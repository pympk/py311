{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Set pandas display options to show more columns and rows\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "# pd.set_option('display.max_rows', 10)       # Limit to 10 rows for readability\n",
    "# pd.set_option('display.width', None)        # Let the display adjust to the window\n",
    "# pd.set_option('display.max_colwidth', None) # Show full content of each cell\n",
    "pd.set_option('display.max_rows', 100)\n",
    "# pd.set_option('display.width', 120)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import date_str, DOWNLOAD_DIR, DEST_DIR\n",
    "\n",
    "path_data = '..\\data\\df_finviz_n_ratios_merged.parquet'\n",
    "path_corr = '..\\data\\df_corr_emv_matrix.parquet'\n",
    "path_cov = '..\\data\\df_cov_emv_matrix.parquet'\n",
    "path_output = f'..\\picks\\{date_str}_portf.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_column_values_above_threshold(df, column_name='Avg Volume, M', threshold=1):\n",
    "  \"\"\"\n",
    "  Analyzes the number and percentage of values in a DataFrame column that are above a specified threshold,\n",
    "  and returns the filtered DataFrame.\n",
    "\n",
    "  Args:\n",
    "    df (pd.DataFrame): The input DataFrame.\n",
    "    column_name (str): The name of the column to analyze. Defaults to 'Avg Volume, M'.\n",
    "    threshold (float): The threshold value to compare against. Defaults to 1.00.\n",
    "\n",
    "  Returns:\n",
    "    pd.DataFrame: A DataFrame containing only the rows where the specified column's value is above the threshold.\n",
    "  \"\"\"\n",
    "  \n",
    "  count_before = len(df)\n",
    "  above_threshold_df = df[df[column_name] > threshold]\n",
    "  count_after = len(above_threshold_df)\n",
    "  percentage = (count_after / len(df)) * 100\n",
    "\n",
    "  print(f\"count_before: {count_before}\")\n",
    "  print(f\"count_after above threshold ({threshold}): {count_after}\")\n",
    "  print(f\"Percentage above threshold ({threshold}): {percentage:.2f}%\")\n",
    "\n",
    "  return above_threshold_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.read_parquet(path_data)\n",
    "\n",
    "# liquidity filter, Avg Volume, M > 0.75M\n",
    "df_data = get_column_values_above_threshold(df_data, column_name='Avg Volume, M', threshold=0.75)\n",
    "\n",
    "# Drop specified columns with NaNs in df_data\n",
    "df_data = df_data.drop(['All-Time High %', 'All-Time Low %', 'Dividend %'], axis=1)\n",
    "\n",
    "df_corr = pd.read_parquet(path_corr)\n",
    "df_cov = pd.read_parquet(path_cov)\n",
    "\n",
    "print(f'\\ndf_cov.shape: {df_cov.shape}')\n",
    "display(df_cov.head())\n",
    "\n",
    "print(f'\\ndf_corr.shape: {df_corr.shape}')\n",
    "display(df_corr.head())\n",
    "\n",
    "print(f'\\ndf_data.shape: {df_data.shape}')\n",
    "display(df_data.head())\n",
    "display((df_data.describe()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check df_corr\n",
    "has_nan_corr = df_corr.isnull().any().any()\n",
    "print(f\"Are there any NaNs in df_corr? {has_nan_corr}\")\n",
    "\n",
    "# Check df_cov\n",
    "has_nan_cov = df_cov.isnull().any().any()\n",
    "print(f\"Are there any NaNs in df_cov? {has_nan_cov}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "output_log = 'output.log'\n",
    "logging.basicConfig(filename=output_log, level=logging.DEBUG, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "def portfolio_optimizer_60days(df_data, df_corr, df_cov, num_clusters=60, num_tickers=100, output_file=\"portfolio_output.txt\"):\n",
    "    \"\"\"\n",
    "    Selects the top N tickers based on a composite score and optimizes a portfolio using cluster analysis.\n",
    "\n",
    "    Args:\n",
    "        df_data (pd.DataFrame): DataFrame containing stock data.\n",
    "        df_corr (pd.DataFrame): DataFrame containing the correlation matrix.\n",
    "        df_cov (pd.DataFrame): DataFrame containing the covariance matrix.\n",
    "        num_clusters (int): The number of clusters to form.\n",
    "        num_tickers (int): The number of top tickers to select.\n",
    "        output_file (str): The name of the log file.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (pd.DataFrame with intermediate values, pd.DataFrame with cluster statistics, \n",
    "                pd.DataFrame with detailed cluster information)\n",
    "\n",
    "    Complete fixed portfolio optimizer with:\n",
    "    - Momentum decay weights\n",
    "    - RSI penalty\n",
    "    - Proper DataFrame handling\n",
    "    \"\"\"\n",
    "    # Configure logging to write to both console and file\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler(output_file, mode='w'),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        logging.info(\"Starting optimized portfolio selection\")\n",
    "\n",
    "        # ===== STAGE 1: DATA PREPARATION =====\n",
    "        # Convert numeric columns\n",
    "        numeric_cols = ['Beta', 'ATR', 'RSI', 'Rel Volume', 'Price']\n",
    "        for col in numeric_cols:\n",
    "            if col in df_data.columns:\n",
    "                df_data[col] = pd.to_numeric(\n",
    "                    df_data[col].astype(str).str.replace(r'[^0-9.-]', '', regex=True),\n",
    "                    errors='coerce'\n",
    "                )\n",
    "\n",
    "        # ===== SCORING CONFIGURATION =====\n",
    "        time_horizons = [3, 5, 10, 15, 30, 60]\n",
    "        feature_weights = {\n",
    "            'sharpe': 0.20,\n",
    "            'sortino': 0.20,\n",
    "            'omega': 0.15,\n",
    "            'momentum': 0.20,  # Reduced from 0.25\n",
    "            'sma': 0.15,\n",
    "            'volatility': -0.10,\n",
    "            'rsi': -0.05  # New RSI penalty\n",
    "        }\n",
    "\n",
    "        # Column definitions\n",
    "        sharpe_cols = [f'Sharpe {days}d' for days in time_horizons]\n",
    "        sortino_cols = [f'Sortino {days}d' for days in time_horizons]\n",
    "        omega_cols = [f'Omega {days}d' for days in time_horizons]\n",
    "        momentum_cols = ['Perf 3D %', 'Perf Week %', 'Perf Month %', 'Perf Quart %']\n",
    "        sma_cols = ['SMA20 %', 'SMA50 %', 'SMA200 %']\n",
    "        volatility_cols = ['Volatility W %', 'Volatility M %']\n",
    "        momentum_weights = np.array([0.4, 0.3, 0.2, 0.1])  # Decay weights\n",
    "\n",
    "        # Data cleaning\n",
    "        required_cols = (sharpe_cols + sortino_cols + omega_cols +\n",
    "                        momentum_cols + sma_cols + volatility_cols + ['RSI'])\n",
    "        clean_mask = df_data[required_cols].notna().all(axis=1)\n",
    "        df_clean = df_data.loc[clean_mask].copy()\n",
    "\n",
    "        if len(df_clean) < num_tickers:\n",
    "            raise ValueError(f\"Only {len(df_clean)} valid tickers after cleaning\")\n",
    "\n",
    "        # ===== SCORE CALCULATION =====\n",
    "        def calculate_weighted_score(df):\n",
    "            components = {}\n",
    "            intermediate_values = {}\n",
    "            raw_values = {}\n",
    "\n",
    "            # Risk-adjusted metrics\n",
    "            for category, cols in [('sharpe', sharpe_cols),\n",
    "                                 ('sortino', sortino_cols),\n",
    "                                 ('omega', omega_cols)]:\n",
    "                z_scores = df[cols].apply(lambda x: (x - x.mean()) / x.std())\n",
    "                intermediate_values[f'{category}_zscores'] = z_scores\n",
    "                raw_values[f'{category}_raw'] = df[cols]\n",
    "                components[category] = z_scores.mean(axis=1) * feature_weights[category]\n",
    "\n",
    "            # Momentum with decay weights\n",
    "            momentum_zscores = df[momentum_cols].apply(lambda x: (x - x.mean()) / x.std())\n",
    "            intermediate_values['momentum_zscores'] = momentum_zscores\n",
    "            raw_values['momentum_raw'] = df[momentum_cols]\n",
    "            components['momentum'] = (momentum_zscores @ momentum_weights) * feature_weights['momentum']\n",
    "\n",
    "            # RSI penalty\n",
    "            if 'RSI' in df.columns:\n",
    "                rsi_penalty = pd.Series(\n",
    "                    np.where(df['RSI'] > 70, feature_weights['rsi'], 0),\n",
    "                    index=df.index\n",
    "                )\n",
    "                components['rsi'] = rsi_penalty\n",
    "            else:\n",
    "                components['rsi'] = pd.Series(0, index=df.index)\n",
    "\n",
    "            # Technical indicators\n",
    "            for category, cols in [('sma', sma_cols),\n",
    "                                 ('volatility', volatility_cols)]:\n",
    "                z_scores = df[cols].apply(lambda x: (x - x.mean()) / x.std())\n",
    "                intermediate_values[f'{category}_zscores'] = z_scores\n",
    "                raw_values[f'{category}_raw'] = df[cols]\n",
    "                components[category] = z_scores.mean(axis=1) * feature_weights[category]\n",
    "\n",
    "            composite_score = pd.concat(components, axis=1).sum(axis=1)\n",
    "            return composite_score, intermediate_values, raw_values\n",
    "\n",
    "        # Calculate scores\n",
    "        df_clean['composite_score'], intermediates, raw_vals = calculate_weighted_score(df_clean)\n",
    "\n",
    "        # ===== STAGE 2: CLUSTERING =====\n",
    "        # top_n = df_clean.nlargest(num_tickers, 'composite_score')\n",
    "        # top_n_tickers = top_n.index.tolist()\n",
    "        # corr_subset = df_corr.loc[top_n_tickers, top_n_tickers]\n",
    "        # distance_matrix = 1 - np.abs(corr_subset)\n",
    "        # np.fill_diagonal(distance_matrix.values, 0)\n",
    "        # linkage_matrix = linkage(squareform(distance_matrix), method='ward')\n",
    "        # clusters = fcluster(linkage_matrix, t=num_clusters, criterion='maxclust')\n",
    "\n",
    "        # ===== STAGE 2: CLUSTERING =====\n",
    "        top_n = df_clean.nlargest(num_tickers, 'composite_score')\n",
    "        top_n_tickers = top_n.index.tolist()\n",
    "        corr_subset = df_corr.loc[top_n_tickers, top_n_tickers]\n",
    "\n",
    "        # Ensure the correlation matrix is symmetric\n",
    "        corr_subset = (corr_subset + corr_subset.T) / 2  # Force symmetry\n",
    "\n",
    "        distance_matrix = 1 - np.abs(corr_subset)\n",
    "        np.fill_diagonal(distance_matrix.values, 0)\n",
    "\n",
    "        # Convert to condensed distance matrix and verify symmetry\n",
    "        condensed_dist = squareform(distance_matrix)\n",
    "        if not np.allclose(condensed_dist, condensed_dist.T, rtol=1e-05, atol=1e-08):\n",
    "            condensed_dist = (condensed_dist + condensed_dist.T) / 2  # Force symmetry\n",
    "\n",
    "        linkage_matrix = linkage(condensed_dist, method='ward')\n",
    "        clusters = fcluster(linkage_matrix, t=num_clusters, criterion='maxclust')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # ===== STAGE 3: PORTFOLIO SELECTION =====\n",
    "        cluster_df = pd.DataFrame({\n",
    "            'ticker': top_n_tickers,\n",
    "            'cluster': clusters,\n",
    "            'score': top_n['composite_score']\n",
    "        }).merge(\n",
    "            df_clean[['Price', 'MktCap AUM, M', 'Volatility M %']],\n",
    "            left_on='ticker',\n",
    "            right_index=True\n",
    "        )\n",
    "\n",
    "        epsilon = 1e-6\n",
    "        cluster_df = cluster_df.assign(\n",
    "            variance=cluster_df['ticker'].apply(lambda x: df_cov.loc[x, x]),\n",
    "            risk_adj_score=lambda x: x['score'] / (np.sqrt(x['variance']) + epsilon),\n",
    "            volatility=lambda x: np.sqrt(x['variance'])\n",
    "        )\n",
    "\n",
    "        # Prepare outputs\n",
    "        detailed_clusters_df = cluster_df.sort_values(['cluster', 'risk_adj_score'], \n",
    "                                                    ascending=[True, False])\n",
    "        detailed_clusters_df = detailed_clusters_df[['cluster', 'ticker', 'score', \n",
    "                                                  'risk_adj_score', 'volatility']]\n",
    "        detailed_clusters_df.columns = ['Cluster_ID', 'Ticker', 'Raw_Score', \n",
    "                                     'Risk_Adj_Score', 'Volatility']\n",
    "        \n",
    "        cluster_stats_df = cluster_df.groupby('cluster').agg(\n",
    "            Size=('ticker', 'count'),\n",
    "            Avg_Correlation=('ticker', lambda x: corr_subset.loc[x,x].values.mean()),\n",
    "            Avg_Raw_Score=('score', 'mean'),\n",
    "            Avg_Risk_Adj_Score=('risk_adj_score', 'mean'),\n",
    "            Avg_Volatility=('volatility', 'mean')\n",
    "        ).reset_index().round(2)\n",
    "        cluster_stats_df.columns = ['Cluster_ID', 'Size', 'Avg_Correlation', \n",
    "                                  'Avg_Raw_Score', 'Avg_Risk_Adj_Score', 'Avg_Volatility']\n",
    "\n",
    "        # ===== FIXED ZSCORE_DF CONSTRUCTION =====\n",
    "        # Convert intermediates to DataFrame\n",
    "        intermediates_df = pd.DataFrame()\n",
    "        for key, values in intermediates.items():\n",
    "            if isinstance(values, pd.DataFrame):\n",
    "                intermediates_df = pd.concat([intermediates_df, values.add_prefix(f'{key}_')], axis=1)\n",
    "            else:\n",
    "                intermediates_df[key] = pd.Series(values, index=df_clean.index)\n",
    "\n",
    "        # Convert raw values to DataFrame\n",
    "        raw_vals_df = pd.DataFrame()\n",
    "        for key, values in raw_vals.items():\n",
    "            if isinstance(values, pd.DataFrame):\n",
    "                raw_vals_df = pd.concat([raw_vals_df, values.add_prefix(f'{key}_')], axis=1)\n",
    "            else:\n",
    "                raw_vals_df[key] = pd.Series(values, index=df_clean.index)\n",
    "\n",
    "        # Construct final output\n",
    "        zscore_df = pd.concat([\n",
    "            intermediates_df,\n",
    "            raw_vals_df,\n",
    "            df_clean['composite_score'].rename('composite_score'),\n",
    "            cluster_df.set_index('ticker')['cluster'].reindex(df_clean.index)\n",
    "        ], axis=1)\n",
    "\n",
    "        # Write results to file\n",
    "        with open(output_file, 'a') as f:\n",
    "            f.write(\"\\n\\n=== CLUSTER STATISTICS ===\\n\")\n",
    "            cluster_stats_df.to_string(f, index=False)\n",
    "            f.write(\"\\n\\n=== DETAILED CLUSTERS ===\\n\")\n",
    "            detailed_clusters_df.to_string(f, index=False)\n",
    "            f.write(\"\\n\\n=== SCORING DETAILS (1-20) ===\\n\")\n",
    "            zscore_df.head(20).to_string(f)  # Only write top 20 rows for brevity\n",
    "\n",
    "        logging.info(\"Portfolio optimization completed successfully\")\n",
    "        return zscore_df, cluster_stats_df, detailed_clusters_df\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Optimization failed: {str(e)}\", exc_info=True)\n",
    "        raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ”§ Running portfolio optimizer...\")\n",
    "\n",
    "zscore_df, cluster_stats_df, detailed_clusters_df = portfolio_optimizer_60days(\n",
    "    df_data, \n",
    "    df_corr,\n",
    "    df_cov,\n",
    "    num_clusters=60,\n",
    "    num_tickers=len(df_data),\n",
    "    output_file=path_output,\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Execution completed.\\nSee output save to: {path_output}\\nOutput log: {output_log}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def select_stocks_from_clusters(cluster_stats_df, detailed_clusters_df, \n",
    "                               num_clusters=3, stocks_per_cluster=5,\n",
    "                               min_cluster_size=5, volatility_threshold=0.3):\n",
    "    \"\"\"\n",
    "    Pipeline to select stocks from better performing clusters\n",
    "    \n",
    "    Parameters:\n",
    "    - cluster_stats_df: DataFrame with cluster statistics\n",
    "    - detailed_clusters_df: DataFrame with detailed cluster information\n",
    "    - num_clusters: Number of top clusters to select\n",
    "    - stocks_per_cluster: Number of stocks to select from each cluster\n",
    "    - min_cluster_size: Minimum size for a cluster to be considered\n",
    "    - volatility_threshold: Maximum allowed volatility for selected stocks\n",
    "    \n",
    "    Returns:\n",
    "    - selected_stocks: DataFrame of selected stocks with their metrics\n",
    "    - cluster_performance: DataFrame of selected clusters with their metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    # ===== 1. Filter and Rank Clusters =====\n",
    "    # Filter clusters by minimum size\n",
    "    qualified_clusters = cluster_stats_df[cluster_stats_df['Size'] >= min_cluster_size].copy()\n",
    "    \n",
    "    # Calculate composite cluster score (weighted average of raw score and correlation)\n",
    "    qualified_clusters['Composite_Cluster_Score'] = (\n",
    "        0.7 * qualified_clusters['Avg_Raw_Score'] +  # Using Raw Score for selection\n",
    "        0.3 * (1 - qualified_clusters['Avg_Correlation'])  # Prefer less correlated clusters\n",
    "    )\n",
    "\n",
    "    # Sort clusters by composite score\n",
    "    ranked_clusters = qualified_clusters.sort_values('Composite_Cluster_Score', ascending=False)\n",
    "    \n",
    "    # Select top N clusters\n",
    "    selected_clusters = ranked_clusters.head(num_clusters)\n",
    "    cluster_ids = selected_clusters['Cluster_ID'].tolist()\n",
    "    \n",
    "    # ===== 2. Select Stocks from Each Cluster =====\n",
    "    selected_stocks_list = []\n",
    "    \n",
    "    for cluster_id in cluster_ids:\n",
    "        # Get stocks from this cluster\n",
    "        cluster_stocks = detailed_clusters_df[detailed_clusters_df['Cluster_ID'] == cluster_id]\n",
    "        \n",
    "        # Filter by volatility threshold\n",
    "        cluster_stocks = cluster_stocks[cluster_stocks['Volatility'] <= volatility_threshold]\n",
    "        \n",
    "        if len(cluster_stocks) > 0:\n",
    "            # Sort by risk-adjusted score and select top stocks\n",
    "            top_stocks = cluster_stocks.sort_values('Risk_Adj_Score', ascending=False).head(stocks_per_cluster)\n",
    "            \n",
    "            # Add cluster metrics to each stock\n",
    "            cluster_metrics = selected_clusters[selected_clusters['Cluster_ID'] == cluster_id].iloc[0]\n",
    "            for col in ['Composite_Cluster_Score', 'Avg_Correlation', 'Avg_Volatility', \n",
    "                       'Avg_Raw_Score', 'Avg_Risk_Adj_Score']:\n",
    "                top_stocks[f'Cluster_{col}'] = cluster_metrics[col]\n",
    "            \n",
    "            selected_stocks_list.append(top_stocks)\n",
    "    \n",
    "    # Combine all selected stocks\n",
    "    if selected_stocks_list:\n",
    "        selected_stocks = pd.concat(selected_stocks_list)\n",
    "        \n",
    "        # Calculate position sizing weights (based on risk-adjusted scores)\n",
    "        selected_stocks['Weight'] = (selected_stocks['Risk_Adj_Score'] / \n",
    "                                   selected_stocks['Risk_Adj_Score'].sum())\n",
    "        \n",
    "        # Sort by cluster then by score\n",
    "        selected_stocks = selected_stocks.sort_values(['Cluster_ID', 'Risk_Adj_Score'], \n",
    "                                                   ascending=[True, False])\n",
    "    else:\n",
    "        selected_stocks = pd.DataFrame()\n",
    "        print(\"Warning: No stocks met selection criteria\")\n",
    "    \n",
    "    # ===== 3. Prepare Enhanced Output Reports =====\n",
    "    # Cluster performance report - now showing both score types\n",
    "    cluster_performance = selected_clusters.copy()\n",
    "    cluster_performance['Stocks_Selected'] = cluster_performance['Cluster_ID'].apply(\n",
    "        lambda x: len(selected_stocks[selected_stocks['Cluster_ID'] == x]) if not selected_stocks.empty else 0)\n",
    "    \n",
    "    # Add diversification metrics\n",
    "    if not selected_stocks.empty:\n",
    "        cluster_performance['Intra_Cluster_Diversification'] = 1 - cluster_performance['Avg_Correlation']\n",
    "    \n",
    "    return selected_stocks, cluster_performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the selection pipeline\n",
    "selected_stocks, cluster_performance = select_stocks_from_clusters(\n",
    "    cluster_stats_df=cluster_stats_df,\n",
    "    detailed_clusters_df=detailed_clusters_df,\n",
    "    num_clusters=3,\n",
    "    stocks_per_cluster=3,\n",
    "    min_cluster_size=5,\n",
    "    volatility_threshold=0.3\n",
    ")\n",
    "\n",
    "# Enhanced Output Display\n",
    "print(\"\\n=== CLUSTER SELECTION CRITERIA ===\")\n",
    "print(\"* Using Avg_Raw_Score for cluster selection\")\n",
    "print(\"* Using Risk_Adj_Score for stock selection within clusters\")\n",
    "print(f\"* Selected top {len(cluster_performance)} clusters from {len(cluster_stats_df)} total\")\n",
    "\n",
    "print(\"\\n=== SELECTED CLUSTERS (RANKED BY RAW SCORE) ===\")\n",
    "display_cols = ['Cluster_ID', 'Size', 'Avg_Raw_Score', 'Avg_Risk_Adj_Score', \n",
    "                'Avg_Correlation', 'Avg_Volatility', 'Composite_Cluster_Score',\n",
    "                'Stocks_Selected', 'Intra_Cluster_Diversification']\n",
    "print(cluster_performance[display_cols].sort_values('Avg_Raw_Score', ascending=False).to_string(index=False))\n",
    "\n",
    "# NEW: Print top 8 stocks by Raw_Score for each selected cluster\n",
    "print(\"\\n=== TOP STOCKS BY RAW SCORE PER CLUSTER ===\")\n",
    "for cluster_id in cluster_performance['Cluster_ID']:\n",
    "    cluster_stocks = detailed_clusters_df[detailed_clusters_df['Cluster_ID'] == cluster_id]\n",
    "    top_raw = cluster_stocks.nlargest(8, 'Raw_Score')[['Ticker', 'Raw_Score', 'Risk_Adj_Score', 'Volatility']]\n",
    "    \n",
    "    print(f\"\\nCluster {cluster_id} - Top 8 by Raw Score:\")\n",
    "    print(top_raw.to_string(index=False))\n",
    "    print(f\"Cluster Avg Raw Score: {cluster_performance[cluster_performance['Cluster_ID'] == cluster_id]['Avg_Raw_Score'].values[0]:.2f}\")\n",
    "    print(f\"Cluster Avg Risk Adj Score: {cluster_performance[cluster_performance['Cluster_ID'] == cluster_id]['Avg_Risk_Adj_Score'].values[0]:.2f}\")\n",
    "\n",
    "print(\"\\n=== FINAL SELECTED STOCKS (BY RISK-ADJ SCORE) ===\")\n",
    "print(\"* Stocks actually selected based on Risk_Adj_Score within each cluster\")\n",
    "print(\"* Position weights assigned based on Risk_Adj_Score\")\n",
    "\n",
    "available_cols = [col for col in ['Cluster_ID', 'Ticker', 'Raw_Score', 'Risk_Adj_Score', \n",
    "                                'Volatility', 'Weight', 'Cluster_Avg_Raw_Score',\n",
    "                                'Cluster_Avg_Risk_Adj_Score'] \n",
    "                  if col in selected_stocks.columns]\n",
    "\n",
    "print(selected_stocks[available_cols].sort_values(['Cluster_ID', 'Risk_Adj_Score'], \n",
    "                                                ascending=[True, False]).to_string(index=False))\n",
    "\n",
    "# Calculate and print portfolio summary\n",
    "if not selected_stocks.empty:\n",
    "    print(\"\\n=== PORTFOLIO SUMMARY ===\")\n",
    "    print(f\"Total Stocks Selected: {len(selected_stocks)}\")\n",
    "    print(f\"Average Raw Score: {selected_stocks['Raw_Score'].mean():.2f}\")\n",
    "    print(f\"Average Risk-Adjusted Score: {selected_stocks['Risk_Adj_Score'].mean():.2f}\")\n",
    "    print(f\"Average Volatility: {selected_stocks['Volatility'].mean():.2f}\")\n",
    "    print(\"\\nCluster Distribution:\")\n",
    "    print(selected_stocks['Cluster_ID'].value_counts().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Drop Tickers \n",
    "drop_tickers = ['BECN']\n",
    "selected_stocks = selected_stocks[~selected_stocks['Ticker'].isin(drop_tickers)].copy()\n",
    "\n",
    "# Step 1: Drop Cluster \n",
    "drop_cluster = 0\n",
    "selected_stocks = selected_stocks[selected_stocks['Cluster_ID'] != drop_cluster].copy()\n",
    "\n",
    "# Step 2: Recalculate weights based on remaining stocks' Risk_Adj_Scores\n",
    "selected_stocks['Weight'] = selected_stocks['Risk_Adj_Score'] / selected_stocks['Risk_Adj_Score'].sum()\n",
    "\n",
    "# Step 3: Display the updated portfolio\n",
    "# print(\"\\n=== UPDATED PORTFOLIO (CLUSTER 46 REMOVED) ===\")\n",
    "print(f\"\\n=== UPDATED PORTFOLIO (CLUSTER {drop_cluster}, TICKER {drop_tickers} REMOVED) ===\")\n",
    "print(selected_stocks[['Cluster_ID', 'Ticker', 'Raw_Score', 'Risk_Adj_Score', 'Weight', 'Volatility']]\n",
    "      .sort_values(['Weight', 'Cluster_ID'], ascending=[False, False])\n",
    "      .to_string(index=False))\n",
    "\n",
    "# Optional: Print weight redistribution summary\n",
    "original_total = 1.0\n",
    "new_total = selected_stocks['Weight'].sum()\n",
    "print(f\"\\nWeights redistributed from Cluster {drop_cluster} & Ticker {drop_tickers}: {original_total - new_total:.1%}\")\n",
    "print(f\"New total weights sum to: {new_total:.0%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Drop Tickers \n",
    "drop_tickers = ['BECN']\n",
    "selected_stocks = selected_stocks[~selected_stocks['Ticker'].isin(drop_tickers)].copy()\n",
    "\n",
    "# Step 1: Drop Cluster \n",
    "drop_cluster = 0\n",
    "selected_stocks = selected_stocks[selected_stocks['Cluster_ID'] != drop_cluster].copy()\n",
    "\n",
    "# Step 2: Recalculate weights based on remaining stocks' Risk_Adj_Scores\n",
    "selected_stocks['Weight'] = selected_stocks['Risk_Adj_Score'] / selected_stocks['Risk_Adj_Score'].sum()\n",
    "\n",
    "# Step 3: Display the updated portfolio\n",
    "print(f\"\\n=== UPDATED PORTFOLIO (CLUSTER {drop_cluster}, TICKER {drop_tickers} REMOVED) ===\")\n",
    "sorted_df = selected_stocks[['Cluster_ID', 'Ticker', 'Raw_Score', 'Risk_Adj_Score', 'Weight', 'Volatility']] \\\n",
    "              .sort_values(['Weight', 'Cluster_ID'], ascending=[False, False])\n",
    "\n",
    "print(sorted_df.to_string(index=False))\n",
    "\n",
    "# Step 4: Extract the 'Ticker' column from the sorted DataFrame\n",
    "ticker_order = sorted_df['Ticker'].tolist()\n",
    "\n",
    "\n",
    "# Optional: Print weight redistribution summary\n",
    "original_total = 1.0\n",
    "new_total = selected_stocks['Weight'].sum()\n",
    "print(f\"\\nWeights redistributed from Cluster {drop_cluster} & Ticker {drop_tickers}: {original_total - new_total:.1%}\")\n",
    "print(f\"New total weights sum to: {new_total:.0%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_data.loc[ticker_order])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_stocks.to_parquet(f'..\\picks\\{date_str}_selected_stocks.parquet')\n",
    "cluster_stats_df.to_parquet(f'..\\picks\\{date_str}_cluster_stats_df.parquet')\n",
    "detailed_clusters_df.to_parquet(f'..\\picks\\{date_str}_detailed_clusters_df.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "PLOT_COLORS = ['lightgreen', 'skyblue', 'salmon', 'gold', 'orchid',\n",
    "                     'lightcoral', 'deepskyblue', 'mediumpurple', 'darkseagreen', 'tan']\n",
    "\n",
    "# Create a figure with subplots\n",
    "fig, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(5, 1, figsize=(12, 12))\n",
    "fig.suptitle('Cluster Statistics Analysis')\n",
    "\n",
    "# Plot 1 Average Risk-Adjusted Score\n",
    "ax1.bar(cluster_stats_df['Cluster_ID'], cluster_stats_df['Avg_Risk_Adj_Score'], color='skyblue')\n",
    "# ax1.bar(cluster_stats_df['Cluster_ID'], cluster_stats_df['Avg_Raw_Score'], color='lightgreen')\n",
    "ax1.set_title('Average Risk-Adjusted Scores by Cluster')\n",
    "ax1.set_xlabel('Cluster_ID')\n",
    "ax1.set_ylabel('Average Risk-Adjusted Score')\n",
    "\n",
    "# Plot 2 Average Raw Score\n",
    "# ax2.bar(cluster_stats_df['Cluster_ID'], cluster_stats_df['Avg_Risk_Adj_Score'], color='lightgreen')\n",
    "ax2.bar(cluster_stats_df['Cluster_ID'], cluster_stats_df['Avg_Raw_Score'], color='lightgreen')\n",
    "ax2.set_title('Average RawScores by Cluster')\n",
    "ax2.set_xlabel('Cluster_ID')\n",
    "ax2.set_ylabel('Average Raw Score')\n",
    "\n",
    "# Plot 3: Average Correlation\n",
    "ax3.bar(cluster_stats_df['Cluster_ID'], cluster_stats_df['Avg_Correlation'], color='salmon')\n",
    "ax3.set_title('Average Correlation within Clusters')\n",
    "ax3.set_xlabel('Cluster_ID')\n",
    "ax3.set_ylabel('Average Correlation')\n",
    "\n",
    "# Plot 4: Average Volatility\n",
    "ax4.bar(cluster_stats_df['Cluster_ID'], cluster_stats_df['Avg_Volatility'], color='gold')\n",
    "ax4.set_title('Average Volatility within Clusters')\n",
    "ax4.set_xlabel('Cluster_ID')\n",
    "ax4.set_ylabel('Average Volatility')\n",
    "\n",
    "# Plot 5 Cluster Size\n",
    "ax5.bar(cluster_stats_df['Cluster_ID'], cluster_stats_df['Size'], color='orchid')\n",
    "ax5.set_title('Cluster Sizes')\n",
    "ax5.set_xlabel('Cluster_ID')\n",
    "ax5.set_ylabel('Number of Members')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate summary statistics for cluster_stats_df\n",
    "stats_summary = pd.DataFrame({\n",
    "    'Count': cluster_stats_df.count(numeric_only=True),\n",
    "    'Sum': cluster_stats_df.sum(numeric_only=True),\n",
    "    'Mean': cluster_stats_df.mean(numeric_only=True),\n",
    "    'Std': cluster_stats_df.std(numeric_only=True),\n",
    "    'Mean+1Std (68%)': cluster_stats_df.mean(numeric_only=True) + cluster_stats_df.std(numeric_only=True),\n",
    "    'Mean-1Std (68%)': cluster_stats_df.mean(numeric_only=True) - cluster_stats_df.std(numeric_only=True),\n",
    "    'Mean+2Std (95%)': cluster_stats_df.mean(numeric_only=True) + 2*cluster_stats_df.std(numeric_only=True),\n",
    "    'Mean-2Std (95%)': cluster_stats_df.mean(numeric_only=True) - 2*cluster_stats_df.std(numeric_only=True),\n",
    "    'Min': cluster_stats_df.min(numeric_only=True),\n",
    "    'Max': cluster_stats_df.max(numeric_only=True),\n",
    "})\n",
    "\n",
    "print(\"Summary Statistics for Cluster Data:\")\n",
    "display(stats_summary.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by Avg_Raw_Score in descending order\n",
    "sorted_by_Avg_Raw_Score = cluster_stats_df.sort_values('Avg_Raw_Score', ascending=False)\n",
    "print(f'sorted_by_Avg_Raw_Score')\n",
    "display(sorted_by_Avg_Raw_Score.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
