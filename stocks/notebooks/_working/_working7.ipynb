{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import os\n",
    "from IPython.display import display, Markdown  # Assuming you use these for display\n",
    "\n",
    "\n",
    "# Set pandas display options to show more columns and rows\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "# pd.set_option('display.max_rows', 10)       # Limit to 10 rows for readability\n",
    "pd.set_option('display.width', 1500)        # Let the display adjust to the window\n",
    "\n",
    "\n",
    "# Notebook cell\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Get root directory (assuming notebook is in root/notebooks/)\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "ROOT_DIR = NOTEBOOK_DIR.parent if NOTEBOOK_DIR.name == 'notebooks' else NOTEBOOK_DIR\n",
    "\n",
    "# Add src directory to Python path\n",
    "sys.path.append(str(ROOT_DIR / 'src'))\n",
    "\n",
    "# Verify path\n",
    "print(f\"Python will look in these locations:\\n{sys.path}\")\n",
    "\n",
    "\n",
    "# --- Execute the processor ---\n",
    "import utils\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_OHLCV, _, _ = utils.main_processor(\n",
    "    data_dir='..\\data',  \n",
    "    # data_dir='output\\selection_results',  # search project ..\\data\n",
    "    downloads_dir=None,  # None searchs Downloads dir, '' omits search1\n",
    "    downloads_limit=60,  # search the first 10 files\n",
    "    clean_name_override=None,  # override filename\n",
    "    start_file_pattern='2025', # search for files starting with 'df_'\n",
    "    contains_pattern='df_OHLCV' # search for files containing 'df_'\n",
    ")\n",
    "\n",
    "print(f'path_OHLCV: {path_OHLCV}')\n",
    "df_OHLCV = pd.read_parquet(path_OHLCV)\n",
    "print(f'df_OHLCV:\\n{df_OHLCV.head()}\\n')\n",
    "print(f'df_OHLCV.info():\\n{df_OHLCV.info()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming your DataFrame is already loaded and named df_data\n",
    "# Example:\n",
    "# df_data = pd.read_pickle('your_data.pkl') # Or however you loaded it\n",
    "\n",
    "# 1. Select the 'Adj Close' column (this returns a Series with the MultiIndex)\n",
    "adj_close_series = df_OHLCV['Adj Close']\n",
    "\n",
    "# 2. Unstack the Ticker level (level 0) of the MultiIndex to become columns\n",
    "#    The Date level (level 1) will remain as the index.\n",
    "df_adj_close = adj_close_series.unstack(level=0)\n",
    "\n",
    "# 3. Optional: Sort the index (Dates) if it's not already sorted\n",
    "df_adj_close = df_adj_close.sort_index()\n",
    "\n",
    "# 4. Optional: Sort columns (Tickers) alphabetically if desired\n",
    "df_adj_close = df_adj_close.sort_index(axis=1)\n",
    "\n",
    "# Display the results\n",
    "print(\"--- Resulting DataFrame for Backtesting (df_adj_close) ---\")\n",
    "print(df_adj_close.info())\n",
    "print(\"\\n--- First 5 rows of df_adj_close: ---\")\n",
    "print(df_adj_close.head())\n",
    "print(\"\\n--- Last 5 rows of df_adj_close: ---\")\n",
    "print(df_adj_close.tail())\n",
    "\n",
    "df_adj_close.to_parquet('df_adj_close.parquet', index=True)\n",
    "print(f\"\\nSaved df_adj_close to 'df_adj_close.parquet' in {Path.cwd()}\")\n",
    "_df = pd.read_parquet('df_adj_close.parquet')\n",
    "print(f\"\\nLoaded df_adj_close from 'df_adj_close.parquet':\\n{_df.head()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, selection_list = utils.main_processor(\n",
    "    # data_dir='..\\ ',  # search project ..\\data\n",
    "    data_dir='output\\selection_results',  # search project ..\\data\n",
    "    downloads_dir=None,  # None searchs Downloads dir, '' omits search1\n",
    "    downloads_limit=60,  # search the first 10 files\n",
    "    clean_name_override=None,  # override filename\n",
    "    start_file_pattern='2025', # search for files starting with 'df_'\n",
    "    contains_pattern='.parquet',  # search for files containing 'df_'\n",
    ")\n",
    "\n",
    "print(f'selection_list: {selection_list}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Method 1: Using os.path.splitext (Recommended) ---\n",
    "# This is generally safer as it correctly handles filenames with multiple dots.\n",
    "params_list = []\n",
    "for parquet_filename in selection_list:\n",
    "    # Split the filename into base and extension\n",
    "    base_name, _ = os.path.splitext(parquet_filename)\n",
    "    # Construct the JSON filename\n",
    "    json_filename = f\"{base_name}_params.json\"\n",
    "    params_list.append(json_filename)\n",
    "\n",
    "print(\"--- Using os.path.splitext ---\")\n",
    "print(f'params_list: {params_list}') # print(params_list)\n",
    "print(f'selection_list: {selection_list}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "import pprint # For cleaner dictionary printing\n",
    "import numpy as np\n",
    "import traceback # Added for detailed error logging\n",
    "from typing import List, Dict, Any, Optional # Added Optional\n",
    "import json\n",
    "import os # For creating directories and checking file existence\n",
    "import logging # For logging instead of print\n",
    "import datetime # For timestamping runs\n",
    "\n",
    "# --- Constants ---\n",
    "RISK_FREE_RATE_DAILY = 0.04 / 365\n",
    "LOG_DIR = 'logs'\n",
    "RESULTS_CSV_PATH = os.path.join(LOG_DIR, 'backtest_parameter_performance.csv')\n",
    "PARAMS_TO_TRACK = [\n",
    "    'n_select_requested',\n",
    "    'inv_vol_col_name',\n",
    "    'filter_min_price',\n",
    "    'filter_min_avg_volume_m',\n",
    "    'filter_min_roe_pct',\n",
    "    'filter_max_debt_eq',\n",
    "    'score_weight_rsi',\n",
    "    'score_weight_change',\n",
    "    'score_weight_rel_volume',\n",
    "    'score_weight_volatility',\n",
    "    # 'weight' (scheme) is handled separately\n",
    "]\n",
    "\n",
    "# --- 1. Setup Logging ---\n",
    "def setup_logging(log_dir: str = LOG_DIR):\n",
    "    \"\"\"Configures logging to write to a file.\"\"\"\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    log_filename = datetime.datetime.now().strftime(\"backtest_run_%Y%m%d_%H%M%S.log\")\n",
    "    log_filepath = os.path.join(log_dir, log_filename)\n",
    "\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO, # Set minimum level to log (DEBUG, INFO, WARNING, ERROR, CRITICAL)\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_filepath), # Log to file\n",
    "            logging.StreamHandler() # Also log to console (optional, remove if only file needed)\n",
    "        ]\n",
    "    )\n",
    "    logging.info(f\"Logging initialized. Log file: {log_filepath}\")\n",
    "    return log_filepath # Return the path for potential reference\n",
    "\n",
    "# --- 2. Refined Functions (using logging instead of print) ---\n",
    "\n",
    "def extract_backtest_setups(\n",
    "    dataframe: pd.DataFrame,\n",
    "    weight_column_names: List[str],\n",
    "    date_str: str, # <<< Still need the date identifier\n",
    "    scheme_separator: str = '_'\n",
    "    ) -> Dict[str, Dict[str, Dict[str, float]]]: # <<< Return type changed\n",
    "    \"\"\"\n",
    "    Extracts Ticker-Weight pairs from specified columns in a DataFrame.\n",
    "\n",
    "    Organizes the data into a nested dictionary structure suitable for running\n",
    "    backtests associated with a specific date. The top-level key is the date string,\n",
    "    and the value is a dictionary containing the individual backtest setups\n",
    "    (scheme name -> {Ticker: Weight}). Outputs are logged.    \n",
    "\n",
    "    Args:\n",
    "        dataframe (pd.DataFrame): The input DataFrame. Must have Tickers as index\n",
    "                                  and contain the specified weight columns.\n",
    "        weight_column_names (List[str]): A list of column names containing the\n",
    "                                         weights for different schemes\n",
    "                                         (e.g., ['Weight_EW', 'Weight_IV']).\n",
    "        date_str (str): A string representing the date (e.g., '20231231') to be\n",
    "                        used as the primary key identifying this batch of\n",
    "                        backtest setups.\n",
    "        scheme_separator (str): The character used to separate the prefix (like 'Weight')\n",
    "                                from the scheme name (like 'EW') in the column name.\n",
    "                                Defaults to '_'.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Dict[str, Dict[str, float]]]:\n",
    "            A dictionary where the single key is the `date_str`.\n",
    "            The value associated with this key is another dictionary:\n",
    "                - Keys are the derived scheme names (e.g., 'EW', 'IV').\n",
    "                - Values are dictionaries mapping Ticker (str) to its weight (float)\n",
    "                  for that scheme.\n",
    "            Returns an empty dictionary `{}` if the input DataFrame is unsuitable\n",
    "            or no valid weights are found for any scheme. If processing occurs\n",
    "            but yields no valid setups, it might return `{date_str: {}}`.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If `date_str` is None or an empty string.\n",
    "\n",
    "    Example Output Structure:\n",
    "        {\n",
    "            '20231231': {\n",
    "                'EW': {'AAPL': 0.5, 'MSFT': 0.5},\n",
    "                'IV': {'AAPL': 0.6, 'MSFT': 0.4}\n",
    "            }\n",
    "        }\n",
    "        # Or {} if no valid columns/data found initially\n",
    "        # Or {'20231231': {}} if processing started but no schemes had valid weights\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # --- Input Validation ---\n",
    "    if not date_str: # Check if date_str is None or empty\n",
    "        # Use logging for errors/warnings\n",
    "        logging.error(\"The 'date_str' argument cannot be None or empty.\")\n",
    "        raise ValueError(\"The 'date_str' argument cannot be None or empty.\")\n",
    "\n",
    "    if dataframe is None or dataframe.empty:\n",
    "        logging.warning(\"Input DataFrame is None or empty. Cannot extract setups.\")\n",
    "        return {} # Return empty dict directly if DataFrame is invalid\n",
    "    # --- End Input Validation ---\n",
    "\n",
    "    scheme_setups: Dict[str, Dict[str, float]] = {}\n",
    "\n",
    "    for col_name in weight_column_names:\n",
    "        if col_name in dataframe.columns:\n",
    "            try:\n",
    "                parts = col_name.split(scheme_separator)\n",
    "                scheme_name = parts[-1] if len(parts) > 1 else col_name\n",
    "\n",
    "                ticker_weights = dataframe[col_name].astype(float).to_dict()\n",
    "                ticker_weights = {\n",
    "                    ticker: weight\n",
    "                    for ticker, weight in ticker_weights.items()\n",
    "                    if pd.notna(weight) # Filter out NaN weights explicitly\n",
    "                }\n",
    "\n",
    "                if ticker_weights:\n",
    "                  if scheme_name in scheme_setups:\n",
    "                        logging.warning(f\"Duplicate scheme name '{scheme_name}' derived. \"\n",
    "                                        f\"Weights from column '{col_name}' might overwrite previous ones.\")\n",
    "                  scheme_setups[scheme_name] = ticker_weights\n",
    "                  logging.info(f\"Successfully extracted weights for scheme: {scheme_name} \"\n",
    "                                f\"({len(ticker_weights)} tickers) for date {date_str}\")\n",
    "                else:\n",
    "                  logging.warning(f\"No valid (non-NaN) weights found for column '{col_name}'. \"\n",
    "                                  f\"Skipping scheme '{scheme_name}' for date '{date_str}'.\")\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing column '{col_name}': {e}\", exc_info=True) # Log traceback\n",
    "        else:\n",
    "            logging.warning(f\"Column '{col_name}' not found in the DataFrame.\")\n",
    "\n",
    "    final_output = {date_str: scheme_setups}\n",
    "\n",
    "    if not scheme_setups:\n",
    "      logging.warning(f\"No valid backtest setups generated for date {date_str}.\")\n",
    "      # Still return the structure, it might be expected upstream\n",
    "      # return {} # Uncomment if you strictly want {} on no setups\n",
    "\n",
    "    return final_output\n",
    "\n",
    "\n",
    "def run_single_backtest(\n",
    "    selection_date: str,\n",
    "    scheme_name: str,\n",
    "    ticker_weights: Dict[str, float],\n",
    "    df_adj_close: pd.DataFrame,\n",
    "    risk_free_rate_daily: float = RISK_FREE_RATE_DAILY,\n",
    "    ) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Runs a simple backtest for a given selection date and ticker weights.\n",
    "    The strategy buys the selected tickers on the next trading day after\n",
    "    selection_date and sells them on the subsequent trading day.\n",
    "    Calculates individual ticker returns and the overall portfolio return\n",
    "    based on the provided weights for the successful trades.\n",
    "    Ensures the input DataFrame's index is sorted by date.\n",
    "    Outputs are logged.\n",
    "\n",
    "    Args:\n",
    "        selection_date (str): The date portfolio weights are determined (YYYY-MM-DD).\n",
    "        scheme_name (str): The name of the weighting scheme (e.g., 'EW', 'IV').\n",
    "        ticker_weights (Dict[str, float]): Dictionary mapping tickers to weights.\n",
    "                                           Weights are used for calculating portfolio return.\n",
    "        df_adj_close (pd.DataFrame): DataFrame of adjusted closing prices, indexed by date.\n",
    "                                     The index should be DatetimeIndex or convertible to it.\n",
    "        risk_free_rate_daily (float): Daily risk-free rate for Sharpe calculation. Defaults to 0.\n",
    "\n",
    "    Returns:\n",
    "        Optional[Dict[str, Any]]: A dictionary containing backtest inputs, trade details,\n",
    "                                  and performance metrics (including portfolio_return),\n",
    "                                  or None if the backtest cannot be run.\n",
    "    \"\"\"\n",
    "\n",
    "    logging.info(\"-\" * 30)\n",
    "    logging.info(f\"Initiating Backtest Run...\")\n",
    "    logging.info(f\"  Date          : {selection_date}\")\n",
    "    logging.info(f\"  Scheme        : {scheme_name}\")\n",
    "    logging.info(f\"  Num Tickers   : {len(ticker_weights)}\")\n",
    "    # Log sample weights less verbosely\n",
    "    sample_weights_str = io.StringIO()\n",
    "    pprint.pprint(dict(list(ticker_weights.items())[:3]), stream=sample_weights_str)\n",
    "    if len(ticker_weights) > 3: sample_weights_str.write(\"    ...\\n\")\n",
    "    logging.debug(f\"  Sample Weights:\\n{sample_weights_str.getvalue()}\") # Use DEBUG level\n",
    "\n",
    "    # --- Input Data Preparation ---\n",
    "    try:\n",
    "        # Ensure index is DatetimeIndex and sorted\n",
    "        if not isinstance(df_adj_close.index, pd.DatetimeIndex):\n",
    "            try:\n",
    "                df_adj_close = df_adj_close.copy() # Avoid modifying original df\n",
    "                df_adj_close.index = pd.to_datetime(df_adj_close.index)\n",
    "                logging.info(\"  Info: Converted DataFrame index to DatetimeIndex.\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"  Error: Failed to convert DataFrame index to DatetimeIndex: {e}\", exc_info=True)\n",
    "                logging.info(\"-\" * 30)\n",
    "                return None\n",
    "\n",
    "        if not df_adj_close.index.is_monotonic_increasing:\n",
    "            logging.info(\"  Info: Sorting DataFrame index by date...\")\n",
    "            df_adj_close = df_adj_close.sort_index()\n",
    "            logging.info(\"  Info: DataFrame index sorted.\")\n",
    "\n",
    "        all_trading_dates = df_adj_close.index\n",
    "        selection_timestamp = pd.Timestamp(selection_date)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"  Error during initial data preparation: {e}\", exc_info=True)\n",
    "        logging.info(\"-\" * 30)\n",
    "        return None\n",
    "\n",
    "    # --- Backtesting Logic ---\n",
    "    try:\n",
    "        # 1. Identify Trading Dates\n",
    "        try:\n",
    "            indexer = all_trading_dates.get_indexer([selection_timestamp])\n",
    "            if indexer[0] == -1:\n",
    "                logging.error(f\"  Error: Selection date {selection_date} not found in price data index.\")\n",
    "                logging.info(\"-\" * 30)\n",
    "                return None\n",
    "            selection_loc = indexer[0]\n",
    "\n",
    "        except KeyError:\n",
    "            logging.error(f\"  Error: Selection date {selection_date} not found in price data index (KeyError).\")\n",
    "            logging.info(\"-\" * 30)\n",
    "            return None\n",
    "\n",
    "        if selection_loc + 1 >= len(all_trading_dates):\n",
    "            logging.error(f\"  Error: No trading date found after selection date {selection_date}.\")\n",
    "            logging.info(\"-\" * 30)\n",
    "            return None\n",
    "        buy_date = all_trading_dates[selection_loc + 1]\n",
    "\n",
    "        if selection_loc + 2 >= len(all_trading_dates):\n",
    "            logging.error(f\"  Error: No trading date found after buy date {buy_date.strftime('%Y-%m-%d')}.\")\n",
    "            logging.info(\"-\" * 30)\n",
    "            return None\n",
    "        sell_date = all_trading_dates[selection_loc + 2]\n",
    "\n",
    "        logging.info(f\"  Selection Date: {selection_date}\")\n",
    "        logging.info(f\"  Buy Date      : {buy_date.strftime('%Y-%m-%d')}\")\n",
    "        logging.info(f\"  Sell Date     : {sell_date.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "        # 2. Simulate Trades & Collect Results\n",
    "        trades = []\n",
    "        returns = []\n",
    "        portfolio_return = 0.0\n",
    "        total_weight_traded = 0.0\n",
    "        valid_tickers_count = 0\n",
    "        missing_price_count = 0\n",
    "\n",
    "        for ticker in ticker_weights.keys():\n",
    "            if ticker not in df_adj_close.columns:\n",
    "                logging.warning(f\"    Warning: Ticker {ticker} not found in price data columns. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            valid_tickers_count += 1\n",
    "            trade_info = { \"ticker\": ticker, \"weight\": ticker_weights[ticker],\n",
    "                          \"buy_date\": buy_date.strftime('%Y-%m-%d'), \"sell_date\": sell_date.strftime('%Y-%m-%d'),\n",
    "                          \"buy_price\": None, \"sell_price\": None, \"return\": None, \"status\": \"Pending\" }\n",
    "\n",
    "            try:\n",
    "                buy_price = df_adj_close.at[buy_date, ticker]\n",
    "                if pd.isna(buy_price) or buy_price <= 0: raise ValueError(f\"Invalid buy price ({buy_price})\")\n",
    "                sell_price = df_adj_close.at[sell_date, ticker]\n",
    "                if pd.isna(sell_price): raise ValueError(f\"Invalid sell price ({sell_price})\")\n",
    "\n",
    "                trade_return = (sell_price - buy_price) / buy_price\n",
    "                trade_info.update({\"buy_price\": buy_price, \"sell_price\": sell_price, \"return\": trade_return, \"status\": \"Success\"})\n",
    "                trades.append(trade_info)\n",
    "                returns.append(trade_return)\n",
    "\n",
    "                current_weight = ticker_weights[ticker]\n",
    "                portfolio_return += trade_return * current_weight\n",
    "                total_weight_traded += current_weight\n",
    "\n",
    "            except KeyError as e:\n",
    "                logging.warning(f\"    Error accessing price for {ticker} on {e}. Skipping trade.\")\n",
    "                trade_info[\"status\"] = f\"Error: Price data missing ({e})\"\n",
    "                trades.append(trade_info)\n",
    "                missing_price_count += 1\n",
    "            except ValueError as e:\n",
    "                logging.warning(f\"    Warning: Invalid price data for {ticker} between {buy_date.strftime('%Y-%m-%d')} and {sell_date.strftime('%Y-%m-%d')} ({e}). Skipping trade.\")\n",
    "                trade_info[\"status\"] = f\"Skipped: Invalid price ({e})\"\n",
    "                try: trade_info[\"buy_price\"] = df_adj_close.at[buy_date, ticker]\n",
    "                except: pass\n",
    "                try: trade_info[\"sell_price\"] = df_adj_close.at[sell_date, ticker]\n",
    "                except: pass\n",
    "                trades.append(trade_info)\n",
    "                missing_price_count += 1\n",
    "            except Exception as e:\n",
    "                logging.error(f\"    Unexpected error processing trade for {ticker}: {e}\", exc_info=True)\n",
    "                trade_info[\"status\"] = f\"Error: Unexpected ({type(e).__name__})\"\n",
    "                trades.append(trade_info)\n",
    "                missing_price_count += 1\n",
    "\n",
    "        # 3. Calculate Metrics\n",
    "        num_attempted_trades = valid_tickers_count\n",
    "        num_successful_trades = len(returns)\n",
    "        metrics = {\n",
    "            'num_selected_tickers': len(ticker_weights),\n",
    "            'num_valid_tickers_in_data': valid_tickers_count,\n",
    "            'num_attempted_trades': num_attempted_trades,\n",
    "            'num_successful_trades': num_successful_trades,\n",
    "            'num_failed_or_skipped_trades': num_attempted_trades - num_successful_trades,\n",
    "            'portfolio_return': portfolio_return if num_successful_trades > 0 else 0.0,\n",
    "            'total_weight_traded': total_weight_traded,\n",
    "            'win_rate': None, 'average_return': None, 'std_dev_return': None, 'sharpe_ratio_period': None,\n",
    "        }\n",
    "\n",
    "        if num_successful_trades > 0:\n",
    "            returns_array = np.array(returns)\n",
    "            metrics['win_rate'] = np.sum(returns_array > 0) / num_successful_trades\n",
    "            metrics['average_return'] = np.mean(returns_array)\n",
    "            metrics['std_dev_return'] = np.std(returns_array, ddof=1) if num_successful_trades > 1 else 0.0\n",
    "\n",
    "            if metrics['std_dev_return'] is not None and metrics['std_dev_return'] > 1e-9:\n",
    "                excess_return = metrics['average_return'] - risk_free_rate_daily\n",
    "                metrics['sharpe_ratio_period'] = excess_return / metrics['std_dev_return']\n",
    "            elif metrics['average_return'] is not None:\n",
    "                # Handle zero standard deviation\n",
    "                excess_return = metrics['average_return'] - risk_free_rate_daily\n",
    "                if abs(excess_return) < 1e-9: metrics['sharpe_ratio_period'] = 0.0\n",
    "                else: metrics['sharpe_ratio_period'] = np.inf * np.sign(excess_return)\n",
    "            else:\n",
    "                metrics['sharpe_ratio_period'] = 0.0 # Or np.nan\n",
    "\n",
    "            logging.info(f\"  Trades Executed: {num_successful_trades}/{num_attempted_trades}\")\n",
    "            logging.info(f\"  Portfolio Return: {metrics['portfolio_return']:.4f} (based on traded weight sum: {metrics['total_weight_traded']:.4f})\")\n",
    "            logging.info(f\"  Win Rate      : {metrics['win_rate']:.2%}\" if metrics['win_rate'] is not None else \"N/A\")\n",
    "            logging.info(f\"  Avg Tkr Return: {metrics['average_return']:.4f}\" if metrics['average_return'] is not None else \"N/A\")\n",
    "            logging.info(f\"  Std Dev Return: {metrics['std_dev_return']:.4f}\" if metrics['std_dev_return'] is not None else \"N/A\")\n",
    "            logging.info(f\"  Period Sharpe : {metrics['sharpe_ratio_period']:.4f}\" if metrics['sharpe_ratio_period'] is not None else \"N/A\")\n",
    "        else:\n",
    "            logging.warning(f\"  No successful trades executed out of {num_attempted_trades} attempted.\")\n",
    "            logging.info(f\"  Portfolio Return: {metrics['portfolio_return']:.4f}\")\n",
    "\n",
    "\n",
    "        # 4. Store Results\n",
    "        backtest_results = {\n",
    "            \"run_inputs\": {\n",
    "                \"selection_date\": selection_date,\n",
    "                \"scheme_name\": scheme_name,\n",
    "                \"num_tickers_input\": len(ticker_weights),\n",
    "                \"risk_free_rate_daily\": risk_free_rate_daily,\n",
    "                \"buy_date\": buy_date.strftime('%Y-%m-%d'),\n",
    "                \"sell_date\": sell_date.strftime('%Y-%m-%d'),\n",
    "            },\n",
    "            \"metrics\": metrics,\n",
    "            \"trades\": trades # Keep trade details if needed, otherwise remove for brevity\n",
    "        }\n",
    "\n",
    "        logging.info(f\"Backtest simulation for '{scheme_name}' on {selection_date} completed.\")\n",
    "        logging.info(\"-\" * 30)\n",
    "        return backtest_results\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.critical(f\"  FATAL ERROR during backtest run for {selection_date}, {scheme_name}: {e}\", exc_info=True)\n",
    "        logging.info(\"-\" * 30)\n",
    "        return None\n",
    "\n",
    "\n",
    "def process_all_backtests(\n",
    "    nested_setups: Dict[str, Dict[str, Dict[str, float]]],\n",
    "    df_adj_close: pd.DataFrame # Pass price data here\n",
    "    ) -> Dict[str, Dict[str, Optional[Dict[str, Any]]]]:\n",
    "    \"\"\"\n",
    "    Iterates through the nested setup dictionary and runs individual backtests.\n",
    "    Outputs are logged.\n",
    "\n",
    "    Args:\n",
    "        nested_setups: The dictionary returned by extract_backtest_setups,\n",
    "                      structured as {date_str: {scheme_name: {Ticker: Weight}}}.\n",
    "        df_adj_close: DataFrame of adjusted closing prices, indexed by date, column by ticker.              \n",
    "\n",
    "    Returns:\n",
    "        A dictionary mirroring the input structure, but containing the\n",
    "        results from `run_single_backtest` instead of the weights.\n",
    "        {date_str: {scheme_name: backtest_results}}.\n",
    "    \"\"\"    \n",
    "    all_results: Dict[str, Dict[str, Optional[Dict[str, Any]]]] = {}\n",
    "\n",
    "    if not nested_setups:\n",
    "        logging.warning(\"Received empty setup dictionary. No backtests to run.\")\n",
    "        return all_results\n",
    "\n",
    "    logging.info(\"\\n===== Starting Batch Backtest Processing =====\")\n",
    "\n",
    "    # Outer loop: Iterate through dates\n",
    "    for date_str, schemes_for_date in nested_setups.items():\n",
    "        logging.info(f\"\\nProcessing date: {date_str}\")\n",
    "        if not schemes_for_date:\n",
    "            logging.warning(f\"  No schemes found for this date. Skipping.\")\n",
    "            all_results[date_str] = {} # Store empty dict for this date\n",
    "            continue\n",
    "\n",
    "        results_for_date: Dict[str, Optional[Dict[str, Any]]] = {}\n",
    "\n",
    "        # Inner loop: Iterate through schemes for the current date\n",
    "        for scheme_name, ticker_weights in schemes_for_date.items():\n",
    "            if not ticker_weights:\n",
    "                logging.warning(f\"  Skipping scheme '{scheme_name}': No ticker weights provided.\")\n",
    "                results_for_date[scheme_name] = None # Mark as skipped or failed\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # --- Call the actual backtest runner ---\n",
    "                backtest_result = run_single_backtest(\n",
    "                    selection_date=date_str,\n",
    "                    scheme_name=scheme_name,\n",
    "                    ticker_weights=ticker_weights,\n",
    "                    df_adj_close=df_adj_close, # Pass the price data\n",
    "                    risk_free_rate_daily = RISK_FREE_RATE_DAILY, # Default or configure elsewhere\n",
    "                )\n",
    "                results_for_date[scheme_name] = backtest_result\n",
    "                # -----------------------------------------\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"!! Error running backtest for {scheme_name} on {date_str}: {e}\", exc_info=True)\n",
    "                results_for_date[scheme_name] = None # Indicate failure\n",
    "\n",
    "        all_results[date_str] = results_for_date\n",
    "\n",
    "    logging.info(\"\\n===== Batch Backtest Processing Finished =====\")\n",
    "    return all_results\n",
    "\n",
    "# --- 3. Function to Extract Parameters and Results for Storage ---\n",
    "def extract_params_and_results(\n",
    "    params: Dict[str, Any],\n",
    "    backtest_results_summary: Dict[str, Dict[str, Optional[Dict[str, Any]]]],\n",
    "    run_timestamp: str,\n",
    "    log_filepath: str\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Extracts relevant parameters and portfolio returns from results.\n",
    "\n",
    "    Args:\n",
    "        params (Dict[str, Any]): The dictionary of parameters used for this run.\n",
    "        backtest_results_summary (Dict): The nested results dictionary from process_all_backtests.\n",
    "        run_timestamp (str): Timestamp for the overall script execution.\n",
    "        log_filepath (str): Path to the log file for this run.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, Any]]: A list of dictionaries, each representing a single\n",
    "                              backtest (date+scheme) with its parameters and results.\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    for date_str, scheme_results in backtest_results_summary.items():\n",
    "        for scheme_name, result in scheme_results.items():\n",
    "            record = {\n",
    "                'run_timestamp': run_timestamp,\n",
    "                'log_file': os.path.basename(log_filepath),\n",
    "                'selection_date': date_str,\n",
    "                'scheme': scheme_name,\n",
    "            }\n",
    "\n",
    "            # Add tracked parameters from the input params dict\n",
    "            for p_key in PARAMS_TO_TRACK:\n",
    "                record[p_key] = params.get(p_key, None) # Use None if param missing\n",
    "\n",
    "            # Add results (handle cases where backtest failed/returned None)\n",
    "            if result and 'metrics' in result:\n",
    "                record['portfolio_return'] = result['metrics'].get('portfolio_return', np.nan)\n",
    "                record['num_successful_trades'] = result['metrics'].get('num_successful_trades', 0)\n",
    "                record['total_weight_traded'] = result['metrics'].get('total_weight_traded', 0.0)\n",
    "                record['win_rate'] = result['metrics'].get('win_rate', np.nan)\n",
    "                record['average_return'] = result['metrics'].get('average_return', np.nan)\n",
    "                # Add n_select_actual if it exists in the input params (from the result file)\n",
    "                record['n_select_actual'] = params.get('n_select_actual', None)\n",
    "            else:\n",
    "                # Backtest failed or skipped for this scheme/date\n",
    "                record['portfolio_return'] = np.nan\n",
    "                record['num_successful_trades'] = 0\n",
    "                record['total_weight_traded'] = 0.0\n",
    "                record['win_rate'] = np.nan\n",
    "                record['average_return'] = np.nan\n",
    "                record['n_select_actual'] = params.get('n_select_actual', None) # Still try to get this\n",
    "\n",
    "            records.append(record)\n",
    "    return records\n",
    "\n",
    "# --- 4. Function to Append Results to CSV ---\n",
    "def append_results_to_csv(records: List[Dict[str, Any]], filepath: str = RESULTS_CSV_PATH):\n",
    "    \"\"\"Appends a list of result records to a CSV file.\"\"\"\n",
    "    if not records:\n",
    "        logging.info(\"No records to append to CSV.\")\n",
    "        return\n",
    "\n",
    "    df_new = pd.DataFrame(records)\n",
    "\n",
    "    # Define column order explicitly for consistency\n",
    "    # Include 'scheme' which represents the weighting method (EW, IV, SW)\n",
    "    column_order = [\n",
    "        'run_timestamp', 'log_file', 'selection_date', 'scheme',\n",
    "        'n_select_requested', 'n_select_actual', # Keep actual close to requested\n",
    "        'inv_vol_col_name', 'filter_min_price', 'filter_min_avg_volume_m',\n",
    "        'filter_min_roe_pct', 'filter_max_debt_eq', 'score_weight_rsi',\n",
    "        'score_weight_change', 'score_weight_rel_volume', 'score_weight_volatility',\n",
    "        'portfolio_return', 'num_successful_trades', 'total_weight_traded',\n",
    "        'win_rate', 'average_return'\n",
    "    ]\n",
    "    # Ensure all expected columns exist, add if missing (e.g., first run)\n",
    "    for col in column_order:\n",
    "        if col not in df_new.columns:\n",
    "            df_new[col] = np.nan # Add missing columns with NaN\n",
    "\n",
    "    # Reorder DataFrame columns\n",
    "    df_new = df_new[column_order]\n",
    "\n",
    "    try:\n",
    "        # Append mode, write header only if file doesn't exist\n",
    "        header = not os.path.exists(filepath)\n",
    "        df_new.to_csv(filepath, mode='a', header=header, index=False)\n",
    "        logging.info(f\"Appended {len(records)} records to {filepath}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error writing to CSV file {filepath}: {e}\", exc_info=True)\n",
    "\n",
    "\n",
    "# === Main Execution Block ===\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Setup ---\n",
    "    log_filepath = setup_logging() # Initialize logging\n",
    "    run_timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    all_performance_records = [] # List to hold records from all file pairs\n",
    "\n",
    "    # --- Load Price Data (Assuming it's loaded once) ---\n",
    "    # !!! IMPORTANT: You need to load your adjusted close price data here !!!\n",
    "    # Example: Replace with your actual loading mechanism\n",
    "    try:\n",
    "        # Example: Load from a parquet file named 'adj_close_prices.parquet'\n",
    "        # Ensure the index is Date/Timestamp and columns are Tickers\n",
    "        adj_close_path = 'df_adj_close.parquet' # <<< CHANGE THIS\n",
    "        if not os.path.exists(adj_close_path):\n",
    "            raise FileNotFoundError(f\"Price data file not found: {adj_close_path}\")\n",
    "        df_adj_close = pd.read_parquet(adj_close_path)\n",
    "\n",
    "        # Basic validation\n",
    "        if not isinstance(df_adj_close.index, pd.DatetimeIndex):\n",
    "            df_adj_close.index = pd.to_datetime(df_adj_close.index)\n",
    "        if not df_adj_close.index.is_monotonic_increasing:\n",
    "            df_adj_close = df_adj_close.sort_index()\n",
    "        logging.info(f\"Successfully loaded and prepared price data from {adj_close_path}\")\n",
    "        logging.info(f\"Price data shape: {df_adj_close.shape}, Date range: {df_adj_close.index.min()} to {df_adj_close.index.max()}\")\n",
    "    except FileNotFoundError as e:\n",
    "        logging.critical(f\"CRITICAL ERROR: {e}. Cannot proceed without price data.\")\n",
    "        exit() # Stop execution if price data is missing\n",
    "    except Exception as e:\n",
    "        logging.critical(f\"CRITICAL ERROR loading price data: {e}\", exc_info=True)\n",
    "        exit()\n",
    "\n",
    "\n",
    "    # --- Find Selection/Parameter File Pairs (Example - adapt to your structure) ---\n",
    "    data_dir = 'output/selection_results/'\n",
    "    try:\n",
    "        all_files = os.listdir(data_dir)\n",
    "        selection_files = sorted([f for f in all_files if f.startswith('2025') and f.endswith('.parquet')])\n",
    "        param_files = sorted([f for f in all_files if f.startswith('2025') and f.endswith('.json')])\n",
    "\n",
    "        # Basic matching logic (assumes corresponding dates)\n",
    "        file_pairs = []\n",
    "        param_map = {utils.extract_date_from_string(pf): pf for pf in param_files} # Map date to param file\n",
    "\n",
    "        for sf in selection_files:\n",
    "            date_str = utils.extract_date_from_string(sf)\n",
    "            if date_str and date_str in param_map:\n",
    "                file_pairs.append((sf, param_map[date_str]))\n",
    "            else:\n",
    "                logging.warning(f\"Could not find matching param file for data file: {sf} (extracted date: {date_str})\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        logging.critical(f\"Data directory not found: {data_dir}. Cannot find files.\")\n",
    "        file_pairs = [] # Ensure it's empty\n",
    "    except Exception as e:\n",
    "        logging.critical(f\"Error listing files in {data_dir}: {e}\", exc_info=True)\n",
    "        file_pairs = []\n",
    "\n",
    "\n",
    "    logging.info(f\"\\n--- Found {len(file_pairs)} Paired Data and Parameter Files ---\")\n",
    "\n",
    "    # --- Iterate Through File Pairs ---\n",
    "    for data_file, param_file in file_pairs:\n",
    "        logging.info(f\"\\nProcessing Pair: Data='{data_file}', Params='{param_file}'\")\n",
    "        date_str = utils.extract_date_from_string(data_file)\n",
    "        if not date_str:\n",
    "            logging.error(f\"Skipping pair due to invalid date extraction from {data_file}\")\n",
    "            continue\n",
    "\n",
    "        logging.info(f\"Extracted date: {date_str}\")\n",
    "\n",
    "        try:\n",
    "            # 1. Read parameters\n",
    "            param_path = os.path.join(data_dir, param_file)\n",
    "            with open(param_path, 'r', encoding='utf-8') as f:\n",
    "                params = json.load(f)\n",
    "                logging.info(f\"Parameters loaded from {param_file}:\")\n",
    "                # Log parameters neatly using pprint to string buffer\n",
    "                params_str = io.StringIO()\n",
    "                pprint.pprint(params, stream=params_str)\n",
    "                logging.info(params_str.getvalue())\n",
    "\n",
    "                # --- ADD 'n_select_actual' to PARAMS_TO_TRACK if it exists ---\n",
    "                # This parameter comes *from* the selection result, not user input,\n",
    "                # but it's useful context for the backtest run.\n",
    "                if 'n_select_actual' in params and 'n_select_actual' not in PARAMS_TO_TRACK:\n",
    "                     # This check prevents adding it multiple times if script reruns partially\n",
    "                     # It's generally better to define PARAMS_TO_TRACK fully at the start,\n",
    "                     # but this handles the specific request.\n",
    "                     # Or just add it manually to the PARAMS_TO_TRACK list initially.\n",
    "                     logging.debug(\"Adding 'n_select_actual' to tracked parameters for this run.\")\n",
    "                     # This modification is local to this run's extraction logic\n",
    "                     # via extract_params_and_results which uses the global list.\n",
    "                     # A cleaner way is to ensure PARAMS_TO_TRACK includes it initially.\n",
    "                     # We will handle it during extraction instead of modifying the global list here.\n",
    "                     pass\n",
    "\n",
    "\n",
    "            # 2. Read Selection DataFrame\n",
    "            selection_path = os.path.join(data_dir, data_file)\n",
    "            selection_df = pd.read_parquet(selection_path)\n",
    "            logging.debug(f'Loaded selection_df from {data_file}. Shape: {selection_df.shape}') # DEBUG level\n",
    "\n",
    "            # 3. Extract Backtest Setups\n",
    "            backtest_setups = extract_backtest_setups(\n",
    "                dataframe=selection_df,\n",
    "                weight_column_names=['Weight_EW', 'Weight_IV', 'Weight_SW'], # Schemes to test\n",
    "                date_str=date_str,\n",
    "            )\n",
    "            # Log setups less verbosely unless debugging\n",
    "            setups_str = io.StringIO()\n",
    "            pprint.pprint(backtest_setups, stream=setups_str, width=120) # Adjust width\n",
    "            logging.debug(\"Extracted Backtest Setups:\\n\" + setups_str.getvalue()) # DEBUG level\n",
    "\n",
    "            if not backtest_setups or not backtest_setups.get(date_str):\n",
    "                 logging.warning(f\"No valid backtest setups extracted for {date_str}. Skipping backtest run.\")\n",
    "                 continue\n",
    "\n",
    "            # 4. Run Backtests for this date/parameter set\n",
    "            logging.info(f\"Running backtests for date: {date_str}\")\n",
    "            # Pass the globally loaded df_adj_close\n",
    "            backtest_results_summary = process_all_backtests(backtest_setups, df_adj_close)\n",
    "\n",
    "            # Log summary results less verbosely\n",
    "            summary_str = io.StringIO()\n",
    "            pprint.pprint(backtest_results_summary, stream=summary_str, width=120)\n",
    "            logging.debug(\"\\n--- Backtest Results Summary (for current date) ---\\n\" + summary_str.getvalue()) # DEBUG level\n",
    "\n",
    "            # 5. Extract parameters and results for storage\n",
    "            run_records = extract_params_and_results(\n",
    "                params=params, # Pass the parameters loaded for this file pair\n",
    "                backtest_results_summary=backtest_results_summary,\n",
    "                run_timestamp=run_timestamp,\n",
    "                log_filepath=log_filepath\n",
    "            )\n",
    "            all_performance_records.extend(run_records) # Collect records from all runs\n",
    "\n",
    "            logging.info(f\"Finished processing for {date_str}.\")\n",
    "\n",
    "        except FileNotFoundError as e:\n",
    "            logging.error(f\"Error finding file for pair ({data_file}, {param_file}): {e}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Unhandled error processing pair ({data_file}, {param_file}): {e}\", exc_info=True)\n",
    "\n",
    "        logging.info(f\"{'=' * 40}\\n\") # Separator\n",
    "\n",
    "    # --- 5. Save all collected results to CSV ---\n",
    "    logging.info(f\"\\n--- Attempting to Save {len(all_performance_records)} Performance Records ---\")\n",
    "    append_results_to_csv(all_performance_records, RESULTS_CSV_PATH)\n",
    "\n",
    "    logging.info(\"=== Script Execution Finished ===\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.shutdown()\n",
    "print(\"Logging system shut down.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
