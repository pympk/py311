{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HplHE-XnwHFz"
   },
   "source": [
    "===== TURN ON POWERTOY AWAKE to KEEP CONNECTION ALIVE =====\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0BAeKmVzAEwE"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DzjWU0MyA8cT"
   },
   "source": [
    "===== SET download_to_PC and end_index ====="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "LnCnXVOGAzi8"
   },
   "outputs": [],
   "source": [
    "# symbols: The string (or sequence) to slice.\n",
    "# start_index: Optional. The starting index of the slice. If None, defaults to 0.\n",
    "# end_index: Optional. The ending index of the slice (exclusive). If None, defaults to the end of the string.\n",
    "# step_value: Optional. The step value for the slice. If None, defaults to 1.\n",
    "\n",
    "# True download to PC, False download to Google Drive\n",
    "download_to_PC = True  # True download to PC\n",
    "# download_to_PC = False  # True download to Google Drive\n",
    "\n",
    "# Download all if end_iddex = None\n",
    "end_index = None\n",
    "end_index = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BWX6sLNsmsCr",
    "outputId": "a3dabbf1-e196-482a-c2c8-63da3a538257"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slice of symbols: symbols[slice(None, 2, None)]\n"
     ]
    }
   ],
   "source": [
    "start_index = None\n",
    "\n",
    "step_value = None\n",
    "slice_obj = slice(start_index, end_index, step_value)  # Create a slice object\n",
    "\n",
    "print(f'slice of symbols: symbols[{slice_obj}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K2xUTYT3wHF4"
   },
   "outputs": [],
   "source": [
    "! pip install fake-useragent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "HTCD6Lo1wHF4"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from fake_useragent import UserAgent\n",
    "\n",
    "selector = '.styled-table-new'\n",
    "ua = UserAgent()  # Initialize UserAgent\n",
    "\n",
    "def download_table(url, selector):\n",
    "    \"\"\"\n",
    "    Downloads table data from a Yahoo Finance page with rate limiting.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Add a User-Agent header to mimic a browser\n",
    "        # headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}  # Example User-Agent\n",
    "        new_user_agent = ua.random\n",
    "        headers = {\"User-Agent\": new_user_agent}\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        table_body = soup.select_one(selector)\n",
    "\n",
    "        if table_body is None:\n",
    "            print(f\"Error: Table body not found using selector: {selector}\")\n",
    "            return None\n",
    "\n",
    "        rows = table_body.find_all('tr')\n",
    "        if not rows:\n",
    "            print(\"Error: No rows found in the table.\")\n",
    "            return None\n",
    "\n",
    "        # Extract headers from the first row (th elements)\n",
    "        headers_list = [th.text.strip() for th in rows[0].find_all('th')]\n",
    "\n",
    "        data = []\n",
    "        for row in rows:\n",
    "            cells = row.find_all('td')\n",
    "            row_data = [cell.text.strip() for cell in cells]\n",
    "            if row_data:  # Only append if the row has data\n",
    "                data.append(row_data)\n",
    "\n",
    "        if not data:\n",
    "            print(\"Error: No data found in the table rows.\")\n",
    "            return None\n",
    "\n",
    "        df = pd.DataFrame(data, columns=headers_list)\n",
    "        return df\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error during request: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RpuXvF2ncotV"
   },
   "outputs": [],
   "source": [
    "# all_etf_columns_sorted_by_AUM\n",
    "# https://finviz.com/screener.ashx?v=152&ft=4&o=-e.assetsundermanagement&c=0,1,2,3,4,5,129,75,14,130,131,31,84,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,134,125,126,59,70,80,83,60,61,63,64,67,81,86,87,88,65,66,103,100,107,108,109,112,113,114,115,116,117,120,121,122,105\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AgZUn7phcotW"
   },
   "outputs": [],
   "source": [
    "# all_stock_columns_sorted_by_market_cap\n",
    "# https://finviz.com/screener.ashx?v=152&ft=4&o=-marketcap&c=0,1,2,79,3,4,5,129,6,7,8,9,10,11,12,13,73,74,75,14,130,131,15,16,77,17,18,19,20,21,23,22,132,133,82,78,127,128,24,25,85,26,27,28,29,30,31,84,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,134,125,126,59,68,70,80,83,76,60,61,62,63,64,67,69,81,86,87,88,65,66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "xyyiAJcecotX"
   },
   "outputs": [],
   "source": [
    "# remove duplicate Dividend with $ value in column 75, kept Dividend with value in % \n",
    "url_all_columns = '0,1,2,79,3,4,5,129,6,7,8,9,10,11,12,13,73,74,14,130,131,15,16,77,17,18,19,20,21,23,22,132,133,82,78,127,128,24,25,85,26,27,28,29,30,31,84,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,134,125,126,59,68,70,80,83,76,60,61,62,63,64,67,69,81,86,87,88,65,66,103,100,107,108,109,112,113,114,115,116,117,120,121,122,105'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "xilLYhetMk-y"
   },
   "outputs": [],
   "source": [
    "url_mktcap ='https://finviz.com/screener.ashx?v=152&ft=4&o=-marketcap&c='\n",
    "# url_mktcap_all_columns = '0,1,2,79,3,4,5,129,6,7,8,9,10,11,12,13,73,74,75,14,130,131,15,16,77,17,18,19,20,21,23,22,132,133,82,78,127,128,24,25,85,26,27,28,29,30,31,84,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,134,125,126,59,68,70,80,83,76,60,61,62,63,64,67,69,81,86,87,88,65,66'\n",
    "url_mktcap_rows = ['&r=1', '&r=21', '&r=41', '&r=61', '&r=81', '&r=101', '&r=121', '&r=141', '&r=161', '&r=181', '&r=201', '&r=221', '&r=241', '&r=261', '&r=281', '&r=301', '&r=321', '&r=341', '&r=361', '&r=381', '&r=401', '&r=421', '&r=441', '&r=461', '&r=481', '&r=501', '&r=521', '&r=541', '&r=561', '&r=581', '&r=601', '&r=621', '&r=641', '&r=661', '&r=681', '&r=701', '&r=721', '&r=741', '&r=761', '&r=781', '&r=801', '&r=821', '&r=841', '&r=861', '&r=881', '&r=901', '&r=921', '&r=941', '&r=961', '&r=981', '&r=1001', '&r=1021','&r=1041','&r=1061','&r=1081','&r=1101','&r=1121','&r=1141',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "p4mC2RrCcotY"
   },
   "outputs": [],
   "source": [
    "url_etfs ='https://finviz.com/screener.ashx?v=152&ft=4&o=-e.assetsundermanagement&c='\n",
    "# url_etfs_all_columns = '0,1,2,3,4,5,129,75,14,130,131,31,84,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,134,125,126,59,70,80,83,60,61,63,64,67,81,86,87,88,65,66,103,100,107,108,109,112,113,114,115,116,117,120,121,122,105'\n",
    "url_etfs_rows = ['&r=1', '&r=21', '&r=41', '&r=61', '&r=81', '&r=101', '&r=121', '&r=141', '&r=161', '&r=181', '&r=201', '&r=221', '&r=241', '&r=261', '&r=281', '&r=301', '&r=321', '&r=341', '&r=361', '&r=381']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BYhnaEoxwHF6",
    "outputId": "def62974-d424-427e-e6b1-f8e8257a0f65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of urls: 58\n",
      "First 3 urls:\n",
      "['https://finviz.com/screener.ashx?v=152&ft=4&o=-marketcap&c=0,1,2,79,3,4,5,129,6,7,8,9,10,11,12,13,73,74,14,130,131,15,16,77,17,18,19,20,21,23,22,132,133,82,78,127,128,24,25,85,26,27,28,29,30,31,84,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,134,125,126,59,68,70,80,83,76,60,61,62,63,64,67,69,81,86,87,88,65,66,103,100,107,108,109,112,113,114,115,116,117,120,121,122,105&r=1', 'https://finviz.com/screener.ashx?v=152&ft=4&o=-marketcap&c=0,1,2,79,3,4,5,129,6,7,8,9,10,11,12,13,73,74,14,130,131,15,16,77,17,18,19,20,21,23,22,132,133,82,78,127,128,24,25,85,26,27,28,29,30,31,84,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,134,125,126,59,68,70,80,83,76,60,61,62,63,64,67,69,81,86,87,88,65,66,103,100,107,108,109,112,113,114,115,116,117,120,121,122,105&r=21', 'https://finviz.com/screener.ashx?v=152&ft=4&o=-marketcap&c=0,1,2,79,3,4,5,129,6,7,8,9,10,11,12,13,73,74,14,130,131,15,16,77,17,18,19,20,21,23,22,132,133,82,78,127,128,24,25,85,26,27,28,29,30,31,84,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,134,125,126,59,68,70,80,83,76,60,61,62,63,64,67,69,81,86,87,88,65,66,103,100,107,108,109,112,113,114,115,116,117,120,121,122,105&r=41']\n"
     ]
    }
   ],
   "source": [
    "urls_mktcap = []\n",
    "\n",
    "for _rows in url_mktcap_rows:\n",
    "    # _url = url_mktcap + url_mktcap_all_columns + _rows\n",
    "    _url = url_mktcap + url_all_columns + _rows\n",
    "    urls_mktcap.append(_url)\n",
    "\n",
    "print(f'Total number of urls: {len(urls_mktcap)}')\n",
    "print(f'First 3 urls:\\n{urls_mktcap[0:3]}')  # Print the length of the list of url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "WJ38HN3UcotZ",
    "outputId": "515f4104-1d5d-41bc-ca5b-5b8b3af952ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of urls: 20\n",
      "First 3 urls:\n",
      "['https://finviz.com/screener.ashx?v=152&ft=4&o=-e.assetsundermanagement&c=0,1,2,79,3,4,5,129,6,7,8,9,10,11,12,13,73,74,14,130,131,15,16,77,17,18,19,20,21,23,22,132,133,82,78,127,128,24,25,85,26,27,28,29,30,31,84,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,134,125,126,59,68,70,80,83,76,60,61,62,63,64,67,69,81,86,87,88,65,66,103,100,107,108,109,112,113,114,115,116,117,120,121,122,105&r=1', 'https://finviz.com/screener.ashx?v=152&ft=4&o=-e.assetsundermanagement&c=0,1,2,79,3,4,5,129,6,7,8,9,10,11,12,13,73,74,14,130,131,15,16,77,17,18,19,20,21,23,22,132,133,82,78,127,128,24,25,85,26,27,28,29,30,31,84,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,134,125,126,59,68,70,80,83,76,60,61,62,63,64,67,69,81,86,87,88,65,66,103,100,107,108,109,112,113,114,115,116,117,120,121,122,105&r=21', 'https://finviz.com/screener.ashx?v=152&ft=4&o=-e.assetsundermanagement&c=0,1,2,79,3,4,5,129,6,7,8,9,10,11,12,13,73,74,14,130,131,15,16,77,17,18,19,20,21,23,22,132,133,82,78,127,128,24,25,85,26,27,28,29,30,31,84,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,134,125,126,59,68,70,80,83,76,60,61,62,63,64,67,69,81,86,87,88,65,66,103,100,107,108,109,112,113,114,115,116,117,120,121,122,105&r=41']\n"
     ]
    }
   ],
   "source": [
    "urls_etfs = []\n",
    "\n",
    "for _rows in url_etfs_rows:\n",
    "    # _url = url_etfs + url_etfs_all_columns + _rows\n",
    "    _url = url_etfs + url_all_columns + _rows\n",
    "    urls_etfs.append(_url)\n",
    "\n",
    "print(f'Total number of urls: {len(urls_etfs)}')\n",
    "print(f'First 3 urls:\\n{urls_etfs[0:3]}')  # Print the length of the list of url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JWfH7PLrwHF7",
    "outputId": "23b628b5-cf9d-4b51-9f38-c55e3044c1d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(shuffled_urls): 78\n",
      "shuffled_urls[0:10]\n",
      "https://finviz.com/screener.ashx?v=152&ft=4&o=-marketcap&c=0,1,2,79,3,4,5,129,6,7,8,9,10,11,12,13,73,74,14,130,131,15,16,77,17,18,19,20,21,23,22,132,133,82,78,127,128,24,25,85,26,27,28,29,30,31,84,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,134,125,126,59,68,70,80,83,76,60,61,62,63,64,67,69,81,86,87,88,65,66,103,100,107,108,109,112,113,114,115,116,117,120,121,122,105&r=781\n",
      "https://finviz.com/screener.ashx?v=152&ft=4&o=-marketcap&c=0,1,2,79,3,4,5,129,6,7,8,9,10,11,12,13,73,74,14,130,131,15,16,77,17,18,19,20,21,23,22,132,133,82,78,127,128,24,25,85,26,27,28,29,30,31,84,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,134,125,126,59,68,70,80,83,76,60,61,62,63,64,67,69,81,86,87,88,65,66,103,100,107,108,109,112,113,114,115,116,117,120,121,122,105&r=241\n",
      "https://finviz.com/screener.ashx?v=152&ft=4&o=-marketcap&c=0,1,2,79,3,4,5,129,6,7,8,9,10,11,12,13,73,74,14,130,131,15,16,77,17,18,19,20,21,23,22,132,133,82,78,127,128,24,25,85,26,27,28,29,30,31,84,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,134,125,126,59,68,70,80,83,76,60,61,62,63,64,67,69,81,86,87,88,65,66,103,100,107,108,109,112,113,114,115,116,117,120,121,122,105&r=141\n",
      "https://finviz.com/screener.ashx?v=152&ft=4&o=-e.assetsundermanagement&c=0,1,2,79,3,4,5,129,6,7,8,9,10,11,12,13,73,74,14,130,131,15,16,77,17,18,19,20,21,23,22,132,133,82,78,127,128,24,25,85,26,27,28,29,30,31,84,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,134,125,126,59,68,70,80,83,76,60,61,62,63,64,67,69,81,86,87,88,65,66,103,100,107,108,109,112,113,114,115,116,117,120,121,122,105&r=301\n",
      "https://finviz.com/screener.ashx?v=152&ft=4&o=-e.assetsundermanagement&c=0,1,2,79,3,4,5,129,6,7,8,9,10,11,12,13,73,74,14,130,131,15,16,77,17,18,19,20,21,23,22,132,133,82,78,127,128,24,25,85,26,27,28,29,30,31,84,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,134,125,126,59,68,70,80,83,76,60,61,62,63,64,67,69,81,86,87,88,65,66,103,100,107,108,109,112,113,114,115,116,117,120,121,122,105&r=161\n",
      "https://finviz.com/screener.ashx?v=152&ft=4&o=-e.assetsundermanagement&c=0,1,2,79,3,4,5,129,6,7,8,9,10,11,12,13,73,74,14,130,131,15,16,77,17,18,19,20,21,23,22,132,133,82,78,127,128,24,25,85,26,27,28,29,30,31,84,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,134,125,126,59,68,70,80,83,76,60,61,62,63,64,67,69,81,86,87,88,65,66,103,100,107,108,109,112,113,114,115,116,117,120,121,122,105&r=101\n",
      "https://finviz.com/screener.ashx?v=152&ft=4&o=-marketcap&c=0,1,2,79,3,4,5,129,6,7,8,9,10,11,12,13,73,74,14,130,131,15,16,77,17,18,19,20,21,23,22,132,133,82,78,127,128,24,25,85,26,27,28,29,30,31,84,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,134,125,126,59,68,70,80,83,76,60,61,62,63,64,67,69,81,86,87,88,65,66,103,100,107,108,109,112,113,114,115,116,117,120,121,122,105&r=481\n",
      "https://finviz.com/screener.ashx?v=152&ft=4&o=-marketcap&c=0,1,2,79,3,4,5,129,6,7,8,9,10,11,12,13,73,74,14,130,131,15,16,77,17,18,19,20,21,23,22,132,133,82,78,127,128,24,25,85,26,27,28,29,30,31,84,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,134,125,126,59,68,70,80,83,76,60,61,62,63,64,67,69,81,86,87,88,65,66,103,100,107,108,109,112,113,114,115,116,117,120,121,122,105&r=861\n",
      "https://finviz.com/screener.ashx?v=152&ft=4&o=-e.assetsundermanagement&c=0,1,2,79,3,4,5,129,6,7,8,9,10,11,12,13,73,74,14,130,131,15,16,77,17,18,19,20,21,23,22,132,133,82,78,127,128,24,25,85,26,27,28,29,30,31,84,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,134,125,126,59,68,70,80,83,76,60,61,62,63,64,67,69,81,86,87,88,65,66,103,100,107,108,109,112,113,114,115,116,117,120,121,122,105&r=181\n",
      "https://finviz.com/screener.ashx?v=152&ft=4&o=-marketcap&c=0,1,2,79,3,4,5,129,6,7,8,9,10,11,12,13,73,74,14,130,131,15,16,77,17,18,19,20,21,23,22,132,133,82,78,127,128,24,25,85,26,27,28,29,30,31,84,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,134,125,126,59,68,70,80,83,76,60,61,62,63,64,67,69,81,86,87,88,65,66,103,100,107,108,109,112,113,114,115,116,117,120,121,122,105&r=501\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "urls =  urls_mktcap + urls_etfs\n",
    "shuffled_urls = random.sample(urls, len(urls))\n",
    "print(f'len(shuffled_urls): {len(shuffled_urls)}')\n",
    "print(f'shuffled_urls[0:10]')\n",
    "for url in shuffled_urls[0:10]:\n",
    "    print(f'{url}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WyA3q6n9wHF7"
   },
   "outputs": [],
   "source": [
    "# test_urls = [\n",
    "#   'https://finviz.com/screener.ashx?v=152&ft=4&o=-marketcap&c=0,1,2,3,4,6,14,42,43,44,45,46,47,48,49,50,51,52,53,54,57,58,59,67,65,66,103,100,109,120,121,122&r=1',\n",
    "#   'https://finviz.com/screener.ashx?v=152&ft=4&o=-e.assetsundermanagement&c=0,1,2,3,4,6,14,42,43,44,45,46,47,48,49,50,51,52,53,54,57,58,59,67,65,66,103,100,109,120,121,122&r=101',\n",
    "#   'https://finviz.com/screener.ashx?v=152&ft=4&o=-marketcap&c=0,1,2,3,4,6,14,42,43,44,45,46,47,48,49,50,51,52,53,54,57,58,59,67,65,66,103,100,109,120,121,122&r=601',\n",
    "#   ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "sp4KYbgEcotc",
    "outputId": "15d64caf-c84a-4d1a-bc03-b104917b3d5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://finviz.com/screener.ashx?v=152&ft=4&o=-marketcap&c=0,1,2,79,3,4,5,129,6,7,8,9,10,11,12,13,73,74,14,130,131,15,16,77,17,18,19,20,21,23,22,132,133,82,78,127,128,24,25,85,26,27,28,29,30,31,84,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,134,125,126,59,68,70,80,83,76,60,61,62,63,64,67,69,81,86,87,88,65,66,103,100,107,108,109,112,113,114,115,116,117,120,121,122,105&r=781. Sleeping for 3.25 seconds.  Processed 1 / 78 urls\n",
      "Downloading https://finviz.com/screener.ashx?v=152&ft=4&o=-marketcap&c=0,1,2,79,3,4,5,129,6,7,8,9,10,11,12,13,73,74,14,130,131,15,16,77,17,18,19,20,21,23,22,132,133,82,78,127,128,24,25,85,26,27,28,29,30,31,84,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,134,125,126,59,68,70,80,83,76,60,61,62,63,64,67,69,81,86,87,88,65,66,103,100,107,108,109,112,113,114,115,116,117,120,121,122,105&r=241. Sleeping for 2.12 seconds.  Processed 2 / 78 urls\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "\n",
    "total_urls_to_download = len(shuffled_urls)\n",
    "\n",
    "df = pd.DataFrame()  # Initialized an empty DataFrame\n",
    "processed_count = 0\n",
    "\n",
    "# for url in test_urls:\n",
    "# for url in shuffled_urls[0:3]:\n",
    "for url in shuffled_urls[slice_obj]:\n",
    "    # Introduce a delay between requests (adjust as needed)\n",
    "    delay_seconds = random.uniform(2, 4.5)  # Sleep between 2 and 5 seconds\n",
    "    # print(f\"Downloading {symbol}. Sleeping for {delay_seconds:.2f} seconds...\")\n",
    "    processed_count += 1\n",
    "    print(f\"Downloading {url}. Sleeping for {delay_seconds:.2f} seconds.  Processed {processed_count} / {total_urls_to_download} urls\")\n",
    "    time.sleep(delay_seconds)\n",
    "\n",
    "    df_temp = download_table(url, selector)\n",
    "\n",
    "    if df_temp is not None:\n",
    "        df = pd.concat([df, df_temp], ignore_index=True)\n",
    "    else:\n",
    "        print(f\"Failed to download data for {url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "TJt8zyFewHF8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_finviz_filename: df_finviz_2025-05-07_stocks_etfs.parquet\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import pytz\n",
    "\n",
    "pst = pytz.timezone('America/Los_Angeles')  # or 'US/Pacific'\n",
    "current_date_pst = datetime.datetime.now(pst).strftime('%Y-%m-%d')\n",
    "\n",
    "# Save the DataFrame as a Parquet file\n",
    "df_finviz_filename = f\"df_finviz_{current_date_pst}_stocks_etfs.parquet\"\n",
    "\n",
    "print(f\"df_finviz_filename: {df_finviz_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "3agEKFzAMX95"
   },
   "outputs": [],
   "source": [
    "import pyarrow\n",
    "\n",
    "df.to_parquet(df_finviz_filename, engine='pyarrow', compression='zstd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "aw1Sw3G-W419"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   No. Ticker                          Company    Index       Sector                        Industry  Country Exchange Market Cap     P/E Fwd P/E   PEG    P/S    P/B     P/C   P/FCF Book/sh Cash/sh Dividend Dividend TTM Dividend Ex Date Payout Ratio    EPS EPS next Q EPS this Y EPS next Y EPS past 5Y EPS next 5Y Sales past 5Y Sales Q/Q  EPS Q/Q EPS YoY TTM Sales YoY TTM    Sales   Income EPS Surprise Revenue Surprise Outstanding    Float Float % Insider Own Insider Trans Inst Own Inst Trans Short Float Short Ratio Short Interest      ROA      ROE     ROI Curr R Quick R LTDebt/Eq Debt/Eq Gross M   Oper M Profit M Perf Week Perf Month Perf Quart Perf Half Perf Year Perf YTD   Beta    ATR Volatility W Volatility M   SMA20   SMA50   SMA200 50D High 50D Low 52W High 52W Low        52W Range All-Time High All-Time Low    RSI  Earnings    IPO Date Optionable Shortable Employees Change from Open     Gap Recom Avg Volume Rel Volume     Volume Target Price Prev Close    Open    High     Low   Price  Change Single Category Asset Type Expense Holdings AUM Flows 1M Flows% 1M Flows 3M Flows% 3M Flows YTD Flows% YTD Return% 1Y Return% 3Y Return% 5Y Tags\n",
      "0  781    PEN                     Penumbra Inc        -   Healthcare                 Medical Devices      USA     NYSE     11.14B  269.80   56.94  6.45   8.98   9.17   29.41   74.10   31.35    9.78        -            -                -        0.00%   1.07       0.83     70.90%     33.17%     -23.21%      41.86%        17.38%    16.32%  258.58%     -54.30%        13.17%    1.24B   42.23M       24.46%            2.66%      38.68M   37.36M  96.59%       3.54%       -12.50%   90.75%     -1.79%       4.20%        3.12          1.57M    2.66%    3.50%   2.98%   6.30    3.68      0.17    0.18  63.46%    9.23%    3.41%    -1.77%      9.18%      8.17%    23.30%    37.94%   21.13%   0.55  10.64        2.49%        4.50%   1.18%   3.34%   20.95%   -5.91%  16.72%   -7.20%  94.37%  148.00 - 310.00       -17.50%      714.57%  53.24  Apr 23/a   9/18/2015        Yes       Yes      4500           -1.01%  -0.46%  1.62    502.88K       0.71    119,553       316.06     291.96  290.61  296.44  287.44  287.67  -1.47%               -          -       -        -   -        -         -        -         -         -          -          -          -          -    -\n",
      "1  782   ALAB                  Astera Labs Inc        -   Technology                  Semiconductors      USA     NASD     11.13B       -   38.88     -  28.08  11.33   12.17  108.63    5.95    5.54        -            -       12/12/2007            -  -0.54       0.32     58.76%     30.13%           -      42.55%       143.61%   179.32%   65.58%    -221.02%       242.24%  396.29M  -83.42M       16.24%            4.14%     162.02M  124.00M  76.53%      24.81%        -8.23%   54.07%     27.03%       8.22%        2.23         10.19M  -13.34%  -14.87%  -8.63%  11.71   11.21      0.00    0.00  76.38%  -29.29%  -21.05%     3.31%     22.32%    -36.69%    -7.13%   -12.33%  -49.06%   0.34   4.98        4.12%        7.82%   6.29%   3.40%  -12.26%  -20.72%  43.17%  -54.22%  86.30%   36.22 - 147.39       -54.22%       86.30%  52.96  May 06/a   3/20/2024        Yes       Yes       440            0.60%  -6.02%  1.40      4.57M       2.62  3,986,892        99.36      71.36   67.07   69.91   63.40   67.47  -5.45%               -          -       -        -   -        -         -        -         -         -          -          -          -          -    -\n",
      "2  783    ZTO     ZTO Express (Cayman) Inc ADR        -  Industrials  Integrated Freight & Logistics    China     NYSE     11.11B   12.82    9.82  0.94   1.81   1.33    3.63       -   14.21    5.22    3.85%         0.70        4/10/2025       45.76%   1.48       0.42     14.70%     13.44%       8.45%      13.58%        14.25%    21.51%   13.66%      -0.32%        12.67%    6.15B    1.22B      -11.92%           10.92%     598.37M  581.59M  97.20%       0.86%         0.00%   25.67%     -4.99%       2.53%        5.48         14.74M    9.75%   14.46%  14.32%   1.07    1.04      0.01    0.28  30.97%   24.90%   19.90%     2.41%      5.13%     -0.08%   -17.95%   -11.80%   -3.09%  -0.16   0.62        2.00%        3.72%   1.11%  -2.15%   -7.66%  -13.93%  15.94%  -31.11%  15.94%    16.34 - 27.50       -49.86%       82.19%  49.84  May 20/a  10/27/2016        Yes       Yes     24471            1.15%  -2.24%  1.31      2.69M       0.60    535,699        26.11      19.16   18.73   18.99   18.56   18.94  -1.12%               -          -       -        -   -        -         -        -         -         -          -          -          -          -    -\n",
      "3  784   POOL                 Pool Corporation  S&P 500  Industrials         Industrial Distribution      USA     NASD     11.10B   27.68   24.71  5.00   2.11   8.97  154.88   22.87   32.89    1.91    1.33%         4.80        5/15/2025       41.58%  10.66       5.16     -0.97%      8.95%      12.04%       5.54%        11.95%    -4.40%  -30.59%     -16.75%        -3.56%    5.26B  406.87M       -9.57%           -2.53%      37.66M   36.45M  96.79%       3.04%        -3.52%  102.79%     -0.34%       8.68%        6.44          3.16M   10.89%   31.49%  16.76%   1.79    0.53      0.96    1.09  29.44%   11.14%    7.73%     0.68%     -2.62%    -12.99%   -18.71%   -19.06%  -13.44%   1.14  10.84        2.80%        4.28%  -2.07%  -7.32%  -14.82%  -21.24%   3.82%  -25.40%   3.82%  284.27 - 395.60       -49.31%    34379.13%  43.07  Apr 24/b  10/13/1995        Yes       Yes      6000           -0.07%   0.00%  2.40    490.80K       0.58     94,932       321.09     295.34  295.34  298.25  293.95  295.13  -0.07%               -          -       -        -   -        -         -        -         -         -          -          -          -          -    -\n",
      "4  785    BNT  Brookfield Wealth Solutions Ltd        -    Financial         Insurance - Diversified  Bermuda     NYSE     11.09B       -       -     -   0.78   0.91       -    2.45   60.79       -    0.60%         0.33        3/14/2025            -      -          -          -          -           -           -       295.23%   146.02%        -           -       102.08%   14.19B    1.12B            -                -      41.44M   18.48M  44.59%      90.81%         0.00%    3.13%          -       0.56%        5.65          0.10M    1.22%   12.32%   6.70%   2.05       -      0.37    0.37       -   11.03%    7.91%     2.74%     21.06%     -6.91%     3.47%    32.49%   -4.02%   1.68   1.70        1.85%        4.14%   6.79%   5.73%    2.58%   -5.99%  26.18%  -12.10%  37.43%    40.12 - 62.72       -20.39%       94.84%  60.30  Feb 13/b   6/28/2021        Yes       Yes      5000            0.05%   0.68%     -     18.38K       0.87      5,230            -      54.73   55.10   55.36   55.09   55.13   0.73%               -          -       -        -   -        -         -        -         -         -          -          -          -          -    -\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.width', 3000) \n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "R6_jh9e9WE_8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 tickers saved to: df_finviz_tickers.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "file_path = '/content/drive/MyDrive/stocks/df_finviz_tickers.pkl'  # Run in Colab\n",
    "file_path = 'df_finviz_tickers.pkl'  # Run in VSCode\n",
    "\n",
    "df_tickers = df[['Ticker']]\n",
    "\n",
    "# Save the DataFrame to a pickle file in the specified location\n",
    "df_tickers.to_pickle(file_path)\n",
    "print(f\"{len(df_tickers)} tickers saved to: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.download(zip_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "78db6cfhuLqT"
   },
   "outputs": [],
   "source": [
    "# ======================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "5VcTdIE3uLqT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slice of symbols: symbols[slice(None, 2, None)]\n"
     ]
    }
   ],
   "source": [
    "# symbols: The string (or sequence) to slice.\n",
    "# start_index: Optional. The starting index of the slice. If None, defaults to 0.\n",
    "# end_index: Optional. The ending index of the slice (exclusive). If None, defaults to the end of the string.\n",
    "# step_value: Optional. The step value for the slice. If None, defaults to 1.\n",
    "\n",
    "# start_index = None\n",
    "# end_index = None\n",
    "# end_index = 3\n",
    "\n",
    "step_value = None\n",
    "slice_obj = slice(start_index, end_index, step_value)  # Create a slice object\n",
    "\n",
    "print(f'slice of symbols: symbols[{slice_obj}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 40 symbols. First 5: ['PEN', 'ALAB', 'ZTO', 'POOL', 'BNT']\n",
      "Symbols to download: symbols[slice(None, 2, None)] (Total: 2)\n",
      "symbols_to_download: ['PEN', 'ALAB']\n",
      "Downloading PEN. Sleeping for 2.65 seconds. Processed 1/2 symbols.\n",
      "Failed to download or process data for PEN using yfinance: Too Many Requests. Rate limited. Try after a while.\n",
      "Downloading ALAB. Sleeping for 2.60 seconds. Processed 2/2 symbols.\n",
      "Failed to download or process data for ALAB using yfinance: Too Many Requests. Rate limited. Try after a while.\n"
     ]
    }
   ],
   "source": [
    "# Code Block 1: Setup and Imports (for yfinance approach)\n",
    "# !pip install yfinance pandas fake_useragent openpyxl\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# For User-Agent rotation if needed with yfinance (though often not required)\n",
    "from fake_useragent import UserAgent\n",
    "ua = UserAgent()\n",
    "\n",
    "# --- Code related to loading symbols (your existing code) ---\n",
    "# Assuming this file exists in your Colab's Google Drive\n",
    "file_path = '/content/drive/MyDrive/stocks/df_finviz_tickers.pkl'\n",
    "file_path = 'df_finviz_tickers.pkl'  # Run in VSCode\n",
    "\n",
    "# Ensure Google Drive is mounted if running in Colab\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    loaded_df = pd.read_pickle(file_path)\n",
    "    symbols = loaded_df['Ticker'].tolist()\n",
    "    print(f\"Loaded {len(symbols)} symbols. First 5: {symbols[:5]}\")\n",
    "else:\n",
    "    print(f\"Error: File not found at {file_path}. Using sample symbols.\")\n",
    "    symbols = [\"AAPL\", \"MSFT\", \"GOOG\", \"HPE\", \"AMZN\"] # Sample symbols if file not found\n",
    "\n",
    "# # --- Define slice_obj and col_names (as they were used in your original code) ---\n",
    "# # Example: Process the first 5 symbols\n",
    "# slice_obj = slice(0, 5) # Or whatever slice you need\n",
    "# # Standard column names from Yahoo Finance history\n",
    "# col_names = ['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', 'Dividends', 'Stock Splits']\n",
    "\n",
    "\n",
    "# --- Main Loop using yfinance ---\n",
    "symbols_to_download = symbols[slice_obj]\n",
    "total_symbols_to_download = len(symbols_to_download)\n",
    "processed_count = 0\n",
    "all_data_frames = [] # List to hold individual DataFrames\n",
    "\n",
    "print(f'Symbols to download: symbols[{slice_obj}] (Total: {total_symbols_to_download})')\n",
    "print(f'symbols_to_download: {symbols_to_download}')\n",
    "\n",
    "for symbol in symbols_to_download:\n",
    "    delay_seconds = random.uniform(1, 3) # yfinance is more efficient, can use shorter delays\n",
    "    processed_count += 1\n",
    "    print(f\"Downloading {symbol}. Sleeping for {delay_seconds:.2f} seconds. Processed {processed_count}/{total_symbols_to_download} symbols.\")\n",
    "    time.sleep(delay_seconds)\n",
    "\n",
    "    try:\n",
    "        ticker = yf.Ticker(symbol)\n",
    "        # Set a user agent for the session yfinance uses\n",
    "        # This can sometimes help if default yfinance requests are blocked\n",
    "        # ticker.session.headers.update({'User-Agent': ua.random})\n",
    "\n",
    "        # Fetch historical market data\n",
    "        # You can specify period (e.g., \"1y\", \"5d\", \"max\") or start/end dates\n",
    "        hist_df = ticker.history(period=\"1y\") # Example: 1 year of data\n",
    "\n",
    "        if hist_df.empty:\n",
    "            print(f\"No data returned for {symbol}. It might be delisted or an invalid ticker.\")\n",
    "            continue\n",
    "\n",
    "        # yfinance returns 'Date' as index. 'Dividends' and 'Stock Splits' are also columns.\n",
    "        # We want to ensure our col_names are reflected or adjusted.\n",
    "        # For MultiIndex:\n",
    "        hist_df.index.name = 'Date' # Ensure the index is named 'Date'\n",
    "        # Create MultiIndex\n",
    "        current_symbol_df = pd.concat({symbol: hist_df}, names=['Symbol'])\n",
    "        all_data_frames.append(current_symbol_df)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download or process data for {symbol} using yfinance: {e}\")\n",
    "        # yfinance sometimes prints more detailed errors itself\n",
    "\n",
    "# # Combine all DataFrames\n",
    "# if all_data_frames:\n",
    "#     final_df = pd.concat(all_data_frames)\n",
    "#     print(\"\\n--- Final Combined DataFrame (first 10 rows) ---\")\n",
    "#     print(final_df.head(10))\n",
    "#     # You can now save final_df to a file, e.g., final_df.to_csv('yahoo_finance_data.csv')\n",
    "# else:\n",
    "#     print(\"No data was successfully downloaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "aZmvZTdHuLqT"
   },
   "outputs": [],
   "source": [
    "from fake_useragent import UserAgent\n",
    "\n",
    "ua = UserAgent()  # Initialize UserAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "NAuxwFy7uLqU"
   },
   "outputs": [],
   "source": [
    "# selector = \"#nimbus-app > section > section > section > article > div.container > div.table-container.yf-1jecxey > table > tbody\"\n",
    "selector = \"#nimbus-app > section > section > section > article > div.container > div.table-container.yf-1jecxey\"\n",
    "col_names = ['Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "hVZTyrX6uLqU"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def read_stock_symbols(dir_path, symbols_stocks_file, symbols_ETFs_file):\n",
    "    \"\"\"\n",
    "    Reads stock and ETF symbols from text files in the specified directory and returns them in two separate lists.\n",
    "\n",
    "    Args:\n",
    "        dir_path (str): The directory path where the symbol files are located.\n",
    "        symbols_stocks_file (str): The name of the file containing stock symbols.\n",
    "        symbols_ETFs_file (str): The name of the file containing ETF symbols.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing two lists: (stock_symbols, etf_symbols).  Returns ([], [])\n",
    "               if any error occurs during file reading.\n",
    "    \"\"\"\n",
    "\n",
    "    stock_symbols = []\n",
    "    etf_symbols = []\n",
    "\n",
    "    try:\n",
    "        # Read stock symbols\n",
    "        with open(os.path.join(dir_path, symbols_stocks_file), 'r') as f:\n",
    "            stock_symbols = [line.strip() for line in f]\n",
    "\n",
    "        # Read ETF symbols\n",
    "        with open(os.path.join(dir_path, symbols_ETFs_file), 'r') as f:\n",
    "            etf_symbols = [line.strip() for line in f]\n",
    "\n",
    "        return stock_symbols, etf_symbols\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: One or more files not found in directory: {dir_path}\")\n",
    "        return [], []\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "NkcX6TakuLqV"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pytz\n",
    "\n",
    "def get_current_pst_time():\n",
    "  \"\"\"\n",
    "  Returns the current time in Pacific Standard Time (PST).\n",
    "\n",
    "  Returns:\n",
    "    A string representing the current time in PST, formatted as\n",
    "    \"YYYY-MM-DD HH:MM:SS\".\n",
    "  \"\"\"\n",
    "\n",
    "  pst_timezone = pytz.timezone('America/Los_Angeles')  # Get the PST timezone\n",
    "  pst_now = datetime.datetime.now(pst_timezone)  # Get the current time in PST\n",
    "\n",
    "  return pst_now.strftime(\"%Y-%m-%d %H:%M:%S\")  # Format the time as a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "gInEy7rtuLqV"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def convert_df_data_types(df):\n",
    "    \"\"\"\n",
    "    Cleans and converts a Pandas DataFrame with a MultiIndex to the specified data types.\n",
    "\n",
    "    Args:\n",
    "        df: The input Pandas DataFrame.  Assumes a MultiIndex with stock ticker (str) and date (str).\n",
    "            Assumes columns 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume' as objects.\n",
    "\n",
    "    Returns:\n",
    "        A Pandas DataFrame with the correct data types.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the second level of the MultiIndex to datetime\n",
    "    try:\n",
    "        df.index = pd.MultiIndex.from_tuples([(i[0], pd.to_datetime(i[1])) for i in df.index], names=df.index.names)\n",
    "    except ValueError as e:\n",
    "        print(f\"Error converting MultiIndex to datetime: {e}\")\n",
    "        return df  # Or handle the error differently, e.g., raise it\n",
    "\n",
    "    # Convert columns to appropriate data types\n",
    "    columns_to_convert = ['Open', 'High', 'Low', 'Close', 'Adj Close']\n",
    "    for col in columns_to_convert:\n",
    "        try:\n",
    "            # Remove commas *before* attempting conversion. CRITICAL.\n",
    "            df[col] = df[col].str.replace(',', '', regex=False)  # Remove commas first\n",
    "            df[col] = df[col].astype(float)\n",
    "        except ValueError as e:\n",
    "            print(f\"Error converting column '{col}' to float: {e}\")\n",
    "            return df #skip this column and return the original df\n",
    "\n",
    "    try:\n",
    "        # Handle '-' values in 'Volume' BEFORE removing commas\n",
    "        df['Volume'] = df['Volume'].replace('-', np.nan)\n",
    "        df['Volume'] = df['Volume'].str.replace(',', '', regex=False).astype(float).astype('Int64') # Use Int64 to store NaN\n",
    "    except ValueError as e:\n",
    "        print(f\"Error converting column 'Volume' to int64: {e}\")\n",
    "        return df\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "OHvVY-VhuLqV"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def adjust_prices(df):\n",
    "    \"\"\"\n",
    "    Adjusts Open, High, Low, and Close prices using Adj Close to account for splits and dividends.\n",
    "\n",
    "    Args:\n",
    "        df: Pandas DataFrame with 'Open', 'High', 'Low', 'Close', and 'Adj Close' columns.\n",
    "            Assumes MultiIndex with stock ticker and datetime.\n",
    "\n",
    "    Returns:\n",
    "        Pandas DataFrame with adjusted 'Open', 'High', and 'Low' prices.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate the adjustment ratio\n",
    "    df['adjustment_ratio'] = df['Adj Close'] / df['Close']\n",
    "\n",
    "    # Adjust Open, High, and Low prices\n",
    "    df['Adj Open'] = df['Open'] * df['adjustment_ratio']\n",
    "    df['Adj High'] = df['High'] * df['adjustment_ratio']\n",
    "    df['Adj Low'] = df['Low'] * df['adjustment_ratio']\n",
    "\n",
    "\n",
    "    # Optionally, drop the adjustment_ratio column if you don't need it\n",
    "    df = df.drop('adjustment_ratio', axis=1)  # axis=1 to drop the column\n",
    "\n",
    "    return df\n",
    "\n",
    "# Example Usage (assuming 'df' is your cleaned DataFrame)\n",
    "# df_adjusted = adjust_prices(df.copy())  # Create a copy\n",
    "# print(df_adjusted.head())\n",
    "# print(df_adjusted.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I79YQzfFuLqV"
   },
   "outputs": [],
   "source": [
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# import time\n",
    "# import random  # For a bit of randomness in the sleep time\n",
    "\n",
    "# def download_yahoo_finance_table(url, selector):\n",
    "#     \"\"\"\n",
    "#     Downloads table data from a Yahoo Finance page with rate limiting.\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         # Add a User-Agent header to mimic a browser\n",
    "#         # headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}  # Example User-Agent\n",
    "#         new_user_agent = ua.random\n",
    "#         headers = {\"User-Agent\": new_user_agent}\n",
    "\n",
    "#         response = requests.get(url, headers=headers)\n",
    "#         response.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)\n",
    "\n",
    "#         soup = BeautifulSoup(response.content, 'html.parser')\n",
    "#         table_body = soup.select_one(selector)\n",
    "\n",
    "#         if table_body is None:\n",
    "#             print(f\"Error: Table body not found using selector: {selector}\")\n",
    "#             return None\n",
    "\n",
    "#         rows = table_body.find_all('tr')\n",
    "#         if not rows:\n",
    "#             print(\"Error: No rows found in the table.\")\n",
    "#             return None\n",
    "\n",
    "#         # Extract headers from the first row (th elements)\n",
    "#         headers_list = [th.text.strip() for th in rows[0].find_all('th')]\n",
    "\n",
    "#         data = []\n",
    "#         for row in rows:\n",
    "#             cells = row.find_all('td')\n",
    "#             row_data = [cell.text.strip() for cell in cells]\n",
    "#             if row_data:  # Only append if the row has data\n",
    "#                 data.append(row_data)\n",
    "\n",
    "#         if not data:\n",
    "#             print(\"Error: No data found in the table rows.\")\n",
    "#             return None\n",
    "\n",
    "#         df = pd.DataFrame(data, columns=headers_list)\n",
    "#         return df\n",
    "\n",
    "#     except requests.exceptions.RequestException as e:\n",
    "#         print(f\"Error during request: {e}\")\n",
    "#         return None\n",
    "#     except Exception as e:\n",
    "#         print(f\"An unexpected error occurred: {e}\")\n",
    "#         return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# import time\n",
    "# import random  # For a bit of randomness in the sleep time\n",
    "# import pandas as pd\n",
    "\n",
    "# def download_yahoo_finance_table(url, selector):\n",
    "#     \"\"\"\n",
    "#     Downloads table data from a Yahoo Finance page with rate limiting.\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         # Chrome browser User-Agent header\n",
    "#         headers = {\n",
    "#             'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36'\n",
    "#         }\n",
    "\n",
    "#         response = requests.get(url, headers=headers)\n",
    "#         response.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)\n",
    "\n",
    "#         soup = BeautifulSoup(response.content, 'html.parser')\n",
    "#         table_body = soup.select_one(selector)\n",
    "\n",
    "#         if table_body is None:\n",
    "#             print(f\"Error: Table body not found using selector: {selector}\")\n",
    "#             return None\n",
    "\n",
    "#         rows = table_body.find_all('tr')\n",
    "#         if not rows:\n",
    "#             print(\"Error: No rows found in the table.\")\n",
    "#             return None\n",
    "\n",
    "#         # Extract headers from the first row (th elements)\n",
    "#         headers_list = [th.text.strip() for th in rows[0].find_all('th')]\n",
    "\n",
    "#         data = []\n",
    "#         for row in rows:\n",
    "#             cells = row.find_all('td')\n",
    "#             row_data = [cell.text.strip() for cell in cells]\n",
    "#             if row_data:  # Only append if the row has data\n",
    "#                 data.append(row_data)\n",
    "\n",
    "#         if not data:\n",
    "#             print(\"Error: No data found in the table rows.\")\n",
    "#             return None\n",
    "\n",
    "#         df = pd.DataFrame(data, columns=headers_list)\n",
    "#         return df\n",
    "\n",
    "#     except requests.exceptions.RequestException as e:\n",
    "#         print(f\"Error during request: {e}\")\n",
    "#         return None\n",
    "#     except Exception as e:\n",
    "#         print(f\"An unexpected error occurred: {e}\")\n",
    "#         return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "bnDeGAN7uLqW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "symbols (len = 40):\n",
      "['PEN', 'ALAB', 'ZTO', 'POOL', 'BNT', 'YMM', 'DRS', 'SUZ', 'FTAI', 'HTHT', 'PPC', 'TAP', 'DVA', 'CAG', 'ENTG', 'CLS', 'CAVA', 'DOCS', 'RVTY', 'SCI', 'LYG', 'HLT', 'ET', 'BCS', 'DLR', 'SNOW', 'CVNA', 'MFC', 'ALL', 'PAYX', 'TEAM', 'AMX', 'CSX', 'FCX', 'TRP', 'PSA', 'ARES', 'LNG', 'DB', 'NWG']\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "file_path = '/content/drive/MyDrive/stocks/df_finviz_tickers.pkl'  # Run in Colab\n",
    "file_path = 'df_finviz_tickers.pkl'  # Run in VSCode\n",
    "\n",
    "loaded_df = pd.read_pickle(file_path)\n",
    "symbols = loaded_df['Ticker'].tolist()\n",
    "print(f\"symbols (len = {len(symbols)}):\")\n",
    "print(symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "5xZ2jju1uLqW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "symbols_to_download: symbols[slice(None, 2, None)]\n",
      "total_symbols_to_download: 2\n",
      "processed_count: 0\n"
     ]
    }
   ],
   "source": [
    "symbols_to_download = symbols[slice_obj]\n",
    "\n",
    "# symbols_to_download = slice_string(symbols, symbol_start, symbol_end, symbol_step)  # Adjust the slice as needed\n",
    "total_symbols_to_download = len(symbols_to_download)\n",
    "processed_count = 0\n",
    "\n",
    "print(f'symbols_to_download: symbols[{slice_obj}]')\n",
    "print(f'total_symbols_to_download: {total_symbols_to_download}')\n",
    "print(f'processed_count: {processed_count}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "q52YTHcquLqW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start OHLCV download at PST time: 2025-05-07 09:02:01\n"
     ]
    }
   ],
   "source": [
    "current_pst_time = get_current_pst_time()\n",
    "print(f\"Start OHLCV download at PST time: {current_pst_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running WebDriver with UI (non-headless mode).\n",
      "Initializing WebDriver...\n",
      "WebDriver initialized successfully.\n",
      "Selenium (Simplified Consent): Navigating to https://finance.yahoo.com/\n",
      "Selenium (Simplified Consent): Successfully navigated. Current URL: https://finance.yahoo.com/. Page title: Yahoo Finance - Stock Market Live, Quotes, Business & Finance News\n",
      "Selenium (Simplified Consent): Body tag found. Page seems to have rendered basic structure.\n",
      "Initial page navigation seems okay. Proceeding to data tables.\n",
      "\n",
      "--- Attempting to download historical data for AAPL using Selenium ---\n",
      "Selenium: Navigating to https://finance.yahoo.com/quote/AAPL/history/?p=AAPL\n",
      "Selenium: Successfully navigated. Current URL: https://finance.yahoo.com/quote/AAPL/history/?p=AAPL. Page title: Apple Inc. (AAPL) Stock Historical Prices & Data - Yahoo Finance\n",
      "Selenium: Pausing for 10 seconds for manual observation of the page. Check if table appears.\n",
      "Selenium: Waiting for the main table container using CSS selector: 'table[data-test='historical-prices']'\n",
      "Selenium: Timed out waiting for PRESENCE of main table container 'table[data-test='historical-prices']' at https://finance.yahoo.com/quote/AAPL/history/?p=AAPL.\n",
      "\n",
      "Failed to download historical data for AAPL with Selenium.\n",
      "--------------------------------------------------\n",
      "Driver session appears to be still valid.\n",
      "\n",
      "--- Attempting to download Valuation Measures for AAPL using Selenium ---\n",
      "Selenium: Navigating to https://finance.yahoo.com/quote/AAPL/key-statistics/?p=AAPL\n",
      "Selenium: Successfully navigated. Current URL: https://finance.yahoo.com/quote/AAPL/key-statistics/?p=AAPL. Page title: Apple Inc. (AAPL) Valuation Measures & Financial Statistics\n",
      "Selenium: Pausing for 10 seconds for manual observation of the page. Check if table appears.\n",
      "Selenium: Waiting for the main table container using CSS selector: 'section[data-test='qsp-statistics'] div > table:nth-of-type(1)'\n",
      "Selenium: Timed out waiting for PRESENCE of main table container 'section[data-test='qsp-statistics'] div > table:nth-of-type(1)' at https://finance.yahoo.com/quote/AAPL/key-statistics/?p=AAPL.\n",
      "\n",
      "Failed to download statistics (Valuation Measures) for AAPL with Selenium.\n",
      "Closing WebDriver...\n",
      "WebDriver closed.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import TimeoutException, WebDriverException, NoSuchElementException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "DRIVER = None\n",
    "# Let's use a slightly older but generally very stable UA\n",
    "CHROME_USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36'\n",
    "\n",
    "def initialize_webdriver(headless=True):\n",
    "    global DRIVER\n",
    "    if DRIVER is not None:\n",
    "        try: DRIVER.quit()\n",
    "        except: pass\n",
    "            \n",
    "    chrome_options = Options()\n",
    "    if headless:\n",
    "        print(\"Running WebDriver in headless mode.\")\n",
    "        chrome_options.add_argument(\"--headless=new\")\n",
    "    else:\n",
    "        print(\"Running WebDriver with UI (non-headless mode).\")\n",
    "    \n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--disable-extensions\")\n",
    "    chrome_options.add_argument(f\"user-agent={CHROME_USER_AGENT}\")\n",
    "    chrome_options.add_argument(\"accept-language=en-US,en;q=0.9\")\n",
    "    chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "    # chrome_options.add_argument(\"--enable-logging\") # Enable basic logging\n",
    "    # chrome_options.add_argument(\"--v=1\") # Verbose logging level\n",
    "    # For more detailed logs (might need specific capabilities)\n",
    "    # logging_prefs = {'browser': 'ALL', 'driver': 'ALL', 'performance': 'ALL'}\n",
    "    # chrome_options.set_capability('goog:loggingPrefs', logging_prefs)\n",
    "\n",
    "\n",
    "    try:\n",
    "        print(\"Initializing WebDriver...\")\n",
    "        # Specify service object to potentially capture chromedriver logs\n",
    "        # service = ChromeService(ChromeDriverManager().install(), log_output=\"chromedriver.log\")\n",
    "        # DRIVER = webdriver.Chrome(service=service, options=chrome_options)\n",
    "        DRIVER = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()), options=chrome_options)\n",
    "        print(\"WebDriver initialized successfully.\")\n",
    "        DRIVER.set_page_load_timeout(60) # Increased page load timeout even more\n",
    "        return True\n",
    "    except WebDriverException as e:\n",
    "        print(f\"Error initializing WebDriver: {e}\")\n",
    "        DRIVER = None\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during WebDriver initialization: {e}\")\n",
    "        DRIVER = None\n",
    "        return False\n",
    "\n",
    "def close_webdriver():\n",
    "    global DRIVER\n",
    "    if DRIVER:\n",
    "        print(\"Closing WebDriver...\")\n",
    "        try: DRIVER.quit()\n",
    "        except: pass\n",
    "        DRIVER = None\n",
    "        print(\"WebDriver closed.\")\n",
    "\n",
    "def simplified_selenium_consent_check(driver, initial_url=\"https://finance.yahoo.com/\"):\n",
    "    try:\n",
    "        print(f\"Selenium (Simplified Consent): Navigating to {initial_url}\")\n",
    "        driver.get(initial_url)\n",
    "        print(f\"Selenium (Simplified Consent): Successfully navigated. Current URL: {driver.current_url}. Page title: {driver.title}\")\n",
    "        WebDriverWait(driver, 25).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "        print(\"Selenium (Simplified Consent): Body tag found. Page seems to have rendered basic structure.\")\n",
    "        time.sleep(3) # Pause\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Selenium (Simplified Consent): Error during initial navigation/check to {initial_url}: {e}\")\n",
    "        try: driver.save_screenshot(\"debug_simplified_consent_error.png\")\n",
    "        except: pass\n",
    "        return False\n",
    "\n",
    "def download_yahoo_finance_table_selenium(url, table_selector_for_bs4, stock_symbol):\n",
    "    global DRIVER\n",
    "    if DRIVER is None or not hasattr(DRIVER, 'window_handles'): # Check if driver is still valid\n",
    "        print(\"WebDriver not initialized or session is invalid. Cannot download table.\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        print(f\"Selenium: Navigating to {url}\")\n",
    "        DRIVER.get(url)\n",
    "        current_url_after_get = DRIVER.current_url\n",
    "        current_title_after_get = DRIVER.title\n",
    "        print(f\"Selenium: Successfully navigated. Current URL: {current_url_after_get}. Page title: {current_title_after_get}\")\n",
    "        \n",
    "        # CRITICAL OBSERVATION POINT if running non-headless:\n",
    "        print(\"Selenium: Pausing for 10 seconds for manual observation of the page. Check if table appears.\")\n",
    "        time.sleep(10) # Long pause to see what the browser does on its own\n",
    "\n",
    "        # SIMPLIFIED WAIT: Just wait for the main table tag itself by its data-test attribute\n",
    "        main_table_selector_css = \"table[data-test='historical-prices']\" # For historical\n",
    "        if \"key-statistics\" in url:\n",
    "            main_table_selector_css = \"section[data-test='qsp-statistics'] div > table:nth-of-type(1)\" # For key-stats first table\n",
    "\n",
    "        print(f\"Selenium: Waiting for the main table container using CSS selector: '{main_table_selector_css}'\")\n",
    "        \n",
    "        try:\n",
    "            wait = WebDriverWait(DRIVER, 20)\n",
    "            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, main_table_selector_css)))\n",
    "            print(f\"Selenium: Main table container '{main_table_selector_css}' is PRESENT.\")\n",
    "        except TimeoutException:\n",
    "            print(f\"Selenium: Timed out waiting for PRESENCE of main table container '{main_table_selector_css}' at {url}.\")\n",
    "            DRIVER.save_screenshot(f\"debug_timeout_no_main_table_tag_{stock_symbol}.png\")\n",
    "            return None # If the main table tag isn't even there, no point proceeding\n",
    "\n",
    "        # If main table is present, now try for content within the target selector (usually tbody tr)\n",
    "        # The table_selector_for_bs4 is what we ultimately want for BS4 (e.g., includes tbody)\n",
    "        if table_selector_for_bs4.endswith(\"tbody\"):\n",
    "            wait_selector_css_for_row = f\"{table_selector_for_bs4} tr\"\n",
    "        else: # Assuming table_selector_for_bs4 points to the table tag\n",
    "             wait_selector_css_for_row = f\"{table_selector_for_bs4} tbody tr\"\n",
    "        \n",
    "        print(f\"Selenium: Waiting for at least one row ('tr') using CSS selector: '{wait_selector_css_for_row}'\")\n",
    "        try:\n",
    "            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, wait_selector_css_for_row)))\n",
    "            print(f\"Selenium: At least one row found for '{wait_selector_css_for_row}'.\")\n",
    "        except TimeoutException:\n",
    "            print(f\"Selenium: Timed out waiting for PRESENCE of a row ('tr') within '{table_selector_for_bs4}' at {url}.\")\n",
    "            DRIVER.save_screenshot(f\"debug_timeout_no_row_in_table_{stock_symbol}.png\")\n",
    "            # Even if no rows, the table might exist (e.g. \"No data available\"). Let's try to parse.\n",
    "            # return None # Might be too strict, let BS4 try if table_element itself was found by main_table_selector\n",
    "\n",
    "        time.sleep(random.uniform(2, 3))\n",
    "\n",
    "        page_source = DRIVER.page_source\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "        \n",
    "        table_element = soup.select_one(table_selector_for_bs4) # Use the original specific selector for BS4\n",
    "\n",
    "        if table_element is None:\n",
    "            print(f\"BS4: Target element '{table_selector_for_bs4}' not found in page source from {url}.\")\n",
    "            # For debugging, let's see if the main_table_selector_css was found by BS4\n",
    "            # main_table_by_bs4 = soup.select_one(main_table_selector_css)\n",
    "            # print(f\"BS4: Main table container '{main_table_selector_css}' found by BS4: {main_table_by_bs4 is not None}\")\n",
    "            # print(f\"DEBUG BS4 Page Source (first 2k): {page_source[:2000]}\")\n",
    "            return None\n",
    "\n",
    "        # ... (rest of your BeautifulSoup parsing logic - same as before) ...\n",
    "        if table_element.name == 'table':\n",
    "            table_body = table_element.find('tbody')\n",
    "            if not table_body: table_body = table_element\n",
    "        elif table_element.name == 'tbody':\n",
    "            table_body = table_element\n",
    "        else:\n",
    "            print(f\"Warning: Selector '{table_selector_for_bs4}' pointed to a '{table_element.name}' element. Attempting to find table within.\")\n",
    "            actual_table = table_element.find('table')\n",
    "            if actual_table:\n",
    "                table_body = actual_table.find('tbody')\n",
    "                if not table_body: table_body = actual_table\n",
    "            else:\n",
    "                print(f\"Error: Could not find a table within the element selected by '{table_selector_for_bs4}'.\")\n",
    "                return None\n",
    "\n",
    "        if table_body is None:\n",
    "            print(f\"Error: Table body could not be definitively identified from '{table_selector_for_bs4}'.\")\n",
    "            return None\n",
    "\n",
    "        rows = table_body.find_all('tr', recursive=False)\n",
    "        if not rows: # This could be a valid case if the table says \"No data available\"\n",
    "            print(f\"Info: No rows (<tr>) found in the table element '{table_selector_for_bs4}'. The table might be legitimately empty or state 'No data'.\")\n",
    "            # Check for a message like \"No data available\"\n",
    "            # no_data_message = table_body.find(text=re.compile(\"No data available\", re.IGNORECASE))\n",
    "            # if no_data_message:\n",
    "            #     print(f\"Confirmed: Table body contains '{no_data_message.strip()}'\")\n",
    "            #     return pd.DataFrame() # Return empty DataFrame\n",
    "            # else: # No rows and no clear \"no data\" message, treat as parsing failure for now\n",
    "            #     return None # Or return empty DF based on requirements\n",
    "\n",
    "        headers_list = []\n",
    "        if rows: # Only try to parse headers if rows exist\n",
    "            first_row_elements = rows[0].find_all(['th', 'td'])\n",
    "            if rows[0].find('th'):\n",
    "                headers_list = [th.get_text(strip=True) for th in rows[0].find_all('th')]\n",
    "                data_rows_to_process = rows[1:]\n",
    "            elif first_row_elements and len(rows) > 1 :\n",
    "                print(f\"Warning: No <th> in first row of '{table_selector_for_bs4}'. Using <td> from first row as headers.\")\n",
    "                headers_list = [cell.get_text(strip=True) for cell in first_row_elements]\n",
    "                data_rows_to_process = rows[1:]\n",
    "            else:\n",
    "                print(f\"Warning: No clear header row found in '{table_selector_for_bs4}'. All rows processed as data.\")\n",
    "                data_rows_to_process = rows\n",
    "        else: # No rows\n",
    "            data_rows_to_process = []\n",
    "\n",
    "\n",
    "        data = []\n",
    "        for row in data_rows_to_process:\n",
    "            cells = row.find_all('td')\n",
    "            if not cells and not headers_list:\n",
    "                cells = row.find_all(['th', 'td'])\n",
    "            row_data = [cell.get_text(strip=True) for cell in cells]\n",
    "            if row_data: data.append(row_data)\n",
    "        \n",
    "        if not data:\n",
    "            if headers_list:\n",
    "                print(f\"Info: Headers found for '{table_selector_for_bs4}', but no data rows. Returning empty DataFrame with headers.\")\n",
    "                return pd.DataFrame(columns=headers_list)\n",
    "            else: # No data and no headers from parsing (or table was truly empty)\n",
    "                if not rows: # If there were no rows to begin with\n",
    "                    print(f\"Info: Table '{table_selector_for_bs4}' had no rows. Returning empty DataFrame.\")\n",
    "                    return pd.DataFrame()\n",
    "                else: # Rows existed but yielded no data cells\n",
    "                    print(f\"Error: No data cells (<td>) extracted and no headers found for '{table_selector_for_bs4}'.\")\n",
    "                    return None\n",
    "\n",
    "\n",
    "        if headers_list:\n",
    "            if data and len(data[0]) != len(headers_list):\n",
    "                print(f\"Warning: Header count ({len(headers_list)}) != data column count ({len(data[0])}) for '{table_selector_for_bs4}'. Using minimum.\")\n",
    "                min_cols = min(len(headers_list), len(data[0]))\n",
    "                headers_list = headers_list[:min_cols]\n",
    "                processed_data = [d_row[:min_cols] for d_row in data]\n",
    "                data = processed_data\n",
    "            df = pd.DataFrame(data, columns=headers_list)\n",
    "        else:\n",
    "            df = pd.DataFrame(data)\n",
    "            print(f\"DataFrame for '{table_selector_for_bs4}' created without explicit column names.\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "    except WebDriverException as e:\n",
    "        # This will catch \"invalid session id\" if browser crashed\n",
    "        print(f\"Selenium: WebDriverException (Browser might have crashed or session lost) for {url}: {e}\")\n",
    "        try:\n",
    "            # Attempting to get current URL or title might also fail if session is truly gone\n",
    "            # print(f\"  Attempting to get current URL: {DRIVER.current_url}\")\n",
    "            DRIVER.save_screenshot(f\"debug_webdriver_exception_{stock_symbol}_{time.strftime('%H%M%S')}.png\")\n",
    "        except Exception as e_screenshot:\n",
    "            print(f\"  Could not save screenshot during WebDriverException: {e_screenshot}\")\n",
    "        return None # Indicate failure\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred with Selenium for {url}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# --- Example Usage ---\n",
    "if __name__ == '__main__':\n",
    "    # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    # !! RUN NON-HEADLESS FIRST TO OBSERVE THE BROWSER BEHAVIOR !!\n",
    "    # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    if not initialize_webdriver(headless=False): # <--- RUNNING NON-HEADLESS\n",
    "        print(\"Failed to initialize WebDriver. Exiting.\")\n",
    "        exit()\n",
    "\n",
    "    stock_symbol = \"AAPL\"\n",
    "    consent_ok = simplified_selenium_consent_check(DRIVER, \"https://finance.yahoo.com/\")\n",
    "    \n",
    "    if not consent_ok:\n",
    "        print(\"Initial page load/consent check failed. Subsequent steps might be unreliable.\")\n",
    "    else:\n",
    "        print(\"Initial page navigation seems okay. Proceeding to data tables.\")\n",
    "    \n",
    "    # Regardless of consent_ok for this test, try to proceed to see where the crash happens\n",
    "    # if it's not on the initial page.\n",
    "\n",
    "    time.sleep(random.uniform(1,2))\n",
    "\n",
    "    # 1. Historical Data\n",
    "    url_historical = f\"https://finance.yahoo.com/quote/{stock_symbol}/history/?p={stock_symbol}\"\n",
    "    selector_historical_tbody = \"table[data-test='historical-prices'] tbody\" # This is what BS4 will parse\n",
    "\n",
    "    print(f\"\\n--- Attempting to download historical data for {stock_symbol} using Selenium ---\")\n",
    "    df_historical = download_yahoo_finance_table_selenium(url_historical, selector_historical_tbody, stock_symbol)\n",
    "\n",
    "    if df_historical is not None and not df_historical.empty:\n",
    "        print(f\"\\nSuccessfully downloaded Historical Data for {stock_symbol} (first 5 rows):\")\n",
    "        print(df_historical.head())\n",
    "    elif df_historical is not None: # Empty DataFrame\n",
    "        print(f\"\\nDownloaded table for {stock_symbol} (Historical Data) with Selenium, but it was empty or had no data rows.\")\n",
    "    else:\n",
    "        print(f\"\\nFailed to download historical data for {stock_symbol} with Selenium.\")\n",
    "\n",
    "    print(\"-\" * 50)\n",
    "    # Check if driver session is still valid before proceeding\n",
    "    driver_still_valid = False\n",
    "    if DRIVER and hasattr(DRIVER, 'window_handles'):\n",
    "        try:\n",
    "            DRIVER.current_url # A simple command to check session\n",
    "            driver_still_valid = True\n",
    "            print(\"Driver session appears to be still valid.\")\n",
    "        except WebDriverException:\n",
    "            print(\"Driver session became invalid after historical data attempt.\")\n",
    "            driver_still_valid = False\n",
    "    else:\n",
    "        print(\"Driver is None or has no window_handles after historical data attempt.\")\n",
    "\n",
    "\n",
    "    if driver_still_valid:\n",
    "        time.sleep(random.uniform(2, 3)) \n",
    "        # 2. Key Statistics\n",
    "        url_stats = f\"https://finance.yahoo.com/quote/{stock_symbol}/key-statistics/?p={stock_symbol}\"\n",
    "        selector_valuation_measures_tbody = \"section[data-test='qsp-statistics'] div > table:nth-of-type(1) tbody\"\n",
    "\n",
    "        print(f\"\\n--- Attempting to download Valuation Measures for {stock_symbol} using Selenium ---\")\n",
    "        df_stats = download_yahoo_finance_table_selenium(url_stats, selector_valuation_measures_tbody, stock_symbol)\n",
    "\n",
    "        if df_stats is not None and not df_stats.empty:\n",
    "            print(f\"\\nSuccessfully downloaded Valuation Measures for {stock_symbol} (first 5 rows):\")\n",
    "            print(df_stats.head())\n",
    "        elif df_stats is not None: # Empty DataFrame\n",
    "            print(f\"\\nDownloaded table for {stock_symbol} (Valuation Measures) with Selenium, but it was empty or had no data rows.\")\n",
    "        else:\n",
    "            print(f\"\\nFailed to download statistics (Valuation Measures) for {stock_symbol} with Selenium.\")\n",
    "    else:\n",
    "        print(\"\\nSkipping Key Statistics download as WebDriver session is invalid.\")\n",
    "\n",
    "    close_webdriver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "5Ro6O4nGuLqW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading PEN. Sleeping for 2.11 seconds.  Processed 3 / 2 symbols.\n",
      "Error during request: 404 Client Error: Not Found for url: https://finance.yahoo.com/quote/PEN/history/\n",
      "Failed to download data for PEN\n",
      "Downloading ALAB. Sleeping for 3.56 seconds.  Processed 4 / 2 symbols.\n",
      "Error during request: 404 Client Error: Not Found for url: https://finance.yahoo.com/quote/ALAB/history/\n",
      "Failed to download data for ALAB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame()  # Initialized an empty DataFrame\n",
    "\n",
    "for symbol in symbols_to_download:\n",
    "    url = f\"https://finance.yahoo.com/quote/{symbol}/history/\"\n",
    "    # Introduce a delay between requests (adjust as needed)\n",
    "    delay_seconds = random.uniform(2, 4.5)  # Sleep between 2 and 5 seconds\n",
    "    # print(f\"Downloading {symbol}. Sleeping for {delay_seconds:.2f} seconds...\")\n",
    "    processed_count += 1\n",
    "    print(f\"Downloading {symbol}. Sleeping for {delay_seconds:.2f} seconds.  Processed {processed_count} / {total_symbols_to_download} symbols.\")\n",
    "    time.sleep(delay_seconds)\n",
    "\n",
    "    df_temp = download_yahoo_finance_table(url, selector)\n",
    "\n",
    "    if df_temp is not None:\n",
    "        df_temp.columns = col_names # Ensure the columns are what is expected\n",
    "\n",
    "        df_temp.set_index('Date', inplace=True) # Set Date as Index\n",
    "        # Create MultiIndex\n",
    "        df_temp.index = pd.MultiIndex.from_product([[symbol], df_temp.index], names=['Symbol', 'Date'])\n",
    "\n",
    "        df = pd.concat([df, df_temp])  # Append to the combined DataFrame\n",
    "\n",
    "    else:\n",
    "        print(f\"Failed to download data for {symbol}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ekzZn5ahuLqW"
   },
   "outputs": [],
   "source": [
    "current_pst_time = get_current_pst_time()\n",
    "print(f\"End OHLCV download at PST time: {current_pst_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nW2J6txeuLqX"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5elcgkAsuLqX"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ed0R2tQduLqX"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pytz\n",
    "\n",
    "pst = pytz.timezone('America/Los_Angeles')  # or 'US/Pacific'\n",
    "current_date_pst = datetime.datetime.now(pst).strftime('%Y-%m-%d')\n",
    "\n",
    "df_OHLCV_filename = f\"df_OHLCV_{current_date_pst}_stocks.parquet\"\n",
    "\n",
    "print(f\"df_OHLCV_filename: {df_OHLCV_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5cWwmLvyuLqc"
   },
   "outputs": [],
   "source": [
    "# Drop rows with any NaN values\n",
    "df_dropna = df.dropna()\n",
    "df_dropna.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gqENdn0huLqc"
   },
   "outputs": [],
   "source": [
    "df_converted = convert_df_data_types(df_dropna.copy())  # Create a copy to avoid modifying the original\n",
    "df_converted.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iEi6efHUuLqd"
   },
   "outputs": [],
   "source": [
    "df_adjusted = adjust_prices(df_converted.copy())  # Create a copy\n",
    "df_adjusted.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vWQqc08yqnfk"
   },
   "outputs": [],
   "source": [
    "df_adjusted.to_parquet(df_OHLCV_filename, engine='pyarrow', compression='zstd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bb6tONh4uLqd"
   },
   "outputs": [],
   "source": [
    "df_adjusted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zvZEbfwqiS0Z"
   },
   "outputs": [],
   "source": [
    "# Create a ZIP file\n",
    "zip_filename = current_date_pst + '_finviz_OHLCV.zip'\n",
    "zip_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2SjosRxFlcwW"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "# from google.colab import files\n",
    "\n",
    "# Example: List of files to download\n",
    "file_list = [df_finviz_filename, df_OHLCV_filename]\n",
    "\n",
    "# Check if files exist before adding to the zip archive\n",
    "with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
    "    for file in file_list:\n",
    "        if os.path.exists(file):\n",
    "            zipf.write(file)\n",
    "        else:\n",
    "            print(f\"Warning: File not found: {file}. Skipping.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XvKKrhq-7UGk"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "from google.colab import files\n",
    "\n",
    "if download_to_PC:\n",
    "  # Download the ZIP file (only 1 prompt)\n",
    "  if os.path.exists(zip_filename):\n",
    "      files.download(zip_filename)\n",
    "      print(f\"File '{zip_filename}' dowloaded to PC\")\n",
    "  else:\n",
    "      print(f\"Error: Zip file not created: {zip_filename}\")\n",
    "\n",
    "else:\n",
    "  destination_path = '/content/drive/MyDrive/stocks/'\n",
    "\n",
    "  # Construct the full path to the destination file\n",
    "  destination_file = os.path.join(destination_path, zip_filename)\n",
    "\n",
    "  # Use shutil.copy2 to preserve metadata (like timestamps)\n",
    "  try:\n",
    "      shutil.copy2(zip_filename, destination_file)\n",
    "      print(f\"File '{zip_filename}' downloaded to '{destination_file}'\")\n",
    "  except FileNotFoundError:\n",
    "      print(f\"Error: The file '{zip_filename}' was not found.\")\n",
    "  except Exception as e:\n",
    "      print(f\"An error occurred during the copy operation: {e}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8gD4Ira3CdUo"
   },
   "outputs": [],
   "source": [
    "# # prompt: calculate the daily return of each ticker in df_adjusted. note the date index in df_adjusted is in reverse order\n",
    "\n",
    "# # THIS CALCULATAION IS CORRECT --- Calculate daily returns, handling the reversed date index\n",
    "# df_returns = df_adjusted['Adj Close'].pct_change(periods=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XjbtUIlQf9SQ"
   },
   "outputs": [],
   "source": [
    "# # prompt: cp: cannot stat 'zip_filename.zip': No such file or directory\n",
    "\n",
    "# import os\n",
    "# import zipfile\n",
    "# from google.colab import files\n",
    "\n",
    "# # Example: List of files to download\n",
    "# file_list = [df_finviz_filename, df_OHLCV_filename]\n",
    "\n",
    "# # Check if files exist before adding to the zip archive\n",
    "# with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
    "#     for file in file_list:\n",
    "#         if os.path.exists(file):\n",
    "#             zipf.write(file)\n",
    "#         else:\n",
    "#             print(f\"Warning: File not found: {file}. Skipping.\")\n",
    "\n",
    "# # Download the ZIP file (only 1 prompt)\n",
    "# if os.path.exists(zip_filename):\n",
    "#     files.download(zip_filename)\n",
    "# else:\n",
    "#     print(f\"Error: Zip file not created: {zip_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cMk4EaASg5ze"
   },
   "outputs": [],
   "source": [
    "# # prompt: copy zipeFile from above cell to /content/drive/MyDrive/stocks/\n",
    "\n",
    "# import shutil\n",
    "# import os\n",
    "\n",
    "# # Assuming zip_filename is defined in the previous code cell\n",
    "# # and contains the name of the created zip file.\n",
    "# # Example: zip_filename = '2024-07-27_finviz_OHLCV.zip'\n",
    "\n",
    "# destination_path = '/content/drive/MyDrive/stocks/'\n",
    "\n",
    "# # Construct the full path to the destination file\n",
    "# destination_file = os.path.join(destination_path, zip_filename)\n",
    "\n",
    "# # Use shutil.copy2 to preserve metadata (like timestamps)\n",
    "# try:\n",
    "#     shutil.copy2(zip_filename, destination_file)\n",
    "#     print(f\"File '{zip_filename}' copied to '{destination_file}'\")\n",
    "# except FileNotFoundError:\n",
    "#     print(f\"Error: The file '{zip_filename}' was not found.\")\n",
    "# except Exception as e:\n",
    "#     print(f\"An error occurred during the copy operation: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1c-CzVo5tGFmNmvnIhfHMJTDBqd0fwr_x",
     "timestamp": 1745777515985
    }
   ]
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
