{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# from pathlib import Path\n",
    "\n",
    "# # Notebook cell\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "# # Get root directory (assuming notebook is in root/notebooks/)\n",
    "# NOTEBOOK_DIR = Path.cwd()\n",
    "# ROOT_DIR = NOTEBOOK_DIR.parent if NOTEBOOK_DIR.name == 'notebooks' else NOTEBOOK_DIR\n",
    "\n",
    "# # Add src directory to Python path\n",
    "# sys.path.append(str(ROOT_DIR / 'src'))\n",
    "\n",
    "# # Verify path\n",
    "# print(f\"Python will look in these locations:\\n{sys.path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Set pandas display options to show more columns and rows\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "# pd.set_option('display.max_rows', 10)       # Limit to 10 rows for readability\n",
    "pd.set_option('display.width', 1000)        # Let the display adjust to the window\n",
    "# pd.set_option('display.max_colwidth', None) # Show full content of each cell\n",
    "pd.set_option('display.max_rows', 200)\n",
    "# pd.set_option('display.width', 120)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zscore_df = pd.read_parquet(f'..\\data\\{date_str}_zscore_df.parquet', engine='pyarrow')\n",
    "cluster_stats_df = pd.read_parquet(f'..\\data\\{date_str}_cluster_stats_df.parquet', engine='pyarrow')\n",
    "detailed_clusters_df = pd.read_parquet(f'..\\data\\{date_str}_detailed_clusters_df.parquet', engine='pyarrow')\n",
    "# df_finviz_merged = pd.read_parquet(f'..\\data\\{date_str}_df_finviz_merged.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detailed_clusters_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd # Assuming pandas is used\n",
    "# import numpy as np\n",
    "\n",
    "# def select_stocks_from_clusters(cluster_stats_df, detailed_clusters_df,\n",
    "#                                 select_top_n_clusters=3, max_selection_per_cluster=5,\n",
    "#                                 min_cluster_size=5, penalty_IntraCluster_Corr=0.3,\n",
    "#                                 date_str=date_str,\n",
    "#                                 min_raw_score=None, # <-- Added argument\n",
    "#                                 min_risk_adj_score=None): # <-- Added argument\n",
    "#     \"\"\"\n",
    "#     Pipeline to select stocks from better performing clusters, with optional score thresholds.\n",
    "\n",
    "#     Parameters:\n",
    "#     - cluster_stats_df: DataFrame with cluster statistics.\n",
    "#     - detailed_clusters_df: DataFrame with detailed cluster information including\n",
    "#                             'Ticker', 'Cluster_ID', 'Raw_Score', 'Risk_Adj_Score', etc.\n",
    "#     - select_top_n_clusters: int, Number of top clusters to select (default=3).\n",
    "#     - max_selection_per_cluster: int, Max number of stocks to select from each cluster (default=5).\n",
    "#     - min_cluster_size: int, Minimum size for a cluster to be considered (default=5).\n",
    "#     - penalty_IntraCluster_Corr: float, Penalty weight for intra-cluster correlation in\n",
    "#                                      composite score (default=0.3).\n",
    "#     - date_str: str, Date string for tracking/parameter storage.\n",
    "#     - min_raw_score: float, optional (default=None)\n",
    "#         Minimum Raw_Score required for a stock to be considered for selection.\n",
    "#         If None, no threshold is applied based on Raw_Score.\n",
    "#     - min_risk_adj_score: float, optional (default=None)\n",
    "#         Minimum Risk_Adj_Score required for a stock to be considered for selection.\n",
    "#         If None, no threshold is applied based on Risk_Adj_Score.\n",
    "\n",
    "#     Returns:\n",
    "#     - dict: A dictionary containing:\n",
    "#         - 'selected_top_n_cluster_ids': List of top selected cluster IDs.\n",
    "#         - 'selected_stocks': DataFrame of selected stocks.\n",
    "#         - 'cluster_performance': DataFrame of selected cluster metrics.\n",
    "#         - 'parameters': Dictionary of the input parameters used.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Store input parameters\n",
    "#     parameters = {\n",
    "#         'date_str': date_str,\n",
    "#         'select_top_n_clusters': select_top_n_clusters,\n",
    "#         'max_selection_per_cluster': max_selection_per_cluster,\n",
    "#         'min_cluster_size': min_cluster_size,\n",
    "#         'min_raw_score': min_raw_score,         # <-- Stored parameter\n",
    "#         'min_risk_adj_score': min_risk_adj_score, # <-- Stored parameter\n",
    "#         'penalty_IntraCluster_Corr': penalty_IntraCluster_Corr,\n",
    "#     }\n",
    "    \n",
    "#     # ===== 1. Filter and Rank Clusters =====\n",
    "#     qualified_clusters = cluster_stats_df[cluster_stats_df['Size'] >= min_cluster_size].copy()\n",
    "#     if qualified_clusters.empty:\n",
    "#         print(f\"Warning: No clusters met the minimum size criteria ({min_cluster_size}).\")\n",
    "#         return {\n",
    "#             'selected_stocks': pd.DataFrame(),\n",
    "#             'cluster_performance': pd.DataFrame(),\n",
    "#             'parameters': parameters\n",
    "#         }\n",
    "\n",
    "#     qualified_clusters['Composite_Cluster_Score'] = (\n",
    "#         (1 - penalty_IntraCluster_Corr) * qualified_clusters['Avg_Raw_Score'] +\n",
    "#         penalty_IntraCluster_Corr * (1 - qualified_clusters['Avg_IntraCluster_Corr'])\n",
    "#     )\n",
    "#     ranked_clusters = qualified_clusters.sort_values('Composite_Cluster_Score', ascending=False)\n",
    "#     selected_clusters = ranked_clusters.head(select_top_n_clusters)\n",
    "#     cluster_ids = selected_clusters['Cluster_ID'].tolist()\n",
    "\n",
    "#     if not cluster_ids:\n",
    "#         print(\"Warning: No clusters were selected based on ranking.\")\n",
    "#         return {\n",
    "#             'selected_stocks': pd.DataFrame(),\n",
    "#             'cluster_performance': selected_clusters, # Return empty selected clusters df\n",
    "#             'parameters': parameters\n",
    "#         }\n",
    "\n",
    "\n",
    "#     # ===== 2. Select Stocks from Each Cluster =====\n",
    "#     selected_stocks_list = []\n",
    "#     for cluster_id in cluster_ids:\n",
    "#         # Get all stocks for the current cluster\n",
    "#         cluster_stocks = detailed_clusters_df[detailed_clusters_df['Cluster_ID'] == cluster_id].copy()\n",
    "\n",
    "#         # ===> Apply Threshold Filters <===\n",
    "#         if min_raw_score is not None:\n",
    "#             cluster_stocks = cluster_stocks[cluster_stocks['Raw_Score'] >= min_raw_score]\n",
    "#         if min_risk_adj_score is not None:\n",
    "#             cluster_stocks = cluster_stocks[cluster_stocks['Risk_Adj_Score'] >= min_risk_adj_score]\n",
    "#         # ===> End of Added Filters <===\n",
    "\n",
    "#         # Proceed only if stocks remain after filtering\n",
    "#         if len(cluster_stocks) > 0:\n",
    "#             # Sort remaining stocks by Risk_Adj_Score and select top N\n",
    "#             top_stocks = cluster_stocks.sort_values('Risk_Adj_Score', ascending=False).head(max_selection_per_cluster)\n",
    "\n",
    "#             # Add cluster-level metrics to the selected stock rows\n",
    "#             cluster_metrics = selected_clusters[selected_clusters['Cluster_ID'] == cluster_id].iloc[0]\n",
    "#             for col in ['Composite_Cluster_Score', 'Avg_IntraCluster_Corr', 'Avg_Volatility',\n",
    "#                       'Avg_Raw_Score', 'Avg_Risk_Adj_Score', 'Size']: # Added Size for context\n",
    "#                 # Use .get() for safety if a column might be missing\n",
    "#                 top_stocks[f'Cluster_{col}'] = cluster_metrics.get(col, None)\n",
    "#             selected_stocks_list.append(top_stocks)\n",
    "\n",
    "#     # Consolidate selected stocks\n",
    "#     if selected_stocks_list:\n",
    "#         selected_stocks = pd.concat(selected_stocks_list)\n",
    "#         # Recalculate weights based on Risk_Adj_Score of the final selection\n",
    "#         # Check if the sum of scores used for weighting is non-zero\n",
    "#         score_sum = selected_stocks['Risk_Adj_Score'].sum()\n",
    "#         if score_sum != 0 and np.isfinite(score_sum): # Check for non-zero and finite sum\n",
    "#             selected_stocks['Weight'] = selected_stocks['Risk_Adj_Score'] / score_sum\n",
    "#         else:\n",
    "#             # Handle case where all selected scores are zero/NaN/Inf or no stocks selected\n",
    "#             # Assign equal weights if possible, otherwise weights remain undefined or zero\n",
    "#             num_stocks = len(selected_stocks)\n",
    "#             if num_stocks > 0:\n",
    "#                 print(f\"Warning ({date_str}): Sum of Risk_Adj_Score for weighting is {score_sum}. Assigning equal weights.\")\n",
    "#                 selected_stocks['Weight'] = 1 / num_stocks\n",
    "#             else:\n",
    "#                 # Should not happen if selected_stocks_list was populated, but defensive coding\n",
    "#                 selected_stocks['Weight'] = 0 # Or handle as appropriate\n",
    "#         selected_stocks = selected_stocks.sort_values(['Cluster_ID', 'Risk_Adj_Score'],\n",
    "#                                                     ascending=[True, False])\n",
    "#     else:\n",
    "#         selected_stocks = pd.DataFrame()\n",
    "#         print(\"Warning: No stocks met selection criteria (including score thresholds if applied).\")\n",
    "\n",
    "\n",
    "#     # ===== 3. Prepare Enhanced Output Reports =====\n",
    "#     cluster_performance = selected_clusters.copy()\n",
    "#     # Calculate how many stocks were actually selected per cluster after filtering\n",
    "#     cluster_performance['Stocks_Selected'] = cluster_performance['Cluster_ID'].apply(\n",
    "#         lambda x: len(selected_stocks[selected_stocks['Cluster_ID'] == x]) if not selected_stocks.empty else 0)\n",
    "\n",
    "#     if not selected_stocks.empty:\n",
    "#         # Ensure Avg_IntraCluster_Corr exists before calculating diversification\n",
    "#         if 'Avg_IntraCluster_Corr' in cluster_performance.columns:\n",
    "#             cluster_performance['Intra_Cluster_Diversification'] = 1 - cluster_performance['Avg_IntraCluster_Corr']\n",
    "#         else:\n",
    "#             cluster_performance['Intra_Cluster_Diversification'] = pd.NA # Or None\n",
    "#     else:\n",
    "#       # Handle case where selected_stocks is empty\n",
    "#         cluster_performance['Intra_Cluster_Diversification'] = pd.NA # Or None\n",
    "\n",
    "#     # ===> Package results and parameters\n",
    "#     results_bundle = {\n",
    "#         'selected_top_n_cluster_ids': cluster_ids,\n",
    "#         'selected_stocks': selected_stocks,\n",
    "#         'cluster_performance': cluster_performance,\n",
    "#         'parameters': parameters\n",
    "#     }\n",
    "\n",
    "#     return results_bundle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from typing import Dict, Any\n",
    "\n",
    "# def print_stock_selection_report(output: Dict[str, Any]) -> None:\n",
    "#     \"\"\"\n",
    "#     Prints a detailed report summarizing the results of the stock selection process,\n",
    "#     extracting all necessary information from the output dictionary.\n",
    "\n",
    "#     Args:\n",
    "#         output (Dict[str, Any]): The dictionary returned by the\n",
    "#                                  select_stocks_from_clusters function, containing:\n",
    "#                                  - 'selected_stocks': DataFrame of selected stocks.\n",
    "#                                  - 'cluster_performance': DataFrame of selected cluster metrics.\n",
    "#                                 #  - 'parameters': Dictionary of the input parameters used.\n",
    "#                                 #  - 'cluster_stats_df': Original cluster stats DataFrame.\n",
    "#                                 #  - 'detailed_clusters_df': Original detailed clusters DataFrame.\n",
    "#     Returns:\n",
    "#         None: This function prints output to the console.\n",
    "#     \"\"\"\n",
    "#     # Extract data from the output dictionary using .get() for safety\n",
    "#     selected_stocks = output.get('selected_stocks', pd.DataFrame())\n",
    "#     cluster_performance = output.get('cluster_performance', pd.DataFrame())\n",
    "#     used_params = output.get('parameters', {})\n",
    "#     # Extract the input DataFrames needed for the report\n",
    "#     # cluster_stats_df = output.get('input_cluster_stats_df') # Might be None\n",
    "#     cluster_stats_df = output.get('cluster_stats_df') # Might be None\n",
    "#     # detailed_clusters_df = output.get('input_detailed_clusters_df') # Might be None\n",
    "#     detailed_clusters_df = output.get('detailed_clusters_df') # Might be None\n",
    "\n",
    "#     # --- Start of Original Code Block (adapted) ---\n",
    "\n",
    "#     print(\"\\n=== CLUSTER SELECTION CRITERIA ===\")\n",
    "#     print(\"* Using Composite_Cluster_Score (balancing Raw Score and diversification) for cluster ranking.\")\n",
    "#     print(\"* Using Risk_Adj_Score for stock selection within clusters.\")\n",
    "\n",
    "#     num_selected_clusters = len(cluster_performance) if not cluster_performance.empty else 0\n",
    "#     # Use the extracted cluster_stats_df\n",
    "#     total_clusters = len(cluster_stats_df) if cluster_stats_df is not None and not cluster_stats_df.empty else 'N/A'\n",
    "\n",
    "#     print(f\"* Selected top {num_selected_clusters} clusters from {total_clusters} total initial clusters.\") # Adjusted wording slightly\n",
    "#     print(f\"* Selection Criteria:\")\n",
    "#     if used_params:\n",
    "#         for key, value in used_params.items():\n",
    "#             # Avoid printing the large input dataframes stored in parameters if they were added there too\n",
    "#             if not isinstance(value, pd.DataFrame):\n",
    "#                  print(f\"    {key}: {value}\")\n",
    "#     else:\n",
    "#         print(\"    Parameters not available.\")\n",
    "\n",
    "\n",
    "#     if not cluster_performance.empty:\n",
    "#         print(\"\\n=== SELECTED CLUSTERS (RANKED BY COMPOSITE SCORE) ===\")\n",
    "#         display_cols_exist = [col for col in [\n",
    "#                                 'Cluster_ID', 'Size', 'Avg_Raw_Score', 'Avg_Risk_Adj_Score',\n",
    "#                                 'Avg_IntraCluster_Corr', 'Avg_Volatility', 'Composite_Cluster_Score',\n",
    "#                                 'Stocks_Selected', 'Intra_Cluster_Diversification']\n",
    "#                               if col in cluster_performance.columns]\n",
    "#         print(cluster_performance[display_cols_exist].sort_values('Composite_Cluster_Score', ascending=False).to_string(index=False))\n",
    "\n",
    "#         # Print top 8 stocks by Raw_Score for each selected cluster\n",
    "#         # Check if detailed_clusters_df was successfully extracted\n",
    "#         if detailed_clusters_df is not None and not detailed_clusters_df.empty:\n",
    "#             print(\"\\n=== TOP STOCKS BY RAW SCORE PER SELECTED CLUSTER ===\")\n",
    "#             print(\"\"\"* Volatility is the standard deviation of daily returns over the past 250 trading days (example context).\n",
    "# * Note: The stocks below are shown ranked by Raw_Score for analysis,\n",
    "# *       but actual selection within the cluster was based on Risk_Adj_Score.\"\"\")\n",
    "\n",
    "#             for cluster_id in cluster_performance['Cluster_ID']:\n",
    "#                  cluster_stocks = detailed_clusters_df[detailed_clusters_df['Cluster_ID'] == cluster_id]\n",
    "#                  if not cluster_stocks.empty:\n",
    "#                     required_cols = ['Ticker', 'Raw_Score', 'Risk_Adj_Score', 'Volatility']\n",
    "#                     if all(col in cluster_stocks.columns for col in required_cols):\n",
    "#                         top_raw = cluster_stocks.nlargest(8, 'Raw_Score')[required_cols]\n",
    "\n",
    "#                         print(f\"\\nCluster {cluster_id} - Top 8 by Raw Score:\")\n",
    "#                         print(top_raw.to_string(index=False))\n",
    "#                         cluster_avg_raw = cluster_performance.loc[cluster_performance['Cluster_ID'] == cluster_id, 'Avg_Raw_Score'].values\n",
    "#                         cluster_avg_risk = cluster_performance.loc[cluster_performance['Cluster_ID'] == cluster_id, 'Avg_Risk_Adj_Score'].values\n",
    "#                         if len(cluster_avg_raw) > 0: print(f\"Cluster Avg Raw Score: {cluster_avg_raw[0]:.2f}\")\n",
    "#                         if len(cluster_avg_risk) > 0: print(f\"Cluster Avg Risk Adj Score: {cluster_avg_risk[0]:.2f}\")\n",
    "#                     else:\n",
    "#                         print(f\"\\nCluster {cluster_id} - Missing required columns in detailed_clusters_df to show top stocks.\")\n",
    "#                  else:\n",
    "#                      print(f\"\\nCluster {cluster_id} - No stocks found in detailed_clusters_df for this cluster.\")\n",
    "#         else:\n",
    "#             print(\"\\n=== TOP STOCKS BY RAW SCORE PER SELECTED CLUSTER ===\")\n",
    "#             print(\"Skipping - Detailed cluster information ('input_detailed_clusters_df') not found in the output dictionary.\")\n",
    "\n",
    "#     else:\n",
    "#         print(\"\\n=== SELECTED CLUSTERS ===\")\n",
    "#         print(\"No clusters were selected based on the criteria.\")\n",
    "\n",
    "\n",
    "#     print(f\"\\n=== FINAL SELECTED STOCKS (FILTERED & WEIGHTED) ===\")\n",
    "#     if not selected_stocks.empty:\n",
    "#         print(\"* Stocks actually selected based on Risk_Adj_Score (and optional thresholds) within each cluster.\")\n",
    "#         print(\"* Position weights assigned based on Risk_Adj_Score within the final selected portfolio.\")\n",
    "\n",
    "#         desired_cols = ['Cluster_ID', 'Ticker', 'Raw_Score', 'Risk_Adj_Score',\n",
    "#                         'Volatility', 'Weight',\n",
    "#                         'Cluster_Avg_Raw_Score', 'Cluster_Avg_Risk_Adj_Score']\n",
    "#         available_cols = [col for col in desired_cols if col in selected_stocks.columns]\n",
    "#         print(selected_stocks[available_cols].sort_values(['Cluster_ID', 'Risk_Adj_Score'],\n",
    "#                                                         ascending=[True, False]).to_string(index=False))\n",
    "\n",
    "#         print(\"\\n=== PORTFOLIO SUMMARY ===\")\n",
    "#         print(f\"Total Stocks Selected: {len(selected_stocks)}\")\n",
    "#         print(f\"Average Raw Score: {selected_stocks.get('Raw_Score', pd.Series(dtype=float)).mean():.2f}\")\n",
    "#         print(f\"Average Risk-Adjusted Score: {selected_stocks.get('Risk_Adj_Score', pd.Series(dtype=float)).mean():.2f}\")\n",
    "#         print(f\"Average Volatility: {selected_stocks.get('Volatility', pd.Series(dtype=float)).mean():.2f}\")\n",
    "#         print(f\"Total Weight (should be close to 1.0): {selected_stocks.get('Weight', pd.Series(dtype=float)).sum():.4f}\")\n",
    "#         print(\"\\nCluster Distribution:\")\n",
    "#         print(selected_stocks['Cluster_ID'].value_counts().to_string())\n",
    "#     else:\n",
    "#         print(\"No stocks were selected after applying all filters and criteria.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools # Import the itertools module\n",
    "\n",
    "# --- Define Factor Ranges ---\n",
    "# Generate the factors using numpy.arange for float steps\n",
    "# Add a small epsilon to the end value to ensure the endpoint is included due to float precision\n",
    "raw_score_factors = np.arange(0.5, 1.2 + 0.01, 0.1)\n",
    "risk_adj_score_factors = np.arange(0.5, 1.2 + 0.01, 0.1)\n",
    "penalty_factors = np.arange(0, 1.0 + 0.01, 0.1) # New factor range\n",
    "\n",
    "print(\"--- Parameter Ranges ---\")\n",
    "print(f\"Raw Score Factors: {np.round(raw_score_factors,1)}\")\n",
    "print(f\"Risk Adj Score Factors: {np.round(risk_adj_score_factors,1)}\")\n",
    "print(f\"Penalty Factors: {np.round(penalty_factors,1)}\")\n",
    "\n",
    "\n",
    "# --- Generate All Combinations ---\n",
    "# Use itertools.product to create an iterator of all combinations\n",
    "parameter_combinations = list(itertools.product(raw_score_factors, risk_adj_score_factors, penalty_factors))\n",
    "total_combinations = len(parameter_combinations)\n",
    "print(f\"\\nTotal parameter combinations to iterate: {total_combinations}\")\n",
    "\n",
    "\n",
    "# --- Store results ---\n",
    "all_portfolios = {} # Dictionary to store portfolios by name\n",
    "\n",
    "# --- Fixed Parameters (that don't vary in this loop) ---\n",
    "select_top_n_clusters = 60\n",
    "max_selection_per_cluster = 2\n",
    "min_cluster_size = 3  # prevent extreme high risk adj scores\n",
    "# You might want to get the date dynamically\n",
    "portf_date_base = date_str # Example date, adjust as needed (Ensure date_str is defined before this)\n",
    "\n",
    "# --- Iteration Loop (Single Loop over Combinations) ---\n",
    "print(\"\\nStarting portfolio generation loop...\")\n",
    "for i, (raw_scale, risk_adj_scale, penalty) in enumerate(parameter_combinations):\n",
    "\n",
    "    # --- Calculate dynamic parameters based on current factors ---\n",
    "    # Round factors slightly to avoid potential floating point representation issues in calculations/names\n",
    "    raw_scale = round(raw_scale, 1)\n",
    "    risk_adj_scale = round(risk_adj_scale, 1)\n",
    "    penalty = round(penalty, 1) # Round the new penalty factor as well\n",
    "\n",
    "    min_raw_score = 2.0 * raw_scale\n",
    "    min_risk_adj_score = 100.0 * risk_adj_scale\n",
    "    # penalty_IntraCluster_Corr is now the 'penalty' variable from the combination\n",
    "\n",
    "    print(f\"\\nRunning combination {i+1}/{total_combinations}: \"\n",
    "          f\"raw_F={raw_scale:.1f}, riskAdj_F={risk_adj_scale:.1f}, penalty={penalty:.1f}\")\n",
    "    print(f\"Resulting thresholds: min_raw_score={min_raw_score:.2f}, min_risk_adj_score={min_risk_adj_score:.1f}\")\n",
    "\n",
    "    # --- Run the selection pipeline --- \n",
    "    try:\n",
    "        output = utils.select_stocks_from_clusters(\n",
    "            cluster_stats_df=cluster_stats_df,      # Ensure this DataFrame is loaded/defined\n",
    "            detailed_clusters_df=detailed_clusters_df, # Ensure this DataFrame is loaded/defined\n",
    "            select_top_n_clusters=select_top_n_clusters,\n",
    "            max_selection_per_cluster=max_selection_per_cluster,\n",
    "            min_cluster_size=min_cluster_size,\n",
    "            penalty_IntraCluster_Corr=penalty, # Use the penalty from the current combination\n",
    "            min_raw_score=min_raw_score,\n",
    "            min_risk_adj_score=min_risk_adj_score,\n",
    "            date_str=portf_date_base # Pass the date\n",
    "        )\n",
    "\n",
    "        # --- Process and Store Results ---\n",
    "        portf_date = output['parameters']['date_str']\n",
    "        portf_raw_score_val = output['parameters']['min_raw_score'] # Use the value returned by the function\n",
    "        portf_risk_adj_score_val = output['parameters']['min_risk_adj_score'] # Use the value returned by the function\n",
    "        portf_penalty_val = output['parameters']['penalty_IntraCluster_Corr'] # Get penalty used\n",
    "        _selected_stocks = output['selected_stocks']\n",
    "\n",
    "        if _selected_stocks is not None and not _selected_stocks.empty:\n",
    "            portf_selected_stocks = _selected_stocks.set_index('Ticker')[['Weight']] # Select only Weight column after setting index\n",
    "\n",
    "            # Format the name using the factors - now includes penalty\n",
    "            portf_name = f'{portf_date}_portf_rawF_{raw_scale:.1f}_riskAdjF_{risk_adj_scale:.1f}_pen_{penalty:.1f}'\n",
    "            # Alternative using resulting thresholds and penalty:\n",
    "            # portf_name = f'{portf_date}_portf_raw_{portf_raw_score_val:.2f}_riskadj_{portf_risk_adj_score_val:.1f}_pen_{portf_penalty_val:.1f}'\n",
    "\n",
    "            print(f'Generated Portfolio:')\n",
    "            print(f'  Name: {portf_name}')\n",
    "            print(f'  Number of stocks: {len(portf_selected_stocks)}')\n",
    "            # print(f'port_selected_stocks:\\n{portf_selected_stocks}') # Can be verbose\n",
    "\n",
    "            # Store the results - using the portfolio name as the key\n",
    "            all_portfolios[portf_name] = {\n",
    "                'parameters': {\n",
    "                    'raw_score_scale_factor': raw_scale,\n",
    "                    'risk_adj_score_scale_factor': risk_adj_scale,\n",
    "                    'penalty_IntraCluster_Corr': penalty, # Store the penalty factor used\n",
    "                    'min_raw_score': portf_raw_score_val,\n",
    "                    'min_risk_adj_score': portf_risk_adj_score_val,\n",
    "                    'select_top_n_clusters': select_top_n_clusters,\n",
    "                    'max_selection_per_cluster': max_selection_per_cluster,\n",
    "                    'min_cluster_size': min_cluster_size,\n",
    "                    'date': portf_date\n",
    "                },\n",
    "                'selected_stocks': portf_selected_stocks\n",
    "            }\n",
    "        else:\n",
    "             print(f\"No stocks selected for raw_F={raw_scale:.1f}, riskAdj_F={risk_adj_scale:.1f}, penalty={penalty:.1f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR processing combination raw_F={raw_scale:.1f}, riskAdj_F={risk_adj_scale:.1f}, penalty={penalty:.1f}: {e}\")\n",
    "        # Decide if you want to continue or stop on error - currently continues\n",
    "\n",
    "print(f\"\\n--- Portfolio Generation Complete ---\")\n",
    "print(f\"Generated {len(all_portfolios)} portfolios out of {total_combinations} combinations attempted.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_portfolios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AI Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys # Needed for sys.exit\n",
    "import traceback # Import traceback for detailed error printing\n",
    "import os\n",
    "\n",
    "# --- Check for Parquet dependency ---\n",
    "try:\n",
    "    import pyarrow\n",
    "    # Optional: Check for zstandard explicitly if needed, often comes with pyarrow\n",
    "    # import zstandard\n",
    "except ImportError:\n",
    "    print(\"ERROR: 'pyarrow' library not found. Please install it (e.g., 'pip install pyarrow' or 'pip install pyarrow zstandard') to use Parquet format.\")\n",
    "    sys.exit(\"Exiting due to missing dependency.\")\n",
    "\n",
    "\n",
    "# --- Parameters ---\n",
    "ADJ_CLOSE_COL = 'Adj Close'\n",
    "POTENTIAL_TICKER_COLS = ['Symbol', 'Ticker']\n",
    "POTENTIAL_DATE_COL = 'Date'\n",
    "# --- Modified: Filenames now use .parquet extension ---\n",
    "FINAL_RETURNS_FILENAME = 'portfolio_returns_final.parquet'\n",
    "FINAL_SUMMARY_FILENAME = 'portfolio_factor_performance_final.parquet'\n",
    "# --- Parameter: Control if final results are saved to file ---\n",
    "SAVE_RESULTS_TO_FILE = True\n",
    "\n",
    "print(\"--- Portfolio Performance Calculation Script ---\")\n",
    "\n",
    "# Assume df_OHLCV and all_portfolios are pre-loaded/defined before this script runs\n",
    "# Example placeholders (replace with your actual loading logic):\n",
    "# df_OHLCV = pd.read_csv('your_ohlcv_data.csv', index_col=[0,1], parse_dates=True) # Example\n",
    "# all_portfolios = {'portfolio_1': {'parameters': {...}, 'selected_stocks': df_stocks1}, ...} # Example\n",
    "\n",
    "# =============================================================================\n",
    "# 1. Data and Index Preparation (NO CHANGE HERE)\n",
    "# =============================================================================\n",
    "print(\"\\nChecking and preparing df_OHLCV index structure...\")\n",
    "ticker_level_name = None\n",
    "date_level_name = None\n",
    "try:\n",
    "    # Check if df_OHLCV exists and is a DataFrame before modification\n",
    "    if 'df_OHLCV' not in locals() or not isinstance(df_OHLCV, pd.DataFrame):\n",
    "        print(\"ERROR: df_OHLCV is not defined or not a DataFrame.\")\n",
    "        sys.exit(\"Exiting due to missing data.\")\n",
    "\n",
    "    # Check if df_OHLCV already has a suitable MultiIndex\n",
    "    if isinstance(df_OHLCV.index, pd.MultiIndex) and len(df_OHLCV.index.levels) >= 2:\n",
    "        actual_names = list(df_OHLCV.index.names)\n",
    "        print(f\"Detected MultiIndex with names: {actual_names}\")\n",
    "        ticker_level_name = actual_names[0] if actual_names[0] is not None else 0\n",
    "        date_level_name = actual_names[1] if actual_names[1] is not None else 1\n",
    "        if isinstance(ticker_level_name, int) or isinstance(date_level_name, int):\n",
    "             print(f\"Warning: Using positional index levels ({ticker_level_name}, {date_level_name}) as names were missing.\")\n",
    "    else:\n",
    "        print(\"Index is not a MultiIndex or has too few levels. Checking columns...\")\n",
    "        ticker_col_to_use = None\n",
    "        for col in POTENTIAL_TICKER_COLS:\n",
    "            if col in df_OHLCV.columns:\n",
    "                ticker_col_to_use = col\n",
    "                break\n",
    "        date_col_to_use = POTENTIAL_DATE_COL if POTENTIAL_DATE_COL in df_OHLCV.columns else None\n",
    "\n",
    "        if ticker_col_to_use and date_col_to_use:\n",
    "            required_cols = [ticker_col_to_use, date_col_to_use]\n",
    "            print(f\"Attempting to set index using columns: {required_cols}...\")\n",
    "            df_OHLCV = df_OHLCV.set_index(required_cols)\n",
    "            ticker_level_name = required_cols[0]\n",
    "            date_level_name = required_cols[1]\n",
    "            print(f\"MultiIndex set successfully.\")\n",
    "        else:\n",
    "            missing = []\n",
    "            if not ticker_col_to_use: missing.extend(POTENTIAL_TICKER_COLS)\n",
    "            if not date_col_to_use: missing.append(POTENTIAL_DATE_COL)\n",
    "            print(f\"ERROR: Cannot set index. Required columns for Ticker ({POTENTIAL_TICKER_COLS}) or Date ({POTENTIAL_DATE_COL}) not found.\")\n",
    "            print(\"df_OHLCV columns:\", df_OHLCV.columns)\n",
    "            sys.exit(\"Exiting due to incorrect df_OHLCV structure.\")\n",
    "\n",
    "    print(f\"Using '{ticker_level_name}' for ticker level and '{date_level_name}' for date level.\")\n",
    "\n",
    "    if ADJ_CLOSE_COL not in df_OHLCV.columns:\n",
    "        print(f\"ERROR: Required price column '{ADJ_CLOSE_COL}' not found in df_OHLCV columns: {df_OHLCV.columns}\")\n",
    "        sys.exit(\"Exiting due to missing price column.\")\n",
    "\n",
    "    # Convert date level to datetime if it's not already\n",
    "    date_level_values = df_OHLCV.index.get_level_values(date_level_name)\n",
    "    if not pd.api.types.is_datetime64_any_dtype(date_level_values):\n",
    "        print(f\"Converting date level '{date_level_name}' to datetime objects...\")\n",
    "        try:\n",
    "            # Get the current index levels and codes\n",
    "            levels = list(df_OHLCV.index.levels)\n",
    "            codes = list(df_OHLCV.index.codes)\n",
    "            names = list(df_OHLCV.index.names)\n",
    "            date_level_idx = names.index(date_level_name)\n",
    "\n",
    "            # Convert only the date level\n",
    "            levels[date_level_idx] = pd.to_datetime(levels[date_level_idx])\n",
    "\n",
    "            # Reconstruct the index\n",
    "            df_OHLCV.index = pd.MultiIndex(levels=levels, codes=codes, names=names)\n",
    "            print(\"Date level converted.\")\n",
    "        except Exception as date_convert_e:\n",
    "             print(f\"ERROR: Could not convert date level '{date_level_name}' to datetime: {date_convert_e}\")\n",
    "             print(\"Attempting slower conversion using reset_index/set_index...\")\n",
    "             try:\n",
    "                 original_columns = df_OHLCV.columns.tolist()\n",
    "                 df_OHLCV = df_OHLCV.reset_index()\n",
    "                 df_OHLCV[date_level_name] = pd.to_datetime(df_OHLCV[date_level_name])\n",
    "                 df_OHLCV = df_OHLCV.set_index([ticker_level_name, date_level_name])\n",
    "                 # Ensure original columns are preserved if reset_index added them\n",
    "                 df_OHLCV = df_OHLCV[original_columns]\n",
    "                 print(\"Date level converted using alternative method.\")\n",
    "             except Exception as fallback_e:\n",
    "                 print(f\"ERROR: Fallback date conversion also failed: {fallback_e}\")\n",
    "                 traceback.print_exc()\n",
    "                 sys.exit(\"Exiting due to date conversion error.\")\n",
    "\n",
    "\n",
    "    if not df_OHLCV.index.is_monotonic_increasing:\n",
    "         print(\"Sorting df_OHLCV index...\")\n",
    "         df_OHLCV = df_OHLCV.sort_index()\n",
    "         print(\"Index sorted.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: An unexpected error occurred during index check/setup: {e}\")\n",
    "    traceback.print_exc()\n",
    "    sys.exit(\"Exiting due to index setup error.\")\n",
    "\n",
    "# --- Get all unique tickers from df_OHLCV (NO CHANGE HERE) ---\n",
    "all_known_tickers_set = set()\n",
    "if 'df_OHLCV' in locals() and isinstance(df_OHLCV, pd.DataFrame) and ticker_level_name is not None:\n",
    "    try:\n",
    "        all_known_tickers = df_OHLCV.index.get_level_values(ticker_level_name).unique().tolist()\n",
    "        all_known_tickers_set = set(all_known_tickers)\n",
    "        print(f\"\\nFound {len(all_known_tickers_set):,} unique tickers in df_OHLCV data.\")\n",
    "        if not all_known_tickers_set:\n",
    "             print(\"Warning: No tickers found in df_OHLCV index. Price fetching will likely fail.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Could not extract unique tickers from df_OHLCV index level '{ticker_level_name}': {e}\")\n",
    "        sys.exit(\"Exiting due to error extracting ticker list.\")\n",
    "else:\n",
    "     print(\"Warning: df_OHLCV not available or ticker level name not set during initialization. Cannot pre-check ticker existence.\")\n",
    "\n",
    "\n",
    "# --- Extract Trading Dates (NO CHANGE HERE) ---\n",
    "try:\n",
    "    all_dates_in_data = df_OHLCV.index.get_level_values(date_level_name).unique()\n",
    "    # Ensure conversion to datetime and sorting\n",
    "    trading_dates = pd.DatetimeIndex(pd.to_datetime(all_dates_in_data)).sort_values()\n",
    "    if not trading_dates.empty:\n",
    "         print(f\"Trading dates extracted using level '{date_level_name}'. Found {len(trading_dates)} unique dates (e.g., {trading_dates[0].date()} to {trading_dates[-1].date()}).\")\n",
    "    else:\n",
    "         print(f\"ERROR: No dates found in the index level '{date_level_name}'. Cannot proceed.\")\n",
    "         sys.exit(\"Exiting - Cannot determine trading dates.\")\n",
    "except (KeyError, IndexError) as e:\n",
    "    print(f\"ERROR: Could not find or access level '{date_level_name}' to extract trading dates: {e}\")\n",
    "    sys.exit(\"Exiting - Cannot determine trading dates.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: An unexpected error occurred extracting trading dates: {e}\")\n",
    "    traceback.print_exc()\n",
    "    sys.exit(\"Exiting - Cannot determine trading dates.\")\n",
    "\n",
    "# =============================================================================\n",
    "# 2. Helper Function for Trading Dates (NO CHANGE HERE)\n",
    "# =============================================================================\n",
    "def get_next_trading_date(current_date, sorted_trading_dates):\n",
    "    \"\"\"Finds the next trading date in the sorted list strictly after current_date.\"\"\"\n",
    "    if not isinstance(sorted_trading_dates, pd.DatetimeIndex) or not sorted_trading_dates.is_monotonic_increasing:\n",
    "        try:\n",
    "            # Ensure it's a sorted DatetimeIndex\n",
    "            sorted_trading_dates = pd.DatetimeIndex(pd.to_datetime(sorted_trading_dates)).sort_values()\n",
    "        except Exception:\n",
    "             print(\"Error: Could not convert input to sorted DatetimeIndex in get_next_trading_date.\")\n",
    "             return None # Indicate error\n",
    "    if sorted_trading_dates.empty:\n",
    "        # print(\"Warning: Empty trading dates list provided to get_next_trading_date.\")\n",
    "        return None\n",
    "\n",
    "    current_date = pd.Timestamp(current_date)\n",
    "    try:\n",
    "        # Use searchsorted to find the insertion point for the current_date\n",
    "        # 'right' ensures we find the index *after* any occurrences of current_date\n",
    "        loc = sorted_trading_dates.searchsorted(current_date, side='right')\n",
    "\n",
    "        # If loc is within the bounds of the array, return that date\n",
    "        if loc < len(sorted_trading_dates):\n",
    "            return sorted_trading_dates[loc]\n",
    "        else:\n",
    "            # If loc is beyond the end, there's no next trading date\n",
    "            # print(f\"Info: No trading date found after {current_date.date()} in the provided list.\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error during searchsorted in get_next_trading_date: {e}\")\n",
    "        return None # Indicate error\n",
    "\n",
    "# =============================================================================\n",
    "# 3. Portfolio Return Calculation Loop (NO CHANGE HERE)\n",
    "# =============================================================================\n",
    "# This list will hold results JUST from the CURRENT run\n",
    "portfolio_returns_data_current_run = []\n",
    "print(\"\\nCalculating portfolio returns for the current run...\")\n",
    "index_mismatch_warning_shown = False\n",
    "\n",
    "if 'all_portfolios' not in locals() or not isinstance(all_portfolios, dict):\n",
    "    print(\"ERROR: 'all_portfolios' dictionary is not defined.\")\n",
    "    all_portfolios = {} # Define as empty to avoid errors below, though likely useless\n",
    "\n",
    "if not all_portfolios:\n",
    "    print(\"Warning: 'all_portfolios' dictionary is empty. No new returns to calculate.\")\n",
    "else:\n",
    "    processed_count = 0\n",
    "    skipped_count = 0\n",
    "    error_count = 0\n",
    "    for portf_name, portf_data in list(all_portfolios.items()): # Use list() for safe iteration if modifying dict\n",
    "        try:\n",
    "            # --- 3.1 Extract Parameters and Holdings ---\n",
    "            params = portf_data.get('parameters')\n",
    "            selected_stocks_df_orig = portf_data.get('selected_stocks') # DataFrame with Ticker/Symbol as index, 'Weight' column\n",
    "\n",
    "            # Basic validation\n",
    "            if not isinstance(params, dict) or not isinstance(selected_stocks_df_orig, pd.DataFrame) or 'Weight' not in selected_stocks_df_orig.columns:\n",
    "                print(f\"Warning: Skipping {portf_name} - Invalid structure or missing 'Weight' column in selected_stocks.\")\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "\n",
    "            # Make a copy to avoid modifying the original dict entry directly\n",
    "            selected_stocks_df = selected_stocks_df_orig.copy()\n",
    "\n",
    "            # Ensure index has a name before comparison\n",
    "            if selected_stocks_df.index.name is None:\n",
    "                # potential_index_names = [name for name in POTENTIAL_TICKER_COLS if name in selected_stocks_df.columns or name.lower() in selected_stocks_df.index.name.lower() if selected_stocks_df.index.name else False]\n",
    "                potential_index_names = [name for name in POTENTIAL_TICKER_COLS if\n",
    "                         (name in selected_stocks_df.columns) or\n",
    "                         (selected_stocks_df.index.name is not None and isinstance(selected_stocks_df.index.name, str) and name.lower() in selected_stocks_df.index.name.lower())]\n",
    "                if potential_index_names:\n",
    "                    selected_stocks_df.index.name = potential_index_names[0] # Use the first match\n",
    "                else:\n",
    "                    # Try setting from columns if Symbol/Ticker is a column\n",
    "                    found_in_col = False\n",
    "                    for col in POTENTIAL_TICKER_COLS:\n",
    "                        if col in selected_stocks_df.columns:\n",
    "                            try:\n",
    "                                selected_stocks_df = selected_stocks_df.set_index(col)\n",
    "                                found_in_col = True\n",
    "                                break\n",
    "                            except Exception as idx_err:\n",
    "                                print(f\"Warning: Could not set index from column {col} for {portf_name}: {idx_err}\")\n",
    "                    if not found_in_col:\n",
    "                         print(f\"Warning: Skipping {portf_name} - Could not determine Ticker index for selected_stocks.\")\n",
    "                         skipped_count += 1\n",
    "                         continue\n",
    "\n",
    "\n",
    "            portf_date_str = params.get('date')\n",
    "            raw_factor = params.get('raw_score_scale_factor')\n",
    "            risk_adj_factor = params.get('risk_adj_score_scale_factor')\n",
    "            penalty_factor = params.get('penalty_IntraCluster_Corr')\n",
    "\n",
    "            if portf_date_str is None or raw_factor is None or risk_adj_factor is None or penalty_factor is None:\n",
    "                print(f\"Warning: Skipping {portf_name} - Missing essential parameter (date, raw_factor, risk_adj_factor, or penalty_factor).\")\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                portf_date = pd.Timestamp(portf_date_str)\n",
    "            except ValueError:\n",
    "                print(f\"Warning: Skipping {portf_name} - Invalid date format for parameter 'date': {portf_date_str}\")\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "\n",
    "\n",
    "            if selected_stocks_df.empty:\n",
    "                # print(f\"Info: Skipping {portf_name} - Portfolio has no selected stocks.\")\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "\n",
    "            # --- 3.2 Ensure Portfolio Index Matches Data Index Name ---\n",
    "            if selected_stocks_df.index.name != ticker_level_name:\n",
    "                if not index_mismatch_warning_shown:\n",
    "                    print(f\"Warning: Portfolio index name ('{selected_stocks_df.index.name}') mismatches data index ('{ticker_level_name}'). Renaming portfolio index for alignment.\")\n",
    "                    index_mismatch_warning_shown = True\n",
    "                try:\n",
    "                    selected_stocks_df.index.name = ticker_level_name\n",
    "                except Exception as rename_e:\n",
    "                    print(f\"Error: Failed to rename portfolio index for {portf_name}: {rename_e}. Skipping.\")\n",
    "                    skipped_count += 1\n",
    "                    continue\n",
    "\n",
    "            original_portfolio_tickers = selected_stocks_df.index.tolist()\n",
    "            if not original_portfolio_tickers: # Double check after potential renaming/copying\n",
    "                 # print(f\"Info: Skipping {portf_name} - Portfolio has no tickers after index handling.\")\n",
    "                 skipped_count += 1\n",
    "                 continue\n",
    "\n",
    "            # --- Check: Verify tickers exist in df_OHLCV's known tickers ---\n",
    "            tickers_in_portfolio_set = set(original_portfolio_tickers)\n",
    "            missing_from_ohlcv = tickers_in_portfolio_set - all_known_tickers_set # Find tickers in portfolio but not in OHLCV data\n",
    "\n",
    "            if missing_from_ohlcv:\n",
    "                # print(f\"Warning for {portf_name}: The following tickers selected for the portfolio are completely missing from the historical price data (df_OHLCV) and will be dropped from calculation: {sorted(list(missing_from_ohlcv))}\")\n",
    "                # Filter the list of tickers to proceed with\n",
    "                tickers_to_process = [t for t in original_portfolio_tickers if t in all_known_tickers_set]\n",
    "\n",
    "                if not tickers_to_process:\n",
    "                    # print(f\"Info: Skipping {portf_name} - No tickers remaining after removing those completely missing from price data.\")\n",
    "                    skipped_count += 1\n",
    "                    continue # Skip to the next portfolio\n",
    "\n",
    "                # IMPORTANT: Filter the selected_stocks_df as well to keep weights consistent\n",
    "                selected_stocks_df = selected_stocks_df.loc[tickers_to_process]\n",
    "                original_portfolio_tickers = tickers_to_process # Update the list for downstream use\n",
    "            # --- End of Check ---\n",
    "\n",
    "            # --- 3.3 Determine Buy and Sell Dates ---\n",
    "            buy_date = get_next_trading_date(portf_date, trading_dates)\n",
    "            if buy_date is None:\n",
    "                # print(f\"Info: Skipping {portf_name} - Cannot find trading date after portfolio generation date {portf_date.date()}.\")\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "            sell_date = get_next_trading_date(buy_date, trading_dates)\n",
    "            if sell_date is None:\n",
    "                # print(f\"Info: Skipping {portf_name} - Cannot find trading date after buy date {buy_date.date()}.\")\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "            required_dates = [buy_date, sell_date]\n",
    "\n",
    "            # --- 3.4 Fetch Price Data & Filter Available Tickers ---\n",
    "            # Use the potentially filtered 'original_portfolio_tickers'\n",
    "            idx = pd.IndexSlice\n",
    "            try:\n",
    "                 # Use reindex for robustness against missing ticker/date combinations\n",
    "                 multi_idx = pd.MultiIndex.from_product([original_portfolio_tickers, required_dates], names=[ticker_level_name, date_level_name])\n",
    "                 potential_data = df_OHLCV.reindex(multi_idx) # This will have NaNs where data is missing\n",
    "\n",
    "            except Exception as slice_error:\n",
    "                 print(f\"Error: Unexpected error during data slicing/reindexing for {portf_name}: {slice_error}. Skipping.\")\n",
    "                 skipped_count+=1\n",
    "                 continue\n",
    "\n",
    "\n",
    "            # Identify tickers that have *valid* data for BOTH required dates\n",
    "            # Group by ticker and check count of non-NA Adj Close prices for the required dates\n",
    "            prices_for_check = potential_data[[ADJ_CLOSE_COL]].dropna() # Drop rows with NaN Adj Close\n",
    "            if prices_for_check.empty:\n",
    "                # print(f\"Info: Skipping {portf_name} - No valid price data found for any selected ticker on required dates {buy_date.date()}, {sell_date.date()}.\")\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "\n",
    "            # Count how many of the required dates each ticker has valid data for\n",
    "            ticker_date_counts = prices_for_check.index.get_level_values(ticker_level_name).value_counts()\n",
    "\n",
    "            # Keep only tickers that have data for *all* required dates (i.e., count == 2)\n",
    "            available_tickers = ticker_date_counts[ticker_date_counts == len(required_dates)].index.tolist()\n",
    "\n",
    "            if not available_tickers:\n",
    "                # print(f\"Info: Skipping {portf_name} - No stocks found with non-NaN '{ADJ_CLOSE_COL}' for BOTH Buy({buy_date.date()}) and Sell({sell_date.date()}) dates.\")\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "\n",
    "            # --- 3.5 Process Prices for Available Tickers ---\n",
    "            try:\n",
    "                # Select 'Adj Close' for the available tickers and dates from potential_data\n",
    "                idx_slice = pd.IndexSlice[available_tickers, required_dates]\n",
    "                # Use .loc on potential_data which is already filtered by dates/tickers\n",
    "                prices = potential_data.loc[idx_slice, ADJ_CLOSE_COL]\n",
    "\n",
    "                prices_unstacked = prices.unstack(level=date_level_name)\n",
    "                prices_unstacked = prices_unstacked.rename(columns={buy_date: 'Buy Price', sell_date: 'Sell Price'})\n",
    "\n",
    "                # Final check for NaNs that might have crept in (though reindex/dropna should handle most)\n",
    "                prices_unstacked = prices_unstacked.dropna()\n",
    "\n",
    "                if prices_unstacked.empty:\n",
    "                    # print(f\"Info: Skipping {portf_name} - Price data became empty after unstacking/final dropna.\")\n",
    "                    skipped_count += 1\n",
    "                    continue\n",
    "\n",
    "                final_available_tickers = prices_unstacked.index.tolist()\n",
    "\n",
    "            except KeyError as e:\n",
    "                 print(f\"Error: KeyError during price processing for {portf_name}: {e}. Index: {idx_slice}. Check slicing or column '{ADJ_CLOSE_COL}'. Skipping.\")\n",
    "                 skipped_count += 1\n",
    "                 continue\n",
    "            except Exception as e:\n",
    "                 print(f\"Error: Unexpected error during price processing for {portf_name}: {e}. Skipping.\")\n",
    "                 traceback.print_exc()\n",
    "                 skipped_count += 1\n",
    "                 continue\n",
    "\n",
    "            # --- 3.6 Calculate Portfolio Return ---\n",
    "            # Align weights using the 'selected_stocks_df' (already filtered for OHLCV presence)\n",
    "            # Index it with the tickers that *actually have price data* for the calculation period.\n",
    "            aligned_weights = selected_stocks_df.loc[final_available_tickers, 'Weight']\n",
    "\n",
    "            # ---> Weight Handling (Normalization) <---\n",
    "            weight_sum = aligned_weights.sum()\n",
    "            if abs(weight_sum) < 1e-9: # Use absolute value for check\n",
    "                 # print(f\"Info: Skipping {portf_name} - Sum of weights for available stocks is zero.\")\n",
    "                 skipped_count += 1\n",
    "                 continue\n",
    "            normalized_weights = aligned_weights / weight_sum\n",
    "            # ---> End of Weight Handling <---\n",
    "\n",
    "            buy_prices = prices_unstacked.loc[final_available_tickers, 'Buy Price']\n",
    "            sell_prices = prices_unstacked.loc[final_available_tickers, 'Sell Price']\n",
    "\n",
    "            # --- Handle potential zero buy prices ---\n",
    "            zero_price_mask = (buy_prices.abs() < 1e-9)\n",
    "            if zero_price_mask.any():\n",
    "                num_zero_prices = zero_price_mask.sum()\n",
    "                # print(f\"Warning for {portf_name}: Found {num_zero_prices} stock(s) with near-zero buy price. They will be excluded from return calculation.\")\n",
    "                valid_price_mask = ~zero_price_mask\n",
    "                if not valid_price_mask.any(): # Check if *any* are True\n",
    "                    # print(f\"Info: Skipping {portf_name} - All remaining stocks had zero buy price.\")\n",
    "                    skipped_count += 1\n",
    "                    continue\n",
    "\n",
    "                # Filter prices and weights\n",
    "                buy_prices = buy_prices.loc[valid_price_mask]\n",
    "                sell_prices = sell_prices.loc[valid_price_mask]\n",
    "                normalized_weights = normalized_weights.loc[valid_price_mask] # Align weights again\n",
    "                final_available_tickers = buy_prices.index.tolist() # Update ticker list\n",
    "\n",
    "                # Re-normalize weights after removing zero-price stocks\n",
    "                weight_sum = normalized_weights.sum()\n",
    "                if abs(weight_sum) < 1e-9:\n",
    "                    # print(f\"Info: Skipping {portf_name} - Sum of weights became zero after removing zero-price stocks.\")\n",
    "                    skipped_count += 1\n",
    "                    continue\n",
    "                normalized_weights = normalized_weights / weight_sum # Renormalize again\n",
    "\n",
    "            # --- Final check for NaN/Inf before calculation ---\n",
    "            if buy_prices.isnull().any() or sell_prices.isnull().any() or normalized_weights.isnull().any() or \\\n",
    "               np.isinf(buy_prices).any() or np.isinf(sell_prices).any() or np.isinf(normalized_weights).any():\n",
    "                print(f\"Warning: Skipping {portf_name} - Found NaN or Inf in final arrays (prices or weights) before calculation.\")\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "\n",
    "            # --- Calculate weighted return ---\n",
    "            individual_returns = (sell_prices / buy_prices) - 1 # Division handles potential remaining zeros gracefully -> inf\n",
    "            individual_returns = individual_returns.replace([np.inf, -np.inf], np.nan).fillna(0) # Replace inf/nan from division with 0 return\n",
    "\n",
    "            portfolio_return = (individual_returns * normalized_weights).sum()\n",
    "\n",
    "             # --- Final check on the calculated return ---\n",
    "            if pd.isna(portfolio_return) or np.isinf(portfolio_return):\n",
    "                print(f\"Warning: Skipping {portf_name} - Final calculated portfolio return is NaN or Inf.\")\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "\n",
    "            # --- 3.7 Store Results for Current Run ---\n",
    "            portfolio_returns_data_current_run.append({\n",
    "                'portfolio_name': portf_name,\n",
    "                'generation_date': portf_date,\n",
    "                'buy_date': buy_date,\n",
    "                'sell_date': sell_date,\n",
    "                'raw_factor': raw_factor,\n",
    "                'risk_adj_factor': risk_adj_factor,\n",
    "                'penalty_factor': penalty_factor,\n",
    "                'portfolio_return': portfolio_return,\n",
    "                # num_stocks_selected: Count from original selection (before any filtering)\n",
    "                'num_stocks_selected': len(tickers_in_portfolio_set),\n",
    "                # num_stocks_in_ohlcv: Count after removing those completely missing from OHLCV\n",
    "                'num_stocks_in_ohlcv': len(selected_stocks_df.index), # Use the df potentially filtered for OHLCV presence\n",
    "                # num_stocks_calc: Count actually used in calculation (passed all filters: OHLCV, dates, zero price)\n",
    "                'num_stocks_calc': len(final_available_tickers)\n",
    "            })\n",
    "            processed_count += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"FATAL Error processing portfolio {portf_name}: {e}\")\n",
    "            traceback.print_exc()\n",
    "            error_count += 1\n",
    "            # Decide whether to continue or stop the loop\n",
    "            # continue # Continue to next portfolio even on fatal error for one\n",
    "\n",
    "    # --- Loop Finished ---\n",
    "    print(f\"\\nCurrent run portfolio return calculation finished.\")\n",
    "    print(f\"Successfully processed: {processed_count}\")\n",
    "    print(f\"Skipped (data issues/filters): {skipped_count}\")\n",
    "    print(f\"Errors during processing: {error_count}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 4. Load Historical Data (if exists), Combine, Clean, and Analyze\n",
    "# =============================================================================\n",
    "print(\"\\n--- Loading Historical Data, Combining with Current Run, and Cleaning ---\")\n",
    "\n",
    "# Initialize DataFrames to hold all data (historical + current) and the summary\n",
    "all_returns_df = pd.DataFrame()\n",
    "performance_summary_df = pd.DataFrame() # Initialize summary DataFrame\n",
    "\n",
    "# --- Try to load existing historical data ---\n",
    "historical_returns_df = pd.DataFrame() # Initialize empty df for historical data\n",
    "if os.path.exists(FINAL_RETURNS_FILENAME):\n",
    "    try:\n",
    "        print(f\"Loading historical data from {FINAL_RETURNS_FILENAME}...\")\n",
    "        # --- *** MODIFIED: Use read_parquet *** ---\n",
    "        historical_returns_df = pd.read_parquet(FINAL_RETURNS_FILENAME, engine='pyarrow')\n",
    "        # --- *** END MODIFICATION *** ---\n",
    "\n",
    "        # Basic check for essential columns (can be expanded)\n",
    "        # Parquet usually preserves types well, including dates, but checks are still good\n",
    "        expected_cols = ['generation_date', 'buy_date', 'sell_date', 'raw_factor', 'risk_adj_factor', 'penalty_factor', 'portfolio_return']\n",
    "        if all(col in historical_returns_df.columns for col in expected_cols):\n",
    "            print(f\"Loaded {len(historical_returns_df)} historical records.\")\n",
    "            # Optional: Verify date types if needed, though pyarrow usually handles this\n",
    "            # for col in ['generation_date', 'buy_date', 'sell_date']:\n",
    "            #     if not pd.api.types.is_datetime64_any_dtype(historical_returns_df[col]):\n",
    "            #         print(f\"Warning: Column {col} in loaded Parquet file is not datetime. Attempting conversion.\")\n",
    "            #         historical_returns_df[col] = pd.to_datetime(historical_returns_df[col], errors='coerce')\n",
    "\n",
    "        else:\n",
    "            print(f\"Warning: Historical data file {FINAL_RETURNS_FILENAME} is missing expected columns. Ignoring historical data.\")\n",
    "            historical_returns_df = pd.DataFrame() # Reset to empty if format is wrong\n",
    "            # Optional: Rename/backup the bad file\n",
    "            # try:\n",
    "            #     os.rename(FINAL_RETURNS_FILENAME, FINAL_RETURNS_FILENAME + f\".bad_format_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}\")\n",
    "            # except OSError as ren_err:\n",
    "            #     print(f\"Could not rename bad format file: {ren_err}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "         # This case is handled by os.path.exists, but included for completeness\n",
    "         print(f\"Historical data file {FINAL_RETURNS_FILENAME} not found.\")\n",
    "         historical_returns_df = pd.DataFrame()\n",
    "    # except pyarrow.lib.ArrowIOError as arrow_io_err: # More specific error for Parquet I/O\n",
    "    #      print(f\"Error reading Parquet file {FINAL_RETURNS_FILENAME}: {arrow_io_err}\")\n",
    "    #      print(\"Warning: File might be corrupted or not a valid Parquet file. Proceeding without historical data.\")\n",
    "    #      historical_returns_df = pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading historical data from {FINAL_RETURNS_FILENAME}: {e}\")\n",
    "        traceback.print_exc()\n",
    "        print(\"Warning: Proceeding without historical data.\")\n",
    "        historical_returns_df = pd.DataFrame()\n",
    "else:\n",
    "    print(f\"Historical data file {FINAL_RETURNS_FILENAME} not found. Starting fresh.\")\n",
    "\n",
    "# --- Combine with current run's data (NO CHANGE HERE) ---\n",
    "current_returns_df = pd.DataFrame() # Initialize empty df for current run\n",
    "if portfolio_returns_data_current_run:\n",
    "    current_returns_df = pd.DataFrame(portfolio_returns_data_current_run)\n",
    "    print(f\"Created DataFrame with {len(current_returns_df)} new records from this run.\")\n",
    "\n",
    "    # Ensure date columns are datetime objects in the new data (should be already, but good practice)\n",
    "    for col in ['generation_date', 'buy_date', 'sell_date']:\n",
    "         if col in current_returns_df.columns:\n",
    "              current_returns_df[col] = pd.to_datetime(current_returns_df[col])\n",
    "else:\n",
    "    print(\"No new portfolio returns were calculated in this run.\")\n",
    "\n",
    "\n",
    "# --- Concatenate historical and current data (NO CHANGE HERE) ---\n",
    "if not historical_returns_df.empty or not current_returns_df.empty:\n",
    "    all_returns_df = pd.concat([historical_returns_df, current_returns_df], ignore_index=True)\n",
    "    print(f\"Combined data contains {len(all_returns_df)} records (before deduplication).\")\n",
    "\n",
    "    # --- Data Cleaning: Remove Duplicates (NO CHANGE HERE) ---\n",
    "    key_cols = ['generation_date', 'buy_date', 'sell_date', 'raw_factor', 'risk_adj_factor', 'penalty_factor']\n",
    "    if all(col in all_returns_df.columns for col in key_cols):\n",
    "        initial_len = len(all_returns_df)\n",
    "        all_returns_df = all_returns_df.drop_duplicates(subset=key_cols, keep='last')\n",
    "        duplicates_removed = initial_len - len(all_returns_df)\n",
    "        if duplicates_removed > 0:\n",
    "            print(f\"Removed {duplicates_removed} duplicate records based on key columns: {key_cols}.\")\n",
    "        else:\n",
    "            print(\"No duplicate records found based on key columns.\")\n",
    "    else:\n",
    "        print(f\"Warning: Cannot perform duplicate check. Not all key columns {key_cols} found in combined data.\")\n",
    "        print(f\"Columns present: {all_returns_df.columns.tolist()}\")\n",
    "\n",
    "else:\n",
    "    print(\"No historical or current return data available to process.\")\n",
    "\n",
    "\n",
    "# --- Perform analysis ONLY if there's combined data (NO CHANGE HERE) ---\n",
    "if not all_returns_df.empty:\n",
    "    print(\"\\n--- Performing Analysis on Combined and Cleaned Data ---\")\n",
    "    # Ensure correct data types before grouping (especially factors and return)\n",
    "    try:\n",
    "        numeric_cols = ['raw_factor', 'risk_adj_factor', 'penalty_factor', 'portfolio_return']\n",
    "        for col in numeric_cols:\n",
    "            if col in all_returns_df.columns:\n",
    "                all_returns_df[col] = pd.to_numeric(all_returns_df[col], errors='coerce') # Coerce errors to NaN\n",
    "\n",
    "        # Optional: Check for NaNs introduced by coercion\n",
    "        nas_after_conversion = all_returns_df[numeric_cols].isnull().sum()\n",
    "        if nas_after_conversion.sum() > 0:\n",
    "            print(\"Warning: Found NaNs after converting numeric columns:\")\n",
    "            print(nas_after_conversion[nas_after_conversion > 0])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting factor/return columns to numeric: {e}. Analysis might be affected.\")\n",
    "\n",
    "\n",
    "    # Sort for easier viewing and consistent grouping (optional, but good practice)\n",
    "    grouping_factors = ['raw_factor', 'risk_adj_factor', 'penalty_factor']\n",
    "    # Ensure grouping factors exist before sorting/grouping\n",
    "    valid_grouping_factors = [f for f in grouping_factors if f in all_returns_df.columns]\n",
    "    if len(valid_grouping_factors) != len(grouping_factors):\n",
    "        print(f\"Warning: Not all grouping factors {grouping_factors} exist in the data. Using: {valid_grouping_factors}\")\n",
    "    grouping_factors = valid_grouping_factors # Use only valid factors\n",
    "\n",
    "    if not grouping_factors:\n",
    "        print(\"Error: No valid grouping factors available for analysis. Skipping summary.\")\n",
    "    else:\n",
    "        try:\n",
    "            # Sort before grouping\n",
    "            all_returns_df = all_returns_df.sort_values(by=['generation_date'] + grouping_factors)\n",
    "\n",
    "            print(f\"\\nTotal unique return instances after cleaning: {len(all_returns_df)}\")\n",
    "            # print(\"Sample of cleaned combined data:\") # Optional\n",
    "            # print(all_returns_df.head())\n",
    "\n",
    "            # --- Group by factor combinations to analyze performance ---\n",
    "            print(\"\\n--- Performance Analysis by Factor Combination (Based on Cleaned Data) ---\")\n",
    "\n",
    "            # Define aggregation functions\n",
    "            def count_valid(x):\n",
    "                return x.dropna().count()\n",
    "            def std_dev(x):\n",
    "                 valid_count = x.dropna().count()\n",
    "                 if valid_count >= 2:\n",
    "                      return x.std(ddof=1) # Use sample std dev (ddof=1)\n",
    "                 else:\n",
    "                      return np.nan\n",
    "            def sharpe_ratio(x, risk_free_rate=0, periods_per_year=252):\n",
    "                 valid_count = x.dropna().count()\n",
    "                 if valid_count < 2:\n",
    "                      return np.nan\n",
    "                 mean_return = x.mean()\n",
    "                 std_return = x.std(ddof=1)\n",
    "                 if std_return is None or pd.isna(std_return) or abs(std_return) < 1e-9:\n",
    "                      return np.nan\n",
    "                 sharpe = (mean_return - risk_free_rate) / std_return\n",
    "                 return sharpe * np.sqrt(periods_per_year)\n",
    "\n",
    "            # Group by all specified factors and aggregate portfolio returns\n",
    "            performance_summary_df = all_returns_df.groupby(grouping_factors)['portfolio_return'].agg(\n",
    "                mean='mean',\n",
    "                median='median',\n",
    "                std=std_dev,\n",
    "                count=count_valid,\n",
    "                sharpe=sharpe_ratio # Use the defined function\n",
    "            ).reset_index() # Reset index to make factors columns again\n",
    "\n",
    "            # --- Calculate additional metrics ---\n",
    "            if 'portfolio_return' in all_returns_df.columns:\n",
    "                 win_rate = all_returns_df[all_returns_df['portfolio_return'] > 0].groupby(grouping_factors).size() / \\\n",
    "                            all_returns_df.groupby(grouping_factors).size()\n",
    "                 performance_summary_df = pd.merge(performance_summary_df, win_rate.rename('win_rate').reset_index(), on=grouping_factors, how='left')\n",
    "                 performance_summary_df['win_rate'] = performance_summary_df['win_rate'].fillna(0) # Fill NaN if no trades for a combo\n",
    "\n",
    "            # Sort by a chosen metric (e.g., Sharpe ratio, mean return)\n",
    "            sort_metric = 'sharpe' if 'sharpe' in performance_summary_df.columns else 'mean'\n",
    "            if sort_metric in performance_summary_df.columns:\n",
    "                performance_summary_df = performance_summary_df.sort_values(by=sort_metric, ascending=False, na_position='last')\n",
    "                print(f\"\\nPerformance per Factor Combination (sorted by {sort_metric}):\")\n",
    "            else:\n",
    "                 print(f\"\\nPerformance per Factor Combination (unsorted):\")\n",
    "\n",
    "            with pd.option_context('display.float_format', '{:.4f}'.format, 'display.max_rows', 200):\n",
    "                print(performance_summary_df)\n",
    "\n",
    "            # --- Analyze the number of stocks used ---\n",
    "            stock_count_cols = ['num_stocks_selected', 'num_stocks_in_ohlcv', 'num_stocks_calc']\n",
    "            valid_stock_cols = [c for c in stock_count_cols if c in all_returns_df.columns]\n",
    "            if valid_stock_cols:\n",
    "                 stock_count_summary = all_returns_df.groupby(grouping_factors)[valid_stock_cols].agg(['mean', 'median', 'min', 'max'])\n",
    "                 print(\"\\nAverage/Median/Min/Max Stocks per Factor Combination (Cleaned Data):\")\n",
    "                 with pd.option_context('display.float_format', '{:.1f}'.format):\n",
    "                      print(stock_count_summary)\n",
    "\n",
    "        except Exception as analysis_err:\n",
    "            print(f\"\\nERROR during performance analysis: {analysis_err}\")\n",
    "            traceback.print_exc()\n",
    "            # Ensure performance_summary_df is empty DataFrame if analysis fails\n",
    "            performance_summary_df = pd.DataFrame()\n",
    "\n",
    "else:\n",
    "    print(\"\\nNo portfolio returns data available (neither historical nor current run) to analyze after cleaning.\")\n",
    "    # Ensure performance_summary_df is defined as empty\n",
    "    performance_summary_df = pd.DataFrame()\n",
    "\n",
    "# =============================================================================\n",
    "# 5. Optionally Save Final Results (Overwrite Mode)\n",
    "# =============================================================================\n",
    "if SAVE_RESULTS_TO_FILE:\n",
    "    print(\"\\n--- Saving Final Results to Parquet ---\")\n",
    "    # --- Save the combined, cleaned returns data (OVERWRITE) ---\n",
    "    if not all_returns_df.empty:\n",
    "        try:\n",
    "            # --- *** MODIFIED: Use to_parquet *** ---\n",
    "            all_returns_df.to_parquet(\n",
    "                FINAL_RETURNS_FILENAME,\n",
    "                engine='pyarrow',\n",
    "                compression='zstd', # Use zstd compression\n",
    "                index=False         # Do not write the DataFrame index\n",
    "            )\n",
    "            # --- *** END MODIFICATION *** ---\n",
    "            print(f\"Saved combined and cleaned returns ({len(all_returns_df)} records) to {FINAL_RETURNS_FILENAME}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving final returns data to {FINAL_RETURNS_FILENAME}: {e}\")\n",
    "            traceback.print_exc() # Print detailed error\n",
    "    else:\n",
    "        print(f\"No combined returns data to save to {FINAL_RETURNS_FILENAME}.\")\n",
    "        # Optional: remove existing file if no data to save\n",
    "        # if os.path.exists(FINAL_RETURNS_FILENAME):\n",
    "        #     try:\n",
    "        #         os.remove(FINAL_RETURNS_FILENAME)\n",
    "        #         print(f\"Removed existing empty file: {FINAL_RETURNS_FILENAME}\")\n",
    "        #     except OSError as rem_err:\n",
    "        #          print(f\"Could not remove existing file {FINAL_RETURNS_FILENAME}: {rem_err}\")\n",
    "\n",
    "\n",
    "    # --- Save the performance summary (OVERWRITE) ---\n",
    "    if not performance_summary_df.empty:\n",
    "        try:\n",
    "            # --- *** MODIFIED: Use to_parquet *** ---\n",
    "            performance_summary_df.to_parquet(\n",
    "                FINAL_SUMMARY_FILENAME,\n",
    "                engine='pyarrow',\n",
    "                compression='zstd', # Use zstd compression\n",
    "                index=False         # Do not write the DataFrame index\n",
    "            )\n",
    "            # --- *** END MODIFICATION *** ---\n",
    "            print(f\"Saved performance summary ({len(performance_summary_df)} factor combinations) to {FINAL_SUMMARY_FILENAME}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving performance summary to {FINAL_SUMMARY_FILENAME}: {e}\")\n",
    "            traceback.print_exc() # Print detailed error\n",
    "    else:\n",
    "        print(f\"No performance summary data to save to {FINAL_SUMMARY_FILENAME}.\")\n",
    "        # Optional: remove existing file if no data to save\n",
    "        # if os.path.exists(FINAL_SUMMARY_FILENAME):\n",
    "        #      try:\n",
    "        #         os.remove(FINAL_SUMMARY_FILENAME)\n",
    "        #         print(f\"Removed existing empty file: {FINAL_SUMMARY_FILENAME}\")\n",
    "        #      except OSError as rem_err:\n",
    "        #          print(f\"Could not remove existing file {FINAL_SUMMARY_FILENAME}: {rem_err}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n--- Skipping Saving Results to File (SAVE_RESULTS_TO_FILE=False) ---\")\n",
    "    print(f\"Returns DataFrame ('all_returns_df') contains {len(all_returns_df)} rows.\")\n",
    "    print(f\"Summary DataFrame ('performance_summary_df') contains {len(performance_summary_df)} rows.\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Script Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_returns_df = pd.read_parquet('portfolio_returns_final.parquet', engine='pyarrow')\n",
    "portfolio_returns_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_factor_performance_final = pd.read_parquet('portfolio_factor_performance_final.parquet', engine='pyarrow')\n",
    "portfolio_factor_performance_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_performance = portfolio_factor_performance_final.sort_values(by='sharpe', ascending=False)\n",
    "display(sorted_performance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # =============================================================================\n",
    "# # 4b. Time-Based Performance Analysis (Example: By Year)\n",
    "# # =============================================================================\n",
    "# if not all_returns_df.empty:\n",
    "#     print(\"\\n--- Performance Analysis by Factor Combination AND Year ---\")\n",
    "\n",
    "#     # Ensure 'sell_date' is datetime and extract the year\n",
    "#     try:\n",
    "#         all_returns_df['sell_year'] = pd.to_datetime(all_returns_df['sell_date']).dt.year\n",
    "        \n",
    "#         # Define grouping factors including the year\n",
    "#         time_grouping_factors = ['sell_year'] + grouping_factors # grouping_factors was ['raw_factor', 'risk_adj_factor', 'penalty_factor']\n",
    "\n",
    "#         # Group by year and factors\n",
    "#         performance_by_year = all_returns_df.groupby(time_grouping_factors)['portfolio_return'].agg(\n",
    "#             mean='mean',\n",
    "#             median='median',\n",
    "#             std=lambda x: x.std(ddof=0) if pd.notna(x).sum() > 1 else np.nan,\n",
    "#             count=lambda x: pd.notna(x).sum()\n",
    "#             # Add other metrics like Sharpe, win rate per year if desired\n",
    "#         ).reset_index()\n",
    "\n",
    "#         # Sort for better readability (e.g., by year, then by mean return)\n",
    "#         performance_by_year = performance_by_year.sort_values(by=['sell_year', 'mean'], ascending=[True, False])\n",
    "\n",
    "#         print(\"\\nPerformance per Factor Combination per Year (sorted by year, then mean return):\")\n",
    "#         with pd.option_context('display.float_format', '{:.4f}'.format, 'display.max_rows', 500): # Show more rows\n",
    "#             print(performance_by_year)\n",
    "\n",
    "#         # You could save this to a separate CSV if needed\n",
    "#         # performance_by_year.to_csv('portfolio_factor_performance_by_year.csv', index=False)\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error during yearly performance analysis: {e}\")\n",
    "#         traceback.print_exc()\n",
    "# else:\n",
    "    # print(\"\\nNo data available for yearly performance analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
