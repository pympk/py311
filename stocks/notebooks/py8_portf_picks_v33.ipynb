{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Notebook cell\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Get root directory (assuming notebook is in root/notebooks/)\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "ROOT_DIR = NOTEBOOK_DIR.parent if NOTEBOOK_DIR.name == 'notebooks' else NOTEBOOK_DIR\n",
    "\n",
    "# Add src directory to Python path\n",
    "sys.path.append(str(ROOT_DIR / 'src'))\n",
    "\n",
    "# Verify path\n",
    "print(f\"Python will look in these locations:\\n{sys.path}\")\n",
    "\n",
    "\n",
    "# --- Execute the processor ---\n",
    "import utils\n",
    "from config import date_str, DOWNLOAD_DIR, DEST_DIR\n",
    "\n",
    "path_data = f'..\\data\\{date_str}_df_finviz_merged.parquet'\n",
    "path_corr = f'..\\data\\{date_str}_df_corr_emv_matrix.parquet'\n",
    "path_cov = f'..\\data\\{date_str}_df_cov_emv_matrix.parquet'\n",
    "path_output = f'..\\picks\\{date_str}_portf.txt'\n",
    "\n",
    "\n",
    "print(utils.__file__)  # Should point to your src/utils.py\n",
    "print(f'path_date: {path_data}')\n",
    "print(f'path_corr: {path_corr}')\n",
    "print(f'path_cov: {path_cov}')  \n",
    "print(f'path_output: {path_output}')\n",
    "print((f'date_str: {date_str}'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Set pandas display options to show more columns and rows\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "# pd.set_option('display.max_rows', 10)       # Limit to 10 rows for readability\n",
    "pd.set_option('display.width', 1000)        # Let the display adjust to the window\n",
    "# pd.set_option('display.max_colwidth', None) # Show full content of each cell\n",
    "pd.set_option('display.max_rows', 200)\n",
    "# pd.set_option('display.width', 120)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_column_values_above_threshold(df, column_name='Avg Volume, M', threshold=1):\n",
    "  \"\"\"\n",
    "  Analyzes the number and percentage of values in a DataFrame column that are above a specified threshold,\n",
    "  and returns the filtered DataFrame.\n",
    "\n",
    "  Args:\n",
    "    df (pd.DataFrame): The input DataFrame.\n",
    "    column_name (str): The name of the column to analyze. Defaults to 'Avg Volume, M'.\n",
    "    threshold (float): The threshold value to compare against. Defaults to 1.00.\n",
    "\n",
    "  Returns:\n",
    "    pd.DataFrame: A DataFrame containing only the rows where the specified column's value is above the threshold.\n",
    "  \"\"\"\n",
    "  \n",
    "  count_before = len(df)\n",
    "  above_threshold_df = df[df[column_name] > threshold]\n",
    "  count_after = len(above_threshold_df)\n",
    "  percentage = (count_after / len(df)) * 100\n",
    "\n",
    "  print(f\"count_before: {count_before}\")\n",
    "  print(f\"count_after above threshold ({threshold}): {count_after}\")\n",
    "  print(f\"Percentage above threshold ({threshold}): {percentage:.2f}%\")\n",
    "\n",
    "  return above_threshold_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.read_parquet(path_data)\n",
    "\n",
    "# liquidity filter, Avg Volume, M > 0.75M\n",
    "df_data = get_column_values_above_threshold(df_data, column_name='Avg Volume, M', threshold=0.75)\n",
    "\n",
    "# Drop specified columns with NaNs in df_data\n",
    "df_data = df_data.drop(['All-Time High %', 'All-Time Low %', 'Dividend %'], axis=1)\n",
    "\n",
    "df_corr = pd.read_parquet(path_corr)\n",
    "df_cov = pd.read_parquet(path_cov)\n",
    "\n",
    "print(f'\\ndf_cov.shape: {df_cov.shape}')\n",
    "display(df_cov.head())\n",
    "\n",
    "print(f'\\ndf_corr.shape: {df_corr.shape}')\n",
    "display(df_corr.head())\n",
    "\n",
    "print(f'\\ndf_data.shape: {df_data.shape}')\n",
    "display(df_data.head())\n",
    "display((df_data.describe()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np # Will be used if we need to handle potential infinite values if any\n",
    "\n",
    "# --- IMPORTANT ---\n",
    "# Ensure your DataFrame is named df_data and contains the columns used below.\n",
    "# Example: (replace this with your actual data loading)\n",
    "# df_data = pd.read_csv('your_market_data.csv')\n",
    "\n",
    "# --- Data Cleaning (Optional but Recommended) ---\n",
    "# Replace potential infinite values with NaN\n",
    "df_data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "# Use a copy to avoid modifying original if needed later\n",
    "df_plot = df_data.copy()\n",
    "\n",
    "# --- Font Size Definitions ---\n",
    "SCALE_FONTSIZE = 1.2  # Scale factor for font sizes\n",
    "TITLE_FONTSIZE = 18 * SCALE_FONTSIZE\n",
    "AXIS_LABEL_FONTSIZE = 14 * SCALE_FONTSIZE\n",
    "TICK_LABEL_FONTSIZE = 12 * SCALE_FONTSIZE\n",
    "LEGEND_FONTSIZE = 12 * SCALE_FONTSIZE\n",
    "SUPTITLE_FONTSIZE = 22 * SCALE_FONTSIZE\n",
    "\n",
    "# --- Visualization ---\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid') # Use a visually appealing style\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 14)) # Increased figure size slightly for larger text\n",
    "fig.suptitle('Visualizing Market Bifurcation Indicators (Larger Text)', fontsize=SUPTITLE_FONTSIZE, y=1.03) # Adjusted y position slightly\n",
    "\n",
    "# 1. Histogram of Yearly Performance ('Perf Year %')\n",
    "sns.histplot(data=df_plot, x='Perf Year %', kde=True, ax=axes[0, 0], bins=50)\n",
    "axes[0, 0].set_title('Distribution of Yearly Performance (%)', fontsize=TITLE_FONTSIZE)\n",
    "axes[0, 0].set_xlabel('Perf Year %', fontsize=AXIS_LABEL_FONTSIZE)\n",
    "axes[0, 0].set_ylabel('Frequency', fontsize=AXIS_LABEL_FONTSIZE) # Added y-axis label\n",
    "axes[0, 0].tick_params(axis='both', which='major', labelsize=TICK_LABEL_FONTSIZE)\n",
    "median_val = df_plot['Perf Year %'].median()\n",
    "mean_val = df_plot['Perf Year %'].mean()\n",
    "axes[0, 0].axvline(median_val, color='red', linestyle='--', label=f'Median: {median_val:.2f}%')\n",
    "axes[0, 0].axvline(mean_val, color='orange', linestyle=':', label=f'Mean: {mean_val:.2f}%')\n",
    "axes[0, 0].legend(fontsize=LEGEND_FONTSIZE)\n",
    "\n",
    "# 2. Histogram of Normalized Volatility ('ATR/Price %')\n",
    "sns.histplot(data=df_plot, x='ATR/Price %', kde=True, ax=axes[0, 1], bins=50)\n",
    "axes[0, 1].set_title('Distribution of Normalized Daily Volatility (ATR/Price %)', fontsize=TITLE_FONTSIZE)\n",
    "axes[0, 1].set_xlabel('ATR/Price %', fontsize=AXIS_LABEL_FONTSIZE)\n",
    "axes[0, 1].set_ylabel('Frequency', fontsize=AXIS_LABEL_FONTSIZE) # Added y-axis label\n",
    "axes[0, 1].tick_params(axis='both', which='major', labelsize=TICK_LABEL_FONTSIZE)\n",
    "median_val = df_plot['ATR/Price %'].median()\n",
    "mean_val = df_plot['ATR/Price %'].mean()\n",
    "axes[0, 1].axvline(median_val, color='red', linestyle='--', label=f'Median: {median_val:.2f}%')\n",
    "axes[0, 1].axvline(mean_val, color='orange', linestyle=':', label=f'Mean: {mean_val:.2f}%')\n",
    "axes[0, 1].legend(fontsize=LEGEND_FONTSIZE)\n",
    "\n",
    "# 3. Scatter Plot: Yearly Performance vs. Normalized Volatility\n",
    "sns.scatterplot(data=df_plot, x='ATR/Price %', y='Perf Year %', ax=axes[1, 0], alpha=0.5, s=25) # Slightly larger points\n",
    "axes[1, 0].set_title('Yearly Performance vs. Daily Volatility', fontsize=TITLE_FONTSIZE)\n",
    "axes[1, 0].set_xlabel('ATR/Price % (Daily Volatility)', fontsize=AXIS_LABEL_FONTSIZE)\n",
    "axes[1, 0].set_ylabel('Perf Year %', fontsize=AXIS_LABEL_FONTSIZE)\n",
    "axes[1, 0].tick_params(axis='both', which='major', labelsize=TICK_LABEL_FONTSIZE)\n",
    "\n",
    "# 4. Scatter Plot: Yearly Performance vs. Distance from 200-Day MA\n",
    "sns.scatterplot(data=df_plot, x='SMA200 %', y='Perf Year %', ax=axes[1, 1], alpha=0.5, s=25) # Slightly larger points\n",
    "axes[1, 1].set_title('Yearly Performance vs. Position Relative to 200-Day MA', fontsize=TITLE_FONTSIZE)\n",
    "axes[1, 1].set_xlabel('Price vs SMA200 (%)', fontsize=AXIS_LABEL_FONTSIZE)\n",
    "axes[1, 1].set_ylabel('Perf Year %', fontsize=AXIS_LABEL_FONTSIZE)\n",
    "axes[1, 1].tick_params(axis='both', which='major', labelsize=TICK_LABEL_FONTSIZE)\n",
    "axes[1, 1].axvline(0, color='grey', linestyle='--') # Line at 0% (on the SMA200)\n",
    "axes[1, 1].axhline(0, color='grey', linestyle='--') # Line at 0% Perf Year\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.97]) # Adjust layout slightly for suptitle\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check df_corr\n",
    "has_nan_corr = df_corr.isnull().any().any()\n",
    "print(f\"Are there any NaNs in df_corr? {has_nan_corr}\")\n",
    "\n",
    "# Check df_cov\n",
    "has_nan_cov = df_cov.isnull().any().any()\n",
    "print(f\"Are there any NaNs in df_cov? {has_nan_cov}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from scipy.spatial.distance import squareform\n",
    "import os # Added for path manipulation\n",
    "\n",
    "# ==============================================================================\n",
    "# Helper Functions\n",
    "# ==============================================================================\n",
    "\n",
    "def setup_logging(log_filepath): # Changed arg name for clarity\n",
    "    \"\"\"Configures logging to write to both console and file.\"\"\"\n",
    "    # Ensure the directory for the log file exists\n",
    "    log_dir = os.path.dirname(log_filepath)\n",
    "    if log_dir and not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "        print(f\"Created log directory: {log_dir}\") # Print confirmation\n",
    "\n",
    "    # Remove existing handlers to avoid duplicate logs if called multiple times\n",
    "    for handler in logging.root.handlers[:]:\n",
    "        logging.root.removeHandler(handler)\n",
    "\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_filepath, mode='w'), # 'w' mode overwrites old file\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "    logging.info(f\"Logging configured. Output file: {log_filepath}\")\n",
    "\n",
    "\n",
    "def _prepare_data(df_data):\n",
    "    \"\"\"\n",
    "    Performs initial data cleaning and preparation.\n",
    "    - Converts specified columns to numeric.\n",
    "    - Assumes 'ATR/Price %' is already present in df_data if needed and includes it in numeric conversion.\n",
    "    - Handles potential errors during conversion.\n",
    "\n",
    "    Args:\n",
    "        df_data (pd.DataFrame): The raw input DataFrame, potentially including 'ATR/Price %'.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (pd.DataFrame: Prepared data, list: Volatility columns used)\n",
    "    \"\"\"\n",
    "    df = df_data.copy() # Work on a copy to avoid modifying the original DataFrame\n",
    "    logging.info(\"Preparing data: Converting numeric columns.\")\n",
    "\n",
    "    # Define columns expected to be numeric\n",
    "    numeric_cols = ['Beta', 'ATR', 'RSI', 'Rel Volume', 'Price', 'MktCap AUM, M', # Added MktCap\n",
    "                    'Volatility W %', 'Volatility M %', 'Perf 3D %',\n",
    "                    'Perf Week %', 'Perf Month %', 'Perf Quart %',\n",
    "                    'SMA20 %', 'SMA50 %', 'SMA200 %', 'ATR/Price %'] # Added 'ATR/Price %' here\n",
    "\n",
    "    # Also include Sharpe, Sortino, Omega columns if they follow a pattern\n",
    "    for col in df.columns:\n",
    "        # Check if column *should* be numeric based on common prefixes/suffixes or explicit list\n",
    "        is_potentially_numeric = any(\n",
    "            col.startswith(prefix) for prefix in\n",
    "            ['Sharpe ', 'Sortino ', 'Omega ', 'Perf ', 'SMA', 'Volatility ']\n",
    "        ) or col in numeric_cols\n",
    "\n",
    "        if is_potentially_numeric and col in df.columns:\n",
    "            try:\n",
    "                # Attempt conversion, removing non-numeric characters except '.' and '-'\n",
    "                df[col] = pd.to_numeric(\n",
    "                    df[col].astype(str).str.replace(r'[^0-9.-]', '', regex=True),\n",
    "                    errors='coerce' # Set invalid parsing as NaN\n",
    "                )\n",
    "                logging.debug(f\"Successfully attempted numeric conversion for column '{col}'.\")\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Could not convert column '{col}' to numeric: {e}\")\n",
    "                df[col] = np.nan # Ensure column exists but filled with NaN if error\n",
    "\n",
    "    # Determine which volatility columns are actually available and potentially numeric\n",
    "    volatility_cols_base = ['Volatility W %', 'Volatility M %']\n",
    "    volatility_cols_used = []\n",
    "\n",
    "    # Check base columns first\n",
    "    for col in volatility_cols_base:\n",
    "         if col in df.columns and pd.api.types.is_numeric_dtype(df[col]):\n",
    "              if df[col].notna().any(): # Check if there's at least one non-NaN value\n",
    "                   volatility_cols_used.append(col)\n",
    "                   logging.debug(f\"Using base volatility column: '{col}'\")\n",
    "              else:\n",
    "                   logging.info(f\"Base volatility column '{col}' found but contains only NaN values. Excluding.\")\n",
    "\n",
    "    # Check if 'ATR/Price %' exists and has numeric data after preparation\n",
    "    atr_price_col = 'ATR/Price %'\n",
    "    if atr_price_col in df.columns and pd.api.types.is_numeric_dtype(df[atr_price_col]):\n",
    "         if df[atr_price_col].notna().any(): # Check if there's at least one non-NaN value after conversion\n",
    "             volatility_cols_used.append(atr_price_col)\n",
    "             logging.info(f\"Found valid numeric data for '{atr_price_col}'. Added to volatility metrics.\")\n",
    "         else:\n",
    "             logging.info(f\"Volatility column '{atr_price_col}' found but contains only NaN values after conversion. Excluding.\")\n",
    "\n",
    "    if not volatility_cols_used:\n",
    "        logging.warning(\"No valid volatility columns found after data preparation.\")\n",
    "    else:\n",
    "        logging.info(f\"Final volatility metrics being used: {volatility_cols_used}\")\n",
    "\n",
    "    return df, volatility_cols_used\n",
    "\n",
    "\n",
    "def _calculate_component_scores(df, cols, weight, name, intermediates, raw_values, apply_zscore=True, custom_weights=None):\n",
    "    \"\"\"\n",
    "    Calculates the weighted score component for a given set of columns.\n",
    "    Handles Z-score calculation and optional custom weighting (like for momentum).\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing the data.\n",
    "        cols (list): List of column names for this component.\n",
    "        weight (float): The feature weight for this component.\n",
    "        name (str): The name of the component (e.g., 'sharpe', 'momentum').\n",
    "        intermediates (dict): Dictionary to store intermediate Z-score values.\n",
    "        raw_values (dict): Dictionary to store raw column values.\n",
    "        apply_zscore (bool): Whether to apply Z-scoring. Default True.\n",
    "        custom_weights (np.array): Optional array of weights for weighted average (e.g., momentum decay).\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: The calculated weighted score for this component. Returns zero series if cols are missing.\n",
    "    \"\"\"\n",
    "    # Check if all required columns exist\n",
    "    missing_cols = [col for col in cols if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        logging.warning(f\"Missing columns for '{name}' component: {missing_cols}. Skipping this component.\")\n",
    "        return pd.Series(0, index=df.index, name=f\"weighted_{name}\")\n",
    "\n",
    "    # Check if provided cols actually exist after potential drops/missing data\n",
    "    valid_cols = [col for col in cols if col in df.columns]\n",
    "    if not valid_cols:\n",
    "        logging.warning(f\"None of the columns for '{name}' component exist: {cols}. Skipping this component.\")\n",
    "        return pd.Series(0, index=df.index, name=f\"weighted_{name}\")\n",
    "\n",
    "    component_data = df[valid_cols].copy() # Use only valid columns\n",
    "    raw_values[f'{name}_raw'] = component_data # Store raw values\n",
    "\n",
    "    # Handle potential NaNs introduced by cleaning/conversion before Z-scoring\n",
    "    component_data = component_data.fillna(component_data.mean()) # Impute NaNs with mean for scoring\n",
    "\n",
    "    if apply_zscore:\n",
    "        # Calculate Z-scores, handle columns with zero standard deviation\n",
    "        z_scores = component_data.apply(lambda x: (x - x.mean()) / x.std() if x.std() > 1e-9 else 0)\n",
    "        intermediates[f'{name}_zscores'] = z_scores\n",
    "        score_basis = z_scores\n",
    "    else:\n",
    "        # Use raw (or imputed) data if no Z-scoring needed (e.g., for pre-calculated scores)\n",
    "        score_basis = component_data\n",
    "\n",
    "    # Calculate the final component score\n",
    "    if custom_weights is not None:\n",
    "        # Adjust custom_weights if some columns were missing\n",
    "        if len(valid_cols) != len(cols):\n",
    "             # Find indices of valid columns within the original list\n",
    "             valid_indices = [i for i, col in enumerate(cols) if col in valid_cols]\n",
    "             # Select corresponding weights\n",
    "             effective_weights = custom_weights[valid_indices]\n",
    "             # Renormalize weights if needed (optional, depends on intent)\n",
    "             # effective_weights = effective_weights / np.sum(effective_weights)\n",
    "             logging.warning(f\"Using subset of custom weights for '{name}' due to missing columns.\")\n",
    "        else:\n",
    "             effective_weights = custom_weights\n",
    "\n",
    "        if len(effective_weights) != score_basis.shape[1]:\n",
    "             logging.error(f\"Length mismatch after adjustment: effective_weights ({len(effective_weights)}) vs columns for {name} ({score_basis.shape[1]})\")\n",
    "             # Handle error: return zero score or raise? Returning zero for now.\n",
    "             return pd.Series(0, index=df.index, name=f\"weighted_{name}\")\n",
    "\n",
    "        # Weighted average using matrix multiplication for efficiency\n",
    "        component_score = (score_basis @ effective_weights) * weight\n",
    "    else:\n",
    "        # Simple mean if no custom weights\n",
    "        component_score = score_basis.mean(axis=1) * weight\n",
    "\n",
    "    return component_score.rename(f\"weighted_{name}\")\n",
    "\n",
    "\n",
    "def _calculate_rsi_score(df, raw_values):\n",
    "    \"\"\"\n",
    "    Calculates the RSI component score with penalties for overbought (>70)\n",
    "    and rewards for oversold (<30).\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing the 'RSI' column.\n",
    "        raw_values (dict): Dictionary to store raw RSI values.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: The calculated RSI score component. Returns zero series if RSI column is missing.\n",
    "    \"\"\"\n",
    "    if 'RSI' not in df.columns:\n",
    "        logging.warning(\"Missing 'RSI' column. Skipping RSI scoring component.\")\n",
    "        return pd.Series(0, index=df.index, name=\"weighted_rsi\")\n",
    "\n",
    "    rsi_values = df['RSI'].copy()\n",
    "    raw_values['rsi_raw'] = rsi_values # Store raw RSI\n",
    "\n",
    "    # Define conditions and corresponding score adjustments\n",
    "    conditions = [\n",
    "        rsi_values > 70,                        # Overbought penalty\n",
    "        rsi_values < 30,                        # Oversold reward\n",
    "        (rsi_values >= 30) & (rsi_values <= 70) # Neutral zone\n",
    "    ]\n",
    "    choices = [\n",
    "        np.clip((rsi_values - 70) * -0.01, a_min=-0.15, a_max=0),  # Penalty capped at -0.15\n",
    "        np.clip((30 - rsi_values) * 0.005, a_min=0, a_max=0.075),  # Reward capped at +0.075\n",
    "        0                                                          # No adjustment in neutral zone\n",
    "    ]\n",
    "\n",
    "    # Use np.select for vectorized conditional assignment\n",
    "    rsi_score = pd.Series(\n",
    "        np.select(conditions, choices, default=0), # Default to 0 if NaN or outside defined ranges\n",
    "        index=df.index,\n",
    "        name=\"weighted_rsi\"\n",
    "    )\n",
    "\n",
    "    # Handle potential NaNs in RSI input - replace resulting NaNs in score with 0\n",
    "    rsi_score = rsi_score.fillna(0)\n",
    "\n",
    "    return rsi_score\n",
    "\n",
    "\n",
    "def calculate_composite_score(df, feature_weights, column_definitions):\n",
    "    \"\"\"\n",
    "    Calculates the composite score based on various weighted metrics.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The cleaned data DataFrame.\n",
    "        feature_weights (dict): Dictionary of weights for each score component.\n",
    "        column_definitions (dict): Dictionary containing lists of column names for each component.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (pd.Series: composite_score,\n",
    "                dict: intermediate_values (like z-scores),\n",
    "                dict: raw_values)\n",
    "    \"\"\"\n",
    "    logging.info(\"Calculating composite scores for tickers.\")\n",
    "    components = {}\n",
    "    intermediate_values = {}\n",
    "    raw_values = {}\n",
    "\n",
    "    # --- Risk-adjusted metrics (Sharpe, Sortino, Omega) ---\n",
    "    for category, cols_key in [('sharpe', 'sharpe'),\n",
    "                               ('sortino', 'sortino'),\n",
    "                               ('omega', 'omega')]:\n",
    "        if category in feature_weights and feature_weights[category] != 0:\n",
    "            components[category] = _calculate_component_scores(\n",
    "                df, column_definitions[cols_key], feature_weights[category], category, intermediate_values, raw_values\n",
    "            )\n",
    "\n",
    "    # --- Momentum with decay weights ---\n",
    "    if 'momentum' in feature_weights and feature_weights['momentum'] != 0:\n",
    "        # Ensure momentum_weights aligns with the actual momentum columns used\n",
    "        components['momentum'] = _calculate_component_scores(\n",
    "            df, column_definitions['momentum'], feature_weights['momentum'], 'momentum',\n",
    "            intermediate_values, raw_values,\n",
    "            custom_weights=np.array(column_definitions['momentum_weights']) # Ensure it's an array\n",
    "        )\n",
    "\n",
    "    # --- Technical indicators (SMA, Volatility) ---\n",
    "    for category, cols_key in [('sma', 'sma'),\n",
    "                              ('volatility', 'volatility')]: # Use dynamic volatility cols\n",
    "        if category in feature_weights and feature_weights[category] != 0:\n",
    "            # Note: Volatility has a negative weight (penalty)\n",
    "            components[category] = _calculate_component_scores(\n",
    "                df, column_definitions[cols_key], feature_weights[category], category, intermediate_values, raw_values\n",
    "            )\n",
    "\n",
    "    # --- Enhanced RSI scoring ---\n",
    "    # No weight needed from feature_weights dict, logic is self-contained\n",
    "    components['rsi'] = _calculate_rsi_score(df, raw_values)\n",
    "\n",
    "    # --- Combine components ---\n",
    "    component_breakdown = pd.concat(components.values(), axis=1) # concat Series directly\n",
    "    intermediate_values['component_breakdown'] = component_breakdown # Store the weighted breakdown\n",
    "    composite_score = component_breakdown.sum(axis=1)\n",
    "\n",
    "    logging.info(\"Composite score calculation finished.\")\n",
    "    # Log intermediate keys for debugging structure\n",
    "    logging.debug(f\"Intermediate keys generated: {list(intermediate_values.keys())}\")\n",
    "    logging.debug(f\"Raw value keys generated: {list(raw_values.keys())}\")\n",
    "\n",
    "    return composite_score, intermediate_values, raw_values\n",
    "\n",
    "\n",
    "def perform_clustering(df_scores, df_corr, num_tickers_to_select, num_clusters, score_col='composite_score'):\n",
    "    \"\"\"\n",
    "    Selects top N tickers based on score and performs hierarchical clustering.\n",
    "    Uses num_tickers_to_select which might be adjusted from the original request.\n",
    "\n",
    "    Args:\n",
    "        df_scores (pd.DataFrame): DataFrame with tickers and scores.\n",
    "        df_corr (pd.DataFrame): Correlation matrix DataFrame.\n",
    "        num_tickers_to_select (int): Actual number of top tickers to select for clustering.\n",
    "        num_clusters (int): Number of clusters to form.\n",
    "        score_col (str): Name of the column containing the scores.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (pd.DataFrame: Top N tickers with scores,\n",
    "                pd.Series: Cluster assignments for top N tickers (index=ticker, value=cluster_id),\n",
    "                pd.DataFrame: Correlation subset used for clustering)\n",
    "                Returns (None, None, None) on failure.\n",
    "    \"\"\"\n",
    "    # This function now assumes num_tickers_to_select <= len(df_scores)\n",
    "    logging.info(f\"Selecting top {num_tickers_to_select} tickers based on '{score_col}'.\")\n",
    "\n",
    "    # Check if enough tickers are available (should be guaranteed by caller, but double-check)\n",
    "    if len(df_scores) < num_tickers_to_select:\n",
    "         logging.error(f\"Internal Error: Trying to select {num_tickers_to_select} tickers, but only {len(df_scores)} are in df_scores.\")\n",
    "         return None, None, None\n",
    "\n",
    "    if num_tickers_to_select <= 1:\n",
    "         logging.warning(f\"Cannot perform clustering with {num_tickers_to_select} tickers. Need at least 2.\")\n",
    "         return None, None, None\n",
    "\n",
    "    top_n = df_scores.nlargest(num_tickers_to_select, score_col)\n",
    "    top_n_tickers = top_n.index.tolist()\n",
    "    logging.info(f\"Selected tickers: {top_n_tickers[:5]}... (first 5 shown)\")\n",
    "\n",
    "    logging.info(f\"Performing hierarchical clustering into {num_clusters} clusters.\")\n",
    "\n",
    "    # --- Robust Subset and Distance Matrix Calculation ---\n",
    "    try:\n",
    "        # Subset correlation matrix for the top N tickers\n",
    "        corr_subset = df_corr.loc[top_n_tickers, top_n_tickers]\n",
    "\n",
    "        # 1. Ensure symmetry (numerical precision issues can sometimes occur)\n",
    "        corr_subset = (corr_subset + corr_subset.T) / 2\n",
    "        # 2. Calculate distance (1 - absolute correlation)\n",
    "        distance_matrix = 1 - np.abs(corr_subset)\n",
    "        # 3. Ensure diagonal is zero\n",
    "        np.fill_diagonal(distance_matrix.values, 0)\n",
    "        # 4. Check for NaNs or Infs in the distance matrix\n",
    "        if distance_matrix.isna().any().any() or np.isinf(distance_matrix.values).any():\n",
    "            logging.warning(\"NaNs or Infs found in distance matrix. Attempting to fill with mean distance.\")\n",
    "            # Replace NaNs/Infs with the mean of finite distances\n",
    "            finite_distances = distance_matrix.values[np.isfinite(distance_matrix.values)]\n",
    "            mean_distance = np.mean(finite_distances) if len(finite_distances) > 0 else 0.5 # Fallback if all are bad\n",
    "            distance_matrix = distance_matrix.fillna(mean_distance)\n",
    "            distance_matrix.values[np.isinf(distance_matrix.values)] = mean_distance\n",
    "            # Ensure diagonal is still zero after fillna\n",
    "            np.fill_diagonal(distance_matrix.values, 0)\n",
    "            # Final check for non-finite values after attempting fill\n",
    "            if not np.all(np.isfinite(distance_matrix.values)):\n",
    "                 logging.error(\"Distance matrix still contains non-finite values after fill attempt. Cannot proceed.\")\n",
    "                 # distance_matrix.to_csv(\"debug_distance_matrix_error.csv\") # Optional: dump for inspection\n",
    "                 return None, None, None\n",
    "\n",
    "    except KeyError as e:\n",
    "        missing_tickers = [t for t in top_n_tickers if t not in df_corr.index or t not in df_corr.columns]\n",
    "        logging.error(f\"KeyError during correlation matrix subsetting. Missing tickers: {missing_tickers}. Error: {e}\")\n",
    "        return None, None, None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during distance matrix preparation: {e}\", exc_info=True)\n",
    "        return None, None, None\n",
    "\n",
    "    # --- Clustering ---\n",
    "    try:\n",
    "        # Convert to condensed distance matrix format required by linkage\n",
    "        condensed_dist = squareform(distance_matrix, checks=True) # checks=True verifies symmetry etc.\n",
    "        linkage_matrix = linkage(condensed_dist, method='ward')\n",
    "\n",
    "        # Adjust num_clusters if it's > number of items being clustered\n",
    "        actual_num_clusters = min(num_clusters, num_tickers_to_select - 1) # Max clusters is N-1\n",
    "        if actual_num_clusters < num_clusters:\n",
    "            logging.warning(f\"Requested {num_clusters} clusters, but only {num_tickers_to_select} tickers selected. Reducing clusters to {actual_num_clusters}.\")\n",
    "        if actual_num_clusters < 1:\n",
    "             logging.error(f\"Cannot form clusters. Number of tickers ({num_tickers_to_select}) is too small.\")\n",
    "             return None, None, None\n",
    "\n",
    "        cluster_ids = fcluster(linkage_matrix, t=actual_num_clusters, criterion='maxclust')\n",
    "        cluster_assignments = pd.Series(cluster_ids, index=top_n_tickers, name='cluster')\n",
    "        logging.info(f\"Clustering successful. Formed {len(cluster_assignments.unique())} clusters.\")\n",
    "\n",
    "    except ValueError as ve:\n",
    "        logging.error(f\"Clustering failed. Error during linkage/fcluster: {ve}\")\n",
    "        logging.error(\"This might be due to issues in the distance matrix (e.g., non-finite values, symmetry problems).\")\n",
    "        # distance_matrix.to_csv(\"debug_distance_matrix_value_error.csv\") # Optional\n",
    "        return None, None, None # Indicate failure\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred during clustering: {e}\", exc_info=True)\n",
    "        return None, None, None # Indicate failure\n",
    "\n",
    "\n",
    "    return top_n, cluster_assignments, corr_subset\n",
    "\n",
    "\n",
    "def analyze_clusters(cluster_assignments, top_n, df_cov, df_corr_subset, df_clean_data, score_col='composite_score'):\n",
    "    \"\"\"\n",
    "    Analyzes the clusters, calculates risk-adjusted scores, and generates summary statistics.\n",
    "\n",
    "    Args:\n",
    "        cluster_assignments (pd.Series): Cluster assignments for top N tickers.\n",
    "        top_n (pd.DataFrame): Top N tickers with scores.\n",
    "        df_cov (pd.DataFrame): Covariance matrix DataFrame.\n",
    "        df_corr_subset (pd.DataFrame): Correlation matrix subset for the top N tickers.\n",
    "        df_clean_data (pd.DataFrame): Original cleaned data (needed for Price, MktCap, etc.).\n",
    "        score_col (str): Name of the column containing the raw scores.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (pd.DataFrame: detailed_clusters_df,\n",
    "                pd.DataFrame: cluster_stats_df)\n",
    "               Returns (None, None) if input is invalid.\n",
    "    \"\"\"\n",
    "    if cluster_assignments is None or top_n is None:\n",
    "         logging.warning(\"Cannot analyze clusters due to invalid input (cluster_assignments or top_n is None).\")\n",
    "         return None, None\n",
    "\n",
    "    logging.info(\"Analyzing clusters and calculating risk-adjusted scores.\")\n",
    "\n",
    "    # Check for required columns in df_clean_data\n",
    "    required_analysis_cols = ['Price', 'MktCap AUM, M', 'Volatility M %']\n",
    "    missing_analysis_cols = [col for col in required_analysis_cols if col not in df_clean_data.columns]\n",
    "    if missing_analysis_cols:\n",
    "        logging.warning(f\"Missing columns in df_clean_data needed for analysis: {missing_analysis_cols}. These columns will be omitted.\")\n",
    "    available_analysis_cols = [col for col in required_analysis_cols if col in df_clean_data.columns]\n",
    "\n",
    "\n",
    "    # Create a DataFrame combining cluster info, scores, and other relevant data\n",
    "    cluster_df = pd.DataFrame({\n",
    "        'ticker': cluster_assignments.index,\n",
    "        'cluster': cluster_assignments.values\n",
    "    }).merge(\n",
    "        top_n[[score_col]], # Get the score from top_n df\n",
    "        left_on='ticker',\n",
    "        right_index=True\n",
    "    ).merge(\n",
    "        df_clean_data[available_analysis_cols], # Add available stats from original clean data\n",
    "        left_on='ticker',\n",
    "        right_index=True,\n",
    "        how='left' # Use left merge in case some columns are missing in df_clean_data for a ticker\n",
    "    )\n",
    "\n",
    "    # --- Calculate Variance and Risk-Adjusted Score ---\n",
    "    # Extract variance (diagonal of covariance matrix) for each ticker\n",
    "    variances = []\n",
    "    tickers_in_cluster_df = cluster_df['ticker'].tolist()\n",
    "    valid_tickers_cov = [t for t in tickers_in_cluster_df if t in df_cov.index and t in df_cov.columns]\n",
    "    missing_tickers_cov = set(tickers_in_cluster_df) - set(valid_tickers_cov)\n",
    "\n",
    "    if missing_tickers_cov:\n",
    "         logging.warning(f\"Tickers missing from covariance matrix diagonals: {list(missing_tickers_cov)}. Variance will be NaN.\")\n",
    "\n",
    "    # Create a variance series aligned with cluster_df's index\n",
    "    variance_series = pd.Series(np.nan, index=cluster_df.index)\n",
    "    try:\n",
    "        # Efficiently get diagonal elements for valid tickers\n",
    "        valid_variances = np.diag(df_cov.loc[valid_tickers_cov, valid_tickers_cov])\n",
    "        # Map these variances back to the cluster_df index based on ticker\n",
    "        ticker_to_variance = dict(zip(valid_tickers_cov, valid_variances))\n",
    "        variance_series = cluster_df['ticker'].map(ticker_to_variance) # Map will put NaN where ticker is missing\n",
    "\n",
    "    except Exception as e:\n",
    "         logging.error(f\"Error accessing covariance matrix diagonal: {e}\", exc_info=True)\n",
    "         # variance_series remains as NaNs\n",
    "\n",
    "    cluster_df['variance'] = variance_series\n",
    "\n",
    "    # Calculate volatility (sqrt of variance), handle negative variance due to numerical issues\n",
    "    cluster_df['volatility'] = np.sqrt(np.maximum(cluster_df['variance'], 0)) # Use maximum to avoid errors with tiny negatives\n",
    "\n",
    "    # Calculate Risk-Adjusted Score (Score / Volatility)\n",
    "    # Add epsilon to denominator to avoid division by zero if variance is exactly zero\n",
    "    epsilon = 1e-9\n",
    "    valid_volatility_mask = cluster_df['volatility'].notna() & (cluster_df['volatility'] > epsilon) # Check for NaN and > epsilon\n",
    "    cluster_df['risk_adj_score'] = np.nan # Initialize column\n",
    "    cluster_df.loc[valid_volatility_mask, 'risk_adj_score'] = (\n",
    "        cluster_df.loc[valid_volatility_mask, score_col] /\n",
    "        cluster_df.loc[valid_volatility_mask, 'volatility'] # Use volatility directly\n",
    "    )\n",
    "    # Log if any risk-adj scores couldn't be calculated\n",
    "    num_nan_risk_adj = cluster_df['risk_adj_score'].isna().sum()\n",
    "    if num_nan_risk_adj > 0:\n",
    "        logging.warning(f\"{num_nan_risk_adj} tickers have NaN risk-adjusted scores (likely due to missing/zero variance/volatility).\")\n",
    "\n",
    "    # --- Prepare Detailed Output ---\n",
    "    detailed_cols_to_include = ['Cluster_ID', 'Ticker', 'Raw_Score', 'Risk_Adj_Score', 'Volatility'] # Basic columns\n",
    "    # Add other analysis cols if available\n",
    "    if 'Price' in cluster_df.columns: detailed_cols_to_include.append('Price')\n",
    "    if 'MktCap AUM, M' in cluster_df.columns: detailed_cols_to_include.append('MktCap AUM, M')\n",
    "\n",
    "    detailed_clusters_df = cluster_df.rename(columns={\n",
    "        'cluster': 'Cluster_ID',\n",
    "        'ticker': 'Ticker',\n",
    "        score_col: 'Raw_Score', # Use the dynamic score_col name\n",
    "        'risk_adj_score': 'Risk_Adj_Score',\n",
    "        'volatility': 'Volatility'\n",
    "    }).sort_values(\n",
    "        ['Cluster_ID', 'Risk_Adj_Score'], ascending=[True, False] # Sort by cluster, then descending risk-adj score\n",
    "    )[detailed_cols_to_include] # Select and order columns\n",
    "\n",
    "\n",
    "    # --- Prepare Cluster Statistics ---\n",
    "    logging.info(\"Calculating cluster summary statistics.\")\n",
    "    # Function to safely calculate mean correlation within a cluster\n",
    "    def safe_mean_corr(tickers_series, corr_matrix):\n",
    "        tickers = tickers_series.tolist()\n",
    "        if len(tickers) <= 1:\n",
    "            return 1.0 # Correlation is 1 for a single ticker cluster\n",
    "        try:\n",
    "             # Ensure all tickers are in the correlation matrix subset\n",
    "             valid_tickers = [t for t in tickers if t in corr_matrix.index and t in corr_matrix.columns]\n",
    "             if len(valid_tickers) <= 1:\n",
    "                 logging.warning(f\"Not enough valid tickers ({len(valid_tickers)}) in corr matrix for cluster: {tickers}. Returning NaN for Avg Corr.\")\n",
    "                 return np.nan\n",
    "\n",
    "             cluster_corr = corr_matrix.loc[valid_tickers, valid_tickers]\n",
    "             # Exclude diagonal (self-correlation) before calculating mean\n",
    "             np.fill_diagonal(cluster_corr.values, np.nan)\n",
    "             # Check if all values became NaN (happens if only 1 valid ticker)\n",
    "             if np.isnan(cluster_corr.values).all():\n",
    "                  return 1.0 # Effectively a single-ticker cluster after filtering\n",
    "             return np.nanmean(cluster_corr.values) # Use nanmean to ignore NaNs from diagonal\n",
    "\n",
    "        except KeyError as e:\n",
    "            logging.warning(f\"KeyError calculating mean correlation for cluster containing: {tickers}. Error: {e}. Returning NaN.\")\n",
    "            return np.nan\n",
    "        except Exception as e:\n",
    "             logging.warning(f\"Error calculating mean correlation for cluster {tickers}: {e}\")\n",
    "             return np.nan\n",
    "\n",
    "    cluster_stats_df = cluster_df.groupby('cluster').agg(\n",
    "        Size=('ticker', 'count'),\n",
    "        Avg_Correlation=('ticker', lambda t: safe_mean_corr(t, df_corr_subset)),\n",
    "        Avg_Raw_Score=(score_col, 'mean'),\n",
    "        Avg_Risk_Adj_Score=('risk_adj_score', 'mean'), # Mean calculation will ignore NaNs by default\n",
    "        Avg_Volatility=('volatility', 'mean')          # Mean calculation will ignore NaNs by default\n",
    "    ).reset_index().round(4) # Increased precision slightly\n",
    "\n",
    "    # Rename columns for clarity\n",
    "    cluster_stats_df.columns = ['Cluster_ID', 'Size', 'Avg_IntraCluster_Corr',\n",
    "                              'Avg_Raw_Score', 'Avg_Risk_Adj_Score', 'Avg_Volatility']\n",
    "\n",
    "\n",
    "    logging.info(\"Cluster analysis complete.\")\n",
    "    return detailed_clusters_df, cluster_stats_df\n",
    "\n",
    "\n",
    "def _assemble_output_df(intermediate_values, raw_values, composite_scores, cluster_assignments, base_index):\n",
    "    \"\"\"\n",
    "    Assembles the final DataFrame containing scores, components, and cluster info.\n",
    "    Handles potential None for cluster_assignments.\n",
    "    \"\"\"\n",
    "    logging.info(\"Assembling final output DataFrame (zscore_df).\")\n",
    "\n",
    "    # --- Intermediate Values (Z-scores, weighted components) ---\n",
    "    intermediates_df = pd.DataFrame(index=base_index)\n",
    "    # Add component breakdown first if it exists\n",
    "    if 'component_breakdown' in intermediate_values:\n",
    "        # Ensure index alignment before concatenating\n",
    "        comp_breakdown = intermediate_values['component_breakdown'].reindex(base_index)\n",
    "        intermediates_df = pd.concat([intermediates_df, comp_breakdown], axis=1)\n",
    "\n",
    "    # Add other intermediate values (like individual z-scores per category)\n",
    "    for key, values in intermediate_values.items():\n",
    "        if key == 'component_breakdown': continue # Already added\n",
    "\n",
    "        if isinstance(values, pd.DataFrame):\n",
    "            # Prefix columns to avoid name collisions and ensure alignment\n",
    "            prefixed_values = values.add_prefix(f'{key}_').reindex(base_index)\n",
    "            intermediates_df = pd.concat([intermediates_df, prefixed_values], axis=1)\n",
    "        elif isinstance(values, pd.Series):\n",
    "             # Ensure index alignment before adding Series\n",
    "             intermediates_df[key] = values.reindex(base_index)\n",
    "        else:\n",
    "            logging.warning(f\"Unexpected data type in intermediate_values for key '{key}': {type(values)}. Skipping.\")\n",
    "\n",
    "    # --- Raw Values ---\n",
    "    raw_vals_df = pd.DataFrame(index=base_index)\n",
    "    for key, values in raw_values.items():\n",
    "        if isinstance(values, pd.DataFrame):\n",
    "             # Prefix columns and ensure alignment\n",
    "            prefixed_values = values.add_prefix(f'{key}_').reindex(base_index)\n",
    "            raw_vals_df = pd.concat([raw_vals_df, prefixed_values], axis=1)\n",
    "        elif isinstance(values, pd.Series):\n",
    "             # Ensure index alignment\n",
    "            raw_vals_df[key] = values.reindex(base_index)\n",
    "        else:\n",
    "            logging.warning(f\"Unexpected data type in raw_values for key '{key}': {type(values)}. Skipping.\")\n",
    "\n",
    "    # --- Cluster Assignments (Handle None) --- FIX APPLIED HERE\n",
    "    if cluster_assignments is not None:\n",
    "        cluster_col = cluster_assignments.reindex(base_index).rename('cluster')\n",
    "    else:\n",
    "        # If clustering failed or wasn't run, create a column of NaNs\n",
    "        cluster_col = pd.Series(np.nan, index=base_index, name='cluster')\n",
    "\n",
    "    # --- Combine all parts ---\n",
    "    final_df = pd.concat([\n",
    "        intermediates_df,\n",
    "        raw_vals_df,\n",
    "        composite_scores.rename('composite_score').reindex(base_index),\n",
    "        cluster_col # Add the potentially NaN cluster column\n",
    "    ], axis=1)\n",
    "\n",
    "    logging.info(\"Final output DataFrame assembled.\")\n",
    "    return final_df\n",
    "\n",
    "\n",
    "def log_results_to_file(results_filepath, cluster_stats_df, detailed_clusters_df, zscore_df, volatility_cols, top_n=20):\n",
    "    \"\"\"Writes the analysis results to the specified results file.\"\"\"\n",
    "    logging.info(f\"Writing detailed results to file: {results_filepath}\")\n",
    "\n",
    "    # Ensure the directory for the results file exists\n",
    "    results_dir = os.path.dirname(results_filepath)\n",
    "    if results_dir and not os.path.exists(results_dir):\n",
    "        try:\n",
    "            os.makedirs(results_dir)\n",
    "            logging.info(f\"Created results directory: {results_dir}\")\n",
    "        except OSError as e:\n",
    "            logging.error(f\"Could not create results directory {results_dir}: {e}. Results will not be saved to file.\")\n",
    "            return # Stop if directory cannot be created\n",
    "\n",
    "    try:\n",
    "        # Use 'w' mode to overwrite previous results, or 'a' to append if desired\n",
    "        with open(results_filepath, 'w') as f:\n",
    "            f.write(\"=\"*30 + \" PORTFOLIO OPTIMIZATION RESULTS \" + \"=\"*30 + \"\\n\")\n",
    "            f.write(f\"Analysis Timestamp: {pd.Timestamp.now()}\\n\")\n",
    "\n",
    "            f.write(\"\\n\\n\" + \"=\"*20 + \" CLUSTER STATISTICS \" + \"=\"*20 + \"\\n\")\n",
    "            if cluster_stats_df is not None and not cluster_stats_df.empty:\n",
    "                cluster_stats_df.to_string(f, index=False, line_width=120)\n",
    "            elif cluster_stats_df is not None and cluster_stats_df.empty:\n",
    "                 f.write(\"Cluster statistics table generated but is empty.\\n\")\n",
    "            else:\n",
    "                f.write(\"Cluster statistics could not be generated (likely due to clustering failure or lack of tickers).\\n\")\n",
    "\n",
    "            f.write(\"\\n\\n\" + \"=\"*20 + \" DETAILED CLUSTERS (ALL SELECTED) \" + \"=\"*20 + \"\\n\")\n",
    "            if detailed_clusters_df is not None and not detailed_clusters_df.empty:\n",
    "                detailed_clusters_df.to_string(f, index=False, line_width=150) # Wider width for more columns\n",
    "            elif detailed_clusters_df is not None and detailed_clusters_df.empty:\n",
    "                 f.write(\"Detailed cluster information table generated but is empty.\\n\")\n",
    "            else:\n",
    "                f.write(\"Detailed cluster information could not be generated.\\n\")\n",
    "\n",
    "            f.write(\"\\n\\n\" + \"=\"*20 + f\" SCORING DETAILS & COMPONENTS (Top {top_n})\" + \"=\"*20 + \"\\n\")\n",
    "            zscore_df_sorted = None\n",
    "            if zscore_df is not None and not zscore_df.empty:\n",
    "                # Check if 'composite_score' exists before sorting\n",
    "                if 'composite_score' in zscore_df.columns:\n",
    "                     # Handle potential all-NaN composite scores if calculation failed badly\n",
    "                     if zscore_df['composite_score'].notna().any():\n",
    "                         try:\n",
    "                             zscore_df_sorted = zscore_df.sort_values('composite_score', ascending=False)\n",
    "                             zscore_df_sorted.head(top_n).to_string(f, line_width=180) # Wider line width\n",
    "                         except Exception as sort_e:\n",
    "                              logging.warning(f\"Could not sort zscore_df by composite_score: {sort_e}. Logging top {top_n} unsorted rows.\")\n",
    "                              zscore_df.head(top_n).to_string(f, line_width=180)\n",
    "                     else:\n",
    "                          f.write(f\"Scoring details available, but 'composite_score' column contains only NaNs. Logging top {top_n} unsorted rows.\\n\")\n",
    "                          zscore_df.head(top_n).to_string(f, line_width=180)\n",
    "                else:\n",
    "                     f.write(f\"Scoring details available, but 'composite_score' column is missing. Logging top {top_n} unsorted rows.\\n\")\n",
    "                     zscore_df.head(top_n).to_string(f, line_width=180)\n",
    "            elif zscore_df is not None and zscore_df.empty:\n",
    "                 f.write(\"Scoring details table (zscore_df) generated but is empty.\\n\")\n",
    "            else:\n",
    "                f.write(\"Scoring details (zscore_df) could not be generated.\\n\")\n",
    "\n",
    "            # --- Add Volatility Breakdown ---\n",
    "            f.write(\"\\n\\n\" + \"=\"*20 + \" VOLATILITY COMPONENT BREAKDOWN (Top 5) \" + \"=\"*20 + \"\\n\")\n",
    "            if zscore_df_sorted is not None and not zscore_df_sorted.empty: # Check if sorted DF exists and is not empty\n",
    "                f.write(f\"Volatility columns used in scoring: {volatility_cols}\\n\")\n",
    "\n",
    "                # Look for the Z-SCORE columns for volatility breakdown (BEFORE weighting)\n",
    "                vol_zscore_cols = [f'volatility_zscores_{col}' for col in volatility_cols if f'volatility_zscores_{col}' in zscore_df_sorted.columns]\n",
    "\n",
    "                if vol_zscore_cols:\n",
    "                    f.write(\"Top 5 Tickers - Z-Score Volatility Components (before weighting):\\n\")\n",
    "                    zscore_df_sorted[vol_zscore_cols].head(5).to_string(f, line_width=120, float_format=\"%.4f\")\n",
    "                else:\n",
    "                    f.write(\"\\nCould not find detailed volatility z-score columns (e.g., 'volatility_zscores_Volatility W %') in output.\\n\")\n",
    "                    logging.warning(\"Volatility z-score columns missing from zscore_df for logging breakdown.\")\n",
    "\n",
    "                # Also show the final weighted volatility contribution\n",
    "                if 'weighted_volatility' in zscore_df_sorted.columns:\n",
    "                    f.write(\"\\n\\nTop 5 Tickers - Final Weighted Volatility Score Component (Lower is better/less penalty):\\n\")\n",
    "                    zscore_df_sorted[['weighted_volatility']].head(5).to_string(f, float_format=\"%.4f\")\n",
    "                else:\n",
    "                    f.write(\"\\n\\nCould not find 'weighted_volatility' column in output.\\n\")\n",
    "                    logging.warning(\"'weighted_volatility' column missing from zscore_df for logging breakdown.\")\n",
    "\n",
    "            elif zscore_df is not None and not zscore_df.empty:\n",
    "                 f.write(\"Scoring details available but could not be sorted by composite_score. Cannot show reliable top 5 volatility breakdown.\\n\")\n",
    "            else:\n",
    "                 f.write(\"Scoring details (zscore_df) not available for volatility breakdown.\\n\")\n",
    "            # --- End Volatility Breakdown ---\n",
    "\n",
    "        logging.info(f\"Successfully wrote results data to {results_filepath}\")\n",
    "\n",
    "    except IOError as e:\n",
    "        logging.error(f\"Failed to write results to file {results_filepath}: {e}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred while writing results to file {results_filepath}: {e}\", exc_info=True)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# Main Refactored Function\n",
    "# ==============================================================================\n",
    "\n",
    "def portfolio_optimizer_60days_refactored(\n",
    "    df_data,\n",
    "    df_corr,\n",
    "    df_cov,\n",
    "    num_clusters=60,\n",
    "    num_tickers=100, # Target number of tickers\n",
    "    log_file=\"portfolio_optimizer.log\",\n",
    "    results_file=\"portfolio_output_refactored.txt\"):\n",
    "    \"\"\"\n",
    "    Refactored: Selects top tickers based on a composite score and optimizes\n",
    "    a portfolio using cluster analysis. Easier to debug and maintain.\n",
    "\n",
    "    Args:\n",
    "        df_data (pd.DataFrame): DataFrame containing stock data with various metrics.\n",
    "                                Must contain columns for scoring and optionally 'Price', 'MktCap AUM, M'.\n",
    "        df_corr (pd.DataFrame): DataFrame containing the correlation matrix (tickers x tickers).\n",
    "        df_cov (pd.DataFrame): DataFrame containing the covariance matrix (tickers x tickers).\n",
    "        num_clusters (int): The target number of clusters to form.\n",
    "        num_tickers (int): The target number of top tickers to select based on score.\n",
    "                           Will be adjusted downwards if not enough tickers pass cleaning.\n",
    "        log_file (str): Path for the process log file.\n",
    "        results_file (str): Path for the final data results file.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Contains three DataFrames:\n",
    "               1. pd.DataFrame: Scoring details, intermediate values, raw values,\n",
    "                  and cluster assignments for all cleaned tickers (`zscore_df`).\n",
    "               2. pd.DataFrame: Statistics summary for each cluster (`cluster_stats_df`).\n",
    "               3. pd.DataFrame: Detailed view of tickers within each cluster,\n",
    "                  sorted by risk-adjusted score (`detailed_clusters_df`).\n",
    "               Returns (None, None, None) if a critical step fails.\n",
    "    \"\"\"\n",
    "    # Initialize return variables to None\n",
    "    zscore_df_final, cluster_stats_df_final, detailed_clusters_df_final = None, None, None\n",
    "    # Initialize intermediate state variables used in exception handling\n",
    "    cluster_stats_df, detailed_clusters_df = None, None\n",
    "    zscore_df = None\n",
    "    volatility_cols_used = []\n",
    "\n",
    "    try:\n",
    "        # Use the log_file argument for logging setup\n",
    "        setup_logging(log_file)\n",
    "        logging.info(\"=\"*30 + \" Starting Portfolio Optimization \" + \"=\"*30)\n",
    "        logging.info(f\"Target number of tickers: {num_tickers}\")\n",
    "        logging.info(f\"Target number of clusters: {num_clusters}\")\n",
    "        logging.info(f\"Detailed results will be written to: {results_file}\")\n",
    "\n",
    "        # ===== STAGE 1: DATA PREPARATION =====\n",
    "        df_prepared, volatility_cols_used = _prepare_data(df_data)\n",
    "\n",
    "        # ===== STAGE 2: SCORING CONFIGURATION =====\n",
    "        logging.info(\"Defining scoring parameters and column groups.\")\n",
    "        time_horizons = [3, 5, 10, 15, 30, 60]\n",
    "        feature_weights = {\n",
    "            'sharpe': 0.20,\n",
    "            'sortino': 0.20,\n",
    "            'omega': 0.15,\n",
    "            'momentum': 0.20,\n",
    "            'sma': 0.15,\n",
    "            'volatility': -0.10, # Negative weight implies lower volatility is better\n",
    "            # RSI is handled separately, no weight needed here.\n",
    "        }\n",
    "        # Ensure weights match the number of columns\n",
    "        momentum_cols_list = ['Perf 3D %', 'Perf Week %', 'Perf Month %', 'Perf Quart %']\n",
    "        momentum_weights_list = [0.4, 0.3, 0.2, 0.1] # Decay weights for momentum\n",
    "        if len(momentum_cols_list) != len(momentum_weights_list):\n",
    "             raise ValueError(f\"Mismatch between momentum columns ({len(momentum_cols_list)}) and weights ({len(momentum_weights_list)})\")\n",
    "\n",
    "        column_definitions = {\n",
    "            'sharpe': [f'Sharpe {days}d' for days in time_horizons],\n",
    "            'sortino': [f'Sortino {days}d' for days in time_horizons],\n",
    "            'omega': [f'Omega {days}d' for days in time_horizons],\n",
    "            'momentum': momentum_cols_list,\n",
    "            'momentum_weights': momentum_weights_list, # Store alongside cols for easy access\n",
    "            'sma': ['SMA20 %', 'SMA50 %', 'SMA200 %'],\n",
    "            'volatility': volatility_cols_used, # Use the dynamically determined list\n",
    "            'rsi': ['RSI'] # Needed for _calculate_rsi_score check\n",
    "        }\n",
    "\n",
    "        # Define all columns required for scoring (used for cleaning)\n",
    "        # --- CORRECTED LOGIC ---\n",
    "        required_cols = []\n",
    "        # Identify components actively used based on non-zero weight OR special cases like RSI\n",
    "        active_components_for_cols = [k for k, v in feature_weights.items() if v != 0]\n",
    "        if 'rsi' not in active_components_for_cols: # RSI score is always calculated if column exists\n",
    "             active_components_for_cols.append('rsi')\n",
    "        # Also include volatility columns if they exist, even if weight is 0,\n",
    "        # as they might be needed for analysis or were explicitly prepared.\n",
    "        if 'volatility' not in active_components_for_cols and volatility_cols_used:\n",
    "            active_components_for_cols.append('volatility')\n",
    "\n",
    "        logging.debug(f\"Components considered for required columns: {active_components_for_cols}\")\n",
    "\n",
    "        for key in active_components_for_cols:\n",
    "            if key in column_definitions:\n",
    "                cols = column_definitions[key]\n",
    "                if isinstance(cols, list):\n",
    "                    # Ensure only strings are added (extra safety)\n",
    "                    string_cols = [item for item in cols if isinstance(item, str)]\n",
    "                    if len(string_cols) != len(cols):\n",
    "                         logging.warning(f\"Non-string items found in column list for key '{key}'. Only adding strings: {string_cols}\")\n",
    "                    required_cols.extend(string_cols)\n",
    "                elif isinstance(cols, str): # Handle case like 'rsi': ['RSI'] where cols could be just 'RSI'\n",
    "                     required_cols.append(cols)\n",
    "            else:\n",
    "                 logging.warning(f\"Component '{key}' listed as active but not found in column_definitions.\")\n",
    "\n",
    "        # Ensure unique and sort (now guaranteed to be only strings)\n",
    "        required_cols = sorted(list(set(required_cols)))\n",
    "        # --- END CORRECTED LOGIC ---\n",
    "\n",
    "        logging.info(f\"Columns required for scoring: {required_cols}\")\n",
    "        logging.info(f\"Volatility columns being used: {volatility_cols_used}\")\n",
    "\n",
    "\n",
    "        # --- Data Cleaning (NaN removal based on required columns) ---\n",
    "        initial_rows = len(df_prepared)\n",
    "        # Check which required columns actually exist in the prepared dataframe\n",
    "        existing_required_cols = [col for col in required_cols if col in df_prepared.columns]\n",
    "        missing_for_clean = sorted(list(set(required_cols) - set(existing_required_cols)))\n",
    "        if missing_for_clean:\n",
    "             logging.warning(f\"Some required columns for cleaning are missing from the data: {missing_for_clean}\")\n",
    "\n",
    "        if not existing_required_cols:\n",
    "             logging.error(\"No required columns found in the data. Cannot proceed with cleaning or scoring.\")\n",
    "             raise ValueError(\"No required columns available for scoring.\")\n",
    "\n",
    "        # Clean based on columns that actually exist AND are required\n",
    "        # Convert relevant columns to numeric before checking for NaNs\n",
    "        for col in existing_required_cols:\n",
    "             if not pd.api.types.is_numeric_dtype(df_prepared[col]):\n",
    "                  df_prepared[col] = pd.to_numeric(df_prepared[col], errors='coerce')\n",
    "\n",
    "        clean_mask = df_prepared[existing_required_cols].notna().all(axis=1)\n",
    "        df_clean = df_prepared.loc[clean_mask].copy()\n",
    "        cleaned_rows = len(df_clean)\n",
    "        logging.info(f\"Data cleaning complete. Retained {cleaned_rows} rows out of {initial_rows} (removed {initial_rows - cleaned_rows} rows with NaNs in required columns: {existing_required_cols}).\")\n",
    "\n",
    "        if cleaned_rows == 0:\n",
    "            raise ValueError(\"No valid tickers remaining after cleaning based on required columns.\")\n",
    "\n",
    "        # ===== ADJUST num_tickers IF NEEDED ===== FIX APPLIED HERE\n",
    "        actual_num_tickers = num_tickers\n",
    "        if cleaned_rows < num_tickers:\n",
    "             logging.warning(f\"Only {cleaned_rows} valid tickers remain after cleaning, which is less than the target num_tickers ({num_tickers}). Adjusting selection to {cleaned_rows}.\")\n",
    "             actual_num_tickers = cleaned_rows # Adjust the number to use\n",
    "\n",
    "        # ===== STAGE 3: SCORE CALCULATION =====\n",
    "        composite_score, intermediate_values, raw_vals = calculate_composite_score(\n",
    "            df_clean, feature_weights, column_definitions\n",
    "        )\n",
    "        # Add composite score to the clean dataframe for clustering selection\n",
    "        df_clean['composite_score'] = composite_score\n",
    "\n",
    "\n",
    "        # ===== STAGE 4: CLUSTERING =====\n",
    "        # Use the potentially adjusted 'actual_num_tickers'\n",
    "        top_n_df, cluster_assignments, corr_subset = perform_clustering(\n",
    "            df_clean, df_corr, actual_num_tickers, num_clusters, score_col='composite_score'\n",
    "        )\n",
    "\n",
    "        # Assemble the comprehensive zscore_df regardless of clustering success\n",
    "        # It now handles None cluster_assignments internally\n",
    "        zscore_df = _assemble_output_df(\n",
    "            intermediate_values, raw_vals, composite_score, cluster_assignments, df_clean.index\n",
    "        )\n",
    "\n",
    "        # Check if clustering failed (indicated by None returns)\n",
    "        if top_n_df is None or cluster_assignments is None:\n",
    "             logging.error(\"Clustering step failed. Portfolio analysis based on clusters cannot proceed.\")\n",
    "             # No cluster_stats or detailed_clusters can be generated\n",
    "             cluster_stats_df = None\n",
    "             detailed_clusters_df = None\n",
    "        else:\n",
    "            # ===== STAGE 5: PORTFOLIO SELECTION & ANALYSIS =====\n",
    "            # Only run analysis if clustering was successful\n",
    "            detailed_clusters_df, cluster_stats_df = analyze_clusters(\n",
    "                cluster_assignments, top_n_df, df_cov, corr_subset, df_clean, score_col='composite_score'\n",
    "            )\n",
    "            if detailed_clusters_df is None or cluster_stats_df is None:\n",
    "                logging.warning(\"Cluster analysis step ran but failed to produce results.\")\n",
    "                # Ensure they are None if analysis failed\n",
    "\n",
    "\n",
    "        # ===== STAGE 6: OUTPUT LOGGING =====\n",
    "        # Log whatever results were successfully generated\n",
    "        log_results_to_file(results_file, cluster_stats_df, detailed_clusters_df, zscore_df, volatility_cols_used)\n",
    "\n",
    "        logging.info(\"=\"*30 + \" Portfolio Optimization Process Completed \" + \"=\"*30)\n",
    "        # Assign final results\n",
    "        zscore_df_final = zscore_df\n",
    "        cluster_stats_df_final = cluster_stats_df\n",
    "        detailed_clusters_df_final = detailed_clusters_df\n",
    "        return zscore_df_final, cluster_stats_df_final, detailed_clusters_df_final\n",
    "\n",
    "    except ValueError as ve:\n",
    "        logging.error(f\"A configuration or data validation error occurred: {str(ve)}\", exc_info=True)\n",
    "        # Attempt to log partial results\n",
    "        try:\n",
    "            log_results_to_file(results_file, cluster_stats_df, detailed_clusters_df, zscore_df, volatility_cols_used)\n",
    "        except Exception as log_e:\n",
    "            logging.error(f\"Additionally, failed to log partial results after ValueError: {log_e}\")\n",
    "        return None, None, None # Indicate failure\n",
    "    except KeyError as ke:\n",
    "         logging.error(f\"A required column or index was not found: {str(ke)}\", exc_info=True)\n",
    "         logging.error(\"Please check input DataFrames (data, corr, cov) and column definitions.\")\n",
    "         try:\n",
    "            log_results_to_file(results_file, cluster_stats_df, detailed_clusters_df, zscore_df, volatility_cols_used)\n",
    "         except Exception as log_e:\n",
    "            logging.error(f\"Additionally, failed to log partial results after KeyError: {log_e}\")\n",
    "         return None, None, None # Indicate failure\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred during portfolio optimization: {str(e)}\", exc_info=True)\n",
    "        # Attempt to log whatever results might have been generated before the error\n",
    "        try:\n",
    "             # FIX APPLIED HERE: Use results_file\n",
    "             log_results_to_file(results_file, cluster_stats_df, detailed_clusters_df, zscore_df, volatility_cols_used)\n",
    "        except Exception as log_e:\n",
    "             logging.error(f\"Additionally, failed to log partial results after unexpected error: {log_e}\", exc_info=True) # Log exception details\n",
    "        return None, None, None # Indicate failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os # Optional: To create directories\n",
    "\n",
    "# Define separate paths\n",
    "log_dir = r\"..\\logs\" # Example directory for logs\n",
    "results_dir = r\"..\\picks\" # Example directory for results\n",
    "log_filename = f\"{date_str}_optimizer.log\"\n",
    "results_filename = f\"{date_str}_portfolio.txt\"\n",
    "\n",
    "log_file_path = os.path.join(log_dir, log_filename)\n",
    "results_file_path = os.path.join(results_dir, results_filename)\n",
    "\n",
    "# --- Call the optimizer function with separate paths ---\n",
    "zscore_df, cluster_stats_df, detailed_clusters_df = portfolio_optimizer_60days_refactored(\n",
    "    df_data=df_data,\n",
    "    df_corr=df_corr,\n",
    "    df_cov=df_cov,\n",
    "    num_clusters=60,\n",
    "    num_tickers=len(df_data),\n",
    "    log_file=log_file_path,\n",
    "    results_file=results_file_path,\n",
    ")\n",
    "\n",
    "# --- !!! KEEP THE SHUTDOWN CALL to close the LOG file handle !!! ---\n",
    "logging.shutdown()\n",
    "# --- !!! KEEP THE SHUTDOWN CALL !!! ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def select_stocks_from_clusters(cluster_stats_df, detailed_clusters_df, \n",
    "                               num_clusters=3, stocks_per_cluster=5,\n",
    "                               min_cluster_size=5):\n",
    "    \"\"\"\n",
    "    REMOVED (2025-04-06)\n",
    "        # # Filter by volatility threshold\n",
    "        # cluster_stocks = cluster_stocks[cluster_stocks['Volatility'] <= volatility_threshold]\n",
    "\n",
    "    Pipeline to select stocks from better performing clusters\n",
    "    \n",
    "    Parameters:\n",
    "    - cluster_stats_df: DataFrame with cluster statistics\n",
    "    - detailed_clusters_df: DataFrame with detailed cluster information\n",
    "    - num_clusters: Number of top clusters to select\n",
    "    - stocks_per_cluster: Number of stocks to select from each cluster\n",
    "    - min_cluster_size: Minimum size for a cluster to be considered\n",
    "    \n",
    "    Returns:\n",
    "    - selected_stocks: DataFrame of selected stocks with their metrics\n",
    "    - cluster_performance: DataFrame of selected clusters with their metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    # ===== 1. Filter and Rank Clusters =====\n",
    "    # Filter clusters by minimum size\n",
    "    qualified_clusters = cluster_stats_df[cluster_stats_df['Size'] >= min_cluster_size].copy()\n",
    "    \n",
    "    # Calculate composite cluster score (weighted average of raw score and correlation)\n",
    "    qualified_clusters['Composite_Cluster_Score'] = (\n",
    "        0.7 * qualified_clusters['Avg_Raw_Score'] +  # Using Raw Score for selection\n",
    "        0.3 * (1 - qualified_clusters['Avg_IntraCluster_Corr'])  # Prefer less correlated clusters\n",
    "    )\n",
    "\n",
    "    # Sort clusters by composite score\n",
    "    ranked_clusters = qualified_clusters.sort_values('Composite_Cluster_Score', ascending=False)\n",
    "    \n",
    "    # Select top N clusters\n",
    "    selected_clusters = ranked_clusters.head(num_clusters)\n",
    "    cluster_ids = selected_clusters['Cluster_ID'].tolist()\n",
    "    \n",
    "    # ===== 2. Select Stocks from Each Cluster =====\n",
    "    selected_stocks_list = []\n",
    "    \n",
    "    for cluster_id in cluster_ids:\n",
    "        # Get stocks from this cluster\n",
    "        cluster_stocks = detailed_clusters_df[detailed_clusters_df['Cluster_ID'] == cluster_id]\n",
    "        \n",
    "        # # Filter by volatility threshold\n",
    "        # cluster_stocks = cluster_stocks[cluster_stocks['Volatility'] <= volatility_threshold]\n",
    "        \n",
    "        if len(cluster_stocks) > 0:\n",
    "            # Sort by risk-adjusted score and select top stocks\n",
    "            top_stocks = cluster_stocks.sort_values('Risk_Adj_Score', ascending=False).head(stocks_per_cluster)\n",
    "            \n",
    "            # Add cluster metrics to each stock\n",
    "            cluster_metrics = selected_clusters[selected_clusters['Cluster_ID'] == cluster_id].iloc[0]\n",
    "            for col in ['Composite_Cluster_Score', 'Avg_IntraCluster_Corr', 'Avg_Volatility', \n",
    "                       'Avg_Raw_Score', 'Avg_Risk_Adj_Score']:\n",
    "                top_stocks[f'Cluster_{col}'] = cluster_metrics[col]\n",
    "            \n",
    "            selected_stocks_list.append(top_stocks)\n",
    "    \n",
    "    # Combine all selected stocks\n",
    "    if selected_stocks_list:\n",
    "        selected_stocks = pd.concat(selected_stocks_list)\n",
    "        \n",
    "        # Calculate position sizing weights (based on risk-adjusted scores)\n",
    "        selected_stocks['Weight'] = (selected_stocks['Risk_Adj_Score'] / \n",
    "                                   selected_stocks['Risk_Adj_Score'].sum())\n",
    "        \n",
    "        # Sort by cluster then by score\n",
    "        selected_stocks = selected_stocks.sort_values(['Cluster_ID', 'Risk_Adj_Score'], \n",
    "                                                   ascending=[True, False])\n",
    "    else:\n",
    "        selected_stocks = pd.DataFrame()\n",
    "        print(\"Warning: No stocks met selection criteria\")\n",
    "    \n",
    "    # ===== 3. Prepare Enhanced Output Reports =====\n",
    "    # Cluster performance report - now showing both score types\n",
    "    cluster_performance = selected_clusters.copy()\n",
    "    cluster_performance['Stocks_Selected'] = cluster_performance['Cluster_ID'].apply(\n",
    "        lambda x: len(selected_stocks[selected_stocks['Cluster_ID'] == x]) if not selected_stocks.empty else 0)\n",
    "    \n",
    "    # Add diversification metrics\n",
    "    if not selected_stocks.empty:\n",
    "        cluster_performance['Intra_Cluster_Diversification'] = 1 - cluster_performance['Avg_IntraCluster_Corr']\n",
    "    \n",
    "    return selected_stocks, cluster_performance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the selection pipeline\n",
    "selected_stocks, cluster_performance = select_stocks_from_clusters(\n",
    "    cluster_stats_df=cluster_stats_df,\n",
    "    detailed_clusters_df=detailed_clusters_df,\n",
    "    num_clusters=3,\n",
    "    stocks_per_cluster=3,\n",
    "    min_cluster_size=5,\n",
    ")\n",
    "\n",
    "# Enhanced Output Display\n",
    "print(\"\\n=== CLUSTER SELECTION CRITERIA ===\")\n",
    "print(\"* Using Avg_Raw_Score for cluster selection\")\n",
    "print(\"* Using Risk_Adj_Score for stock selection within clusters\")\n",
    "print(f\"* Selected top {len(cluster_performance)} clusters from {len(cluster_stats_df)} total\")\n",
    "\n",
    "print(\"\\n=== SELECTED CLUSTERS (RANKED BY RAW SCORE) ===\")\n",
    "display_cols = ['Cluster_ID', 'Size', 'Avg_Raw_Score', 'Avg_Risk_Adj_Score', \n",
    "                'Avg_IntraCluster_Corr', 'Avg_Volatility', 'Composite_Cluster_Score',\n",
    "                'Stocks_Selected', 'Intra_Cluster_Diversification']\n",
    "print(cluster_performance[display_cols].sort_values('Avg_Raw_Score', ascending=False).to_string(index=False))\n",
    "\n",
    "# NEW: Print top 8 stocks by Raw_Score for each selected cluster\n",
    "print(\"\\n=== TOP STOCKS BY RAW SCORE PER CLUSTER ===\")\n",
    "print(\"\"\"* Volatility is the standard deviation of daily returns over the past 250 trading days\n",
    "* Volatility for ' GLD' = sqrt(df_cov.loc['GLD', 'GLD'])\n",
    "* volatility_threshold = 0.3 \n",
    "* A standard deviation of 0.3 means the stock's price typically moves +/- 30% per day relative to its average daily return.\n",
    "* Annualized Volatility  Daily Volatility * sqrt(252)   0.3 * 15.87  4.76 or 476%\"\"\") \n",
    "\n",
    "for cluster_id in cluster_performance['Cluster_ID']:\n",
    "    cluster_stocks = detailed_clusters_df[detailed_clusters_df['Cluster_ID'] == cluster_id]\n",
    "    top_raw = cluster_stocks.nlargest(8, 'Raw_Score')[['Ticker', 'Raw_Score', 'Risk_Adj_Score', 'Volatility']]\n",
    "    \n",
    "    print(f\"Cluster {cluster_id} - Top 8 by Raw Score:\")\n",
    "    print(top_raw.to_string(index=False))\n",
    "    print(f\"Cluster Avg Raw Score: {cluster_performance[cluster_performance['Cluster_ID'] == cluster_id]['Avg_Raw_Score'].values[0]:.2f}\")\n",
    "    print(f\"Cluster Avg Risk Adj Score: {cluster_performance[cluster_performance['Cluster_ID'] == cluster_id]['Avg_Risk_Adj_Score'].values[0]:.2f}\")\n",
    "\n",
    "print(\"\\n=== FINAL SELECTED STOCKS (BY RISK-ADJ SCORE) ===\")\n",
    "print(\"* Stocks actually selected based on Risk_Adj_Score within each cluster\")\n",
    "print(\"* Position weights assigned based on Risk_Adj_Score\")\n",
    "\n",
    "available_cols = [col for col in ['Cluster_ID', 'Ticker', 'Raw_Score', 'Risk_Adj_Score', \n",
    "                                'Volatility', 'Weight', 'Cluster_Avg_Raw_Score',\n",
    "                                'Cluster_Avg_Risk_Adj_Score'] \n",
    "                  if col in selected_stocks.columns]\n",
    "\n",
    "print(selected_stocks[available_cols].sort_values(['Cluster_ID', 'Risk_Adj_Score'], \n",
    "                                                ascending=[True, False]).to_string(index=False))\n",
    "\n",
    "# Calculate and print portfolio summary\n",
    "if not selected_stocks.empty:\n",
    "    print(\"\\n=== PORTFOLIO SUMMARY ===\")\n",
    "    print(f\"Total Stocks Selected: {len(selected_stocks)}\")\n",
    "    print(f\"Average Raw Score: {selected_stocks['Raw_Score'].mean():.2f}\")\n",
    "    print(f\"Average Risk-Adjusted Score: {selected_stocks['Risk_Adj_Score'].mean():.2f}\")\n",
    "    print(f\"Average Volatility: {selected_stocks['Volatility'].mean():.2f}\")\n",
    "    print(\"\\nCluster Distribution:\")\n",
    "    print(selected_stocks['Cluster_ID'].value_counts().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Columns in cluster_performance:\", cluster_performance.columns)\n",
    "# Original line 22:\n",
    "print(cluster_performance[display_cols].sort_values('Avg_Raw_Score', ascending=False).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if ticker 'AM' exists in detailed_clusters_df\n",
    "has_ticker = 'AM' in detailed_clusters_df['Ticker'].values\n",
    "\n",
    "# Print the result\n",
    "if has_ticker:\n",
    "  # Get the row for AM\n",
    "  am_details = detailed_clusters_df[detailed_clusters_df['Ticker'] == 'AM']\n",
    "  print(f\"Ticker 'AM' found in cluster {am_details['Cluster_ID'].values[0]}\")\n",
    "  print(\"\\nDetails for AM:\")\n",
    "  print(am_details.to_string())\n",
    "else:\n",
    "  print(\"Ticker 'AM' not found in detailed_clusters_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if 'AM' is present in df_cov\n",
    "has_ticker_cov = 'AM' in df_cov.index and 'AM' in df_cov.columns\n",
    "\n",
    "print(f\"Is 'AM' present in covariance matrix? {has_ticker_cov}\")\n",
    "\n",
    "if has_ticker_cov:\n",
    "  # Print covariance for AM vs itself (variance) if exists\n",
    "  print(f\"\\nVariance for AM: {df_cov.loc['AM', 'AM']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cov.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_cluster_ids =selected_stocks['Cluster_ID'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "PLOT_COLORS = ['lightgreen', 'skyblue', 'salmon', 'gold', 'orchid',\n",
    "               'lightcoral', 'deepskyblue', 'mediumpurple', 'darkseagreen', 'tan']\n",
    "HIGHLIGHT_COLOR = 'red'  # Color for highlighting selected clusters\n",
    "HIGHLIGHT_COLOR = 'green'  # Color for highlighting selected clusters\n",
    "\n",
    "# Create a figure with subplots\n",
    "fig, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(5, 1, figsize=(12, 12))\n",
    "fig.suptitle('Cluster Statistics Analysis')\n",
    "\n",
    "# Function to create bars with highlighted clusters\n",
    "def create_bars(ax, x, y, color, highlight_ids):\n",
    "    bars = ax.bar(x, y, color=[HIGHLIGHT_COLOR if cl_id in highlight_ids else color for cl_id in x])\n",
    "    return bars\n",
    "\n",
    "# Plot 1 Average Risk-Adjusted Score\n",
    "create_bars(ax1, cluster_stats_df['Cluster_ID'], cluster_stats_df['Avg_Risk_Adj_Score'], \n",
    "            'skyblue', selected_cluster_ids)\n",
    "ax1.set_title('Average Risk-Adjusted Scores by Cluster')\n",
    "ax1.set_xlabel('Cluster_ID')\n",
    "ax1.set_ylabel('Average Risk-Adjusted Score')\n",
    "\n",
    "# Plot 2 Average Raw Score\n",
    "create_bars(ax2, cluster_stats_df['Cluster_ID'], cluster_stats_df['Avg_Raw_Score'], \n",
    "            'lightgreen', selected_cluster_ids)\n",
    "ax2.set_title('Average RawScores by Cluster')\n",
    "ax2.set_xlabel('Cluster_ID')\n",
    "ax2.set_ylabel('Average Raw Score')\n",
    "\n",
    "# Plot 3: Average Correlation\n",
    "create_bars(ax3, cluster_stats_df['Cluster_ID'], cluster_stats_df['Avg_IntraCluster_Corr'], \n",
    "            'salmon', selected_cluster_ids)\n",
    "ax3.set_title('Average Correlation within Clusters')\n",
    "ax3.set_xlabel('Cluster_ID')\n",
    "ax3.set_ylabel('Average Correlation')\n",
    "\n",
    "# Plot 4: Average Volatility\n",
    "create_bars(ax4, cluster_stats_df['Cluster_ID'], cluster_stats_df['Avg_Volatility'], \n",
    "            'gold', selected_cluster_ids)\n",
    "ax4.set_title('Average Volatility within Clusters')\n",
    "ax4.set_xlabel('Cluster_ID')\n",
    "ax4.set_ylabel('Average Volatility')\n",
    "\n",
    "# Plot 5 Cluster Size\n",
    "create_bars(ax5, cluster_stats_df['Cluster_ID'], cluster_stats_df['Size'], \n",
    "            'orchid', selected_cluster_ids)\n",
    "ax5.set_title('Cluster Sizes')\n",
    "ax5.set_xlabel('Cluster_ID')\n",
    "ax5.set_ylabel('Number of Members')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Drop Tickers \n",
    "drop_tickers = ['BECN']\n",
    "selected_stocks = selected_stocks[~selected_stocks['Ticker'].isin(drop_tickers)].copy()\n",
    "\n",
    "# Step 1: Drop Cluster \n",
    "drop_cluster = 0\n",
    "selected_stocks = selected_stocks[selected_stocks['Cluster_ID'] != drop_cluster].copy()\n",
    "\n",
    "# Step 2: Recalculate weights based on remaining stocks' Risk_Adj_Scores\n",
    "selected_stocks['Weight'] = selected_stocks['Risk_Adj_Score'] / selected_stocks['Risk_Adj_Score'].sum()\n",
    "\n",
    "# Step 3: Display the updated portfolio\n",
    "print(f\"\\n=== UPDATED PORTFOLIO (CLUSTER {drop_cluster}, TICKER {drop_tickers} REMOVED) ===\")\n",
    "# sorted_selected_stocks_df = selected_stocks[['Cluster_ID', 'Ticker', 'Raw_Score', 'Risk_Adj_Score', 'Weight', 'Volatility']] \\\n",
    "#               .sort_values(['Weight', 'Cluster_ID'], ascending=[False, False])\n",
    "# Select columns, sort, and then set 'Ticker' as the index\n",
    "sorted_selected_stocks_df = selected_stocks[['Cluster_ID', 'Ticker', 'Raw_Score', 'Risk_Adj_Score', 'Weight', 'Volatility']] \\\n",
    "              .sort_values(['Weight', 'Cluster_ID'], ascending=[False, False]) \\\n",
    "              .set_index('Ticker')\n",
    "\n",
    "\n",
    "\n",
    "print(sorted_selected_stocks_df.to_string(index=True))\n",
    "\n",
    "# Step 4: Extract the 'Ticker' column from the sorted DataFrame\n",
    "ticker_order = sorted_selected_stocks_df.index.tolist()\n",
    "\n",
    "\n",
    "# Optional: Print weight redistribution summary\n",
    "original_total = 1.0\n",
    "new_total = selected_stocks['Weight'].sum()\n",
    "print(f\"\\nWeights redistributed from Cluster {drop_cluster} & Ticker {drop_tickers}: {original_total - new_total:.1%}\")\n",
    "print(f\"New total weights sum to: {new_total:.0%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_selected_stocks_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_selected_stocks = pd.concat([sorted_selected_stocks_df, df_data.loc[ticker_order]], axis=1)\n",
    "final_selected_stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_cluster_ids = final_selected_stocks['Cluster_ID'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "PLOT_COLORS = ['lightgreen', 'skyblue', 'salmon', 'gold', 'orchid',\n",
    "               'lightcoral', 'deepskyblue', 'mediumpurple', 'darkseagreen', 'tan']\n",
    "HIGHLIGHT_COLOR = 'red'  # Color for highlighting selected clusters\n",
    "HIGHLIGHT_COLOR = 'green'  # Color for highlighting selected clusters\n",
    "\n",
    "# Create a figure with subplots\n",
    "fig, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(5, 1, figsize=(12, 12))\n",
    "fig.suptitle('Cluster Statistics Analysis')\n",
    "\n",
    "# Function to create bars with highlighted clusters\n",
    "def create_bars(ax, x, y, color, highlight_ids):\n",
    "    bars = ax.bar(x, y, color=[HIGHLIGHT_COLOR if cl_id in highlight_ids else color for cl_id in x])\n",
    "    return bars\n",
    "\n",
    "# Plot 1 Average Risk-Adjusted Score\n",
    "create_bars(ax1, cluster_stats_df['Cluster_ID'], cluster_stats_df['Avg_Risk_Adj_Score'], \n",
    "            'skyblue', final_cluster_ids)\n",
    "ax1.set_title('Average Risk-Adjusted Scores by Cluster')\n",
    "ax1.set_xlabel('Cluster_ID')\n",
    "ax1.set_ylabel('Average Risk-Adjusted Score')\n",
    "\n",
    "# Plot 2 Average Raw Score\n",
    "create_bars(ax2, cluster_stats_df['Cluster_ID'], cluster_stats_df['Avg_Raw_Score'], \n",
    "            'lightgreen', final_cluster_ids)\n",
    "ax2.set_title('Average RawScores by Cluster')\n",
    "ax2.set_xlabel('Cluster_ID')\n",
    "ax2.set_ylabel('Average Raw Score')\n",
    "\n",
    "# Plot 3: Average Correlation\n",
    "create_bars(ax3, cluster_stats_df['Cluster_ID'], cluster_stats_df['Avg_IntraCluster_Corr'], \n",
    "            'salmon', final_cluster_ids)\n",
    "ax3.set_title('Average Correlation within Clusters')\n",
    "ax3.set_xlabel('Cluster_ID')\n",
    "ax3.set_ylabel('Average Correlation')\n",
    "\n",
    "# Plot 4: Average Volatility\n",
    "create_bars(ax4, cluster_stats_df['Cluster_ID'], cluster_stats_df['Avg_Volatility'], \n",
    "            'gold', final_cluster_ids)\n",
    "ax4.set_title('Average Volatility within Clusters')\n",
    "ax4.set_xlabel('Cluster_ID')\n",
    "ax4.set_ylabel('Average Volatility')\n",
    "\n",
    "# Plot 5 Cluster Size\n",
    "create_bars(ax5, cluster_stats_df['Cluster_ID'], cluster_stats_df['Size'], \n",
    "            'orchid', final_cluster_ids)\n",
    "ax5.set_title('Cluster Sizes')\n",
    "ax5.set_xlabel('Cluster_ID')\n",
    "ax5.set_ylabel('Number of Members')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_filename_tsv = f\"..\\picks\\{date_str}_selected_stocks.tsv\"\n",
    "\n",
    "final_selected_stocks.to_csv(output_filename_tsv, sep='\\t', encoding='utf-8', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_selected_stocks.to_parquet(f'..\\picks\\{date_str}_selected_stocks.parquet', engine='pyarrow', compression='zstd')\n",
    "cluster_stats_df.to_parquet(f'..\\picks\\{date_str}_cluster_stats_df.parquet', engine='pyarrow', compression='zstd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zscore_df.to_parquet(f'..\\data\\{date_str}_zscore_df.parquet', engine='pyarrow', compression='zstd')\n",
    "detailed_clusters_df.to_parquet(f'..\\data\\{date_str}_detailed_clusters_df.parquet', engine='pyarrow', compression='zstd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate summary statistics for cluster_stats_df\n",
    "stats_summary = pd.DataFrame({\n",
    "    'Count': cluster_stats_df.count(numeric_only=True),\n",
    "    'Sum': cluster_stats_df.sum(numeric_only=True),\n",
    "    'Mean': cluster_stats_df.mean(numeric_only=True),\n",
    "    'Std': cluster_stats_df.std(numeric_only=True),\n",
    "    'Mean+1Std (68%)': cluster_stats_df.mean(numeric_only=True) + cluster_stats_df.std(numeric_only=True),\n",
    "    'Mean-1Std (68%)': cluster_stats_df.mean(numeric_only=True) - cluster_stats_df.std(numeric_only=True),\n",
    "    'Mean+2Std (95%)': cluster_stats_df.mean(numeric_only=True) + 2*cluster_stats_df.std(numeric_only=True),\n",
    "    'Mean-2Std (95%)': cluster_stats_df.mean(numeric_only=True) - 2*cluster_stats_df.std(numeric_only=True),\n",
    "    'Min': cluster_stats_df.min(numeric_only=True),\n",
    "    'Max': cluster_stats_df.max(numeric_only=True),\n",
    "})\n",
    "\n",
    "print(\"Summary Statistics for Cluster Data:\")\n",
    "display(stats_summary.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by Avg_Raw_Score in descending order\n",
    "sorted_by_Avg_Raw_Score = cluster_stats_df.sort_values('Avg_Raw_Score', ascending=False)\n",
    "print(f'sorted_by_Avg_Raw_Score')\n",
    "display(sorted_by_Avg_Raw_Score.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 200)\n",
    "\n",
    "# --- Add this verification line ---\n",
    "print(f\"Use sorted_df_desc.head(200) to prevent truncation and see the top 200 rows\\n\")\n",
    "# ---\n",
    "\n",
    "sorted_df_desc = detailed_clusters_df.sort_values(by='Risk_Adj_Score', ascending=False)\n",
    "print(sorted_df_desc.head(200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 200)\n",
    "\n",
    "print(detailed_clusters_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if 'GLD' is present in df_cov\n",
    "has_ticker_cov = 'GLD' in df_cov.index and 'GLD' in df_cov.columns\n",
    "\n",
    "print(f\"Is 'GLD' present in covariance matrix? {has_ticker_cov}\")\n",
    "\n",
    "if has_ticker_cov:\n",
    "  # Print covariance for GLD vs itself (variance) if exists\n",
    "  print(f\"\\nVariance for GLD: {df_cov.loc['GLD', 'GLD']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if 'GLD' is present in df_corr\n",
    "has_ticker_cov = 'GLD' in df_corr.index and 'GLD' in df_corr.columns\n",
    "\n",
    "print(f\"Is 'GLD' present in covariance matrix? {has_ticker_cov}\")\n",
    "\n",
    "if has_ticker_cov:\n",
    "  # Print covariance for GLD vs itself (variance) if exists\n",
    "  print(f\"\\nVariance for GLD: {df_corr.loc['GLD', 'GLD']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zscore_df.loc['GLD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
