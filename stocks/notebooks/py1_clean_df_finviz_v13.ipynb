{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process data in finviz download\n",
    "#### Concatenate columns [\"Sector\", \"Industry\", \"Single Category\", \"Asset Type\"] to Info column\n",
    "#### Create \"MktCap AUM\" column by combining \"Market Cap\" and \"AUM\" columns\n",
    "#### Process columns with values end on K,M,B,T\n",
    "- convert to numeric in units of million\n",
    "- add suffix \", M' to their column names\n",
    "#### Process columns with values end in %\n",
    "- convert to numeric\n",
    "- add suffix \" %\" to their column names\n",
    "#### Sort by \"MktCap AUM, M\" in descending order\n",
    "#### Set Ticker as index\n",
    "#### Add Rank column with largest \"MktCap AUM\" ranked as 1     \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Python will look in these locations:\n",
      "['C:\\\\Users\\\\ping\\\\.pyenv\\\\pyenv-win\\\\versions\\\\3.11.9\\\\python311.zip', 'C:\\\\Users\\\\ping\\\\.pyenv\\\\pyenv-win\\\\versions\\\\3.11.9\\\\DLLs', 'C:\\\\Users\\\\ping\\\\.pyenv\\\\pyenv-win\\\\versions\\\\3.11.9\\\\Lib', 'C:\\\\Users\\\\ping\\\\.pyenv\\\\pyenv-win\\\\versions\\\\3.11.9', 'c:\\\\Users\\\\ping\\\\Files_win10\\\\python\\\\py311\\\\.venv', '', 'c:\\\\Users\\\\ping\\\\Files_win10\\\\python\\\\py311\\\\.venv\\\\Lib\\\\site-packages', 'c:\\\\Users\\\\ping\\\\Files_win10\\\\python\\\\py311\\\\.venv\\\\Lib\\\\site-packages\\\\win32', 'c:\\\\Users\\\\ping\\\\Files_win10\\\\python\\\\py311\\\\.venv\\\\Lib\\\\site-packages\\\\win32\\\\lib', 'c:\\\\Users\\\\ping\\\\Files_win10\\\\python\\\\py311\\\\.venv\\\\Lib\\\\site-packages\\\\Pythonwin', 'c:\\\\Users\\\\ping\\\\Files_win10\\\\python\\\\py311\\\\stocks\\\\src', 'c:\\\\Users\\\\ping\\\\Files_win10\\\\python\\\\py311\\\\.venv\\\\Lib\\\\site-packages\\\\setuptools\\\\_vendor', 'c:\\\\Users\\\\ping\\\\Files_win10\\\\python\\\\py311\\\\stocks\\\\src']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import os\n",
    "from IPython.display import display, Markdown  # Assuming you use these for display\n",
    "\n",
    "# Set pandas display options to show more columns and rows\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.max_rows', 200)       # Limit to 10 rows for readability\n",
    "pd.set_option('display.width', 2500) \n",
    "\n",
    "# Notebook cell\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Get root directory (assuming notebook is in root/notebooks/)\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "ROOT_DIR = NOTEBOOK_DIR.parent if NOTEBOOK_DIR.name == 'notebooks' else NOTEBOOK_DIR\n",
    "\n",
    "# Add src directory to Python path\n",
    "sys.path.append(str(ROOT_DIR / 'src'))\n",
    "\n",
    "# Verify path\n",
    "print(f\"Python will look in these locations:\\n{sys.path}\")\n",
    "\n",
    "\n",
    "# --- Execute the processor ---\n",
    "import utils\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'DATE_STR' from 'config' (c:\\Users\\ping\\Files_win10\\python\\py311\\stocks\\notebooks\\config.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# process_files.py\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DATE_STR, DOWNLOAD_DIR, DEST_DIR\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path  \u001b[38;5;66;03m# Better path handling\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# ###########################\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# DATE_STR = '2025-04-25'  # Example date string, replace with your actual date string\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# ###########################\u001b[39;00m\n\u001b[32m      9\u001b[39m \n\u001b[32m     10\u001b[39m \n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Build paths\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'DATE_STR' from 'config' (c:\\Users\\ping\\Files_win10\\python\\py311\\stocks\\notebooks\\config.py)"
     ]
    }
   ],
   "source": [
    "# process_files.py\n",
    "from config import DATE_STR, DOWNLOAD_DIR, DEST_DIR\n",
    "from pathlib import Path  # Better path handling\n",
    "\n",
    "\n",
    "# ###########################\n",
    "# DATE_STR = '2025-04-25'  # Example date string, replace with your actual date string\n",
    "# ###########################\n",
    "\n",
    "\n",
    "# Build paths\n",
    "source_path = Path(DOWNLOAD_DIR) / f'df_finviz_{DATE_STR}_stocks_etfs.parquet'\n",
    "dest_path = Path(DEST_DIR) / f'{DATE_STR}_df_finviz_stocks_etfs.parquet'\n",
    "\n",
    "print(f\"source_path: {source_path}\")\n",
    "print(f\"dest_path: {dest_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(source_path, engine='pyarrow')\n",
    "display(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _df_etfs = df.sort_values(by=\"AUM, M\", ascending=False)\n",
    "_df_etfs = df.sort_values(by=\"AUM\", ascending=False)\n",
    "print(_df_etfs.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _df_stocks = df.sort_values(by=\"AUM, M\", ascending=False)\n",
    "_df_stocks = df.sort_values(by=\"Market Cap\", ascending=False)\n",
    "print(_df_stocks.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to concatenate\n",
    "columns_to_concat = [\"Sector\", \"Industry\", \"Single Category\", \"Asset Type\"]\n",
    "\n",
    "# Replace '-' with empty string in specified columns\n",
    "for col in columns_to_concat:\n",
    "    df[col] = df[col].replace('-', '')\n",
    "\n",
    "# Concatenate the columns, handling empty strings, and remove extra spaces\n",
    "df['Info'] = df[columns_to_concat].apply(lambda row: ', '.join(filter(None, row.astype(str))), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the columns 'Market Cap' and 'AUM'\n",
    "# Replace '-' with empty string in both columns\n",
    "\n",
    "df['MktCap AUM'] = df['Market Cap'].replace('-', '') + df['AUM'].replace('-', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re # Keep this if using the regex-based check from previous step\n",
    "\n",
    "# --- Assume df is your DataFrame from the previous step ---\n",
    "# --- Re-run identification or use the provided list ---\n",
    "\n",
    "# Option 1: Re-run the identification (Recommended for robustness)\n",
    "# Define the suffixes we are looking for abbreviation\n",
    "abbreviation_suffixes = ('B', 'M', 'K', 'T')\n",
    "\n",
    "def check_numeric_abbreviation(series):\n",
    "    \"\"\"\n",
    "    Checks if a Series contains string values that look like\n",
    "    abbreviated numbers (e.g., '10.5M', '2B', '500K').\n",
    "    It verifies both the suffix and that the prefix is numeric.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        s_str = series.dropna().astype(str).str.strip()\n",
    "        if s_str.empty: return False\n",
    "        ends_with_suffix_mask = s_str.str.upper().str.endswith(abbreviation_suffixes)\n",
    "        if not ends_with_suffix_mask.any(): return False\n",
    "        candidates = s_str[ends_with_suffix_mask]\n",
    "        prefixes = candidates.str[:-1].str.strip()\n",
    "        # Allow for potential negative signs or commas if needed, pd.to_numeric is good\n",
    "        numeric_prefixes = pd.to_numeric(prefixes, errors='coerce')\n",
    "        return numeric_prefixes.notna().any()\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "is_numeric_abbreviation_col = df.apply(check_numeric_abbreviation, axis=0)\n",
    "columns_to_convert = is_numeric_abbreviation_col[is_numeric_abbreviation_col].index.tolist()\n",
    "\n",
    "# Option 2: Use the list you provided (if you are certain it's correct)\n",
    "# columns_to_convert = [\n",
    "#     'Market Cap', 'Sales', 'Income', 'Outstanding', 'Float',\n",
    "#     'Short Interest', 'Avg Volume', 'AUM', 'Flows 1M', 'Flows 3M', 'Flows YTD'\n",
    "# ]\n",
    "# Note: 'Short Interest' might often be a percentage, double-check if it truly belongs here.\n",
    "# Let's assume the dynamically generated list from Option 1 is more accurate for the example.\n",
    "\n",
    "print(f\"Columns identified for conversion: {columns_to_convert}\")\n",
    "\n",
    "# --- Conversion Logic ---\n",
    "\n",
    "# Define multipliers to convert to Millions\n",
    "multipliers = {\n",
    "    'T': 1_000_000, # Trillion to Million\n",
    "    'B': 1_000,     # Billion to Million\n",
    "    'M': 1,         # Million to Million\n",
    "    'K': 0.001      # Thousand to Million (1/1000)\n",
    "}\n",
    "\n",
    "def convert_to_millions(value):\n",
    "    \"\"\"Converts a string with T/B/M/K suffix to millions.\"\"\"\n",
    "    if pd.isna(value):\n",
    "        return np.nan\n",
    "    \n",
    "    value_str = str(value).strip().upper()\n",
    "    if not value_str:\n",
    "        return np.nan\n",
    "\n",
    "    suffix = value_str[-1]\n",
    "    \n",
    "    if suffix in multipliers:\n",
    "        number_part = value_str[:-1]\n",
    "        try:\n",
    "            # Attempt to convert the part before the suffix to a float\n",
    "            number = float(number_part)\n",
    "            # Apply the multiplier\n",
    "            return number * multipliers[suffix]\n",
    "        except ValueError:\n",
    "            # The part before the suffix wasn't a valid number\n",
    "            return np.nan\n",
    "    else:\n",
    "        # No recognized suffix (T, B, M, K)\n",
    "        # Optionally, handle plain numbers if necessary, otherwise return NaN\n",
    "        try:\n",
    "            # Could it be a plain number already (treat as raw value)?\n",
    "            # If you want plain numbers (e.g., 5000000) to be converted to millions:\n",
    "            # return float(value_str) / 1_000_000\n",
    "            # ---\n",
    "            # For now, only convert if suffix is present, return NaN otherwise\n",
    "            return np.nan \n",
    "        except ValueError:\n",
    "            return np.nan # It wasn't a plain number either\n",
    "\n",
    "# --- Apply Conversion and Rename ---\n",
    "\n",
    "new_column_names = {}\n",
    "print(\"\\nConverting columns to Millions:\")\n",
    "\n",
    "for col in columns_to_convert:\n",
    "    if col in df.columns: # Ensure column still exists\n",
    "        print(f\"- Processing: {col}\")\n",
    "        original_dtype = df[col].dtype\n",
    "        \n",
    "        # Apply the conversion function\n",
    "        converted_series = df[col].apply(convert_to_millions)\n",
    "        \n",
    "        # Update the DataFrame column\n",
    "        df[col] = converted_series\n",
    "        \n",
    "        # Prepare new name\n",
    "        new_name = f\"{col}, M\"\n",
    "        new_column_names[col] = new_name\n",
    "        print(f\"  ...Converted values and prepared rename to: {new_name}\")\n",
    "    else:\n",
    "         print(f\"- Warning: Column '{col}' not found in DataFrame. Skipping.\")\n",
    "\n",
    "\n",
    "# Rename columns in one go (more efficient)\n",
    "df.rename(columns=new_column_names, inplace=True)\n",
    "\n",
    "print(\"\\nConversion Complete. DataFrame head after conversion:\")\n",
    "print(df.head())\n",
    "\n",
    "# Optionally, check dtypes\n",
    "print(\"\\nData types after conversion:\")\n",
    "print(df.dtypes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'df.dtypes: {df.dtypes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def process_percentage_columns(df):\n",
    "    \"\"\"\n",
    "    Identifies and processes columns in a DataFrame where values end with '%'.\n",
    "    The function cleans, converts to numeric, renames, and prints which columns were modified.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The modified DataFrame with processed percentage columns.\n",
    "                      Returns the original DataFrame if no percentage columns are found.\n",
    "    \"\"\"\n",
    "\n",
    "    # Identify columns where values END WITH '%'\n",
    "    percent_cols = [\n",
    "        col for col in df.columns\n",
    "        if df[col].dtype == 'object'\n",
    "        and df[col].str.strip().str.endswith('%', na=False).any()\n",
    "    ]\n",
    "\n",
    "    # If no percentage columns are found, return the original DataFrame\n",
    "    if not percent_cols:\n",
    "        print(\"No percentage columns found to modify.\")  # Print message\n",
    "        return df\n",
    "\n",
    "    print(\"The following columns ending with % were modified:\") #Print message before the loop\n",
    "\n",
    "    # Process identified percentage columns\n",
    "    for col in percent_cols:\n",
    "        # Clean data: (1) Strip whitespace, (2) Handle '-', (3) Remove trailing %\n",
    "        cleaned_series = (\n",
    "            df[col].str.strip()\n",
    "            .replace('-', np.nan)  # Convert '-' to NaN\n",
    "            .str.replace(r'%$', '', regex=True)  # Remove only ENDING %\n",
    "        )\n",
    "        \n",
    "        # Convert to numeric (coerce invalid values to NaN)\n",
    "        df[col] = pd.to_numeric(cleaned_series, errors='coerce')\n",
    "        \n",
    "        # Rename column\n",
    "        # Check if the specific pattern \"%\" is already present in the column name\n",
    "        if \"%\" not in col:\n",
    "            # If it's NOT present, then rename by appending \" %\"\n",
    "            new_col_name = f\"{col} %\"\n",
    "            df.rename(columns={col: new_col_name}, inplace=True)\n",
    "            print(f\"- Renamed: {col} -> {new_col_name}\") # Indicate the rename happened\n",
    "        else:\n",
    "            # If it IS already present, do nothing to the name, just print\n",
    "            print(f\"- Kept as is: {col} (already contains %)\") # Indicate no rename needed\n",
    "\n",
    "        \n",
    "    return df\n",
    "\n",
    "# Example Usage:\n",
    "# Assuming you have a DataFrame called 'df'\n",
    "# df = process_percentage_columns(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = process_percentage_columns(df)\n",
    "print(f'df.dtypes: {df.dtypes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = [\n",
    "    \"No.\",\n",
    "    \"P/E\",\n",
    "    \"Fwd P/E\",\n",
    "    \"PEG\",\n",
    "    \"P/S\",\n",
    "    \"P/B\",\n",
    "    \"P/C\",\n",
    "    \"P/FCF\",\n",
    "    \"Book/sh\",\n",
    "    \"Cash/sh\",\n",
    "    \"Dividend TTM\",\n",
    "    \"EPS\",\n",
    "    \"EPS next Q\",\n",
    "    \"Short Ratio\",\n",
    "    \"Curr R\",\n",
    "    \"Quick R\",\n",
    "    \"LTDebt/Eq\",\n",
    "    \"Debt/Eq\",\n",
    "    \"Beta\",\n",
    "    \"ATR\",\n",
    "    \"RSI\",\n",
    "    \"Employees\",\n",
    "    \"Recom\",\n",
    "    \"Rel Volume\",\n",
    "    \"Volume\",\n",
    "    \"Target Price\",\n",
    "    \"Prev Close\",\n",
    "    \"Open\",\n",
    "    \"High\",\n",
    "    \"Low\",\n",
    "    \"Price\",\n",
    "    \"Holdings\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert columns with numeric values to float\n",
    "for col in numeric_columns:\n",
    "  # Clean and convert to numeric\n",
    "  df[col] = pd.to_numeric(\n",
    "    df[col].str.replace('$', '').str.replace(',', ''),\n",
    "    errors='coerce'  # Convert invalid values to NaN\n",
    "  )\n",
    "\n",
    "# Verify the conversion\n",
    "for col in numeric_columns:\n",
    "  print(f\"{col}: {df[col].dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_etfs = df.sort_values(by=\"AUM, M\", ascending=False)\n",
    "df_etfs = df.sort_values(by=\"AUM, M\", ascending=False)\n",
    "print(df_etfs.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"df.dtypes:\\n{df.dtypes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df.describe())\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming 'df' is your DataFrame\n",
    "\n",
    "print(\"Original DataFrame head:\")\n",
    "print(df.head())\n",
    "print(\"-\" * 30)\n",
    "\n",
    "print(\"DataFrame Columns:\", df.columns.tolist())\n",
    "print(\"-\" * 30)\n",
    "\n",
    "try:\n",
    "    # 1. Sort the DataFrame by 'MktCap AUM, M' in descending order, in place.\n",
    "    print(f\"Sorting DataFrame by 'MktCap AUM, M'...\")\n",
    "    df.sort_values(by='MktCap AUM, M', ascending=False, inplace=True)\n",
    "    print(\"DataFrame sorted.\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # 2. Set 'Ticker' as the index, if the column exists.\n",
    "    if 'Ticker' in df.columns:\n",
    "        df.set_index('Ticker', inplace=True)\n",
    "        print(\"'Ticker' column successfully set as index.\")\n",
    "    else:\n",
    "        print(\"Warning: 'Ticker' column not found. Skipping setting it as index.\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # 3. Display the head of the modified DataFrame.\n",
    "    print(\"Modified DataFrame head (first 20 rows):\")\n",
    "    print(df.head(20))\n",
    "\n",
    "except KeyError as e:\n",
    "    print(f\"\\nKeyError: The column {e} was not found in the DataFrame.\")\n",
    "    print(\"Please ensure the column name used for sorting ('MktCap AUM, M') exists and is spelled correctly.\")\n",
    "    print(\"Available columns:\", df.columns.tolist())\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn unexpected error occurred: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming your DataFrame 'df' already exists\n",
    "\n",
    "# Sort the DataFrame by the \"MktCap AUM, M\" column in descending order\n",
    "df_sorted = df.sort_values(by=\"MktCap AUM, M\", ascending=False).copy()\n",
    "# Using .copy() here creates a new, non-fragmented DataFrame\n",
    "\n",
    "# Add the \"Rank\" column\n",
    "df_sorted[\"Rank\"] = range(1, len(df_sorted) + 1)\n",
    "\n",
    "# If you still want to update the original DataFrame (though generally not recommended\n",
    "# after sorting), you could do:\n",
    "# df[\"Rank\"] = df.sort_values(by=\"MktCap AUM, M\", ascending=False).reset_index(drop=True).index + 1\n",
    "\n",
    "# Now df_sorted has the \"Rank\" column\n",
    "print(f'df_sorted.head(3):\\n{df_sorted.head(3)}')\n",
    "print(f'\\ndf_sorted.tail(3):\\n{df_sorted.tail(3)}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df.describe())\n",
    "print(df_sorted.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using PyArrow (default, recommended for most cases)\n",
    "df_sorted.to_parquet(dest_path, engine='pyarrow', compression='zstd')\n",
    "print(f'save df_sorted to {dest_path}')\n",
    "\n",
    "# To load it later:\n",
    "loaded_df = pd.read_parquet(dest_path, engine='pyarrow')\n",
    "print(f'loaded_df.head(20):\\n{loaded_df.head(20)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
