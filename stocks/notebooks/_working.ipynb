{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sorted_selected_stocks_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 37\u001b[39m\n\u001b[32m     32\u001b[39m ticker_order = [\u001b[33m'\u001b[39m\u001b[33mMSFT\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mAAPL\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mGOOG\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mAMZN\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;66;03m# Example order\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# --- Solution ---\u001b[39;00m\n\u001b[32m     35\u001b[39m \n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# 1. Select the desired columns from sorted_selected_stocks_df\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m new_cols_df = \u001b[43msorted_selected_stocks_df\u001b[49m[[\u001b[33m'\u001b[39m\u001b[33mWeight\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mCluster_ID\u001b[39m\u001b[33m'\u001b[39m]]\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# 2. Reindex both the new columns and the existing df_data to the desired ticker_order.\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m#    This ensures rows align correctly before concatenation.\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m#    Use errors='ignore' in drop just in case Weight/Cluster_ID weren't in df_data.\u001b[39;00m\n\u001b[32m     42\u001b[39m new_cols_ordered = new_cols_df.reindex(ticker_order)\n",
      "\u001b[31mNameError\u001b[39m: name 'sorted_selected_stocks_df' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- Assumptions (replace with your actual data) ---\n",
    "# Assume sorted_selected_stocks_df looks something like this (already indexed by Ticker):\n",
    "#         Cluster_ID  Raw_Score  Risk_Adj_Score  Weight  Volatility\n",
    "# Ticker\n",
    "# AAPL             1       0.85            0.75    0.15        0.25\n",
    "# MSFT             1       0.80            0.70    0.12        0.22\n",
    "# GOOG             0       0.90            0.80    0.10        0.30\n",
    "# AMZN             0       0.75            0.65    0.08        0.35\n",
    "# ...\n",
    "\n",
    "# Assume df_data exists and might have different columns and potentially a different index/order\n",
    "# Example:\n",
    "data_for_df = {\n",
    "    'Price': [155, 290, 2800, 3400, 500],\n",
    "    'Volume': [1e6, 5e5, 2e5, 3e5, 1e5]\n",
    "}\n",
    "index_for_df = ['AAPL', 'MSFT', 'GOOG', 'AMZN', 'META'] # Example index\n",
    "df_data = pd.DataFrame(data_for_df, index=index_for_df)\n",
    "# df_data might look like:\n",
    "#       Price  Volume\n",
    "# AAPL    155  1000000.0\n",
    "# MSFT    290   500000.0\n",
    "# GOOG   2800   200000.0\n",
    "# AMZN   3400   300000.0\n",
    "# META    500   100000.0\n",
    "\n",
    "\n",
    "# Assume ticker_order is defined (the desired final order)\n",
    "# Example: must contain tickers present in BOTH dataframes you want to combine\n",
    "ticker_order = ['MSFT', 'AAPL', 'GOOG', 'AMZN'] # Example order\n",
    "\n",
    "# --- Solution ---\n",
    "\n",
    "# 1. Select the desired columns from sorted_selected_stocks_df\n",
    "new_cols_df = sorted_selected_stocks_df[['Weight', 'Cluster_ID']]\n",
    "\n",
    "# 2. Reindex both the new columns and the existing df_data to the desired ticker_order.\n",
    "#    This ensures rows align correctly before concatenation.\n",
    "#    Use errors='ignore' in drop just in case Weight/Cluster_ID weren't in df_data.\n",
    "new_cols_ordered = new_cols_df.reindex(ticker_order)\n",
    "other_cols_ordered = df_data.drop(columns=['Weight', 'Cluster_ID'], errors='ignore').reindex(ticker_order)\n",
    "\n",
    "# 3. Concatenate along columns (axis=1), putting the new columns first.\n",
    "#    The index will be ticker_order because both DataFrames were reindexed.\n",
    "final_df = pd.concat([new_cols_ordered, other_cols_ordered], axis=1)\n",
    "\n",
    "# --- Verification ---\n",
    "print(\"Original df_data (example):\")\n",
    "print(df_data)\n",
    "print(\"\\nSelected columns from sorted_selected_stocks_df (example):\")\n",
    "# Example data for sorted_selected_stocks_df if needed for running the code\n",
    "example_sorted_data = {\n",
    "    'Cluster_ID': [1, 1, 0, 0],\n",
    "    'Raw_Score': [0.85, 0.80, 0.90, 0.75],\n",
    "    'Risk_Adj_Score': [0.75, 0.70, 0.80, 0.65],\n",
    "    'Weight': [0.15, 0.12, 0.10, 0.08],\n",
    "    'Volatility': [0.25, 0.22, 0.30, 0.35]\n",
    "}\n",
    "sorted_selected_stocks_df = pd.DataFrame(example_sorted_data, index=['AAPL', 'MSFT', 'GOOG', 'AMZN'])\n",
    "sorted_selected_stocks_df.index.name = 'Ticker'\n",
    "print(sorted_selected_stocks_df[['Weight', 'Cluster_ID']])\n",
    "\n",
    "print(\"\\nFinal DataFrame:\")\n",
    "print(final_df)\n",
    "print(\"\\nFinal DataFrame Index:\")\n",
    "print(final_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Final EWM Covariance (adjust=True) ---\n",
      "          Asset_A   Asset_B   Asset_C   Asset_D   Asset_E\n",
      "Asset_A  0.000076  0.000014  0.000006 -0.000025 -0.000027\n",
      "Asset_B  0.000014  0.000131  0.000031 -0.000020  0.000008\n",
      "Asset_C  0.000006  0.000031  0.000082  0.000003  0.000012\n",
      "Asset_D -0.000025 -0.000020  0.000003  0.000139  0.000054\n",
      "Asset_E -0.000027  0.000008  0.000012  0.000054  0.000123\n",
      "\n",
      "--- Final EWM Correlation (adjust=True) ---\n",
      "          Asset_A   Asset_B   Asset_C   Asset_D   Asset_E\n",
      "Asset_A  1.000000  0.137990  0.075930 -0.243463 -0.282015\n",
      "Asset_B  0.137990  1.000000  0.298636 -0.147612  0.064539\n",
      "Asset_C  0.075930  0.298636  1.000000  0.024381  0.116358\n",
      "Asset_D -0.243463 -0.147612  0.024381  1.000000  0.409850\n",
      "Asset_E -0.282015  0.064539  0.116358  0.409850  1.000000\n",
      "\n",
      "--- Final EWM Covariance Only ---\n",
      "          Asset_A   Asset_B   Asset_C   Asset_D   Asset_E\n",
      "Asset_A  0.000076  0.000014  0.000006 -0.000025 -0.000027\n",
      "Asset_B  0.000014  0.000131  0.000031 -0.000020  0.000008\n",
      "Asset_C  0.000006  0.000031  0.000082  0.000003  0.000012\n",
      "Asset_D -0.000025 -0.000020  0.000003  0.000139  0.000054\n",
      "Asset_E -0.000027  0.000008  0.000012  0.000054  0.000123\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Modified Function Definition ---\n",
    "def get_final_ewm_cov_corr_matrices(df, span=21, return_corr=True, return_cov=True):\n",
    "    \"\"\"\n",
    "    Calculates the *final* EWM covariance and/or correlation matrix\n",
    "    using the standard pandas df.ewm(adjust=True, ignore_na=False).cov() method.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame with time series data (e.g., returns).\n",
    "                           Index should be time-ordered.\n",
    "        span (int): The span for the EWM calculation.\n",
    "        return_corr (bool): Whether to return the correlation matrix.\n",
    "        return_cov (bool): Whether to return the covariance matrix.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame or tuple(pd.DataFrame, pd.DataFrame):\n",
    "            - If both return_cov and return_corr are True, returns (covariance, correlation).\n",
    "            - If only one is True, returns that specific matrix.\n",
    "            - Returns NaN-filled DataFrame(s) if calculation is not possible (e.g., empty df).\n",
    "    \"\"\"\n",
    "    if df.empty or len(df) < 2: # Need at least 2 points for covariance\n",
    "        # Handle empty or too short DataFrame input\n",
    "        nan_res = pd.DataFrame(np.nan, index=df.columns, columns=df.columns)\n",
    "        results = []\n",
    "        if return_cov: results.append(nan_res)\n",
    "        if return_corr: results.append(nan_res)\n",
    "        # Ensure returning tuple if both requested, even if NaN\n",
    "        if len(results) > 1:\n",
    "             return tuple(results)\n",
    "        elif len(results) == 1:\n",
    "             return results[0]\n",
    "        else:\n",
    "             return None # Or raise error if neither requested?\n",
    "\n",
    "    # --- Core Calculation using Pandas EWM ---\n",
    "    try:\n",
    "        # Calculate the full EWM covariance time series\n",
    "        # adjust=True: Standard bias correction for EWM\n",
    "        # ignore_na=False: Propagate NaNs if insufficient data at a point in time\n",
    "        ewm_cov_ts = df.ewm(span=span, adjust=True, ignore_na=False).cov()\n",
    "\n",
    "        # Check if the result is empty or all NaN (e.g., df shorter than min_periods derived from span)\n",
    "        if ewm_cov_ts.empty or ewm_cov_ts.isnull().all().all():\n",
    "             raise ValueError(\"Pandas EWM calculation resulted in all NaNs.\")\n",
    "\n",
    "        # Extract the *final* covariance matrix (corresponding to the last index)\n",
    "        last_index = df.index[-1]\n",
    "        # Use .iloc[-n_assets*n_assets:] as a robust way to get the last block\n",
    "        # in case of duplicate indices or multi-index issues after EWM.\n",
    "        # However, direct loc is usually cleaner if index is unique and standard.\n",
    "        try:\n",
    "            cov_matrix_df = ewm_cov_ts.loc[last_index]\n",
    "        except KeyError:\n",
    "            # Fallback if last_index isn't found directly (e.g., dropped due to NaNs earlier)\n",
    "            # Find the last available index in the EWM result\n",
    "            last_ewm_index = ewm_cov_ts.index.get_level_values(0)[-1]\n",
    "            cov_matrix_df = ewm_cov_ts.loc[last_ewm_index]\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "         # Handle potential errors during EWM calculation or extraction\n",
    "         print(f\"Error during pandas EWM calculation or final matrix extraction: {e}\")\n",
    "         # Return NaN matrices in case of error\n",
    "         nan_res = pd.DataFrame(np.nan, index=df.columns, columns=df.columns)\n",
    "         results = []\n",
    "         if return_cov: results.append(nan_res)\n",
    "         if return_corr: results.append(nan_res)\n",
    "         if len(results) > 1:\n",
    "             return tuple(results)\n",
    "         elif len(results) == 1:\n",
    "             return results[0]\n",
    "         else:\n",
    "             return None\n",
    "    # --- End of Core Calculation ---\n",
    "\n",
    "\n",
    "    # Prepare results\n",
    "    results = []\n",
    "    if return_cov:\n",
    "        results.append(cov_matrix_df)\n",
    "\n",
    "    if return_corr:\n",
    "        # Calculate correlation from the final covariance matrix\n",
    "        cov_matrix = cov_matrix_df.values # Use numpy array for calculation\n",
    "        variances = np.diag(cov_matrix).copy()\n",
    "\n",
    "        # Handle non-positive variances robustly\n",
    "        variances[variances <= 1e-14] = 1e-14 # Use a small positive floor\n",
    "        std_devs = np.sqrt(variances)\n",
    "\n",
    "        # Calculate correlation matrix\n",
    "        inv_std_devs = 1.0 / std_devs\n",
    "        corr_matrix = cov_matrix * np.outer(inv_std_devs, inv_std_devs)\n",
    "\n",
    "        # Clip diagonal to exactly 1.0 and off-diagonal to [-1.0, 1.0]\n",
    "        # Handles potential floating point inaccuracies\n",
    "        np.fill_diagonal(corr_matrix, 1.0)\n",
    "        corr_matrix = np.clip(corr_matrix, -1.0, 1.0)\n",
    "\n",
    "        corr_matrix_df = pd.DataFrame(corr_matrix,\n",
    "                                      index=df.columns,\n",
    "                                      columns=df.columns)\n",
    "        results.append(corr_matrix_df)\n",
    "\n",
    "    # Return tuple if both requested, otherwise the single DataFrame\n",
    "    return tuple(results) if len(results) > 1 else results[0]\n",
    "\n",
    "# --- Example Usage ---\n",
    "np.random.seed(42) # for reproducibility\n",
    "n_obs = 100\n",
    "n_assets = 5\n",
    "data = np.random.randn(n_obs, n_assets) * 0.01 + 0.0001 # Simulate returns\n",
    "dates = pd.date_range(start='2023-01-01', periods=n_obs, freq='B')\n",
    "df_test = pd.DataFrame(data, index=dates, columns=[f'Asset_{chr(65+i)}' for i in range(n_assets)])\n",
    "\n",
    "# Get both final covariance and correlation\n",
    "cov_final, corr_final = get_final_ewm_cov_corr_matrices(df_test, span=21, return_corr=True, return_cov=True)\n",
    "\n",
    "print(\"--- Final EWM Covariance (adjust=True) ---\")\n",
    "print(cov_final)\n",
    "print(\"\\n--- Final EWM Correlation (adjust=True) ---\")\n",
    "print(corr_final)\n",
    "\n",
    "# Get only final covariance\n",
    "cov_only = get_final_ewm_cov_corr_matrices(df_test, span=21, return_corr=False, return_cov=True)\n",
    "print(\"\\n--- Final EWM Covariance Only ---\")\n",
    "print(cov_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running Your Custom Function ---\n",
      "\n",
      "Covariance Matrix from Your Function:\n",
      "          Asset_A   Asset_B   Asset_C   Asset_D   Asset_E\n",
      "Asset_A  0.000066  0.000012  0.000005 -0.000022 -0.000024\n",
      "Asset_B  0.000012  0.000114  0.000027 -0.000017  0.000007\n",
      "Asset_C  0.000005  0.000027  0.000071  0.000002  0.000010\n",
      "Asset_D -0.000022 -0.000017  0.000002  0.000120  0.000046\n",
      "Asset_E -0.000024  0.000007  0.000010  0.000046  0.000107\n",
      "\n",
      "--- Running Standard Pandas EWM (adjust=True) ---\n",
      "\n",
      "Final Covariance Matrix from Pandas EWM (adjust=True):\n",
      "          Asset_A   Asset_B   Asset_C   Asset_D   Asset_E\n",
      "Asset_A  0.000076  0.000014  0.000006 -0.000025 -0.000027\n",
      "Asset_B  0.000014  0.000131  0.000031 -0.000020  0.000008\n",
      "Asset_C  0.000006  0.000031  0.000082  0.000003  0.000012\n",
      "Asset_D -0.000025 -0.000020  0.000003  0.000139  0.000054\n",
      "Asset_E -0.000027  0.000008  0.000012  0.000054  0.000123\n",
      "\n",
      "--- Comparison ---\n",
      "\n",
      "Are the matrices numerically close? False\n",
      "\n",
      "Difference Matrix (Your Function - Pandas EWM):\n",
      "              Asset_A   Asset_B       Asset_C       Asset_D   Asset_E\n",
      "Asset_A -1.018789e-05 -0.000002 -7.987896e-07  3.355449e-06  0.000004\n",
      "Asset_B -1.848969e-06 -0.000018 -4.151905e-06  2.671388e-06 -0.000001\n",
      "Asset_C -7.987896e-07 -0.000004 -1.096153e-05 -3.393875e-07 -0.000002\n",
      "Asset_D  3.355449e-06  0.000003 -3.393875e-07 -1.858638e-05 -0.000007\n",
      "Asset_E  3.661325e-06 -0.000001 -1.569864e-06 -7.192017e-06 -0.000017\n",
      "\n",
      "Explanation:\n",
      "The matrices are expected to be DIFFERENT.\n",
      "Key reasons for differences include:\n",
      "1. Calculation Method: Your code computes a single, full-history weighted average of cross-products of *point-in-time* demeaned values. Standard EWM is recursive.\n",
      "2. Demeaning: Your code uses point-in-time `adjust=False` EWM mean for demeaning. Standard `adjust=True` EWM Covariance effectively uses bias-corrected means in its recursive update, and the final 'adjust=True' value reflects a different normalization/weighting.\n",
      "4. Weight Normalization: The exact normalization factor applied in your direct sum differs from the implicit normalization within the standard EWM recursion, especially the 'adjust=True' variant.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Your Function Definition ---\n",
    "def get_cov_corr_ewm_matrices_chunked(df, span=21, return_corr=True, return_cov=True, chunk_size=100):\n",
    "    \"\"\"\n",
    "    Robust chunked calculation of EWM covariance and correlation matrices.\n",
    "    Handles edge cases and ensures proper broadcasting.\n",
    "    (NOTE: Calculates a specific weighted average, not standard recursive EWM Cov)\n",
    "    \"\"\"\n",
    "    alpha = 2 / (span + 1)\n",
    "\n",
    "    # Clean data - remove inf and drop rows with any NaN\n",
    "    # IMPORTANT: This dropna() can differ significantly from pandas EWM NaN handling\n",
    "    clean_df = df.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    if clean_df.empty:\n",
    "         # Handle empty DataFrame after cleaning\n",
    "        empty_res = pd.DataFrame(np.nan, index=df.columns, columns=df.columns)\n",
    "        results = []\n",
    "        if return_cov: results.append(empty_res)\n",
    "        if return_corr: results.append(empty_res)\n",
    "        return tuple(results) if len(results) > 1 else results[0]\n",
    "\n",
    "    n_assets = len(clean_df.columns)\n",
    "    n_obs = len(clean_df)\n",
    "\n",
    "    # Calculate EWM mean (point-in-time, adjust=False) and demean\n",
    "    ewm_mean = clean_df.ewm(alpha=alpha, adjust=False).mean()\n",
    "    demeaned = clean_df - ewm_mean\n",
    "\n",
    "    # Compute weights as a column vector (for the entire history)\n",
    "    weights = (1 - alpha) ** np.arange(n_obs - 1, -1, -1) # Corrected range N-1 down to 0\n",
    "    # Denominator for normalization (adjust=True like EWM variance)\n",
    "    # Note: Standard EWM variance/covariance adjust=True uses a different recursive normalization\n",
    "    # This is one interpretation of a full-history adjusted weight sum\n",
    "    # It ensures weights roughly sum to 1 for large N, similar to adjust=True spirit\n",
    "    norm_factor = np.sum(weights) # Simple sum normalization\n",
    "    weights /= norm_factor\n",
    "    weights = weights.reshape(-1, 1)  # Shape (n_obs, 1)\n",
    "\n",
    "    # Initialize covariance matrix\n",
    "    cov_matrix = np.zeros((n_assets, n_assets))\n",
    "\n",
    "    # Process in chunks\n",
    "    demeaned_vals = demeaned.values # Use numpy array for faster access\n",
    "    for i in range(0, n_assets, chunk_size):\n",
    "        i_end = min(i + chunk_size, n_assets)\n",
    "        chunk_i = demeaned_vals[:, i:i_end]  # Shape (n_obs, chunk_size_i)\n",
    "\n",
    "        # Apply weights to chunk_i (broadcasting works automatically)\n",
    "        weighted_chunk_i = chunk_i * weights  # Shape (n_obs, chunk_size_i)\n",
    "\n",
    "        for j in range(i, n_assets, chunk_size):  # Start from i for upper triangle\n",
    "            j_end = min(j + chunk_size, n_assets)\n",
    "            chunk_j = demeaned_vals[:, j:j_end]  # Shape (n_obs, chunk_size_j)\n",
    "\n",
    "            # Calculate weighted products for this chunk pair\n",
    "            # weighted_chunk_i.T @ chunk_j performs Sum [ w_t * demeaned_i_t * demeaned_j_t ]\n",
    "            cov_chunk = np.dot(weighted_chunk_i.T, chunk_j) # Shape (chunk_size_i, chunk_size_j)\n",
    "\n",
    "            # Fill the covariance matrix\n",
    "            cov_matrix[i:i_end, j:j_end] = cov_chunk\n",
    "\n",
    "            # Fill symmetric part if not on diagonal\n",
    "            if i != j:\n",
    "                cov_matrix[j:j_end, i:i_end] = cov_chunk.T\n",
    "\n",
    "    # Prepare results\n",
    "    results = []\n",
    "    cov_matrix_df = pd.DataFrame(cov_matrix,\n",
    "                               index=clean_df.columns,\n",
    "                               columns=clean_df.columns)\n",
    "    if return_cov:\n",
    "        results.append(cov_matrix_df)\n",
    "\n",
    "    if return_corr:\n",
    "        # Handle zero variances\n",
    "        variances = np.diag(cov_matrix).copy()\n",
    "        # Use a larger epsilon for numerical stability if variances are calculated near zero\n",
    "        variances[variances <= 1e-12] = 1e-12\n",
    "        std_devs = np.sqrt(variances)\n",
    "\n",
    "        # Ensure diagonal is exactly 1 and handle potential division by near-zero std devs\n",
    "        inv_std_devs = 1.0 / std_devs\n",
    "        corr_matrix = cov_matrix * np.outer(inv_std_devs, inv_std_devs)\n",
    "        np.fill_diagonal(corr_matrix, 1.0) # Clip diagonal for perfect 1s\n",
    "        corr_matrix = np.clip(corr_matrix, -1.0, 1.0) # Clip off-diagonal due to potential fp errors\n",
    "\n",
    "        corr_matrix_df = pd.DataFrame(corr_matrix,\n",
    "                                    index=clean_df.columns,\n",
    "                                    columns=clean_df.columns)\n",
    "        results.append(corr_matrix_df)\n",
    "\n",
    "    return tuple(results) if len(results) > 1 else results[0]\n",
    "# --- End of Your Function ---\n",
    "\n",
    "\n",
    "# --- Comparison Setup ---\n",
    "# 1. Generate Sample Data\n",
    "np.random.seed(42) # for reproducibility\n",
    "n_obs = 100\n",
    "n_assets = 5\n",
    "data = np.random.randn(n_obs, n_assets) * 0.01 + 0.0001 # Simulate returns\n",
    "dates = pd.date_range(start='2023-01-01', periods=n_obs, freq='B')\n",
    "df = pd.DataFrame(data, index=dates, columns=[f'Asset_{chr(65+i)}' for i in range(n_assets)])\n",
    "# Introduce a NaN to see handling (your code drops row, pandas ignores pair/propagates)\n",
    "# df.iloc[10, 1] = np.nan\n",
    "\n",
    "# 2. Define Parameters\n",
    "span = 21\n",
    "chunk_size = 2 # Small chunk size for testing chunking logic\n",
    "\n",
    "# --- Run Calculations ---\n",
    "\n",
    "# 3. Your Code's Calculation\n",
    "print(\"--- Running Your Custom Function ---\")\n",
    "# Request only covariance\n",
    "cov_custom = get_cov_corr_ewm_matrices_chunked(\n",
    "    df.copy(), # Pass copy as your function modifies internally (dropna)\n",
    "    span=span,\n",
    "    return_corr=False,\n",
    "    return_cov=True,\n",
    "    chunk_size=chunk_size\n",
    ")\n",
    "print(\"\\nCovariance Matrix from Your Function:\")\n",
    "print(cov_custom)\n",
    "\n",
    "# 4. Standard Pandas EWM Calculation (adjust=True)\n",
    "print(\"\\n--- Running Standard Pandas EWM (adjust=True) ---\")\n",
    "# Calculate the full EWM covariance time series\n",
    "# Note: Pandas default for ignore_na changed over versions. Explicitly set.\n",
    "# ignore_na=False propagates NaNs. ignore_na=True tries to compute using available pairs.\n",
    "ewm_cov_std_all = df.ewm(span=span, adjust=True, ignore_na=False).cov()\n",
    "\n",
    "# Extract the *final* covariance matrix (corresponding to the last date)\n",
    "final_date = df.index[-1]\n",
    "cov_std_final = ewm_cov_std_all.loc[final_date]\n",
    "\n",
    "print(\"\\nFinal Covariance Matrix from Pandas EWM (adjust=True):\")\n",
    "print(cov_std_final)\n",
    "\n",
    "# --- Compare Results ---\n",
    "print(\"\\n--- Comparison ---\")\n",
    "\n",
    "# Check if the DataFrames are numerically close\n",
    "# Note: We expect them to be DIFFERENT due to calculation method differences\n",
    "are_close = np.allclose(cov_custom.values, cov_std_final.values, atol=1e-8) # Use tolerance for float comparison\n",
    "print(f\"\\nAre the matrices numerically close? {are_close}\")\n",
    "\n",
    "# Show the difference\n",
    "difference = cov_custom - cov_std_final\n",
    "print(\"\\nDifference Matrix (Your Function - Pandas EWM):\")\n",
    "print(difference)\n",
    "\n",
    "print(\"\\nExplanation:\")\n",
    "print(\"The matrices are expected to be DIFFERENT.\")\n",
    "print(\"Key reasons for differences include:\")\n",
    "print(\"1. Calculation Method: Your code computes a single, full-history weighted average of cross-products of *point-in-time* demeaned values. Standard EWM is recursive.\")\n",
    "print(\"2. Demeaning: Your code uses point-in-time `adjust=False` EWM mean for demeaning. Standard `adjust=True` EWM Covariance effectively uses bias-corrected means in its recursive update, and the final 'adjust=True' value reflects a different normalization/weighting.\")\n",
    "# print(\"3. NaN Handling: Your code uses dropna() (removes entire row), while pandas EWM has `ignore_na` options (if NaNs were present).\") # Uncomment if you add NaNs\n",
    "print(\"4. Weight Normalization: The exact normalization factor applied in your direct sum differs from the implicit normalization within the standard EWM recursion, especially the 'adjust=True' variant.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Python will look in these locations:\n",
      "['C:\\\\Users\\\\ping\\\\.pyenv\\\\pyenv-win\\\\versions\\\\3.11.9\\\\python311.zip', 'C:\\\\Users\\\\ping\\\\.pyenv\\\\pyenv-win\\\\versions\\\\3.11.9\\\\DLLs', 'C:\\\\Users\\\\ping\\\\.pyenv\\\\pyenv-win\\\\versions\\\\3.11.9\\\\Lib', 'C:\\\\Users\\\\ping\\\\.pyenv\\\\pyenv-win\\\\versions\\\\3.11.9', 'c:\\\\Users\\\\ping\\\\Files_win10\\\\python\\\\py311\\\\.venv', '', 'c:\\\\Users\\\\ping\\\\Files_win10\\\\python\\\\py311\\\\.venv\\\\Lib\\\\site-packages', 'c:\\\\Users\\\\ping\\\\Files_win10\\\\python\\\\py311\\\\.venv\\\\Lib\\\\site-packages\\\\win32', 'c:\\\\Users\\\\ping\\\\Files_win10\\\\python\\\\py311\\\\.venv\\\\Lib\\\\site-packages\\\\win32\\\\lib', 'c:\\\\Users\\\\ping\\\\Files_win10\\\\python\\\\py311\\\\.venv\\\\Lib\\\\site-packages\\\\Pythonwin', 'c:\\\\Users\\\\ping\\\\Files_win10\\\\python\\\\py311\\\\stocks\\\\src', 'c:\\\\Users\\\\ping\\\\Files_win10\\\\python\\\\py311\\\\.venv\\\\Lib\\\\site-packages\\\\setuptools\\\\_vendor', 'c:\\\\Users\\\\ping\\\\Files_win10\\\\python\\\\py311\\\\stocks\\\\src', 'c:\\\\Users\\\\ping\\\\Files_win10\\\\python\\\\py311\\\\stocks\\\\src']\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<span style='color:#00ffff;font-weight:500'>[Downloads] Scanned latest 60 files • Found 14 'df_finviz_2025' matches</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Available 'df_finviz_2025' files:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "- (1) `[DOWNLOADS]` `df_finviz_2025-04-01.parquet` <span style='color:#00ffff'>(6.88 MB, 2025-04-01 16:37)</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "- (2) `[DOWNLOADS]` `df_finviz_2025-03-31.pkl` <span style='color:#00ffff'>(0.51 MB, 2025-03-31 23:53)</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "- (3) `[DOWNLOADS]` `df_finviz_2025-03-28.pkl` <span style='color:#00ffff'>(0.51 MB, 2025-03-28 17:41)</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "- (4) `[DOWNLOADS]` `df_finviz_2025-03-27.pkl` <span style='color:#00ffff'>(0.51 MB, 2025-03-27 15:09)</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "- (5) `[DOWNLOADS]` `df_finviz_2025-03-26.pkl` <span style='color:#00ffff'>(0.51 MB, 2025-03-26 15:12)</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "- (6) `[DOWNLOADS]` `df_finviz_2025-03-25.pkl` <span style='color:#00ffff'>(0.51 MB, 2025-03-25 15:23)</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "- (7) `[DOWNLOADS]` `df_finviz_2025-03-24 (1).pkl` <span style='color:#00ffff'>(0.51 MB, 2025-03-24 13:20)</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "- (8) `[DOWNLOADS]` `df_finviz_2025-03-24.pkl` <span style='color:#00ffff'>(0.51 MB, 2025-03-24 13:12)</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "- (9) `[DOWNLOADS]` `df_finviz_2025-03-21.pkl` <span style='color:#00ffff'>(0.51 MB, 2025-03-21 13:09)</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "- (10) `[DOWNLOADS]` `df_finviz_2025-03-20.pkl` <span style='color:#00ffff'>(0.51 MB, 2025-03-20 13:18)</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "- (11) `[DOWNLOADS]` `df_finviz_2025-03-19.pkl` <span style='color:#00ffff'>(0.51 MB, 2025-03-19 13:14)</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "- (12) `[DOWNLOADS]` `df_finviz_2025-03-18.pkl` <span style='color:#00ffff'>(0.51 MB, 2025-03-18 13:38)</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "- (13) `[DOWNLOADS]` `df_finviz_2025-03-17.pkl` <span style='color:#00ffff'>(0.46 MB, 2025-03-17 13:06)</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "- (14) `[DOWNLOADS]` `df_finviz_2025-03-14.pkl` <span style='color:#00ffff'>(0.46 MB, 2025-03-14 15:30)</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input a number to select file (1-14)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "    **Selected paths:**\n",
       "    - Source: `C:\\Users\\ping\\Downloads\\df_finviz_2025-04-01.parquet`  \n",
       "    - Destination: `..\\data\\df_finviz.pkl`\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Notebook cell\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Get root directory (assuming notebook is in root/notebooks/)\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "ROOT_DIR = NOTEBOOK_DIR.parent if NOTEBOOK_DIR.name == 'notebooks' else NOTEBOOK_DIR\n",
    "\n",
    "# Add src directory to Python path\n",
    "sys.path.append(str(ROOT_DIR / 'src'))\n",
    "\n",
    "# Verify path\n",
    "print(f\"Python will look in these locations:\\n{sys.path}\")\n",
    "\n",
    "\n",
    "# --- Execute the processor ---\n",
    "import utils\n",
    "\n",
    "# SOURCE_PATH_OHLCV = '..\\data\\df_OHLCV_2025-03-07_clean.pkl'\n",
    "# SOURCE_PATH_STOCK = '..\\data\\df_finviz_stocks_n_ratios.pkl'\n",
    "# SOURCE_PATH_ETF = '..\\data\\df_finviz_etfs_n_ratios.pkl'\n",
    "\n",
    "SOURCE_PATH, DEST_PATH = utils.main_processor(\n",
    "    data_dir='..\\data',  # search project ..\\data\n",
    "    downloads_dir=None,  # None searchs Downloads dir, '' omits search1\n",
    "    downloads_limit=60,  # search the first 10 files\n",
    "    clean_name_override='df_finviz.pkl',  # override filename\n",
    "    start_file_pattern='df_finviz_2025', # search for files starting with 'df_'\n",
    "    # start_file_pattern='df_OHLCV_2025', # search for files starting with 'df_'    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the Parquet file\n",
    "df = pd.read_parquet(SOURCE_PATH, engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 394579 entries, ('TBIL', 'Apr 1, 2025') to ('IEI', 'Apr 2, 2024')\n",
      "Data columns (total 6 columns):\n",
      " #   Column     Non-Null Count   Dtype \n",
      "---  ------     --------------   ----- \n",
      " 0   Open       394579 non-null  object\n",
      " 1   High       389418 non-null  object\n",
      " 2   Low        389418 non-null  object\n",
      " 3   Close      389418 non-null  object\n",
      " 4   Adj Close  389418 non-null  object\n",
      " 5   Volume     389418 non-null  object\n",
      "dtypes: object(6)\n",
      "memory usage: 19.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted date: 2025-03-14\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Extract date using regex pattern\n",
    "date_pattern = r'(\\d{4}-\\d{2}-\\d{2})'\n",
    "match = re.search(date_pattern, SOURCE_PATH)\n",
    "if match:\n",
    "  date_str = match.group(1)\n",
    "  print(f\"Extracted date: {date_str}\")\n",
    "else:\n",
    "  print(\"No date found in the path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_config.py\n",
    "def create_config_file(date_str):\n",
    "    \"\"\"Create config.py with date_str as a string variable\"\"\"\n",
    "    config_content = f\"\"\"# config.py\n",
    "# Automatically generated date configuration\n",
    "date_str = '{date_str}'  # Date in YYYY-MM-DD format\n",
    "\"\"\"\n",
    "    \n",
    "    with open('config.py', 'w') as f:\n",
    "        f.write(config_content)\n",
    "    \n",
    "    print(f\"config.py created successfully with date: {date_str}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.py created successfully with date: 2025-03-14\n"
     ]
    }
   ],
   "source": [
    "create_config_file(date_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date string from config: 2025-03-14\n"
     ]
    }
   ],
   "source": [
    "# script_using_config.py\n",
    "from config import date_str\n",
    "\n",
    "print(f\"Date string from config: {date_str}\")  # Direct string access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
