{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a675977a",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Papermill injects parameters here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a586c430",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# --- 1. PANDAS & IPYTHON OPTIONS ---\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 3000*5)\n",
    "pd.set_option('display.float_format', '{:.6f}'.format)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# --- 2. PROJECT PATH CONFIGURATION ---\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "PARENT_DIR = NOTEBOOK_DIR.parent\n",
    "ROOT_DIR = NOTEBOOK_DIR.parent.parent  # Adjust if your notebook is in a 'notebooks' subdirectory\n",
    "DATA_DIR = ROOT_DIR / 'data'\n",
    "SRC_DIR = ROOT_DIR / 'src'\n",
    "\n",
    "# Add 'src' to the Python path to import custom modules\n",
    "if str(SRC_DIR) not in sys.path:\n",
    "    sys.path.append(str(SRC_DIR))\n",
    "\n",
    "# --- 3. IMPORT CUSTOM MODULES ---\n",
    "import utils\n",
    "import plotting_utils\n",
    "\n",
    "# --- 4. INITIAL_CAPITAL ---\n",
    "INITIAL_CAPITAL = 100000\n",
    "\n",
    "# --- 5. RISK FREE ANNUAL RATE ---\n",
    "RISK_FREE_ANNUAL_RATE = 0.04\n",
    "\n",
    "# --- 6. VERIFICATION ---\n",
    "print(\"--- Path Configuration ---\")\n",
    "print(f\"✅ Project Root: {ROOT_DIR}\")\n",
    "print(f\"✅ Parent Dir:   {PARENT_DIR}\")\n",
    "print(f\"✅ Notebook Dir: {NOTEBOOK_DIR}\")\n",
    "print(f\"✅ Data Dir:     {DATA_DIR}\")\n",
    "print(f\"✅ Source Dir:   {SRC_DIR}\")\n",
    "assert all([ROOT_DIR.exists(), DATA_DIR.exists(), SRC_DIR.exists()]), \"A key directory was not found!\"\n",
    "\n",
    "print(\"\\n--- Module Verification ---\")\n",
    "print(f\"✅ Successfully imported 'utils' and 'plotting_utils'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7955686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the papermill injected parameters to read the files\n",
    "returns_train = pd.read_parquet(returns_train_path)\n",
    "returns_test = pd.read_parquet(returns_test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0923044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Read the Parquet file into a DataFrame\n",
    "# returns_train = pd.read_parquet('returns_train.parquet', engine='pyarrow')\n",
    "# returns_test = pd.read_parquet('returns_test.parquet', engine='pyarrow')\n",
    "\n",
    "# parameters injected by papermill\n",
    "# returns_train = pd.read_csv(returns_train_path, index_col=0, parse_dates=True)\n",
    "# returns_test = pd.read_csv(returns_test_path, index_col=0, parse_dates=True)\n",
    "\n",
    "# returns_train = pd.read_parquet(NOTEBOOK_DIR / '_returns_train')\n",
    "# returns_test = pd.read_parquet(NOTEBOOK_DIR / '_returns_test')\n",
    "\n",
    "# returns_train = pd.read_parquet(returns_train_path)\n",
    "# returns_test = pd.read_parquet(returns_test_path)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96836dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = utils.get_recent_files(\n",
    "    directory_path = DATA_DIR,\n",
    "    extension = 'parquet', \n",
    "    prefix = None,\n",
    "    contains_pattern = 'df_finviz_merged_stocks_etfs',\n",
    "    count = None\n",
    ")\n",
    "\n",
    "df_finviz = pd.read_parquet(DATA_DIR / file_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfd9532",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'df_finviz:\\n{df_finviz}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd50c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns_train = train_chunks[i].iloc[:n_test_rows]\n",
    "# returns_test = train_chunks[i].iloc[n_test_rows:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07adabb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'returns_train.shape: {returns_train.shape}')\n",
    "print(f'returns_test.shape: {returns_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03341863",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming returns_train is your DataFrame with daily returns\n",
    "\n",
    "# Extract the last 30, 60, 120, and 240 rows\n",
    "last_30 = returns_train.iloc[-30:]\n",
    "last_60 = returns_train.iloc[-60:]\n",
    "last_120 = returns_train.iloc[-120:]\n",
    "last_240 = returns_train.iloc[-240:]\n",
    "\n",
    "# Function to calculate Sharpe ratio\n",
    "def calculate_sharpe_ratio(returns):\n",
    "    mean_return = returns.mean()\n",
    "    std_dev = returns.std()\n",
    "    sharpe_ratio = mean_return / std_dev\n",
    "    return sharpe_ratio\n",
    "\n",
    "# Calculate Sharpe ratios for each subset\n",
    "sharpe_ratio_30 = calculate_sharpe_ratio(last_30)\n",
    "sharpe_ratio_60 = calculate_sharpe_ratio(last_60)\n",
    "sharpe_ratio_120 = calculate_sharpe_ratio(last_120)\n",
    "sharpe_ratio_240 = calculate_sharpe_ratio(last_240)\n",
    "\n",
    "# Print the results\n",
    "print(\"Sharpe Ratios for the last 30 rows:\")\n",
    "print(sharpe_ratio_30.head())  # Display the first few tickers for brevity\n",
    "\n",
    "print(\"Sharpe Ratios for the last 60 rows:\")\n",
    "print(sharpe_ratio_60.head())  # Display the first few tickers for brevity\n",
    "\n",
    "print(\"\\nSharpe Ratios for the last 120 rows:\")\n",
    "print(sharpe_ratio_120.head())  # Display the first few tickers for brevity\n",
    "\n",
    "print(\"\\nSharpe Ratios for the last 240 rows:\")\n",
    "print(sharpe_ratio_240.head())  # Display the first few tickers for brevity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf8681d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the Sharpe ratio of 'VGT' for each period\n",
    "vgt_sharpe_30 = sharpe_ratio_30['VGT']\n",
    "vgt_sharpe_60 = sharpe_ratio_60['VGT']\n",
    "vgt_sharpe_120 = sharpe_ratio_120['VGT']\n",
    "vgt_sharpe_240 = sharpe_ratio_240['VGT']\n",
    "\n",
    "# Find tickers with higher Sharpe ratio than 'VGT' for all four periods\n",
    "tickers_with_higher_sharpe = sharpe_ratio_30[\n",
    "    (sharpe_ratio_30 > vgt_sharpe_30) &    \n",
    "    (sharpe_ratio_60 > vgt_sharpe_60) &\n",
    "    (sharpe_ratio_120 > vgt_sharpe_120) &\n",
    "    (sharpe_ratio_240 > vgt_sharpe_240)\n",
    "].index\n",
    "\n",
    "n_higher_sharpe_tickers = len(tickers_with_higher_sharpe)\n",
    "total_ticker_columns = len(returns_train.columns)\n",
    "pct_higher_sharpe_tickers = n_higher_sharpe_tickers  / total_ticker_columns * 100\n",
    "\n",
    "print(f\"Percentage of tickers with Sharpe Ratio > Benchmark's Sharpe Ratio: {pct_higher_sharpe_tickers:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145ae373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a boolean array\n",
    "boolean_array_30 = sharpe_ratio_30 > vgt_sharpe_30\n",
    "boolean_array_60 = sharpe_ratio_60 > vgt_sharpe_60\n",
    "boolean_array_120 = sharpe_ratio_120 > vgt_sharpe_120\n",
    "boolean_array_240 = sharpe_ratio_240 > vgt_sharpe_240\n",
    "\n",
    "# Count the number of True values\n",
    "count_true_30 = np.sum(boolean_array_30)\n",
    "count_true_60 = np.sum(boolean_array_60)\n",
    "count_true_120 = np.sum(boolean_array_120)\n",
    "count_true_240 = np.sum(boolean_array_240)\n",
    "\n",
    "print(f'count sharpe_ratio_30 > vgt_sharpe_30: {count_true_30}')\n",
    "print(f'count sharpe_ratio_60 > vgt_sharpe_60: {count_true_60}')\n",
    "print(f'count sharpe_ratio_120 > vgt_sharpe_120: {count_true_120}')\n",
    "print(f'count sharpe_ratio_240 > vgt_sharpe_240: {count_true_240}')\n",
    "\n",
    "# Print the tickers\n",
    "print(f\"\\n{n_higher_sharpe_tickers} tickers, or {pct_higher_sharpe_tickers:.2f}%, have higher Sharpe Ratio than Benchmark for all four periods:\")\n",
    "print(tickers_with_higher_sharpe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888f34c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.cluster.hierarchy import linkage, to_tree\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "# Filter returns_train to only include tickers in tickers_with_higher_sharpe\n",
    "returns_train_filtered = returns_train[tickers_with_higher_sharpe]\n",
    "\n",
    "# Calculate the covariance matrix\n",
    "cov_matrix = returns_train_filtered.cov()\n",
    "\n",
    "# Calculate the distance matrix\n",
    "dist_matrix = pdist(cov_matrix, metric='correlation')\n",
    "\n",
    "# Perform hierarchical clustering\n",
    "linkage_matrix = linkage(dist_matrix, method='single')\n",
    "\n",
    "# Convert the linkage matrix to a tree structure\n",
    "tree = to_tree(linkage_matrix)\n",
    "\n",
    "# Function to get the order of leaves in the dendrogram\n",
    "def get_leaf_order(tree):\n",
    "    if tree.is_leaf():\n",
    "        return [tree.id]\n",
    "    return get_leaf_order(tree.left) + get_leaf_order(tree.right)\n",
    "\n",
    "# Get the order of leaves\n",
    "leaf_order = get_leaf_order(tree)\n",
    "\n",
    "# Function to allocate risk proportionally\n",
    "def allocate_risk(cov_matrix, leaf_order):\n",
    "    n = len(cov_matrix)\n",
    "    weights = np.zeros(n)\n",
    "    cluster_var = np.diag(cov_matrix)\n",
    "    total_var = np.sum(cluster_var)\n",
    "    for i, ticker in enumerate(leaf_order):\n",
    "        weights[ticker] = cluster_var[i] / total_var\n",
    "    return weights\n",
    "\n",
    "# Allocate risk proportionally\n",
    "weights = allocate_risk(cov_matrix, leaf_order)\n",
    "\n",
    "# Select the top 10 tickers with the highest weights\n",
    "selected_tickers = returns_train_filtered.columns[np.argsort(weights)[-10:][::-1]]\n",
    "\n",
    "# Get the weights for the selected tickers\n",
    "selected_weights = weights[np.argsort(weights)[-10:][::-1]]\n",
    "\n",
    "# Normalize the selected weights to ensure they sum to 1\n",
    "selected_weights /= np.sum(selected_weights)\n",
    "\n",
    "if len(selected_tickers) < 10:\n",
    "    print(f'===== There are only {len(selected_tickers)} selected_tickers. It is less than 10. Stay in CASH =====')\n",
    "    print(f'===== They are {selected_tickers.tolist()} =====\\n')\n",
    "    selected_tickers = ['CASH']\n",
    "    selected_weights = [1.0]    \n",
    "\n",
    "# Print the selected tickers and their normalized weights\n",
    "print(\"Selected tickers for the diversified portfolio using HRP:\")\n",
    "print(selected_tickers)\n",
    "print(\"Normalized portfolio weights for the selected tickers:\")\n",
    "print(selected_weights)\n",
    "\n",
    "# Verify that the weights sum to 1\n",
    "print(\"Sum of weights:\", np.sum(selected_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8c36c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_tickers = ['CASH']\n",
    "# selected_weights = [1.0]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2e5c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure returns_test is a DataFrame with daily returns for all tickers\n",
    "# returns_test should have a DatetimeIndex and columns for each ticker\n",
    "\n",
    "# Extract the daily returns for the selected tickers\n",
    "selected_returns = returns_test[selected_tickers]\n",
    "\n",
    "# Extract the daily returns for 'VGT'\n",
    "vgt_returns = returns_test['VGT']\n",
    "\n",
    "# Calculate the daily portfolio value\n",
    "portfolio_value = selected_returns.dot(selected_weights)\n",
    "\n",
    "# Create a DataFrame to store the daily values\n",
    "daily_values = pd.DataFrame({\n",
    "    'Portfolio': portfolio_value,\n",
    "    'VGT': vgt_returns\n",
    "})\n",
    "\n",
    "# Calculate the cumulative returns for the portfolio and 'VGT'\n",
    "daily_values['Portfolio_Cumulative'] = (1 + daily_values['Portfolio']).cumprod()\n",
    "daily_values['VGT_Cumulative'] = (1 + daily_values['VGT']).cumprod()\n",
    "\n",
    "# Print the daily values\n",
    "print(\"Daily values of the portfolio and 'VGT':\")\n",
    "print(daily_values[['Portfolio_Cumulative', 'VGT_Cumulative']])\n",
    "\n",
    "# Plot the cumulative returns for comparison\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(daily_values.index, daily_values['Portfolio_Cumulative'], label='Portfolio')\n",
    "plt.plot(daily_values.index, daily_values['VGT_Cumulative'], label='VGT')\n",
    "plt.title('Daily Cumulative Returns: Portfolio vs VGT')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Cumulative Returns')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f0afaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Portfolio and Benchmark daily Value to save to csv\n",
    "portfolio_daily_value = daily_values[['Portfolio_Cumulative', 'VGT_Cumulative']]\n",
    "\n",
    "# Define the path for the temporary directory\n",
    "temp_data_dir = NOTEBOOK_DIR / \"temp\"\n",
    "\n",
    "# Define the CSV file name\n",
    "# csv_file_name = 'portfolio daily value.csv'\n",
    "csv_file_name = temp_data_dir / 'portfolio daily value.csv'\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(csv_file_name):\n",
    "    # Read the existing CSV file into a DataFrame\n",
    "    existing_data = pd.read_csv(csv_file_name, index_col='Date', parse_dates=True)\n",
    "    \n",
    "    # Ensure the existing DataFrame has the same columns as portfolio_daily_value\n",
    "    existing_data = existing_data[portfolio_daily_value.columns]\n",
    "    \n",
    "    # Append the new data to the existing DataFrame\n",
    "    updated_data = pd.concat([existing_data, portfolio_daily_value], ignore_index=False)\n",
    "    \n",
    "    # Save the updated DataFrame back to the CSV file\n",
    "    updated_data.to_csv(csv_file_name, index=True)\n",
    "    print(f\"Data appended to {csv_file_name}\")\n",
    "else:\n",
    "    # Save the new DataFrame to the CSV file, including the date index\n",
    "    portfolio_daily_value.to_csv(csv_file_name, index=True)\n",
    "    print(f\"New file created and data saved to {csv_file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab15e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Create a list of counts\n",
    "# counts = [count_true_30, count_true_60, count_true_120, count_true_240]\n",
    "counts = [count_true_240, count_true_120, count_true_60, count_true_30]\n",
    "\n",
    "# Create a list of labels for the x-axis\n",
    "# labels = ['30', '60', '120', '240']\n",
    "labels = ['240', '120', '60', '30']\n",
    "\n",
    "# # Create a bar chart\n",
    "# plt.bar(labels, counts, color=['blue', 'green', 'red', 'purple'])\n",
    "\n",
    "# Create line chart\n",
    "plt.plot(labels, counts, marker='o', linestyle='-', color='blue')\n",
    "\n",
    "# Add titles and labels\n",
    "# plt.title('Count of True Values for Different Time Periods')\n",
    "plt.title('Tickers with Sharpe Ratio > Benchmark for Past 240, 120, 60, 30 Days')\n",
    "plt.xlabel('Past Period (days)')\n",
    "plt.ylabel('Count of Tickers')\n",
    "\n",
    "# # Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fe3083",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "if selected_tickers[0] != 'CASH':\n",
    "    # Define the columns you want to include in the new DataFrame\n",
    "    cols = [\n",
    "        'Company',\n",
    "        'Index',\n",
    "        'Sector',\n",
    "        'Industry',\n",
    "        'Info',\n",
    "    ]\n",
    "\n",
    "    missing_tickers = []\n",
    "    \n",
    "    # Filter out tickers that are not in the DataFrame's index\n",
    "    valid_tickers = [ticker for ticker in selected_tickers if ticker in df_finviz.index]\n",
    "    missing_tickers = [ticker for ticker in selected_tickers if ticker not in df_finviz.index]\n",
    "\n",
    "    # Print missing tickers\n",
    "    if missing_tickers:\n",
    "        print(f\"The following tickers are not in the df_finviz's index and will be excluded: {missing_tickers}\\n\")\n",
    "\n",
    "    # Check if there are any valid tickers left\n",
    "    if not valid_tickers:\n",
    "        print(\"None of the selected tickers are in the DataFrame's index.\")\n",
    "    else:\n",
    "        # Filter the DataFrame to include only the valid tickers\n",
    "        df_selected = df_finviz.loc[valid_tickers]\n",
    "\n",
    "        # Select the specified columns\n",
    "        df_selected = df_selected[cols]\n",
    "\n",
    "        # Print the resulting DataFrame\n",
    "        print(\"DataFrame with selected tickers and specified columns:\")\n",
    "        print(df_selected)\n",
    "else:\n",
    "    print(f'All CASH portfolio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a544da9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'count sharpe_ratio_30 > vgt_sharpe_30: {count_true_30}')\n",
    "print(f'count sharpe_ratio_60 > vgt_sharpe_60: {count_true_60}')\n",
    "print(f'count sharpe_ratio_120 > vgt_sharpe_120: {count_true_120}')\n",
    "print(f'count sharpe_ratio_240 > vgt_sharpe_240: {count_true_240}')\n",
    "\n",
    "# Print the tickers\n",
    "print(f\"\\n{n_higher_sharpe_tickers} tickers, or {pct_higher_sharpe_tickers:.2f}%, have higher Sharpe Ratio than Benchmark for all four periods:\")\n",
    "print(tickers_with_higher_sharpe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd66f243",
   "metadata": {},
   "outputs": [],
   "source": [
    "if selected_tickers[0] != 'CASH':\n",
    "    print(f'returns[missing_tickers]:\\n{returns_train[missing_tickers]}')\n",
    "else:\n",
    "    print(f'returns[\"CASH\"]:\\n{returns_train[\"CASH\"]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
