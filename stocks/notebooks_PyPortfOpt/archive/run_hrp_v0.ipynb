{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2156999f",
   "metadata": {},
   "source": [
    "## Hierarchical Risk Parity (HRP) Kimi-K2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ebd390",
   "metadata": {},
   "source": [
    "Think of HRP as having **two separate steps**:\n",
    "\n",
    "1. **“Where do we cut the tree?”** (build the dendrogram)  \n",
    "   → depends **only** on the correlation matrix you feed it.\n",
    "\n",
    "2. **“How risky is each branch?”** (recursive bisection)  \n",
    "   → uses the **covariance matrix** of the period you will actually trade.\n",
    "\n",
    "`hrp_weights` and `hrp_quick` differ only in **which period supplies the correlation matrix for step 1**.\n",
    "\n",
    "---\n",
    "\n",
    "### Two practical recipes\n",
    "\n",
    "| Method | Step-1 tree built on | Step-2 cov matrix built on | When to use |\n",
    "|---|---|---|---|\n",
    "| **hrp_weights(df_train)** | same as step-2 (`df_train`) | `df_train` | *Pure in-sample* – tree and risk both estimated on the same data. Simple, but may over-fit. |\n",
    "| **hrp_quick(df_train)** | **long history (`df`)** | `df_train` | *Hybrid* – stable tree from long history, fresh risk estimates from recent data. Lower turnover, more robust to regime changes. |\n",
    "\n",
    "---\n",
    "\n",
    "### Pros & cons in plain language\n",
    "\n",
    "| Aspect | hrp_weights(df_train) | hrp_quick(df_train) |\n",
    "|---|---|---|\n",
    "| **Stability** | Tree changes every time you roll the window → higher turnover, more re-balancing costs. | Tree is “anchored” by long history → weights evolve slowly, cheaper to trade. |\n",
    "| **Over-fitting** | Correlations estimated on short window can be noisy; clusters flip on a whim. | Long window smooths out noise; structure is less jumpy. |\n",
    "| **Regime sensitivity** | Reacts quickly if correlation breaks down. | May miss a *genuine* regime shift because the tree is “locked”. |\n",
    "| **Implementation effort** | One line; nothing to cache. | You must decide *how long* the “long” window should be and store the tree. |\n",
    "\n",
    "---\n",
    "\n",
    "### What do practitioners do?\n",
    "\n",
    "Most **quant desks / multi-asset funds** use the **hybrid approach** (i.e., the logic behind `hrp_quick`):\n",
    "\n",
    "- **Tree** = 3-5 years of daily returns (or even longer for low-turnover mandates).  \n",
    "- **Covariance** = rolling 6-12 months (Ledoit-Wolf shrinkage).  \n",
    "- Re-estimate the tree only once a year or when major structural breaks are detected.\n",
    "\n",
    "They do *not* rebuild the tree every month because:\n",
    "\n",
    "- Execution costs outweigh the marginal improvement.  \n",
    "- Correlation structure is fairly persistent at the **cluster** level (sectors, styles).  \n",
    "- Risk (volatility) is what really moves month-to-month, not the *order* of assets in the tree.\n",
    "\n",
    "---\n",
    "\n",
    "### Rule of thumb for you\n",
    "\n",
    "- If you are **learning / prototyping**, `hrp_weights(df_train)` is fine—easy to code, easier to debug.  \n",
    "- If you are **running live capital**, adopt the hybrid recipe:  \n",
    "  `linkage_cache = linkage(long_history)`  \n",
    "  `hrp_quick(df_train)`  \n",
    "  and refresh the cache only **quarterly or annually**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff29b931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Path Configuration ---\n",
      "✅ Project Root: c:\\Users\\ping\\Files_win10\\python\\py311\\stocks\n",
      "✅ Parent Dir:   c:\\Users\\ping\\Files_win10\\python\\py311\\stocks\\notebooks_PyPortfOpt\n",
      "✅ Notebook Dir: c:\\Users\\ping\\Files_win10\\python\\py311\\stocks\\notebooks_PyPortfOpt\\_working\n",
      "✅ Data Dir:     c:\\Users\\ping\\Files_win10\\python\\py311\\stocks\\data\n",
      "✅ Source Dir:   c:\\Users\\ping\\Files_win10\\python\\py311\\stocks\\src\n",
      "\n",
      "--- Module Verification ---\n",
      "✅ Successfully imported 'utils' and 'plotting_utils'.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pprint\n",
    "import inspect  # <--- ADD THIS LINE\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# --- 1. PANDAS & IPYTHON OPTIONS ---\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 3000)\n",
    "pd.set_option('display.float_format', '{:.6f}'.format)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# --- 2. PROJECT PATH CONFIGURATION ---\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "PARENT_DIR = NOTEBOOK_DIR.parent\n",
    "ROOT_DIR = NOTEBOOK_DIR.parent.parent  # Adjust if your notebook is in a 'notebooks' subdirectory\n",
    "DATA_DIR = ROOT_DIR / 'data'\n",
    "SRC_DIR = ROOT_DIR / 'src'\n",
    "\n",
    "# Add 'src' to the Python path to import custom modules\n",
    "if str(SRC_DIR) not in sys.path:\n",
    "    sys.path.append(str(SRC_DIR))\n",
    "\n",
    "# --- 3. IMPORT CUSTOM MODULES ---\n",
    "import utils\n",
    "\n",
    "# --- 4. CONSTANTS ---\n",
    "INITIAL_CAPITAL = 100_000  # 100,000\n",
    "RISK_FREE_ANNUAL_RATE = 0.04\n",
    "BENCHMARK_TICKER = \"VGT\"\n",
    "\n",
    "# --- 5. VERIFICATION ---\n",
    "print(\"--- Path Configuration ---\")\n",
    "print(f\"✅ Project Root: {ROOT_DIR}\")\n",
    "print(f\"✅ Parent Dir:   {PARENT_DIR}\")\n",
    "print(f\"✅ Notebook Dir: {NOTEBOOK_DIR}\")\n",
    "print(f\"✅ Data Dir:     {DATA_DIR}\")\n",
    "print(f\"✅ Source Dir:   {SRC_DIR}\")\n",
    "assert all([ROOT_DIR.exists(), DATA_DIR.exists(), SRC_DIR.exists()]), \"A key directory was not found!\"\n",
    "\n",
    "print(\"\\n--- Module Verification ---\")\n",
    "print(f\"✅ Successfully imported 'utils' and 'plotting_utils'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4dd962df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(DATA_DIR / 'df_adj_close.parquet')\n",
    "# print(f'df:\\n{df}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7742a0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 2. --- This is the code to split your DataFrame ---\n",
    "\n",
    "# Create a boolean mask for the years 2023 and 2024\n",
    "mask = df.index.year.isin([2023, 2024])\n",
    "\n",
    "# Apply the mask to get the first DataFrame\n",
    "df_train = df[mask]\n",
    "\n",
    "# Apply the inverse of the mask (using ~) to get the second DataFrame\n",
    "df_remain = df[~mask]\n",
    "\n",
    "test_mask = (df_remain.index.year == 2025) & df_remain.index.month.isin([1])\n",
    "df_test = df_remain.loc[test_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7b00092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 644 entries, 2023-01-03 to 2025-07-29\n",
      "Columns: 1524 entries, A to ZWS\n",
      "dtypes: float64(1524)\n",
      "memory usage: 7.5 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb92b0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_start_date: 2023-01-03\n",
      "train_end_date: 2024-12-31\n"
     ]
    }
   ],
   "source": [
    "train_start_date = df_train.index.min().strftime('%Y-%m-%d')\n",
    "train_end_date = df_train.index.max().strftime('%Y-%m-%d')\n",
    "print(f'train_start_date: {train_start_date}')\n",
    "print(f'train_end_date: {train_end_date}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5beca26f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_start_date: 2025-01-02\n",
      "test_end_date: 2025-01-31\n"
     ]
    }
   ],
   "source": [
    "test_start_date = df_test.index.min().strftime('%Y-%m-%d')\n",
    "test_end_date = df_test.index.max().strftime('%Y-%m-%d')\n",
    "print(f'test_start_date: {test_start_date}')\n",
    "print(f'test_end_date: {test_end_date}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdfd4ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "from scipy.spatial.distance import squareform\n",
    "from sklearn.covariance import ledoit_wolf\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Helpers\n",
    "# ------------------------------------------------------------------\n",
    "def _log_returns(price_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Log returns, drop first NA row.\"\"\"\n",
    "    return np.log(price_df / price_df.shift(1)).dropna()\n",
    "\n",
    "def _cov2corr(cov: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Covariance matrix → correlation matrix.\"\"\"\n",
    "    std = np.sqrt(np.diag(cov))\n",
    "    corr = cov / np.outer(std, std)\n",
    "    corr[corr < -1], corr[corr > 1] = -1, 1\n",
    "    return corr\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Core HRP\n",
    "# ------------------------------------------------------------------\n",
    "def _quasi_diagonal(link) -> np.ndarray:\n",
    "    \"\"\"Return reordering indices from linkage.\"\"\"\n",
    "    return np.array(dendrogram(link, no_plot=True)[\"leaves\"])\n",
    "\n",
    "def _recursive_bisection(cov: np.ndarray, perm: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Allocate top-down via inverse-variance weighting.\"\"\"\n",
    "    w = np.ones(len(perm))\n",
    "    clusters = [perm]\n",
    "    while clusters:\n",
    "        new_clusters = []\n",
    "        for idx in clusters:\n",
    "            if len(idx) == 1:\n",
    "                continue\n",
    "            half = len(idx) // 2\n",
    "            left, right = idx[:half], idx[half:]\n",
    "            # variance of each branch\n",
    "            var_l = np.linalg.inv(cov[np.ix_(left, left)]).sum()\n",
    "            var_r = np.linalg.inv(cov[np.ix_(right, right)]).sum()\n",
    "            alpha = 1 - var_l / (var_l + var_r)\n",
    "            w[left] *= alpha\n",
    "            w[right] *= (1 - alpha)\n",
    "            new_clusters.extend([left, right])\n",
    "        clusters = new_clusters\n",
    "    return w\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Public entry point\n",
    "# ------------------------------------------------------------------\n",
    "def hrp_weights(price_df: pd.DataFrame,\n",
    "                linkage_method: str = \"single\") -> pd.Series:\n",
    "    \"\"\"\n",
    "    Build HRP portfolio weights from a DataFrame of daily adjusted-close prices.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    price_df : pd.DataFrame\n",
    "        Columns = tickers, index = DatetimeIndex of daily prices.\n",
    "    linkage_method : str\n",
    "        Any valid scipy linkage method (\"single\", \"ward\", etc.).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.Series\n",
    "        Index = tickers, values = portfolio weights (sum = 1).\n",
    "    \"\"\"\n",
    "    returns = _log_returns(price_df)\n",
    "    cov, _ = ledoit_wolf(returns)\n",
    "    corr = _cov2corr(cov)\n",
    "\n",
    "    # distance matrix → condensed 1-D vector\n",
    "    dist = np.sqrt(np.clip((1 - corr) / 2, 0, 1))\n",
    "    dist_vec = squareform(dist, checks=False)\n",
    "\n",
    "    link = linkage(dist_vec, method=linkage_method)\n",
    "    perm = _quasi_diagonal(link)\n",
    "\n",
    "    raw = _recursive_bisection(cov, perm)\n",
    "    w = pd.Series(raw, index=returns.columns).sort_index()\n",
    "    return w / w.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d90968af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ticker\n",
      "A      0.000488\n",
      "AA     0.000251\n",
      "AAL    0.001555\n",
      "AAON   0.000715\n",
      "AAPL   0.000574\n",
      "         ...   \n",
      "ZM     0.002229\n",
      "ZS     0.003353\n",
      "ZTO    0.000943\n",
      "ZTS    0.000045\n",
      "ZWS    0.000533\n",
      "Length: 1524, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "weights = hrp_weights(df_train)          # df is your 643×1518 price DataFrame\n",
    "# print(weights[weights > 0.001].round(4))\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7243ce05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ticker\n",
      "U        0.008131\n",
      "MARA     0.008011\n",
      "TPG      0.006788\n",
      "EQT      0.006164\n",
      "MNDY     0.005975\n",
      "CWEN-A   0.005819\n",
      "YUMC     0.005505\n",
      "FND      0.005062\n",
      "NEE      0.004829\n",
      "CF       0.004727\n",
      "dtype: float64\n",
      "Total tickers in portfolio : 1524\n",
      "Sum of weights             : 1.0\n"
     ]
    }
   ],
   "source": [
    "print(weights.sort_values(ascending=False).head(10))\n",
    "print(\"Total tickers in portfolio :\", len(weights))\n",
    "print(\"Sum of weights             :\", weights.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6dc2798c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0011 291\n",
      "0.0012100000000000001 261\n",
      "0.0013310000000000002 235\n",
      "0.0014641000000000003 211\n",
      "0.0016105100000000005 179\n",
      "0.0017715610000000007 150\n",
      "0.0019487171000000009 141\n",
      "0.002143588810000001 122\n",
      "0.0023579476910000016 103\n",
      "0.002593742460100002 90\n",
      "0.0028531167061100022 71\n",
      "0.003138428376721003 59\n",
      "0.0034522712143931033 46\n",
      "0.003797498335832414 30\n",
      "0.004177248169415656 22\n",
      "0.004594972986357222 10\n",
      "\n",
      "Cutoff that yields 10 tickers: 0.004595\n",
      "Tickers kept: 10\n",
      "Tickers kept: 10\n",
      "Sum of re-balanced weights: 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Ticker\n",
       "U        0.133264\n",
       "MARA     0.131311\n",
       "TPG      0.111259\n",
       "EQT      0.101024\n",
       "MNDY     0.097938\n",
       "CWEN-A   0.095382\n",
       "YUMC     0.090225\n",
       "FND      0.082968\n",
       "NEE      0.079153\n",
       "CF       0.077476\n",
       "dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Run HRP once\n",
    "w = hrp_weights(df_train)          # or hrp_quick(df_train)\n",
    "\n",
    "# 2. Drop anything < 5 %\n",
    "target_ticker_count = 10           # set your desired count\n",
    "cutoff = 0.001                     # starting threshold\n",
    "\n",
    "while len(w[w >= cutoff]) > target_ticker_count:\n",
    "    # coarse bisection: shrink or grow cutoff\n",
    "    if len(w[w >= cutoff]) < target_ticker_count:\n",
    "        cutoff *= 0.9                 # too many left → raise threshold\n",
    "    else:\n",
    "        cutoff *= 1.1                 # too few left → lower threshold\n",
    "    print(cutoff, len(w[w >= cutoff]))\n",
    "\n",
    "w_kept = w[w >= cutoff]\n",
    "print(f\"\\nCutoff that yields {target_ticker_count} tickers: {cutoff:.6f}\")\n",
    "print(\"Tickers kept:\", len(w_kept))\n",
    "\n",
    "# 3. Re-balance so the remaining weights sum to 1\n",
    "w_balanced = w_kept / w_kept.sum()\n",
    "\n",
    "# 4. How many tickers survived?\n",
    "print(\"Tickers kept:\", len(w_balanced))\n",
    "print(\"Sum of re-balanced weights:\", w_balanced.sum())\n",
    "\n",
    "# 5. View the new weights\n",
    "w_balanced.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a87511d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- after you have w_balanced (the re-balanced Series) ---\n",
    "last_prices = df_train.loc[df_train.index[-1], w_balanced.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c5ffdd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "Index: 10 entries, CF to YUMC\n",
      "Series name: None\n",
      "Non-Null Count  Dtype  \n",
      "--------------  -----  \n",
      "10 non-null     float64\n",
      "dtypes: float64(1)\n",
      "memory usage: 160.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "\n",
    "initial_shares = INITIAL_CAPITAL * w_balanced / last_prices\n",
    "initial_shares.info() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b259e011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# daily portfolio value (scalar for each day)\n",
    "portfolio_value = (\n",
    "    df_test[initial_shares.index]          # prices of the 10 tickers\n",
    "    .mul(initial_shares)                  # shares × price\n",
    "    .sum(axis=1)                          # sum across tickers\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce3047f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Total Return (%)             2.560000\n",
       "Annualized Return (%)       39.810000\n",
       "Annualized Volatility (%)   27.610000\n",
       "Sharpe Ratio                 1.440000\n",
       "Max Drawdown (%)            -6.960000\n",
       "Sortino Ratio                1.820000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1) daily portfolio value series (already computed)\n",
    "# ------------------------------------------------------------------\n",
    "# portfolio_value = df_test[initial_shares.index].mul(initial_shares).sum(axis=1)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2) daily arithmetic returns of the portfolio\n",
    "# ------------------------------------------------------------------\n",
    "daily_ret = portfolio_value.pct_change().dropna()\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3) annualisation factor\n",
    "# ------------------------------------------------------------------\n",
    "ANN_FACTOR = 252          # trading days per year\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4) performance metrics\n",
    "# ------------------------------------------------------------------\n",
    "total_return   = (portfolio_value.iloc[-1] / portfolio_value.iloc[0] - 1)\n",
    "ann_return     = (1 + total_return) ** (ANN_FACTOR / len(daily_ret)) - 1\n",
    "ann_vol        = daily_ret.std() * np.sqrt(ANN_FACTOR)\n",
    "sharpe_ratio   = ann_return / ann_vol\n",
    "max_dd         = (portfolio_value / portfolio_value.cummax() - 1).min()\n",
    "sortino        = ann_return / (daily_ret[daily_ret < 0].std() * np.sqrt(ANN_FACTOR))\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 5) summary\n",
    "# ------------------------------------------------------------------\n",
    "summary = {\n",
    "    \"Total Return (%)\":         round(total_return * 100, 2),\n",
    "    \"Annualized Return (%)\":    round(ann_return * 100, 2),\n",
    "    \"Annualized Volatility (%)\":round(ann_vol * 100, 2),\n",
    "    \"Sharpe Ratio\":             round(sharpe_ratio, 2),\n",
    "    \"Max Drawdown (%)\":         round(max_dd * 100, 2),\n",
    "    \"Sortino Ratio\":            round(sortino, 2)\n",
    "}\n",
    "\n",
    "pd.Series(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "910da291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total Return (%)</th>\n",
       "      <th>Annualized Return (%)</th>\n",
       "      <th>Annualized Volatility (%)</th>\n",
       "      <th>Sharpe Ratio</th>\n",
       "      <th>Max Drawdown (%)</th>\n",
       "      <th>Sortino Ratio</th>\n",
       "      <th>Tracking Error (%)</th>\n",
       "      <th>Information Ratio</th>\n",
       "      <th>Train Start Date</th>\n",
       "      <th>Train End Date</th>\n",
       "      <th>Test Start Date</th>\n",
       "      <th>Test End Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Portfolio</th>\n",
       "      <td>2.560000</td>\n",
       "      <td>39.810000</td>\n",
       "      <td>27.610000</td>\n",
       "      <td>1.440000</td>\n",
       "      <td>-6.960000</td>\n",
       "      <td>1.820000</td>\n",
       "      <td>18.050000</td>\n",
       "      <td>2.370000</td>\n",
       "      <td>2023-01-03</td>\n",
       "      <td>2024-12-31</td>\n",
       "      <td>2025-01-02</td>\n",
       "      <td>2025-01-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VGT</th>\n",
       "      <td>-0.760000</td>\n",
       "      <td>-9.640000</td>\n",
       "      <td>30.630000</td>\n",
       "      <td>-0.310000</td>\n",
       "      <td>-6.120000</td>\n",
       "      <td>-0.390000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-01-03</td>\n",
       "      <td>2024-12-31</td>\n",
       "      <td>2025-01-02</td>\n",
       "      <td>2025-01-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Total Return (%) Annualized Return (%) Annualized Volatility (%) Sharpe Ratio Max Drawdown (%) Sortino Ratio Tracking Error (%) Information Ratio Train Start Date Train End Date Test Start Date Test End Date\n",
       "Portfolio         2.560000             39.810000                 27.610000     1.440000        -6.960000      1.820000          18.050000          2.370000       2023-01-03     2024-12-31      2025-01-02    2025-01-31\n",
       "VGT              -0.760000             -9.640000                 30.630000    -0.310000        -6.120000     -0.390000                NaN               NaN       2023-01-03     2024-12-31      2025-01-02    2025-01-31"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1) daily portfolio value series\n",
    "# ------------------------------------------------------------------\n",
    "portfolio_value = df_test[initial_shares.index].mul(initial_shares).sum(axis=1)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2) daily returns – portfolio vs benchmark (VGT)\n",
    "# ------------------------------------------------------------------\n",
    "daily_ret = portfolio_value.pct_change().dropna()\n",
    "bench_ret = df_test['VGT'].pct_change().dropna()\n",
    "\n",
    "# align dates\n",
    "common_dates = daily_ret.index.intersection(bench_ret.index)\n",
    "daily_ret = daily_ret.loc[common_dates]\n",
    "bench_ret = bench_ret.loc[common_dates]\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3) annualisation\n",
    "# ------------------------------------------------------------------\n",
    "ANN_FACTOR = 252\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4) helper\n",
    "# ------------------------------------------------------------------\n",
    "def _annualised_metrics(price_series):\n",
    "    ret = price_series.pct_change().dropna()\n",
    "    total = (price_series.iloc[-1] / price_series.iloc[0] - 1)\n",
    "    ann_ret = (1 + total) ** (ANN_FACTOR / len(ret)) - 1\n",
    "    ann_vol = ret.std() * np.sqrt(ANN_FACTOR)\n",
    "    sharpe  = ann_ret / ann_vol\n",
    "    max_dd  = (price_series / price_series.cummax() - 1).min()\n",
    "    sortino = ann_ret / (ret[ret < 0].std() * np.sqrt(ANN_FACTOR))\n",
    "    return {\n",
    "        \"Total Return (%)\": round(total * 100, 2),\n",
    "        \"Annualized Return (%)\": round(ann_ret * 100, 2),\n",
    "        \"Annualized Volatility (%)\": round(ann_vol * 100, 2),\n",
    "        \"Sharpe Ratio\": round(sharpe, 2),\n",
    "        \"Max Drawdown (%)\": round(max_dd * 100, 2),\n",
    "        \"Sortino Ratio\": round(sortino, 2)\n",
    "    }\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 5) metrics\n",
    "# ------------------------------------------------------------------\n",
    "port_metrics = _annualised_metrics(portfolio_value)\n",
    "bench_metrics = _annualised_metrics(df_test['VGT'])\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 6) tracking error & information ratio\n",
    "# ------------------------------------------------------------------\n",
    "tracking_error = (daily_ret - bench_ret).std() * np.sqrt(ANN_FACTOR)\n",
    "info_ratio     = (daily_ret.mean() - bench_ret.mean()) * ANN_FACTOR / tracking_error\n",
    "\n",
    "port_metrics[\"Tracking Error (%)\"] = round(tracking_error * 100, 2)\n",
    "port_metrics[\"Information Ratio\"]  = round(info_ratio, 2)\n",
    "port_metrics[\"Train Start Date\"] = train_start_date\n",
    "port_metrics[\"Train End Date\"] = train_end_date\n",
    "port_metrics[\"Test Start Date\"] = test_start_date\n",
    "port_metrics[\"Test End Date\"] = test_end_date\n",
    "\n",
    "bench_metrics[\"Train Start Date\"] = train_start_date\n",
    "bench_metrics[\"Train End Date\"] = train_end_date\n",
    "bench_metrics[\"Test Start Date\"] = test_start_date\n",
    "bench_metrics[\"Test End Date\"] = test_end_date\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 7) summary table\n",
    "# ------------------------------------------------------------------\n",
    "summary_df = pd.DataFrame({\n",
    "    \"Portfolio\": port_metrics,\n",
    "    \"VGT\": bench_metrics\n",
    "}).T\n",
    "\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "701dfbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary_df.to_csv('prices.csv', index=True)\n",
    "# append without writing the header again\n",
    "summary_df.to_csv('prices.csv', mode='a', header=False, index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311-py3.11 (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
