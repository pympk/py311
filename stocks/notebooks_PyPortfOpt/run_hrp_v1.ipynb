{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2156999f",
   "metadata": {},
   "source": [
    "## Hierarchical Risk Parity (HRP) Kimi-K2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ebd390",
   "metadata": {},
   "source": [
    "Think of HRP as having **two separate steps**:\n",
    "\n",
    "1. **“Where do we cut the tree?”** (build the dendrogram)  \n",
    "   → depends **only** on the correlation matrix you feed it.\n",
    "\n",
    "2. **“How risky is each branch?”** (recursive bisection)  \n",
    "   → uses the **covariance matrix** of the period you will actually trade.\n",
    "\n",
    "`hrp_weights` and `hrp_quick` differ only in **which period supplies the correlation matrix for step 1**.\n",
    "\n",
    "---\n",
    "\n",
    "### Two practical recipes\n",
    "\n",
    "| Method | Step-1 tree built on | Step-2 cov matrix built on | When to use |\n",
    "|---|---|---|---|\n",
    "| **hrp_weights(df_train)** | same as step-2 (`df_train`) | `df_train` | *Pure in-sample* – tree and risk both estimated on the same data. Simple, but may over-fit. |\n",
    "| **hrp_quick(df_train)** | **long history (`df`)** | `df_train` | *Hybrid* – stable tree from long history, fresh risk estimates from recent data. Lower turnover, more robust to regime changes. |\n",
    "\n",
    "---\n",
    "\n",
    "### Pros & cons in plain language\n",
    "\n",
    "| Aspect | hrp_weights(df_train) | hrp_quick(df_train) |\n",
    "|---|---|---|\n",
    "| **Stability** | Tree changes every time you roll the window → higher turnover, more re-balancing costs. | Tree is “anchored” by long history → weights evolve slowly, cheaper to trade. |\n",
    "| **Over-fitting** | Correlations estimated on short window can be noisy; clusters flip on a whim. | Long window smooths out noise; structure is less jumpy. |\n",
    "| **Regime sensitivity** | Reacts quickly if correlation breaks down. | May miss a *genuine* regime shift because the tree is “locked”. |\n",
    "| **Implementation effort** | One line; nothing to cache. | You must decide *how long* the “long” window should be and store the tree. |\n",
    "\n",
    "---\n",
    "\n",
    "### What do practitioners do?\n",
    "\n",
    "Most **quant desks / multi-asset funds** use the **hybrid approach** (i.e., the logic behind `hrp_quick`):\n",
    "\n",
    "- **Tree** = 3-5 years of daily returns (or even longer for low-turnover mandates).  \n",
    "- **Covariance** = rolling 6-12 months (Ledoit-Wolf shrinkage).  \n",
    "- Re-estimate the tree only once a year or when major structural breaks are detected.\n",
    "\n",
    "They do *not* rebuild the tree every month because:\n",
    "\n",
    "- Execution costs outweigh the marginal improvement.  \n",
    "- Correlation structure is fairly persistent at the **cluster** level (sectors, styles).  \n",
    "- Risk (volatility) is what really moves month-to-month, not the *order* of assets in the tree.\n",
    "\n",
    "---\n",
    "\n",
    "### Rule of thumb for you\n",
    "\n",
    "- If you are **learning / prototyping**, `hrp_weights(df_train)` is fine—easy to code, easier to debug.  \n",
    "- If you are **running live capital**, adopt the hybrid recipe:  \n",
    "  `linkage_cache = linkage(long_history)`  \n",
    "  `hrp_quick(df_train)`  \n",
    "  and refresh the cache only **quarterly or annually**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc4f971",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# parameters injected by papermill\n",
    "df_train = pd.read_csv(df_train_path, index_col=0, parse_dates=True)\n",
    "df_test = pd.read_csv(df_test_path, index_col=0, parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff29b931",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pprint\n",
    "import inspect  # <--- ADD THIS LINE\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# --- 1. PANDAS & IPYTHON OPTIONS ---\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 3000)\n",
    "pd.set_option('display.float_format', '{:.6f}'.format)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# --- 2. PROJECT PATH CONFIGURATION ---\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "PARENT_DIR = NOTEBOOK_DIR.parent\n",
    "ROOT_DIR = NOTEBOOK_DIR.parent.parent  # Adjust if your notebook is in a 'notebooks' subdirectory\n",
    "DATA_DIR = ROOT_DIR / 'data'\n",
    "SRC_DIR = ROOT_DIR / 'src'\n",
    "\n",
    "# Add 'src' to the Python path to import custom modules\n",
    "if str(SRC_DIR) not in sys.path:\n",
    "    sys.path.append(str(SRC_DIR))\n",
    "\n",
    "# --- 3. IMPORT CUSTOM MODULES ---\n",
    "import utils\n",
    "\n",
    "# --- 4. CONSTANTS ---\n",
    "INITIAL_CAPITAL = 100_000  # 100,000\n",
    "RISK_FREE_ANNUAL_RATE = 0.04\n",
    "BENCHMARK_TICKER = \"VGT\"\n",
    "\n",
    "# --- 5. VERIFICATION ---\n",
    "print(\"--- Path Configuration ---\")\n",
    "print(f\"✅ Project Root: {ROOT_DIR}\")\n",
    "print(f\"✅ Parent Dir:   {PARENT_DIR}\")\n",
    "print(f\"✅ Notebook Dir: {NOTEBOOK_DIR}\")\n",
    "print(f\"✅ Data Dir:     {DATA_DIR}\")\n",
    "print(f\"✅ Source Dir:   {SRC_DIR}\")\n",
    "assert all([ROOT_DIR.exists(), DATA_DIR.exists(), SRC_DIR.exists()]), \"A key directory was not found!\"\n",
    "\n",
    "print(\"\\n--- Module Verification ---\")\n",
    "print(f\"✅ Successfully imported 'utils' and 'plotting_utils'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd962df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_parquet(DATA_DIR / 'df_adj_close.parquet')\n",
    "# print(f'df:\\n{df}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7742a0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# # 2. --- This is the code to split your DataFrame ---\n",
    "\n",
    "# # Create a boolean mask for the years 2023 and 2024\n",
    "# mask = df.index.year.isin([2023, 2024])\n",
    "\n",
    "# # Apply the mask to get the first DataFrame\n",
    "# df_train = df[mask]\n",
    "\n",
    "# # Apply the inverse of the mask (using ~) to get the second DataFrame\n",
    "# df_remain = df[~mask]\n",
    "\n",
    "# test_mask = (df_remain.index.year == 2025) & df_remain.index.month.isin([1])\n",
    "# df_test = df_remain.loc[test_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb92b0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_start_date = df_train.index.min().strftime('%Y-%m-%d')\n",
    "train_end_date = df_train.index.max().strftime('%Y-%m-%d')\n",
    "print(f'train_start_date: {train_start_date}')\n",
    "print(f'train_end_date: {train_end_date}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5beca26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_start_date = df_test.index.min().strftime('%Y-%m-%d')\n",
    "test_end_date = df_test.index.max().strftime('%Y-%m-%d')\n",
    "print(f'test_start_date: {test_start_date}')\n",
    "print(f'test_end_date: {test_end_date}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfd4ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "from scipy.spatial.distance import squareform\n",
    "from sklearn.covariance import ledoit_wolf\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Helpers\n",
    "# ------------------------------------------------------------------\n",
    "def _log_returns(price_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Log returns, drop first NA row.\"\"\"\n",
    "    return np.log(price_df / price_df.shift(1)).dropna()\n",
    "\n",
    "def _cov2corr(cov: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Covariance matrix → correlation matrix.\"\"\"\n",
    "    std = np.sqrt(np.diag(cov))\n",
    "    corr = cov / np.outer(std, std)\n",
    "    corr[corr < -1], corr[corr > 1] = -1, 1\n",
    "    return corr\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Core HRP\n",
    "# ------------------------------------------------------------------\n",
    "def _quasi_diagonal(link) -> np.ndarray:\n",
    "    \"\"\"Return reordering indices from linkage.\"\"\"\n",
    "    return np.array(dendrogram(link, no_plot=True)[\"leaves\"])\n",
    "\n",
    "def _recursive_bisection(cov: np.ndarray, perm: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Allocate top-down via inverse-variance weighting.\"\"\"\n",
    "    w = np.ones(len(perm))\n",
    "    clusters = [perm]\n",
    "    while clusters:\n",
    "        new_clusters = []\n",
    "        for idx in clusters:\n",
    "            if len(idx) == 1:\n",
    "                continue\n",
    "            half = len(idx) // 2\n",
    "            left, right = idx[:half], idx[half:]\n",
    "            # variance of each branch\n",
    "            var_l = np.linalg.inv(cov[np.ix_(left, left)]).sum()\n",
    "            var_r = np.linalg.inv(cov[np.ix_(right, right)]).sum()\n",
    "            alpha = 1 - var_l / (var_l + var_r)\n",
    "            w[left] *= alpha\n",
    "            w[right] *= (1 - alpha)\n",
    "            new_clusters.extend([left, right])\n",
    "        clusters = new_clusters\n",
    "    return w\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Public entry point\n",
    "# ------------------------------------------------------------------\n",
    "def hrp_weights(price_df: pd.DataFrame,\n",
    "                linkage_method: str = \"single\") -> pd.Series:\n",
    "    \"\"\"\n",
    "    Build HRP portfolio weights from a DataFrame of daily adjusted-close prices.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    price_df : pd.DataFrame\n",
    "        Columns = tickers, index = DatetimeIndex of daily prices.\n",
    "    linkage_method : str\n",
    "        Any valid scipy linkage method (\"single\", \"ward\", etc.).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.Series\n",
    "        Index = tickers, values = portfolio weights (sum = 1).\n",
    "    \"\"\"\n",
    "    returns = _log_returns(price_df)\n",
    "    cov, _ = ledoit_wolf(returns)\n",
    "    corr = _cov2corr(cov)\n",
    "\n",
    "    # distance matrix → condensed 1-D vector\n",
    "    dist = np.sqrt(np.clip((1 - corr) / 2, 0, 1))\n",
    "    dist_vec = squareform(dist, checks=False)\n",
    "\n",
    "    link = linkage(dist_vec, method=linkage_method)\n",
    "    perm = _quasi_diagonal(link)\n",
    "\n",
    "    raw = _recursive_bisection(cov, perm)\n",
    "    w = pd.Series(raw, index=returns.columns).sort_index()\n",
    "    return w / w.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90968af",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = hrp_weights(df_train)          # df is your 643×1518 price DataFrame\n",
    "# print(weights[weights > 0.001].round(4))\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7243ce05",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(weights.sort_values(ascending=False).head(10))\n",
    "print(\"Total tickers in portfolio :\", len(weights))\n",
    "print(\"Sum of weights             :\", weights.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc2798c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Run HRP once\n",
    "w = hrp_weights(df_train)          # or hrp_quick(df_train)\n",
    "\n",
    "# 2. Drop anything < 5 %\n",
    "target_ticker_count = 10           # set your desired count\n",
    "cutoff = 0.001                     # starting threshold\n",
    "\n",
    "while len(w[w >= cutoff]) > target_ticker_count:\n",
    "    # coarse bisection: shrink or grow cutoff\n",
    "    if len(w[w >= cutoff]) < target_ticker_count:\n",
    "        cutoff *= 0.9                 # too many left → raise threshold\n",
    "    else:\n",
    "        cutoff *= 1.1                 # too few left → lower threshold\n",
    "    print(cutoff, len(w[w >= cutoff]))\n",
    "\n",
    "w_kept = w[w >= cutoff]\n",
    "print(f\"\\nCutoff that yields {target_ticker_count} tickers: {cutoff:.6f}\")\n",
    "print(\"Tickers kept:\", len(w_kept))\n",
    "\n",
    "# 3. Re-balance so the remaining weights sum to 1\n",
    "w_balanced = w_kept / w_kept.sum()\n",
    "\n",
    "# 4. How many tickers survived?\n",
    "print(\"Tickers kept:\", len(w_balanced))\n",
    "print(\"Sum of re-balanced weights:\", w_balanced.sum())\n",
    "\n",
    "# 5. View the new weights\n",
    "w_balanced.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a87511d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- after you have w_balanced (the re-balanced Series) ---\n",
    "last_prices = df_train.loc[df_train.index[-1], w_balanced.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5ffdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "initial_shares = INITIAL_CAPITAL * w_balanced / last_prices\n",
    "initial_shares.info() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b259e011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# daily portfolio value (scalar for each day)\n",
    "portfolio_value = (\n",
    "    df_test[initial_shares.index]          # prices of the 10 tickers\n",
    "    .mul(initial_shares)                  # shares × price\n",
    "    .sum(axis=1)                          # sum across tickers\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3047f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1) daily portfolio value series (already computed)\n",
    "# ------------------------------------------------------------------\n",
    "# portfolio_value = df_test[initial_shares.index].mul(initial_shares).sum(axis=1)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2) daily arithmetic returns of the portfolio\n",
    "# ------------------------------------------------------------------\n",
    "daily_ret = portfolio_value.pct_change().dropna()\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3) annualisation factor\n",
    "# ------------------------------------------------------------------\n",
    "ANN_FACTOR = 252          # trading days per year\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4) performance metrics\n",
    "# ------------------------------------------------------------------\n",
    "total_return   = (portfolio_value.iloc[-1] / portfolio_value.iloc[0] - 1)\n",
    "ann_return     = (1 + total_return) ** (ANN_FACTOR / len(daily_ret)) - 1\n",
    "ann_vol        = daily_ret.std() * np.sqrt(ANN_FACTOR)\n",
    "sharpe_ratio   = ann_return / ann_vol\n",
    "max_dd         = (portfolio_value / portfolio_value.cummax() - 1).min()\n",
    "sortino        = ann_return / (daily_ret[daily_ret < 0].std() * np.sqrt(ANN_FACTOR))\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 5) summary\n",
    "# ------------------------------------------------------------------\n",
    "summary = {\n",
    "    \"Total Return (%)\":         round(total_return * 100, 2),\n",
    "    \"Annualized Return (%)\":    round(ann_return * 100, 2),\n",
    "    \"Annualized Volatility (%)\":round(ann_vol * 100, 2),\n",
    "    \"Sharpe Ratio\":             round(sharpe_ratio, 2),\n",
    "    \"Max Drawdown (%)\":         round(max_dd * 100, 2),\n",
    "    \"Sortino Ratio\":            round(sortino, 2)\n",
    "}\n",
    "\n",
    "pd.Series(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910da291",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1) daily portfolio value series\n",
    "# ------------------------------------------------------------------\n",
    "portfolio_value = df_test[initial_shares.index].mul(initial_shares).sum(axis=1)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2) daily returns – portfolio vs benchmark (VGT)\n",
    "# ------------------------------------------------------------------\n",
    "daily_ret = portfolio_value.pct_change().dropna()\n",
    "bench_ret = df_test['VGT'].pct_change().dropna()\n",
    "\n",
    "# align dates\n",
    "common_dates = daily_ret.index.intersection(bench_ret.index)\n",
    "daily_ret = daily_ret.loc[common_dates]\n",
    "bench_ret = bench_ret.loc[common_dates]\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3) annualisation\n",
    "# ------------------------------------------------------------------\n",
    "ANN_FACTOR = 252\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4) helper\n",
    "# ------------------------------------------------------------------\n",
    "def _annualised_metrics(price_series):\n",
    "    ret = price_series.pct_change().dropna()\n",
    "    total = (price_series.iloc[-1] / price_series.iloc[0] - 1)\n",
    "    ann_ret = (1 + total) ** (ANN_FACTOR / len(ret)) - 1\n",
    "    ann_vol = ret.std() * np.sqrt(ANN_FACTOR)\n",
    "    sharpe  = ann_ret / ann_vol\n",
    "    max_dd  = (price_series / price_series.cummax() - 1).min()\n",
    "    sortino = ann_ret / (ret[ret < 0].std() * np.sqrt(ANN_FACTOR))\n",
    "    return {\n",
    "        \"Total Return (%)\": round(total * 100, 2),\n",
    "        \"Annualized Return (%)\": round(ann_ret * 100, 2),\n",
    "        \"Annualized Volatility (%)\": round(ann_vol * 100, 2),\n",
    "        \"Sharpe Ratio\": round(sharpe, 2),\n",
    "        \"Max Drawdown (%)\": round(max_dd * 100, 2),\n",
    "        \"Sortino Ratio\": round(sortino, 2)\n",
    "    }\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 5) metrics\n",
    "# ------------------------------------------------------------------\n",
    "port_metrics = _annualised_metrics(portfolio_value)\n",
    "bench_metrics = _annualised_metrics(df_test['VGT'])\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 6) tracking error & information ratio\n",
    "# ------------------------------------------------------------------\n",
    "tracking_error = (daily_ret - bench_ret).std() * np.sqrt(ANN_FACTOR)\n",
    "info_ratio     = (daily_ret.mean() - bench_ret.mean()) * ANN_FACTOR / tracking_error\n",
    "\n",
    "port_metrics[\"Tracking Error (%)\"] = round(tracking_error * 100, 2)\n",
    "port_metrics[\"Information Ratio\"]  = round(info_ratio, 2)\n",
    "port_metrics[\"Train Start Date\"] = train_start_date\n",
    "port_metrics[\"Train End Date\"] = train_end_date\n",
    "port_metrics[\"Test Start Date\"] = test_start_date\n",
    "port_metrics[\"Test End Date\"] = test_end_date\n",
    "\n",
    "bench_metrics[\"Train Start Date\"] = train_start_date\n",
    "bench_metrics[\"Train End Date\"] = train_end_date\n",
    "bench_metrics[\"Test Start Date\"] = test_start_date\n",
    "bench_metrics[\"Test End Date\"] = test_end_date\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 7) summary table\n",
    "# ------------------------------------------------------------------\n",
    "summary_df = pd.DataFrame({\n",
    "    \"Portfolio\": port_metrics,\n",
    "    \"VGT\": bench_metrics\n",
    "}).T\n",
    "\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701dfbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary_df.to_csv('prices.csv', index=True)\n",
    "# append without writing the header again\n",
    "summary_df.to_csv('prices.csv', mode='a', header=False, index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311-py3.11 (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
