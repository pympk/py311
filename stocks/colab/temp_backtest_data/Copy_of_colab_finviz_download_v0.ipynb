{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HplHE-XnwHFz"
      },
      "source": [
        "===== TURN ON POWERTOY AWAKE to KEEP CONNECTION ALIVE =====\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7SsL7bwocxCj",
        "outputId": "dfa15ebd-fe19-4d86-edd1-59d762537ca2"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJrn6gNKYrBo",
        "outputId": "8ca4a72f-d9d5-4951-a666-5e6aa9beabbf"
      },
      "outputs": [],
      "source": [
        "!pip install cloudscraper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewLHfyexZEYd",
        "outputId": "5dc7407f-e900-4238-e944-3312b2fbce4e"
      },
      "outputs": [],
      "source": [
        " !pip install curl_cffi pandas lxml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from curl_cffi import requests\n",
        "import pandas as pd\n",
        "from io import StringIO\n",
        "\n",
        "\n",
        "# def scrape_finviz_curl():\n",
        "#     url = (\n",
        "#         \"https://finviz.com/screener.ashx?v=152&ft=4&o=-marketcap\"\n",
        "#         \"&c=0,1,2,79,3,4,5,129,6,7,8,9,10,11,12,13,73,74,14,130,131,\"\n",
        "#         \"15,16,77,17,18,19,20,21,23,22,132,133,82,78,127,128,24,25,85,\"\n",
        "#         \"26,27,28,29,30,31,84,32,33,34,35,36,37,38,39,40,41,42,43,44,\"\n",
        "#         \"45,46,47,48,49,50,51,52,53,54,55,56,57,58,134,125,126,59,68,\"\n",
        "#         \"70,80,83,76,60,61,62,63,64,67,69,81,86,87,88,65,66,103,100,\"\n",
        "#         \"107,108,109,112,113,114,115,116,117,120,121,122,105&r=921\"\n",
        "#     )\n",
        "\n",
        "#     try:\n",
        "#         print(\"Fetching data with curl_cffi (mimicking Chrome)...\")\n",
        "#         # 'impersonate' makes the server think we are a real Chrome browser\n",
        "#         response = requests.get(url, impersonate=\"chrome120\")\n",
        "\n",
        "#         if response.status_code != 200:\n",
        "#             print(f\"Failed. Status Code: {response.status_code}\")\n",
        "#             return None\n",
        "\n",
        "#         # Parse HTML\n",
        "#         dfs = pd.read_html(StringIO(response.text))\n",
        "\n",
        "#         # Find the right table (usually the largest one)\n",
        "#         screener_df = max(dfs, key=len)\n",
        "\n",
        "#         # Cleanup headers if needed\n",
        "#         if \"Ticker\" not in screener_df.columns:\n",
        "#             screener_df.columns = screener_df.iloc[0]\n",
        "#             screener_df = screener_df[1:]\n",
        "\n",
        "#         screener_df = screener_df.reset_index(drop=True)\n",
        "\n",
        "#         print(f\"Success! Scraped {len(screener_df)} rows.\")\n",
        "#         print(screener_df.head())\n",
        "\n",
        "#         return screener_df\n",
        "\n",
        "#     except Exception as e:\n",
        "#         print(f\"Error: {e}\")\n",
        "#         # Debug tip: If this fails, Finviz might be serving a CAPTCHA page.\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     scrape_finviz_curl()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VyKSKQBVZH8q",
        "outputId": "2cbe165d-e983-47bf-f140-7ed7f1e73329"
      },
      "outputs": [],
      "source": [
        "from curl_cffi import requests\n",
        "import pandas as pd\n",
        "from io import StringIO\n",
        "\n",
        "\n",
        "def scrape_finviz_curl(url):\n",
        "    # url = (\n",
        "    #     \"https://finviz.com/screener.ashx?v=152&ft=4&o=-marketcap\"\n",
        "    #     \"&c=0,1,2,79,3,4,5,129,6,7,8,9,10,11,12,13,73,74,14,130,131,\"\n",
        "    #     \"15,16,77,17,18,19,20,21,23,22,132,133,82,78,127,128,24,25,85,\"\n",
        "    #     \"26,27,28,29,30,31,84,32,33,34,35,36,37,38,39,40,41,42,43,44,\"\n",
        "    #     \"45,46,47,48,49,50,51,52,53,54,55,56,57,58,134,125,126,59,68,\"\n",
        "    #     \"70,80,83,76,60,61,62,63,64,67,69,81,86,87,88,65,66,103,100,\"\n",
        "    #     \"107,108,109,112,113,114,115,116,117,120,121,122,105&r=921\"\n",
        "    # )\n",
        "\n",
        "    try:\n",
        "        print(\"Fetching data with curl_cffi (mimicking Chrome)...\")\n",
        "        # 'impersonate' makes the server think we are a real Chrome browser\n",
        "        response = requests.get(url, impersonate=\"chrome120\")\n",
        "\n",
        "        if response.status_code != 200:\n",
        "            print(f\"Failed. Status Code: {response.status_code}\")\n",
        "            return None\n",
        "\n",
        "        # Parse HTML\n",
        "        dfs = pd.read_html(StringIO(response.text))\n",
        "\n",
        "        # Find the right table (usually the largest one)\n",
        "        screener_df = max(dfs, key=len)\n",
        "\n",
        "        # Cleanup headers if needed\n",
        "        if \"Ticker\" not in screener_df.columns:\n",
        "            screener_df.columns = screener_df.iloc[0]\n",
        "            screener_df = screener_df[1:]\n",
        "\n",
        "        screener_df = screener_df.reset_index(drop=True)\n",
        "\n",
        "        print(f\"Success! Scraped {len(screener_df)} rows.\")\n",
        "        print(screener_df.head())\n",
        "\n",
        "        return screener_df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        # Debug tip: If this fails, Finviz might be serving a CAPTCHA page.\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     scrape_finviz_curl()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipBGKQUI84d_"
      },
      "source": [
        "==== SET Download Location ===="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRjhn4tM8eYO"
      },
      "outputs": [],
      "source": [
        "download_to_PC = True\n",
        "download_to_Google_Drive = False\n",
        "\n",
        "# download_to_PC = False\n",
        "# download_to_Google_Drive = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzjWU0MyA8cT"
      },
      "source": [
        "===== SET download_to_PC and end_index ====="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LnCnXVOGAzi8"
      },
      "outputs": [],
      "source": [
        "# symbols: The string (or sequence) to slice.\n",
        "# start_index: Optional. The starting index of the slice. If None, defaults to 0.\n",
        "# end_index: Optional. The ending index of the slice (exclusive). If None, defaults to the end of the string.\n",
        "# step_value: Optional. The step value for the slice. If None, defaults to 1.\n",
        "\n",
        "# True download to PC, False download to Google Drive\n",
        "download_to_PC = True  # True download to PC\n",
        "# download_to_PC = False  # True download to Google Drive\n",
        "\n",
        "# Download all if end_iddex = None\n",
        "end_index = None\n",
        "# end_index = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWX6sLNsmsCr",
        "outputId": "2688d4e1-e8c7-47a1-bb66-b1b21a681b70"
      },
      "outputs": [],
      "source": [
        "start_index = None\n",
        "\n",
        "step_value = None\n",
        "slice_obj = slice(start_index, end_index, step_value)  # Create a slice object\n",
        "\n",
        "print(f\"slice of symbols: symbols[{slice_obj}]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2xUTYT3wHF4",
        "outputId": "b84b7470-5404-4bf9-9caf-615c5825704c"
      },
      "outputs": [],
      "source": [
        "! pip install fake-useragent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTCD6Lo1wHF4"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from fake_useragent import UserAgent\n",
        "\n",
        "\n",
        "# selector = '.styled-table-new'\n",
        "\n",
        "# selector = 'screener-table'\n",
        "\n",
        "# screener-table\n",
        "# screener-table\n",
        "\n",
        "\n",
        "ua = UserAgent()  # Initialize UserAgent\n",
        "\n",
        "\n",
        "# def download_table(url, selector):\n",
        "def scrape_finviz_curl(url, selector):\n",
        "    \"\"\"\n",
        "    Downloads table data from a Yahoo Finance page with rate limiting.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Add a User-Agent header to mimic a browser\n",
        "        # headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}  # Example User-Agent\n",
        "        new_user_agent = ua.random\n",
        "        headers = {\"User-Agent\": new_user_agent}\n",
        "        response = requests.get(url, headers=headers)\n",
        "        response.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "        table_body = soup.select_one(selector)\n",
        "\n",
        "        if table_body is None:\n",
        "            print(f\"Error: Table body not found using selector: {selector}\")\n",
        "            return None\n",
        "\n",
        "        rows = table_body.find_all(\"tr\")\n",
        "        if not rows:\n",
        "            print(\"Error: No rows found in the table.\")\n",
        "            return None\n",
        "\n",
        "        # Extract headers from the first row (th elements)\n",
        "        headers_list = [th.text.strip() for th in rows[0].find_all(\"th\")]\n",
        "\n",
        "        data = []\n",
        "        for row in rows:\n",
        "            cells = row.find_all(\"td\")\n",
        "            row_data = [cell.text.strip() for cell in cells]\n",
        "            if row_data:  # Only append if the row has data\n",
        "                data.append(row_data)\n",
        "\n",
        "        if not data:\n",
        "            print(\"Error: No data found in the table rows.\")\n",
        "            return None\n",
        "\n",
        "        df = pd.DataFrame(data, columns=headers_list)\n",
        "        return df\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error during request: {e}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dPYRMTmCS1s6"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from fake_useragent import UserAgent\n",
        "\n",
        "# selector = '.styled-table-new'\n",
        "\n",
        "selector = \"screener-table\"\n",
        "\n",
        "\n",
        "ua = UserAgent()  # Initialize UserAgent\n",
        "\n",
        "\n",
        "def download_table(url, selector):\n",
        "    \"\"\"\n",
        "    Downloads table data from a Yahoo Finance page with rate limiting.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Add a User-Agent header to mimic a browser\n",
        "        # headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}  # Example User-Agent\n",
        "        new_user_agent = ua.random\n",
        "        headers = {\"User-Agent\": new_user_agent}\n",
        "        response = requests.get(url, headers=headers)\n",
        "        response.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "        table_body = soup.select_one(selector)\n",
        "\n",
        "        if table_body is None:\n",
        "            print(f\"Error: Table body not found using selector: {selector}\")\n",
        "            return None\n",
        "\n",
        "        rows = table_body.find_all(\"tr\")\n",
        "        if not rows:\n",
        "            print(\"Error: No rows found in the table.\")\n",
        "            return None\n",
        "\n",
        "        # Extract headers from the first row (th elements)\n",
        "        headers_list = [th.text.strip() for th in rows[0].find_all(\"th\")]\n",
        "\n",
        "        data = []\n",
        "        for row in rows:\n",
        "            cells = row.find_all(\"td\")\n",
        "            row_data = [cell.text.strip() for cell in cells]\n",
        "            if row_data:  # Only append if the row has data\n",
        "                data.append(row_data)\n",
        "\n",
        "        if not data:\n",
        "            print(\"Error: No data found in the table rows.\")\n",
        "            return None\n",
        "\n",
        "        df = pd.DataFrame(data, columns=headers_list)\n",
        "        return df\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error during request: {e}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7WcYi3BecotV"
      },
      "outputs": [],
      "source": [
        "# all_stocks_and_etfs_columns\n",
        "# https://finviz.com/screener.ashx?v=152&ft=4&o=-marketcap&r=721&c=0,1,2,79,3,4,5,129,6,7,8,9,10,11,12,13,73,74,75,14,130,131,15,16,77,17,18,19,20,21,23,22,132,133,82,78,127,128,24,25,85,26,27,28,29,30,31,84,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,134,125,126,59,68,70,80,83,76,60,61,62,63,64,67,69,81,86,87,88,65,66,103,100,107,108,109,112,113,114,115,116,117,120,121,122,105"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RpuXvF2ncotV"
      },
      "outputs": [],
      "source": [
        "# all_etf_columns_sorted_by_AUM\n",
        "# https://finviz.com/screener.ashx?v=152&ft=4&o=-e.assetsundermanagement&c=0,1,2,3,4,5,129,75,14,130,131,31,84,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,134,125,126,59,70,80,83,60,61,63,64,67,81,86,87,88,65,66,103,100,107,108,109,112,113,114,115,116,117,120,121,122,105"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AgZUn7phcotW"
      },
      "outputs": [],
      "source": [
        "# all_stock_columns_sorted_by_market_cap\n",
        "# https://finviz.com/screener.ashx?v=152&ft=4&o=-marketcap&c=0,1,2,79,3,4,5,129,6,7,8,9,10,11,12,13,73,74,75,14,130,131,15,16,77,17,18,19,20,21,23,22,132,133,82,78,127,128,24,25,85,26,27,28,29,30,31,84,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,134,125,126,59,68,70,80,83,76,60,61,62,63,64,67,69,81,86,87,88,65,66"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyyiAJcecotX"
      },
      "outputs": [],
      "source": [
        "# url_all_columns = '0,1,2,79,3,4,5,129,6,7,8,9,10,11,12,13,73,74,75,14,130,131,15,16,77,17,18,19,20,21,23,22,132,133,82,78,127,128,24,25,85,26,27,28,29,30,31,84,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,134,125,126,59,68,70,80,83,76,60,61,62,63,64,67,69,81,86,87,88,65,66,103,100,107,108,109,112,113,114,115,116,117,120,121,122,105'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4cZ5NJQXE2I"
      },
      "outputs": [],
      "source": [
        "# remove dupicate Dividend column 75 with $ values, kept Dividend column with % value\n",
        "url_all_columns = \"0,1,2,79,3,4,5,129,6,7,8,9,10,11,12,13,73,74,14,130,131,15,16,77,17,18,19,20,21,23,22,132,133,82,78,127,128,24,25,85,26,27,28,29,30,31,84,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,134,125,126,59,68,70,80,83,76,60,61,62,63,64,67,69,81,86,87,88,65,66,103,100,107,108,109,112,113,114,115,116,117,120,121,122,105\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xilLYhetMk-y",
        "outputId": "84ffc03a-45da-4be8-bf0f-7f704673c3b0"
      },
      "outputs": [],
      "source": [
        "print(f\"download the top 1160 stocks by market cap\")\n",
        "url_mktcap = \"https://finviz.com/screener.ashx?v=152&ft=4&o=-marketcap&c=\"\n",
        "# url_mktcap_all_columns = '0,1,2,79,3,4,5,129,6,7,8,9,10,11,12,13,73,74,75,14,130,131,15,16,77,17,18,19,20,21,23,22,132,133,82,78,127,128,24,25,85,26,27,28,29,30,31,84,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,134,125,126,59,68,70,80,83,76,60,61,62,63,64,67,69,81,86,87,88,65,66'\n",
        "url_mktcap_rows = [\n",
        "    \"&r=1\",\n",
        "    \"&r=21\",\n",
        "    \"&r=41\",\n",
        "    \"&r=61\",\n",
        "    \"&r=81\",\n",
        "    \"&r=101\",\n",
        "    \"&r=121\",\n",
        "    \"&r=141\",\n",
        "    \"&r=161\",\n",
        "    \"&r=181\",\n",
        "    \"&r=201\",\n",
        "    \"&r=221\",\n",
        "    \"&r=241\",\n",
        "    \"&r=261\",\n",
        "    \"&r=281\",\n",
        "    \"&r=301\",\n",
        "    \"&r=321\",\n",
        "    \"&r=341\",\n",
        "    \"&r=361\",\n",
        "    \"&r=381\",\n",
        "    \"&r=401\",\n",
        "    \"&r=421\",\n",
        "    \"&r=441\",\n",
        "    \"&r=461\",\n",
        "    \"&r=481\",\n",
        "    \"&r=501\",\n",
        "    \"&r=521\",\n",
        "    \"&r=541\",\n",
        "    \"&r=561\",\n",
        "    \"&r=581\",\n",
        "    \"&r=601\",\n",
        "    \"&r=621\",\n",
        "    \"&r=641\",\n",
        "    \"&r=661\",\n",
        "    \"&r=681\",\n",
        "    \"&r=701\",\n",
        "    \"&r=721\",\n",
        "    \"&r=741\",\n",
        "    \"&r=761\",\n",
        "    \"&r=781\",\n",
        "    \"&r=801\",\n",
        "    \"&r=821\",\n",
        "    \"&r=841\",\n",
        "    \"&r=861\",\n",
        "    \"&r=881\",\n",
        "    \"&r=901\",\n",
        "    \"&r=921\",\n",
        "    \"&r=941\",\n",
        "    \"&r=961\",\n",
        "    \"&r=981\",\n",
        "    \"&r=1001\",\n",
        "    \"&r=1021\",\n",
        "    \"&r=1041\",\n",
        "    \"&r=1061\",\n",
        "    \"&r=1081\",\n",
        "    \"&r=1101\",\n",
        "    \"&r=1121\",\n",
        "    \"&r=1141\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4mC2RrCcotY",
        "outputId": "c3cbca39-bee4-4ec3-9a64-542b1f533228"
      },
      "outputs": [],
      "source": [
        "print(f\"download the top 400 ETFs by AUM\")\n",
        "url_etfs = \"https://finviz.com/screener.ashx?v=152&ft=4&o=-e.assetsundermanagement&c=\"\n",
        "# url_etfs_all_columns = '0,1,2,3,4,5,129,75,14,130,131,31,84,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,134,125,126,59,70,80,83,60,61,63,64,67,81,86,87,88,65,66,103,100,107,108,109,112,113,114,115,116,117,120,121,122,105'\n",
        "url_etfs_rows = [\n",
        "    \"&r=1\",\n",
        "    \"&r=21\",\n",
        "    \"&r=41\",\n",
        "    \"&r=61\",\n",
        "    \"&r=81\",\n",
        "    \"&r=101\",\n",
        "    \"&r=121\",\n",
        "    \"&r=141\",\n",
        "    \"&r=161\",\n",
        "    \"&r=181\",\n",
        "    \"&r=201\",\n",
        "    \"&r=221\",\n",
        "    \"&r=241\",\n",
        "    \"&r=261\",\n",
        "    \"&r=281\",\n",
        "    \"&r=301\",\n",
        "    \"&r=321\",\n",
        "    \"&r=341\",\n",
        "    \"&r=361\",\n",
        "    \"&r=381\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYhnaEoxwHF6",
        "outputId": "4ba300fe-fa58-46bb-e15c-026df28a0210"
      },
      "outputs": [],
      "source": [
        "urls_mktcap = []\n",
        "\n",
        "for _rows in url_mktcap_rows:\n",
        "    # _url = url_mktcap + url_mktcap_all_columns + _rows\n",
        "    _url = url_mktcap + url_all_columns + _rows\n",
        "    urls_mktcap.append(_url)\n",
        "\n",
        "print(f\"Total number of urls: {len(urls_mktcap)}\")\n",
        "print(f\"First 3 urls:\\n{urls_mktcap[0:3]}\")  # Print the length of the list of url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJ38HN3UcotZ",
        "outputId": "8bf5d3ec-02d7-4497-e091-c23c7b7ed575"
      },
      "outputs": [],
      "source": [
        "urls_etfs = []\n",
        "\n",
        "for _rows in url_etfs_rows:\n",
        "    # _url = url_etfs + url_etfs_all_columns + _rows\n",
        "    _url = url_etfs + url_all_columns + _rows\n",
        "    urls_etfs.append(_url)\n",
        "\n",
        "print(f\"Total number of urls: {len(urls_etfs)}\")\n",
        "print(f\"First 3 urls:\\n{urls_etfs[0:3]}\")  # Print the length of the list of url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWfH7PLrwHF7",
        "outputId": "19a1fab0-86a7-4bed-cd16-6c1f06b38225"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "print(\n",
        "    f\"===== -marketcap and -e.assetsundermanagement sort marketcap and AUM in descendiing order =====\"\n",
        ")\n",
        "print(f\"===== download 1160 largest marketcap stocks and 400 largest AUM ETFs =====\")\n",
        "urls = urls_mktcap + urls_etfs\n",
        "shuffled_urls = random.sample(urls, len(urls))\n",
        "print(f\"len(shuffled_urls): {len(shuffled_urls)}\")\n",
        "print(f\"shuffled_urls[0:10]\")\n",
        "for url in shuffled_urls[0:10]:\n",
        "    print(f\"{url}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WyA3q6n9wHF7"
      },
      "outputs": [],
      "source": [
        "# test_urls = [\n",
        "#   'https://finviz.com/screener.ashx?v=152&ft=4&o=-marketcap&c=0,1,2,3,4,6,14,42,43,44,45,46,47,48,49,50,51,52,53,54,57,58,59,67,65,66,103,100,109,120,121,122&r=1',\n",
        "#   'https://finviz.com/screener.ashx?v=152&ft=4&o=-e.assetsundermanagement&c=0,1,2,3,4,6,14,42,43,44,45,46,47,48,49,50,51,52,53,54,57,58,59,67,65,66,103,100,109,120,121,122&r=101',\n",
        "#   'https://finviz.com/screener.ashx?v=152&ft=4&o=-marketcap&c=0,1,2,3,4,6,14,42,43,44,45,46,47,48,49,50,51,52,53,54,57,58,59,67,65,66,103,100,109,120,121,122&r=601',\n",
        "#   ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "id": "sp4KYbgEcotc",
        "outputId": "5bec4b0f-3146-4a04-b0d6-1d1b05259b5b"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "import random\n",
        "\n",
        "total_urls_to_download = len(shuffled_urls)\n",
        "\n",
        "df = pd.DataFrame()  # Initialized an empty DataFrame\n",
        "processed_count = 0\n",
        "\n",
        "# for url in test_urls:\n",
        "# for url in shuffled_urls[0:3]:\n",
        "for url in shuffled_urls[slice_obj]:\n",
        "    # Introduce a delay between requests (adjust as needed)\n",
        "    delay_seconds = random.uniform(2, 4.5)  # Sleep between 2 and 5 seconds\n",
        "    # print(f\"Downloading {symbol}. Sleeping for {delay_seconds:.2f} seconds...\")\n",
        "    processed_count += 1\n",
        "    print(\n",
        "        f\"Downloading {url}. Sleeping for {delay_seconds:.2f} seconds.  Processed {processed_count} / {total_urls_to_download} urls\"\n",
        "    )\n",
        "    time.sleep(delay_seconds)\n",
        "\n",
        "    # selector = '#screener-output'\n",
        "    # selector = \"table.styled-table-new.screener_table\"\n",
        "    selector = \"table.styled-table-new.is-rounded.is-tabular-nums.w-full.screener_table\"\n",
        "    # selector =\n",
        "    df_temp = scrape_finviz_curl(url)\n",
        "    # df_temp = download_table(url, selector)\n",
        "\n",
        "    if df_temp is not None:\n",
        "        df = pd.concat([df, df_temp], ignore_index=True)\n",
        "    else:\n",
        "        print(f\"Failed to download data for {url}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dd4c0f02"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from fake_useragent import UserAgent\n",
        "\n",
        "ua = UserAgent()\n",
        "url_to_inspect = shuffled_urls[0]\n",
        "\n",
        "print(f\"Re-inspecting URL: {url_to_inspect}\")\n",
        "\n",
        "try:\n",
        "    headers = {\"User-Agent\": ua.random}\n",
        "    response = requests.get(url_to_inspect, headers=headers)\n",
        "    response.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)\n",
        "\n",
        "    print(f\"Length of response content: {len(response.content)} bytes\")\n",
        "    print(\"First 500 characters of response content:\")\n",
        "    print(response.text[:500])\n",
        "\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Error during request: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9327b957"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from fake_useragent import UserAgent\n",
        "\n",
        "ua = UserAgent()\n",
        "url_to_inspect = shuffled_urls[0] # Use the first URL from the shuffled list\n",
        "\n",
        "print(f\"Inspecting URL: {url_to_inspect}\")\n",
        "\n",
        "try:\n",
        "    headers = {\"User-Agent\": ua.random}\n",
        "    response = requests.get(url_to_inspect, headers=headers)\n",
        "    response.raise_for_status() # Raise HTTPError for bad responses (4xx or 5xx)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Try to find the table by common Finviz attributes\n",
        "    # The main screener table usually has these attributes\n",
        "    table_body = soup.find('table', attrs={'width': '100%', 'cellpadding': '3', 'cellspacing': '1'})\n",
        "\n",
        "    if table_body:\n",
        "        print(\"Found a potential table using attributes width='100%', cellpadding='3', cellspacing='1'.\")\n",
        "        # You can print a snippet of the table or its rows to confirm\n",
        "        # print(table_body.prettify())\n",
        "        # To get the column headers as a hint, let's try to extract them\n",
        "        headers = [th.text.strip() for th in table_body.find_all('th')]\n",
        "        print(f\"Table Headers: {headers}\")\n",
        "        print(\"\\nBased on this, the correct selector appears to be 'table[width=\"100%\"][cellpadding=\"3\"][cellspacing=\"1\"]'.\")\n",
        "        print(\"I will update the `selector` variable in cell `HTCD6Lo1wHF4`.\")\n",
        "\n",
        "    else:\n",
        "        print(\"Could not find table using common Finviz attributes. Further manual inspection of HTML may be needed.\")\n",
        "        print(\"Printing first 5000 characters of the page source for manual inspection:\")\n",
        "        print(soup.prettify()[:5000]) # Print a snippet of the HTML\n",
        "\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Error during request: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e515b1c7"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from fake_useragent import UserAgent\n",
        "import os\n",
        "\n",
        "ua = UserAgent()\n",
        "url_to_inspect = shuffled_urls[0]  # Use the first URL from the shuffled list\n",
        "\n",
        "print(f\"Inspecting URL: {url_to_inspect}\")\n",
        "\n",
        "try:\n",
        "    headers = {\"User-Agent\": ua.random}\n",
        "    response = requests.get(url_to_inspect, headers=headers)\n",
        "    response.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)\n",
        "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "    # Try to find the table by common Finviz attributes\n",
        "    # The main screener table usually has these attributes\n",
        "    table_body = soup.find(\n",
        "        \"table\", attrs={\"width\": \"100%\", \"cellpadding\": \"3\", \"cellspacing\": \"1\"}\n",
        "    )\n",
        "\n",
        "    if table_body:\n",
        "        print(\n",
        "            \"Found a potential table using attributes width='100%', cellpadding='3', cellspacing='1'.\"\n",
        "        )\n",
        "        headers = [th.text.strip() for th in table_body.find_all(\"th\")]\n",
        "        print(f\"Table Headers: {headers}\")\n",
        "        print(\n",
        "            f'\\nBased on this, the correct selector appears to be \\'table[width=\"100%\"][cellpadding=\"3\"][cellspacing=\"1\"]\\'.'\n",
        "        )\n",
        "        print(\"I will update the `selector` variable in cell `HTCD6Lo1wHF4`.\")\n",
        "\n",
        "    else:\n",
        "        print(\n",
        "            \"Could not find table using common Finviz attributes. Further manual inspection of HTML may be needed.\"\n",
        "        )\n",
        "        html_output_filename = \"finviz_page_source.html\"\n",
        "        with open(html_output_filename, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(soup.prettify())\n",
        "        print(\n",
        "            f\"Full page source saved to {html_output_filename} for manual inspection.\"\n",
        "        )\n",
        "        print(\"Please review this file to find the correct CSS selector for the table.\")\n",
        "\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Error during request: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJt8zyFewHF8"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "import pytz\n",
        "\n",
        "pst = pytz.timezone(\"America/Los_Angeles\")  # or 'US/Pacific'\n",
        "current_date_pst = datetime.datetime.now(pst).strftime(\"%Y-%m-%d\")\n",
        "\n",
        "# Save the DataFrame as a Parquet file\n",
        "df_finviz_filename = f\"df_finviz_{current_date_pst}_stocks_etfs.parquet\"\n",
        "\n",
        "# Save the ticker as a text file\n",
        "ticker_filename = f\"ticker_{current_date_pst}_stocks_etfs.csv\"\n",
        "\n",
        "print(f\"df_finviz_filename: {df_finviz_filename}\")\n",
        "print(f\"ticker_filename: {ticker_filename}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3agEKFzAMX95"
      },
      "outputs": [],
      "source": [
        "import pyarrow\n",
        "\n",
        "df.to_parquet(df_finviz_filename, engine=\"pyarrow\", compression=\"zstd\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vcxj1LmwLBcr"
      },
      "outputs": [],
      "source": [
        "# prompt: save ticker in df[\"Ticker'] as a csv file\n",
        "\n",
        "# Assuming df is your DataFrame and it contains a column named \"Ticker\"\n",
        "df.to_csv(ticker_filename, columns=[\"Ticker\"], header=False, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aw1Sw3G-W419"
      },
      "outputs": [],
      "source": [
        "pd.set_option(\"display.max_columns\", None)  # Show all columns\n",
        "# pd.set_option('display.max_rows', 10)       # Limit to 10 rows for readability\n",
        "pd.set_option(\"display.width\", 2000)\n",
        "\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBNjjBhqMXAj"
      },
      "outputs": [],
      "source": [
        "def zip_files(filenames, zip_filename):\n",
        "    \"\"\"Zips multiple files into a single archive.\"\"\"\n",
        "    import zipfile\n",
        "\n",
        "    with zipfile.ZipFile(zip_filename, \"w\", zipfile.ZIP_DEFLATED) as zipf:\n",
        "        for filename in filenames:\n",
        "            zipf.write(filename)\n",
        "    print(f\"Files zipped into {zip_filename}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvF6jYWHq1Te"
      },
      "source": [
        "Download to PC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QbZeqCKcVAfp"
      },
      "outputs": [],
      "source": [
        "if download_to_PC:\n",
        "    from google.colab import files\n",
        "\n",
        "    # Example usage, replace with actual filenames\n",
        "    filenames = [df_finviz_filename, ticker_filename]\n",
        "    zip_filename = (\n",
        "        f\"{current_date_pst}_combined_files.zip\"  # Name for the zipped archive\n",
        "    )\n",
        "\n",
        "    zip_files(filenames, zip_filename)\n",
        "\n",
        "    # Download the zip file\n",
        "    files.download(zip_filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0g8j5BFpq9QP"
      },
      "source": [
        "Downlaod to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ittr2hWsG0vI"
      },
      "outputs": [],
      "source": [
        "if download_to_Google_Drive:\n",
        "    import shutil\n",
        "\n",
        "    # Define the source and destination paths\n",
        "    source_path = zip_filename\n",
        "    destination_path = \"/content/drive/MyDrive/00_colab_download/\" + zip_filename\n",
        "\n",
        "    # Copy the file\n",
        "    shutil.copy(source_path, destination_path)\n",
        "\n",
        "    print(f\"File '{zip_filename}' copied to Google Drive.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78db6cfhuLqT"
      },
      "outputs": [],
      "source": [
        "# ======================"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5185fad4"
      },
      "source": [
        "# Task\n",
        "The previous attempt to scrape Finviz data failed because the HTML table could not be found with the `.styled-table-new` CSS selector. An inspection of the page source suggested that the correct selector should be `'table[width=\"100%\"][cellpadding=\"3\"][cellspacing=\"1\"]'`.\n",
        "\n",
        "To resolve this, I will:\n",
        "\n",
        "1.  Update the `selector` variable in cell `HTCD6Lo1wHF4` to the newly identified selector.\n",
        "2.  Modify the `download_table` function in cell `HTCD6Lo1wHF4` to include a `Referer` header and print the HTTP status code and a snippet of the response content for improved debugging.\n",
        "3.  Execute the modified `download_table` function on a single URL (`shuffled_urls[0]`) to verify that it can now successfully retrieve the table.\n",
        "4.  If the test is successful, re-run the main data scraping loop in cell `sp4KYbgEcotc` to download all the data.\n",
        "5.  Summarize the resolution of the scraping issue and confirm the successful download of Finviz data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3885ebd8"
      },
      "source": [
        "## Update Download Function\n",
        "\n",
        "### Subtask:\n",
        "Modify the `download_table` function in cell `HTCD6Lo1wHF4` to include a more complete set of request headers (including a specific User-Agent and Referer) and print the HTTP status code and initial response content for debugging.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc9936ab"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to update the `selector` variable and modify the `download_table` function in cell `HTCD6Lo1wHF4` to include a `Referer` header and print debugging information (status code and initial response content).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b35a6e03"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from fake_useragent import UserAgent\n",
        "\n",
        "selector = 'table[width=\"100%\"][cellpadding=\"3\"][cellspacing=\"1\"]'\n",
        "ua = UserAgent()  # Initialize UserAgent\n",
        "\n",
        "\n",
        "def download_table(url, selector):\n",
        "    \"\"\"\n",
        "    Downloads table data from a Yahoo Finance page with rate limiting.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Add a User-Agent header to mimic a browser\n",
        "        new_user_agent = ua.random\n",
        "        headers = {\n",
        "            \"User-Agent\": new_user_agent,\n",
        "            \"Referer\": \"https://finviz.com/screener.ashx\",\n",
        "        }\n",
        "        response = requests.get(url, headers=headers)\n",
        "        print(f\"HTTP Status Code: {response.status_code}\")\n",
        "        print(f\"Initial response content (first 500 chars): {response.text[:500]}\")\n",
        "        response.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "        table_body = soup.select_one(selector)\n",
        "\n",
        "        if table_body is None:\n",
        "            print(f\"Error: Table body not found using selector: {selector}\")\n",
        "            return None\n",
        "\n",
        "        rows = table_body.find_all(\"tr\")\n",
        "        if not rows:\n",
        "            print(\"Error: No rows found in the table.\")\n",
        "            return None\n",
        "\n",
        "        # Extract headers from the first row (th elements)\n",
        "        headers_list = [th.text.strip() for th in rows[0].find_all(\"th\")]\n",
        "\n",
        "        data = []\n",
        "        for row in rows:\n",
        "            cells = row.find_all(\"td\")\n",
        "            row_data = [cell.text.strip() for cell in cells]\n",
        "            if row_data:  # Only append if the row has data\n",
        "                data.append(row_data)\n",
        "\n",
        "        if not data:\n",
        "            print(\"Error: No data found in the table rows.\")\n",
        "            return None\n",
        "\n",
        "        df = pd.DataFrame(data, columns=headers_list)\n",
        "        return df\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error during request: {e}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07be55cc"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code block successfully updated the `selector` variable and modified the `download_table` function as requested. Now, I will proceed to the next instruction in the main task, which is to execute the modified `download_table` function on a single URL to verify its functionality.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18a42dd4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "import random\n",
        "\n",
        "total_urls_to_download = len(shuffled_urls)\n",
        "\n",
        "df = pd.DataFrame()  # Initialized an empty DataFrame\n",
        "processed_count = 0\n",
        "\n",
        "# Test the download_table function with a single URL to verify\n",
        "url_to_test = shuffled_urls[0]\n",
        "print(f\"Testing download_table with URL: {url_to_test}\")\n",
        "df_test = download_table(url_to_test, selector)\n",
        "\n",
        "if df_test is not None:\n",
        "    print(\"Test download successful. Displaying first 5 rows of the test DataFrame:\")\n",
        "    print(df_test.head())\n",
        "else:\n",
        "    print(\"Test download failed. Please review the output above for errors.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1e2b41b"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous test download showed an empty 'Initial response content' despite an HTTP Status Code of 200, and the table selector still failed to find the table body. This suggests the content being received by `requests.get` might be empty or problematic. I will modify the `download_table` function to explicitly print the length of the raw response content to diagnose if the content itself is empty.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70970bcc"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from fake_useragent import UserAgent\n",
        "\n",
        "selector = 'table[width=\"100%\"][cellpadding=\"3\"][cellspacing=\"1\"]'\n",
        "ua = UserAgent()  # Initialize UserAgent\n",
        "\n",
        "\n",
        "def download_table(url, selector):\n",
        "    \"\"\"\n",
        "    Downloads table data from a Yahoo Finance page with rate limiting.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Add a User-Agent header to mimic a browser\n",
        "        new_user_agent = ua.random\n",
        "        headers = {\n",
        "            \"User-Agent\": new_user_agent,\n",
        "            \"Referer\": \"https://finviz.com/screener.ashx\",\n",
        "        }\n",
        "        response = requests.get(url, headers=headers)\n",
        "        print(f\"HTTP Status Code: {response.status_code}\")\n",
        "        print(\n",
        "            f\"Length of response content: {len(response.content)} bytes\"\n",
        "        )  # Added for debugging\n",
        "        print(f\"Initial response content (first 500 chars): {response.text[:500]}\")\n",
        "        response.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "        table_body = soup.select_one(selector)\n",
        "\n",
        "        if table_body is None:\n",
        "            print(f\"Error: Table body not found using selector: {selector}\")\n",
        "            return None\n",
        "\n",
        "        rows = table_body.find_all(\"tr\")\n",
        "        if not rows:\n",
        "            print(\"Error: No rows found in the table.\")\n",
        "            return None\n",
        "\n",
        "        # Extract headers from the first row (th elements)\n",
        "        headers_list = [th.text.strip() for th in rows[0].find_all(\"th\")]\n",
        "\n",
        "        data = []\n",
        "        for row in rows:\n",
        "            cells = row.find_all(\"td\")\n",
        "            row_data = [cell.text.strip() for cell in cells]\n",
        "            if row_data:  # Only append if the row has data\n",
        "                data.append(row_data)\n",
        "\n",
        "        if not data:\n",
        "            print(\"Error: No data found in the table rows.\")\n",
        "            return None\n",
        "\n",
        "        df = pd.DataFrame(data, columns=headers_list)\n",
        "        return df\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error during request: {e}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e3b98d2"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code block was intended to update the `download_table` function with additional debugging information, including the length of the response content. To ensure this updated function is active in the environment, I am providing the same code block again.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64c18b24"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from fake_useragent import UserAgent\n",
        "\n",
        "selector = 'table[width=\"100%\"][cellpadding=\"3\"][cellspacing=\"1\"]'\n",
        "ua = UserAgent()  # Initialize UserAgent\n",
        "\n",
        "\n",
        "def download_table(url, selector):\n",
        "    \"\"\"\n",
        "    Downloads table data from a Yahoo Finance page with rate limiting.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Add a User-Agent header to mimic a browser\n",
        "        new_user_agent = ua.random\n",
        "        headers = {\n",
        "            \"User-Agent\": new_user_agent,\n",
        "            \"Referer\": \"https://finviz.com/screener.ashx\",\n",
        "        }\n",
        "        response = requests.get(url, headers=headers)\n",
        "        print(f\"HTTP Status Code: {response.status_code}\")\n",
        "        print(\n",
        "            f\"Length of response content: {len(response.content)} bytes\"\n",
        "        )  # Added for debugging\n",
        "        print(f\"Initial response content (first 500 chars): {response.text[:500]}\")\n",
        "        response.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "        table_body = soup.select_one(selector)\n",
        "\n",
        "        if table_body is None:\n",
        "            print(f\"Error: Table body not found using selector: {selector}\")\n",
        "            return None\n",
        "\n",
        "        rows = table_body.find_all(\"tr\")\n",
        "        if not rows:\n",
        "            print(\"Error: No rows found in the table.\")\n",
        "            return None\n",
        "\n",
        "        # Extract headers from the first row (th elements)\n",
        "        headers_list = [th.text.strip() for th in rows[0].find_all(\"th\")]\n",
        "\n",
        "        data = []\n",
        "        for row in rows:\n",
        "            cells = row.find_all(\"td\")\n",
        "            row_data = [cell.text.strip() for cell in cells]\n",
        "            if row_data:  # Only append if the row has data\n",
        "                data.append(row_data)\n",
        "\n",
        "        if not data:\n",
        "            print(\"Error: No data found in the table rows.\")\n",
        "            return None\n",
        "\n",
        "        df = pd.DataFrame(data, columns=headers_list)\n",
        "        return df\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error during request: {e}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41cda291"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the `download_table` function has been updated to include printing the length of the response content for debugging, I will re-run the test with a single URL to observe the new debugging output.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6c79088"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "import random\n",
        "\n",
        "total_urls_to_download = len(shuffled_urls)\n",
        "\n",
        "df = pd.DataFrame()  # Initialized an empty DataFrame\n",
        "processed_count = 0\n",
        "\n",
        "# Test the download_table function with a single URL to verify\n",
        "url_to_test = shuffled_urls[0]\n",
        "print(f\"Testing download_table with URL: {url_to_test}\")\n",
        "df_test = download_table(url_to_test, selector)\n",
        "\n",
        "if df_test is not None:\n",
        "    print(\"Test download successful. Displaying first 5 rows of the test DataFrame:\")\n",
        "    print(df_test.head())\n",
        "else:\n",
        "    print(\"Test download failed. Please review the output above for errors.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1db82d9d"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous test indicated that `requests.get` is returning a `200` status code but with an empty response body (`Length of response content: 0 bytes`). This suggests Finviz might be blocking or serving an empty page to automated requests. I will add more browser-like headers (Accept-Language, Accept-Encoding) to the `download_table` function to make the request appear more legitimate and explicitly handle cases where no content is received before attempting to parse with BeautifulSoup.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28238e65"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from fake_useragent import UserAgent\n",
        "\n",
        "selector = 'table[width=\"100%\"][cellpadding=\"3\"][cellspacing=\"1\"]'\n",
        "ua = UserAgent()  # Initialize UserAgent\n",
        "\n",
        "\n",
        "def download_table(url, selector):\n",
        "    \"\"\"\n",
        "    Downloads table data from a Yahoo Finance page with rate limiting.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Add a User-Agent header to mimic a browser\n",
        "        new_user_agent = ua.random\n",
        "        headers = {\n",
        "            \"User-Agent\": new_user_agent,\n",
        "            \"Referer\": \"https://finviz.com/screener.ashx\",\n",
        "            \"Accept-Language\": \"en-US,en;q=0.9\",\n",
        "            \"Accept-Encoding\": \"gzip, deflate, br\",\n",
        "        }\n",
        "        response = requests.get(url, headers=headers, timeout=10)  # Added a timeout\n",
        "        print(f\"HTTP Status Code: {response.status_code}\")\n",
        "        print(\n",
        "            f\"Length of response content: {len(response.content)} bytes\"\n",
        "        )  # Added for debugging\n",
        "        print(f\"Initial response content (first 500 chars): {response.text[:500]}\")\n",
        "\n",
        "        response.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)\n",
        "\n",
        "        if len(response.content) == 0:\n",
        "            print(\"Error: Received empty response content from the server.\")\n",
        "            return None\n",
        "\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "        table_body = soup.select_one(selector)\n",
        "\n",
        "        if table_body is None:\n",
        "            print(f\"Error: Table body not found using selector: {selector}\")\n",
        "            return None\n",
        "\n",
        "        rows = table_body.find_all(\"tr\")\n",
        "        if not rows:\n",
        "            print(\"Error: No rows found in the table.\")\n",
        "            return None\n",
        "\n",
        "        # Extract headers from the first row (th elements)\n",
        "        headers_list = [th.text.strip() for th in rows[0].find_all(\"th\")]\n",
        "\n",
        "        data = []\n",
        "        for row in rows:\n",
        "            cells = row.find_all(\"td\")\n",
        "            row_data = [cell.text.strip() for cell in cells]\n",
        "            if row_data:  # Only append if the row has data\n",
        "                data.append(row_data)\n",
        "\n",
        "        if not data:\n",
        "            print(\"Error: No data found in the table rows.\")\n",
        "            return None\n",
        "\n",
        "        df = pd.DataFrame(data, columns=headers_list)\n",
        "        return df\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error during request: {e}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8da24ac5"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code block updated the `download_table` function to include more browser-like headers and a timeout. To verify the changes and proceed with testing, I will re-run the single URL test.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6161834d"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "import random\n",
        "\n",
        "total_urls_to_download = len(shuffled_urls)\n",
        "\n",
        "df = pd.DataFrame()  # Initialized an empty DataFrame\n",
        "processed_count = 0\n",
        "\n",
        "# Test the download_table function with a single URL to verify\n",
        "url_to_test = shuffled_urls[0]\n",
        "print(f\"Testing download_table with URL: {url_to_test}\")\n",
        "df_test = download_table(url_to_test, selector)\n",
        "\n",
        "if df_test is not None:\n",
        "    print(\"Test download successful. Displaying first 5 rows of the test DataFrame:\")\n",
        "    print(df_test.head())\n",
        "else:\n",
        "    print(\"Test download failed. Please review the output above for errors.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3af250d"
      },
      "source": [
        "## Implement Headless Browser Scraping\n",
        "\n",
        "### Subtask:\n",
        "Modify the `download_table` function to use Selenium with a headless Chrome browser to bypass Finviz's anti-scraping measures and successfully retrieve page content.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b987782b"
      },
      "source": [
        "**Reasoning**:\n",
        "The first step is to install the `selenium` and `webdriver-manager` libraries as they are necessary for headless browser scraping with Chrome.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ad2886f4"
      },
      "outputs": [],
      "source": [
        "pip install selenium webdriver-manager"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "903c6b36"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempts with `requests` failed to retrieve the page content, indicating that Finviz is likely blocking standard HTTP requests or requires JavaScript rendering. Therefore, I will implement headless browser scraping using Selenium to bypass these restrictions. This step involves importing the necessary Selenium modules and updating the `download_table` function as specified in the instructions to use a headless Chrome WebDriver.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6457171c"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from fake_useragent import UserAgent\n",
        "\n",
        "# Import Selenium modules\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "\n",
        "selector = 'table[width=\"100%\"][cellpadding=\"3\"][cellspacing=\"1\"]'  # Updated selector\n",
        "ua = UserAgent()  # Initialize UserAgent\n",
        "\n",
        "\n",
        "def download_table(url, selector):\n",
        "    \"\"\"\n",
        "    Downloads table data from a Finviz page using Selenium with a headless Chrome browser.\n",
        "    \"\"\"\n",
        "    driver = None  # Initialize driver to None for cleanup in finally block\n",
        "    try:\n",
        "        # Setup Chrome options for headless browsing\n",
        "        chrome_options = Options()\n",
        "        chrome_options.add_argument(\"--headless\")\n",
        "        chrome_options.add_argument(\"--no-sandbox\")\n",
        "        chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "\n",
        "        # Add a random User-Agent header to mimic a browser\n",
        "        new_user_agent = ua.random\n",
        "        chrome_options.add_argument(f\"user-agent={new_user_agent}\")\n",
        "\n",
        "        # Initialize WebDriver\n",
        "        service = Service(ChromeDriverManager().install())\n",
        "        driver = webdriver.Chrome(service=service, options=chrome_options)\n",
        "\n",
        "        print(f\"Navigating to URL: {url}\")\n",
        "        driver.get(url)\n",
        "\n",
        "        # Wait for the table element to be present (adjust timeout as needed)\n",
        "        # The selector here targets the table element itself, not its content\n",
        "        wait = WebDriverWait(driver, 20)  # Wait up to 20 seconds\n",
        "        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, selector)))\n",
        "\n",
        "        # Retrieve the page source after JavaScript has rendered the content\n",
        "        page_source = driver.page_source\n",
        "        print(f\"Page source retrieved. Length: {len(page_source)} bytes\")\n",
        "\n",
        "        soup = BeautifulSoup(page_source, \"html.parser\")\n",
        "        table_body = soup.select_one(selector)\n",
        "\n",
        "        if table_body is None:\n",
        "            print(f\"Error: Table body not found using selector: {selector}\")\n",
        "            return None\n",
        "\n",
        "        rows = table_body.find_all(\"tr\")\n",
        "        if not rows:\n",
        "            print(\"Error: No rows found in the table.\")\n",
        "            return None\n",
        "\n",
        "        # Extract headers from the first row (th elements)\n",
        "        headers_list = [th.text.strip() for th in rows[0].find_all(\"th\")]\n",
        "\n",
        "        data = []\n",
        "        for row in rows:\n",
        "            cells = row.find_all(\"td\")\n",
        "            row_data = [cell.text.strip() for cell in cells]\n",
        "            if row_data:  # Only append if the row has data\n",
        "                data.append(row_data)\n",
        "\n",
        "        if not data:\n",
        "            print(\"Error: No data found in the table rows.\")\n",
        "            return None\n",
        "\n",
        "        df = pd.DataFrame(data, columns=headers_list)\n",
        "        return df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during Selenium scraping: {e}\")\n",
        "        return None\n",
        "    finally:\n",
        "        if driver:  # Ensure driver is closed even if an error occurs\n",
        "            driver.quit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5a836ea"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the `download_table` function has been updated to use Selenium, I will re-run the test with a single URL to verify that it can successfully retrieve table data from Finviz.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ee770a1f"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "import random\n",
        "\n",
        "total_urls_to_download = len(shuffled_urls)\n",
        "\n",
        "df = pd.DataFrame()  # Initialized an empty DataFrame\n",
        "processed_count = 0\n",
        "\n",
        "# Test the download_table function with a single URL to verify\n",
        "url_to_test = shuffled_urls[0]\n",
        "print(f\"Testing download_table with URL: {url_to_test}\")\n",
        "df_test = download_table(url_to_test, selector)\n",
        "\n",
        "if df_test is not None:\n",
        "    print(\"Test download successful. Displaying first 5 rows of the test DataFrame:\")\n",
        "    print(df_test.head())\n",
        "else:\n",
        "    print(\"Test download failed. Please review the output above for errors.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cebf5ba7"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt to use Selenium failed because the Chrome binary was not found. To resolve this, I need to install Google Chrome in the Colab environment. After installation, the `download_table` function can properly utilize the Chrome driver.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24977aca"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "# Install Chrome in the Colab environment\n",
        "!apt-get update\n",
        "!apt-get install -y google-chrome-stable\n",
        "\n",
        "print(\"Google Chrome installed successfully.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b27a9662"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt to install `google-chrome-stable` failed because the package could not be located. I will try an alternative installation method by downloading the `.deb` package directly and installing it with `dpkg`. I will also add the Chrome binary path to the system path, although `webdriver-manager` should ideally find it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95c74c12"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# Install Chrome in the Colab environment using wget and dpkg\n",
        "!wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | apt-key add -\n",
        "!echo \"deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main\" >> /etc/apt/sources.list.d/google-chrome.list\n",
        "!apt-get update\n",
        "!apt-get install -y google-chrome-stable\n",
        "\n",
        "# Set the path to the Chrome binary for webdriver_manager (though it often finds it automatically)\n",
        "# This might not be strictly necessary, but good for debugging if issues persist\n",
        "chrome_binary_path = \"/usr/bin/google-chrome\"\n",
        "if os.path.exists(chrome_binary_path):\n",
        "    os.environ['CHROME_PATH'] = chrome_binary_path\n",
        "    print(f\"Chrome binary found and CHROME_PATH set to: {os.environ['CHROME_PATH']}\")\n",
        "else:\n",
        "    print(f\"Chrome binary not found at expected path: {chrome_binary_path}\")\n",
        "\n",
        "print(\"Google Chrome installation process completed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef8644ba"
      },
      "source": [
        "**Reasoning**:\n",
        "Google Chrome has been successfully installed, and the `CHROME_PATH` environment variable has been set. Now, I will re-run the test of the `download_table` function with a single URL to verify that Selenium can now properly find and use the Chrome binary to scrape the table data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3004b26"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "import random\n",
        "\n",
        "total_urls_to_download = len(shuffled_urls)\n",
        "\n",
        "df = pd.DataFrame()  # Initialized an empty DataFrame\n",
        "processed_count = 0\n",
        "\n",
        "# Test the download_table function with a single URL to verify\n",
        "url_to_test = shuffled_urls[0]\n",
        "print(f\"Testing download_table with URL: {url_to_test}\")\n",
        "df_test = download_table(url_to_test, selector)\n",
        "\n",
        "if df_test is not None:\n",
        "    print(\"Test download successful. Displaying first 5 rows of the test DataFrame:\")\n",
        "    print(df_test.head())\n",
        "else:\n",
        "    print(\"Test download failed. Please review the output above for errors.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be3318f0"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt resulted in a generic Selenium error. I will modify the `download_table` function to explicitly pass the Chrome binary path to the `Service` constructor to ensure Selenium can find and use the installed Chrome browser, and add more specific error handling to diagnose potential `WebDriverException` issues.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f81f1be7"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from fake_useragent import UserAgent\n",
        "\n",
        "# Import Selenium modules\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "from selenium.common.exceptions import WebDriverException\n",
        "\n",
        "selector = 'table[width=\"100%\"][cellpadding=\"3\"][cellspacing=\"1\"]'  # Updated selector\n",
        "ua = UserAgent()  # Initialize UserAgent\n",
        "\n",
        "\n",
        "def download_table(url, selector):\n",
        "    \"\"\"\n",
        "    Downloads table data from a Finviz page using Selenium with a headless Chrome browser.\n",
        "    \"\"\"\n",
        "    driver = None  # Initialize driver to None for cleanup in finally block\n",
        "    try:\n",
        "        # Setup Chrome options for headless browsing\n",
        "        chrome_options = Options()\n",
        "        chrome_options.add_argument(\"--headless\")\n",
        "        chrome_options.add_argument(\"--no-sandbox\")\n",
        "        chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "\n",
        "        # Add a random User-Agent header to mimic a browser\n",
        "        new_user_agent = ua.random\n",
        "        chrome_options.add_argument(f\"user-agent={new_user_agent}\")\n",
        "\n",
        "        # Explicitly set the path to the Chrome binary\n",
        "        chrome_service = Service(\n",
        "            executable_path=ChromeDriverManager().install(), service_args=[\"--verbose\"]\n",
        "        )\n",
        "        driver = webdriver.Chrome(service=chrome_service, options=chrome_options)\n",
        "\n",
        "        print(f\"Navigating to URL: {url}\")\n",
        "        driver.get(url)\n",
        "\n",
        "        # Wait for the table element to be present (adjust timeout as needed)\n",
        "        # The selector here targets the table element itself, not its content\n",
        "        wait = WebDriverWait(driver, 20)  # Wait up to 20 seconds\n",
        "        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, selector)))\n",
        "\n",
        "        # Retrieve the page source after JavaScript has rendered the content\n",
        "        page_source = driver.page_source\n",
        "        print(f\"Page source retrieved. Length: {len(page_source)} bytes\")\n",
        "\n",
        "        soup = BeautifulSoup(page_source, \"html.parser\")\n",
        "        table_body = soup.select_one(selector)\n",
        "\n",
        "        if table_body is None:\n",
        "            print(f\"Error: Table body not found using selector: {selector}\")\n",
        "            return None\n",
        "\n",
        "        rows = table_body.find_all(\"tr\")\n",
        "        if not rows:\n",
        "            print(\"Error: No rows found in the table.\")\n",
        "            return None\n",
        "\n",
        "        # Extract headers from the first row (th elements)\n",
        "        headers_list = [th.text.strip() for th in rows[0].find_all(\"th\")]\n",
        "\n",
        "        data = []\n",
        "        for row in rows:\n",
        "            cells = row.find_all(\"td\")\n",
        "            row_data = [cell.text.strip() for cell in cells]\n",
        "            if row_data:  # Only append if the row has data\n",
        "                data.append(row_data)\n",
        "\n",
        "        if not data:\n",
        "            print(\"Error: No data found in the table rows.\")\n",
        "            return None\n",
        "\n",
        "        df = pd.DataFrame(data, columns=headers_list)\n",
        "        return df\n",
        "\n",
        "    except WebDriverException as e:\n",
        "        print(f\"An error occurred with the WebDriver: {e}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred during Selenium scraping: {e}\")\n",
        "        return None\n",
        "    finally:\n",
        "        if driver:  # Ensure driver is closed even if an error occurs\n",
        "            driver.quit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34ac7f6b"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt resulted in a generic Selenium error. I will modify the `download_table` function to explicitly pass the Chrome binary path to the `Service` constructor to ensure Selenium can find and use the installed Chrome browser, and add more specific error handling to diagnose potential `WebDriverException` issues.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8462f9c"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from fake_useragent import UserAgent\n",
        "\n",
        "# Import Selenium modules\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "from selenium.common.exceptions import WebDriverException\n",
        "\n",
        "selector = 'table[width=\"100%\"][cellpadding=\"3\"][cellspacing=\"1\"]'  # Updated selector\n",
        "ua = UserAgent()  # Initialize UserAgent\n",
        "\n",
        "\n",
        "def download_table(url, selector):\n",
        "    \"\"\"\n",
        "    Downloads table data from a Finviz page using Selenium with a headless Chrome browser.\n",
        "    \"\"\"\n",
        "    driver = None  # Initialize driver to None for cleanup in finally block\n",
        "    try:\n",
        "        # Setup Chrome options for headless browsing\n",
        "        chrome_options = Options()\n",
        "        chrome_options.add_argument(\"--headless\")\n",
        "        chrome_options.add_argument(\"--no-sandbox\")\n",
        "        chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "\n",
        "        # Add a random User-Agent header to mimic a browser\n",
        "        new_user_agent = ua.random\n",
        "        chrome_options.add_argument(f\"user-agent={new_user_agent}\")\n",
        "\n",
        "        # Explicitly set the path to the Chrome binary\n",
        "        chrome_service = Service(\n",
        "            executable_path=ChromeDriverManager().install(), service_args=[\"--verbose\"]\n",
        "        )\n",
        "        driver = webdriver.Chrome(service=chrome_service, options=chrome_options)\n",
        "\n",
        "        print(f\"Navigating to URL: {url}\")\n",
        "        driver.get(url)\n",
        "\n",
        "        # Wait for the table element to be present (adjust timeout as needed)\n",
        "        # The selector here targets the table element itself, not its content\n",
        "        wait = WebDriverWait(driver, 20)  # Wait up to 20 seconds\n",
        "        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, selector)))\n",
        "\n",
        "        # Retrieve the page source after JavaScript has rendered the content\n",
        "        page_source = driver.page_source\n",
        "        print(f\"Page source retrieved. Length: {len(page_source)} bytes\")\n",
        "\n",
        "        soup = BeautifulSoup(page_source, \"html.parser\")\n",
        "        table_body = soup.select_one(selector)\n",
        "\n",
        "        if table_body is None:\n",
        "            print(f\"Error: Table body not found using selector: {selector}\")\n",
        "            return None\n",
        "\n",
        "        rows = table_body.find_all(\"tr\")\n",
        "        if not rows:\n",
        "            print(\"Error: No rows found in the table.\")\n",
        "            return None\n",
        "\n",
        "        # Extract headers from the first row (th elements)\n",
        "        headers_list = [th.text.strip() for th in rows[0].find_all(\"th\")]\n",
        "\n",
        "        data = []\n",
        "        for row in rows:\n",
        "            cells = row.find_all(\"td\")\n",
        "            row_data = [cell.text.strip() for cell in cells]\n",
        "            if row_data:  # Only append if the row has data\n",
        "                data.append(row_data)\n",
        "\n",
        "        if not data:\n",
        "            print(\"Error: No data found in the table rows.\")\n",
        "            return None\n",
        "\n",
        "        df = pd.DataFrame(data, columns=headers_list)\n",
        "        return df\n",
        "\n",
        "    except WebDriverException as e:\n",
        "        print(f\"An error occurred with the WebDriver: {e}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred during Selenium scraping: {e}\")\n",
        "        return None\n",
        "    finally:\n",
        "        if driver:  # Ensure driver is closed even if an error occurs\n",
        "            driver.quit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2a160f9"
      },
      "source": [
        "**Reasoning**:\n",
        "The `download_table` function has been updated with explicit Chrome binary path handling and enhanced error reporting. Now, I will re-run the test with a single URL to verify that Selenium can now properly find and use the Chrome binary to scrape the table data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24692a68"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "import random\n",
        "\n",
        "total_urls_to_download = len(shuffled_urls)\n",
        "\n",
        "df = pd.DataFrame()  # Initialized an empty DataFrame\n",
        "processed_count = 0\n",
        "\n",
        "# Test the download_table function with a single URL to verify\n",
        "url_to_test = shuffled_urls[0]\n",
        "print(f\"Testing download_table with URL: {url_to_test}\")\n",
        "df_test = download_table(url_to_test, selector)\n",
        "\n",
        "if df_test is not None:\n",
        "    print(\"Test download successful. Displaying first 5 rows of the test DataFrame:\")\n",
        "    print(df_test.head())\n",
        "else:\n",
        "    print(\"Test download failed. Please review the output above for errors.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9681131"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt to run Selenium still resulted in an empty error message, indicating a potential issue with `ChromeDriverManager().install()` or how the service is being initialized. I will modify the `download_table` function to directly use the `chrome_binary_path` that was verified earlier and remove the `service_args=['--verbose']` for a cleaner log, hoping to resolve the WebDriver initialization issue.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9f8e413"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from fake_useragent import UserAgent\n",
        "\n",
        "# Import Selenium modules\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "from selenium.common.exceptions import WebDriverException\n",
        "\n",
        "selector = 'table[width=\"100%\"][cellpadding=\"3\"][cellspacing=\"1\"]'  # Updated selector\n",
        "ua = UserAgent()  # Initialize UserAgent\n",
        "\n",
        "\n",
        "def download_table(url, selector):\n",
        "    \"\"\"\n",
        "    Downloads table data from a Finviz page using Selenium with a headless Chrome browser.\n",
        "    \"\"\"\n",
        "    driver = None  # Initialize driver to None for cleanup in finally block\n",
        "    try:\n",
        "        # Setup Chrome options for headless browsing\n",
        "        chrome_options = Options()\n",
        "        chrome_options.add_argument(\"--headless\")\n",
        "        chrome_options.add_argument(\"--no-sandbox\")\n",
        "        chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "\n",
        "        # Add a random User-Agent header to mimic a browser\n",
        "        new_user_agent = ua.random\n",
        "        chrome_options.add_argument(f\"user-agent={new_user_agent}\")\n",
        "\n",
        "        # Explicitly set the path to the Chrome binary and ChromeDriver\n",
        "        # Use the global chrome_binary_path variable if it exists\n",
        "        chromedriver_path = ChromeDriverManager().install()\n",
        "        service = Service(executable_path=chromedriver_path)\n",
        "        driver = webdriver.Chrome(service=service, options=chrome_options)\n",
        "\n",
        "        print(f\"Navigating to URL: {url}\")\n",
        "        driver.get(url)\n",
        "\n",
        "        # Wait for the table element to be present (adjust timeout as needed)\n",
        "        # The selector here targets the table element itself, not its content\n",
        "        wait = WebDriverWait(driver, 20)  # Wait up to 20 seconds\n",
        "        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, selector)))\n",
        "\n",
        "        # Retrieve the page source after JavaScript has rendered the content\n",
        "        page_source = driver.page_source\n",
        "        print(f\"Page source retrieved. Length: {len(page_source)} bytes\")\n",
        "\n",
        "        soup = BeautifulSoup(page_source, \"html.parser\")\n",
        "        table_body = soup.select_one(selector)\n",
        "\n",
        "        if table_body is None:\n",
        "            print(f\"Error: Table body not found using selector: {selector}\")\n",
        "            return None\n",
        "\n",
        "        rows = table_body.find_all(\"tr\")\n",
        "        if not rows:\n",
        "            print(\"Error: No rows found in the table.\")\n",
        "            return None\n",
        "\n",
        "        # Extract headers from the first row (th elements)\n",
        "        headers_list = [th.text.strip() for th in rows[0].find_all(\"th\")]\n",
        "\n",
        "        data = []\n",
        "        for row in rows:\n",
        "            cells = row.find_all(\"td\")\n",
        "            row_data = [cell.text.strip() for cell in cells]\n",
        "            if row_data:  # Only append if the row has data\n",
        "                data.append(row_data)\n",
        "\n",
        "        if not data:\n",
        "            print(\"Error: No data found in the table rows.\")\n",
        "            return None\n",
        "\n",
        "        df = pd.DataFrame(data, columns=headers_list)\n",
        "        return df\n",
        "\n",
        "    except WebDriverException as e:\n",
        "        print(f\"An error occurred with the WebDriver: {e}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred during Selenium scraping: {e}\")\n",
        "        return None\n",
        "    finally:\n",
        "        if driver:  # Ensure driver is closed even if an error occurs\n",
        "            driver.quit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9926d618"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt to run Selenium still resulted in an empty error message, indicating a potential issue with `ChromeDriverManager().install()` or how the service is being initialized. I will modify the `download_table` function to directly use the `chrome_binary_path` that was verified earlier and remove the `service_args=['--verbose']` for a cleaner log, hoping to resolve the WebDriver initialization issue.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5e954998"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from fake_useragent import UserAgent\n",
        "\n",
        "# Import Selenium modules\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "from selenium.common.exceptions import WebDriverException\n",
        "\n",
        "selector = 'table[width=\"100%\"][cellpadding=\"3\"][cellspacing=\"1\"]'  # Updated selector\n",
        "ua = UserAgent()  # Initialize UserAgent\n",
        "\n",
        "\n",
        "def download_table(url, selector):\n",
        "    \"\"\"\n",
        "    Downloads table data from a Finviz page using Selenium with a headless Chrome browser.\n",
        "    \"\"\"\n",
        "    driver = None  # Initialize driver to None for cleanup in finally block\n",
        "    try:\n",
        "        # Setup Chrome options for headless browsing\n",
        "        chrome_options = Options()\n",
        "        chrome_options.add_argument(\"--headless\")\n",
        "        chrome_options.add_argument(\"--no-sandbox\")\n",
        "        chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "\n",
        "        # Add a random User-Agent header to mimic a browser\n",
        "        new_user_agent = ua.random\n",
        "        chrome_options.add_argument(f\"user-agent={new_user_agent}\")\n",
        "\n",
        "        # Explicitly set the path to the Chrome binary and ChromeDriver\n",
        "        # Use the global chrome_binary_path variable if it exists\n",
        "        chromedriver_path = ChromeDriverManager().install()\n",
        "        service = Service(executable_path=chromedriver_path)\n",
        "        driver = webdriver.Chrome(service=service, options=chrome_options)\n",
        "\n",
        "        print(f\"Navigating to URL: {url}\")\n",
        "        driver.get(url)\n",
        "\n",
        "        # Wait for the table element to be present (adjust timeout as needed)\n",
        "        # The selector here targets the table element itself, not its content\n",
        "        wait = WebDriverWait(driver, 20)  # Wait up to 20 seconds\n",
        "        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, selector)))\n",
        "\n",
        "        # Retrieve the page source after JavaScript has rendered the content\n",
        "        page_source = driver.page_source\n",
        "        print(f\"Page source retrieved. Length: {len(page_source)} bytes\")\n",
        "\n",
        "        soup = BeautifulSoup(page_source, \"html.parser\")\n",
        "        table_body = soup.select_one(selector)\n",
        "\n",
        "        if table_body is None:\n",
        "            print(f\"Error: Table body not found using selector: {selector}\")\n",
        "            return None\n",
        "\n",
        "        rows = table_body.find_all(\"tr\")\n",
        "        if not rows:\n",
        "            print(\"Error: No rows found in the table.\")\n",
        "            return None\n",
        "\n",
        "        # Extract headers from the first row (th elements)\n",
        "        headers_list = [th.text.strip() for th in rows[0].find_all(\"th\")]\n",
        "\n",
        "        data = []\n",
        "        for row in rows:\n",
        "            cells = row.find_all(\"td\")\n",
        "            row_data = [cell.text.strip() for cell in cells]\n",
        "            if row_data:  # Only append if the row has data\n",
        "                data.append(row_data)\n",
        "\n",
        "        if not data:\n",
        "            print(\"Error: No data found in the table rows.\")\n",
        "            return None\n",
        "\n",
        "        df = pd.DataFrame(data, columns=headers_list)\n",
        "        return df\n",
        "\n",
        "    except WebDriverException as e:\n",
        "        print(f\"An error occurred with the WebDriver: {e}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred during Selenium scraping: {e}\")\n",
        "        return None\n",
        "    finally:\n",
        "        if driver:  # Ensure driver is closed even if an error occurs\n",
        "            driver.quit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe6f84e3"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempts to initialize the Selenium WebDriver failed because the Chrome browser binary was not being found. To address this, I will explicitly set the `binary_location` for the Chrome options to the `chrome_binary_path` that was successfully installed and verified earlier, within the `download_table` function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2e3821ca"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from fake_useragent import UserAgent\n",
        "\n",
        "# Import Selenium modules\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "from selenium.common.exceptions import WebDriverException\n",
        "\n",
        "selector = 'table[width=\"100%\"][cellpadding=\"3\"][cellspacing=\"1\"]'  # Updated selector\n",
        "ua = UserAgent()  # Initialize UserAgent\n",
        "\n",
        "\n",
        "def download_table(url, selector):\n",
        "    \"\"\"\n",
        "    Downloads table data from a Finviz page using Selenium with a headless Chrome browser.\n",
        "    \"\"\"\n",
        "    driver = None  # Initialize driver to None for cleanup in finally block\n",
        "    try:\n",
        "        # Setup Chrome options for headless browsing\n",
        "        chrome_options = Options()\n",
        "        chrome_options.add_argument(\"--headless\")\n",
        "        chrome_options.add_argument(\"--no-sandbox\")\n",
        "        chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "\n",
        "        # Explicitly set the path to the Chrome binary\n",
        "        global chrome_binary_path  # Access the global variable\n",
        "        if \"CHROME_PATH\" in os.environ:\n",
        "            chrome_options.binary_location = os.environ[\"CHROME_PATH\"]\n",
        "        elif chrome_binary_path and os.path.exists(chrome_binary_path):\n",
        "            chrome_options.binary_location = chrome_binary_path\n",
        "        else:\n",
        "            print(\n",
        "                \"Warning: Chrome binary path not explicitly set. Relying on default discovery.\"\n",
        "            )\n",
        "\n",
        "        # Add a random User-Agent header to mimic a browser\n",
        "        new_user_agent = ua.random\n",
        "        chrome_options.add_argument(f\"user-agent={new_user_agent}\")\n",
        "\n",
        "        # Explicitly set the path to the Chrome driver\n",
        "        chromedriver_path = ChromeDriverManager().install()\n",
        "        service = Service(executable_path=chromedriver_path)\n",
        "        driver = webdriver.Chrome(service=service, options=chrome_options)\n",
        "\n",
        "        print(f\"Navigating to URL: {url}\")\n",
        "        driver.get(url)\n",
        "\n",
        "        # Wait for the table element to be present (adjust timeout as needed)\n",
        "        # The selector here targets the table element itself, not its content\n",
        "        wait = WebDriverWait(driver, 20)  # Wait up to 20 seconds\n",
        "        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, selector)))\n",
        "\n",
        "        # Retrieve the page source after JavaScript has rendered the content\n",
        "        page_source = driver.page_source\n",
        "        print(f\"Page source retrieved. Length: {len(page_source)} bytes\")\n",
        "\n",
        "        soup = BeautifulSoup(page_source, \"html.parser\")\n",
        "        table_body = soup.select_one(selector)\n",
        "\n",
        "        if table_body is None:\n",
        "            print(f\"Error: Table body not found using selector: {selector}\")\n",
        "            return None\n",
        "\n",
        "        rows = table_body.find_all(\"tr\")\n",
        "        if not rows:\n",
        "            print(\"Error: No rows found in the table.\")\n",
        "            return None\n",
        "\n",
        "        # Extract headers from the first row (th elements)\n",
        "        headers_list = [th.text.strip() for th in rows[0].find_all(\"th\")]\n",
        "\n",
        "        data = []\n",
        "        for row in rows:\n",
        "            cells = row.find_all(\"td\")\n",
        "            row_data = [cell.text.strip() for cell in cells]\n",
        "            if row_data:  # Only append if the row has data\n",
        "                data.append(row_data)\n",
        "\n",
        "        if not data:\n",
        "            print(\"Error: No data found in the table rows.\")\n",
        "            return None\n",
        "\n",
        "        df = pd.DataFrame(data, columns=headers_list)\n",
        "        return df\n",
        "\n",
        "    except WebDriverException as e:\n",
        "        print(f\"An error occurred with the WebDriver: {e}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred during Selenium scraping: {e}\")\n",
        "        return None\n",
        "    finally:\n",
        "        if driver:  # Ensure driver is closed even if an error occurs\n",
        "            driver.quit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03a09243"
      },
      "source": [
        "**Reasoning**:\n",
        "The `download_table` function has been updated to explicitly set the Chrome binary location. Now, I will re-run the test with a single URL to verify that Selenium can properly find and use the Chrome binary to scrape the table data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ac910a95"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "import random\n",
        "\n",
        "total_urls_to_download = len(shuffled_urls)\n",
        "\n",
        "df = pd.DataFrame()  # Initialized an empty DataFrame\n",
        "processed_count = 0\n",
        "\n",
        "# Test the download_table function with a single URL to verify\n",
        "url_to_test = shuffled_urls[0]\n",
        "print(f\"Testing download_table with URL: {url_to_test}\")\n",
        "df_test = download_table(url_to_test, selector)\n",
        "\n",
        "if df_test is not None:\n",
        "    print(\"Test download successful. Displaying first 5 rows of the test DataFrame:\")\n",
        "    print(df_test.head())\n",
        "else:\n",
        "    print(\"Test download failed. Please review the output above for errors.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bba262d1"
      },
      "source": [
        "**Reasoning**:\n",
        "The 'Read timed out' error suggests a communication issue between Selenium and the Chrome driver, possibly due to repeated driver installation within the loop or insufficient timeouts. I will modify the setup to initialize the ChromeDriver path once globally to avoid redundancy and explicitly set page load and script timeouts within the WebDriver options for better stability.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d09e5139"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from fake_useragent import UserAgent\n",
        "\n",
        "# Import Selenium modules\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "from selenium.common.exceptions import WebDriverException\n",
        "\n",
        "selector = 'table[width=\"100%\"][cellpadding=\"3\"][cellspacing=\"1\"]'  # Updated selector\n",
        "ua = UserAgent()  # Initialize UserAgent\n",
        "\n",
        "# Initialize ChromeDriverManager once globally\n",
        "try:\n",
        "    global chromedriver_path\n",
        "    chromedriver_path = ChromeDriverManager().install()\n",
        "    print(f\"ChromeDriver installed/located at: {chromedriver_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error installing ChromeDriver: {e}\")\n",
        "    chromedriver_path = None\n",
        "\n",
        "\n",
        "def download_table(url, selector):\n",
        "    \"\"\"\n",
        "    Downloads table data from a Finviz page using Selenium with a headless Chrome browser.\n",
        "    \"\"\"\n",
        "    driver = None  # Initialize driver to None for cleanup in finally block\n",
        "    if not chromedriver_path:\n",
        "        print(\"ChromeDriver not available, cannot proceed with scraping.\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        # Setup Chrome options for headless browsing\n",
        "        chrome_options = Options()\n",
        "        chrome_options.add_argument(\"--headless\")\n",
        "        chrome_options.add_argument(\"--no-sandbox\")\n",
        "        chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "\n",
        "        # Explicitly set the path to the Chrome binary\n",
        "        global chrome_binary_path  # Access the global variable\n",
        "        if \"CHROME_PATH\" in os.environ:\n",
        "            chrome_options.binary_location = os.environ[\"CHROME_PATH\"]\n",
        "        elif chrome_binary_path and os.path.exists(chrome_binary_path):\n",
        "            chrome_options.binary_location = chrome_binary_path\n",
        "        else:\n",
        "            print(\n",
        "                \"Warning: Chrome binary path not explicitly set. Relying on default discovery.\"\n",
        "            )\n",
        "\n",
        "        # Add a random User-Agent header to mimic a browser\n",
        "        new_user_agent = ua.random\n",
        "        chrome_options.add_argument(f\"user-agent={new_user_agent}\")\n",
        "        chrome_options.add_argument(\n",
        "            \"--remote-debugging-port=9222\"\n",
        "        )  # Recommended for Colab\n",
        "\n",
        "        # Explicitly set the path to the Chrome driver and use longer timeouts\n",
        "        service = Service(executable_path=chromedriver_path)\n",
        "        driver = webdriver.Chrome(service=service, options=chrome_options)\n",
        "        driver.set_page_load_timeout(60)  # Set page load timeout to 60 seconds\n",
        "        driver.set_script_timeout(60)  # Set script timeout to 60 seconds\n",
        "\n",
        "        print(f\"Navigating to URL: {url}\")\n",
        "        driver.get(url)\n",
        "\n",
        "        # Wait for the table element to be present (adjust timeout as needed)\n",
        "        wait = WebDriverWait(driver, 30)  # Increased WebDriverWait to 30 seconds\n",
        "        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, selector)))\n",
        "\n",
        "        # Retrieve the page source after JavaScript has rendered the content\n",
        "        page_source = driver.page_source\n",
        "        print(f\"Page source retrieved. Length: {len(page_source)} bytes\")\n",
        "\n",
        "        soup = BeautifulSoup(page_source, \"html.parser\")\n",
        "        table_body = soup.select_one(selector)\n",
        "\n",
        "        if table_body is None:\n",
        "            print(f\"Error: Table body not found using selector: {selector}\")\n",
        "            return None\n",
        "\n",
        "        rows = table_body.find_all(\"tr\")\n",
        "        if not rows:\n",
        "            print(\"Error: No rows found in the table.\")\n",
        "            return None\n",
        "\n",
        "        # Extract headers from the first row (th elements)\n",
        "        headers_list = [th.text.strip() for th in rows[0].find_all(\"th\")]\n",
        "\n",
        "        data = []\n",
        "        for row in rows:\n",
        "            cells = row.find_all(\"td\")\n",
        "            row_data = [cell.text.strip() for cell in cells]\n",
        "            if row_data:  # Only append if the row has data\n",
        "                data.append(row_data)\n",
        "\n",
        "        if not data:\n",
        "            print(\"Error: No data found in the table rows.\")\n",
        "            return None\n",
        "\n",
        "        df = pd.DataFrame(data, columns=headers_list)\n",
        "        return df\n",
        "\n",
        "    except WebDriverException as e:\n",
        "        print(f\"An error occurred with the WebDriver: {e}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred during Selenium scraping: {e}\")\n",
        "        return None\n",
        "    finally:\n",
        "        if driver:  # Ensure driver is closed even if an error occurs\n",
        "            driver.quit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b4dae4e"
      },
      "source": [
        "**Reasoning**:\n",
        "The `download_table` function has been updated with global ChromeDriver initialization, explicit Chrome binary location, and increased timeouts. Now, I will re-run the test with a single URL to verify that Selenium can properly scrape the table data without timing out.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "edf42649"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "import random\n",
        "import os  # Import os for environment variable checks\n",
        "\n",
        "total_urls_to_download = len(shuffled_urls)\n",
        "\n",
        "df = pd.DataFrame()  # Initialized an empty DataFrame\n",
        "processed_count = 0\n",
        "\n",
        "# Test the download_table function with a single URL to verify\n",
        "url_to_test = shuffled_urls[0]\n",
        "print(f\"Testing download_table with URL: {url_to_test}\")\n",
        "df_test = download_table(url_to_test, selector)\n",
        "\n",
        "if df_test is not None:\n",
        "    print(\"Test download successful. Displaying first 5 rows of the test DataFrame:\")\n",
        "    print(df_test.head())\n",
        "else:\n",
        "    print(\"Test download failed. Please review the output above for errors.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d171cfd3"
      },
      "source": [
        "# Task\n",
        "Okay, I will proceed with the plan.\n",
        "\n",
        "First, I will modify the `download_table` function to enable verbose ChromeDriver logging. This will help diagnose the 'Read timed out' error previously encountered. I will add `service_args=['--verbose', '--log-path=/tmp/chromedriver.log']` to the `Service` initialization.\n",
        "\n",
        "```python\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from fake_useragent import UserAgent\n",
        "import os\n",
        "\n",
        "# Import Selenium modules\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "from selenium.common.exceptions import WebDriverException\n",
        "\n",
        "selector = 'table[width=\"100%\"][cellpadding=\"3\"][cellspacing=\"1\"]' # Updated selector\n",
        "ua = UserAgent()  # Initialize UserAgent\n",
        "\n",
        "# Initialize ChromeDriverManager once globally\n",
        "try:\n",
        "    global chromedriver_path\n",
        "    chromedriver_path = ChromeDriverManager().install()\n",
        "    print(f\"ChromeDriver installed/located at: {chromedriver_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error installing ChromeDriver: {e}\")\n",
        "    chromedriver_path = None\n",
        "\n",
        "def download_table(url, selector):\n",
        "    \"\"\"\n",
        "    Downloads table data from a Finviz page using Selenium with a headless Chrome browser.\n",
        "    \"\"\"\n",
        "    driver = None # Initialize driver to None for cleanup in finally block\n",
        "    if not chromedriver_path:\n",
        "        print(\"ChromeDriver not available, cannot proceed with scraping.\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        # Setup Chrome options for headless browsing\n",
        "        chrome_options = Options()\n",
        "        chrome_options.add_argument('--headless')\n",
        "        chrome_options.add_argument('--no-sandbox')\n",
        "        chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "\n",
        "        # Explicitly set the path to the Chrome binary\n",
        "        global chrome_binary_path # Access the global variable\n",
        "        if 'CHROME_PATH' in os.environ:\n",
        "            chrome_options.binary_location = os.environ['CHROME_PATH']\n",
        "        elif 'chrome_binary_path' in globals() and chrome_binary_path and os.path.exists(chrome_binary_path):\n",
        "            chrome_options.binary_location = chrome_binary_path\n",
        "        else:\n",
        "            print(\"Warning: Chrome binary path not explicitly set. Relying on default discovery.\")\n",
        "\n",
        "        # Add a random User-Agent header to mimic a browser\n",
        "        new_user_agent = ua.random\n",
        "        chrome_options.add_argument(f'user-agent={new_user_agent}')\n",
        "        chrome_options.add_argument('--remote-debugging-port=9222') # Recommended for Colab\n",
        "\n",
        "        # Explicitly set the path to the Chrome driver and use longer timeouts\n",
        "        # Enable verbose logging for ChromeDriver\n",
        "        service = Service(executable_path=chromedriver_path, service_args=['--verbose', '--log-path=/tmp/chromedriver.log'])\n",
        "        driver = webdriver.Chrome(service=service, options=chrome_options)\n",
        "        driver.set_page_load_timeout(60) # Set page load timeout to 60 seconds\n",
        "        driver.set_script_timeout(60)    # Set script timeout to 60 seconds\n",
        "\n",
        "        print(f\"Navigating to URL: {url}\")\n",
        "        driver.get(url)\n",
        "\n",
        "        # Wait for the table element to be present (adjust timeout as needed)\n",
        "        wait = WebDriverWait(driver, 30) # Increased WebDriverWait to 30 seconds\n",
        "        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, selector)))\n",
        "\n",
        "        # Retrieve the page source after JavaScript has rendered the content\n",
        "        page_source = driver.page_source\n",
        "        print(f\"Page source retrieved. Length: {len(page_source)} bytes\")\n",
        "\n",
        "        soup = BeautifulSoup(page_source, 'html.parser')\n",
        "        table_body = soup.select_one(selector)\n",
        "\n",
        "        if table_body is None:\n",
        "            print(f\"Error: Table body not found using selector: {selector}\")\n",
        "            return None\n",
        "\n",
        "        rows = table_body.find_all('tr')\n",
        "        if not rows:\n",
        "            print(\"Error: No rows found in the table.\")\n",
        "            return None\n",
        "\n",
        "        # Extract headers from the first row (th elements)\n",
        "        headers_list = [th.text.strip() for th in rows[0].find_all('th')]\n",
        "\n",
        "        data = []\n",
        "        for row in rows:\n",
        "            cells = row.find_all('td')\n",
        "            row_data = [cell.text.strip() for cell in cells]\n",
        "            if row_data:  # Only append if the row has data\n",
        "                data.append(row_data)\n",
        "\n",
        "        if not data:\n",
        "            print(\"Error: No data found in the table rows.\")\n",
        "            return None\n",
        "\n",
        "        df = pd.DataFrame(data, columns=headers_list)\n",
        "        return df\n",
        "\n",
        "    except WebDriverException as e:\n",
        "        print(f\"An error occurred with the WebDriver: {e}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred during Selenium scraping: {e}\")\n",
        "        return None\n",
        "    finally:\n",
        "        if driver: # Ensure driver is closed even if an error occurs\n",
        "            driver.quit()\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "862c28f7"
      },
      "source": [
        "## Enable Verbose ChromeDriver Logging\n",
        "\n",
        "### Subtask:\n",
        "Modify the `download_table` function to enable verbose logging for the ChromeDriver service. This will provide detailed output on the WebDriver's interactions and help identify the cause of the 'Read timed out' error.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1524c5d8"
      },
      "source": [
        "**Reasoning**:\n",
        "To enable verbose logging for ChromeDriver and help diagnose the 'Read timed out' error, I will modify the `download_table` function in cell `d09e5139` by adding `service_args=['--verbose', '--log-path=/tmp/chromedriver.log']` to the `Service` initialization.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aef5578d"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from fake_useragent import UserAgent\n",
        "\n",
        "# Import Selenium modules\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "from selenium.common.exceptions import WebDriverException\n",
        "\n",
        "selector = 'table[width=\"100%\"][cellpadding=\"3\"][cellspacing=\"1\"]'  # Updated selector\n",
        "ua = UserAgent()  # Initialize UserAgent\n",
        "\n",
        "# Initialize ChromeDriverManager once globally\n",
        "try:\n",
        "    global chromedriver_path\n",
        "    chromedriver_path = ChromeDriverManager().install()\n",
        "    print(f\"ChromeDriver installed/located at: {chromedriver_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error installing ChromeDriver: {e}\")\n",
        "    chromedriver_path = None\n",
        "\n",
        "\n",
        "def download_table(url, selector):\n",
        "    \"\"\"\n",
        "    Downloads table data from a Finviz page using Selenium with a headless Chrome browser.\n",
        "    \"\"\"\n",
        "    driver = None  # Initialize driver to None for cleanup in finally block\n",
        "    if not chromedriver_path:\n",
        "        print(\"ChromeDriver not available, cannot proceed with scraping.\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        # Setup Chrome options for headless browsing\n",
        "        chrome_options = Options()\n",
        "        chrome_options.add_argument(\"--headless\")\n",
        "        chrome_options.add_argument(\"--no-sandbox\")\n",
        "        chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "\n",
        "        # Explicitly set the path to the Chrome binary\n",
        "        global chrome_binary_path  # Access the global variable\n",
        "        if \"CHROME_PATH\" in os.environ:\n",
        "            chrome_options.binary_location = os.environ[\"CHROME_PATH\"]\n",
        "        elif chrome_binary_path and os.path.exists(chrome_binary_path):\n",
        "            chrome_options.binary_location = chrome_binary_path\n",
        "        else:\n",
        "            print(\n",
        "                \"Warning: Chrome binary path not explicitly set. Relying on default discovery.\"\n",
        "            )\n",
        "\n",
        "        # Add a random User-Agent header to mimic a browser\n",
        "        new_user_agent = ua.random\n",
        "        chrome_options.add_argument(f\"user-agent={new_user_agent}\")\n",
        "        chrome_options.add_argument(\n",
        "            \"--remote-debugging-port=9222\"\n",
        "        )  # Recommended for Colab\n",
        "\n",
        "        # Explicitly set the path to the Chrome driver and use longer timeouts\n",
        "        # Enable verbose logging for ChromeDriver\n",
        "        service = Service(\n",
        "            executable_path=chromedriver_path,\n",
        "            service_args=[\"--verbose\", \"--log-path=/tmp/chromedriver.log\"],\n",
        "        )\n",
        "        driver = webdriver.Chrome(service=service, options=chrome_options)\n",
        "        driver.set_page_load_timeout(60)  # Set page load timeout to 60 seconds\n",
        "        driver.set_script_timeout(60)  # Set script timeout to 60 seconds\n",
        "\n",
        "        print(f\"Navigating to URL: {url}\")\n",
        "        driver.get(url)\n",
        "\n",
        "        # Wait for the table element to be present (adjust timeout as needed)\n",
        "        wait = WebDriverWait(driver, 30)  # Increased WebDriverWait to 30 seconds\n",
        "        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, selector)))\n",
        "\n",
        "        # Retrieve the page source after JavaScript has rendered the content\n",
        "        page_source = driver.page_source\n",
        "        print(f\"Page source retrieved. Length: {len(page_source)} bytes\")\n",
        "\n",
        "        soup = BeautifulSoup(page_source, \"html.parser\")\n",
        "        table_body = soup.select_one(selector)\n",
        "\n",
        "        if table_body is None:\n",
        "            print(f\"Error: Table body not found using selector: {selector}\")\n",
        "            return None\n",
        "\n",
        "        rows = table_body.find_all(\"tr\")\n",
        "        if not rows:\n",
        "            print(\"Error: No rows found in the table.\")\n",
        "            return None\n",
        "\n",
        "        # Extract headers from the first row (th elements)\n",
        "        headers_list = [th.text.strip() for th in rows[0].find_all(\"th\")]\n",
        "\n",
        "        data = []\n",
        "        for row in rows:\n",
        "            cells = row.find_all(\"td\")\n",
        "            row_data = [cell.text.strip() for cell in cells]\n",
        "            if row_data:  # Only append if the row has data\n",
        "                data.append(row_data)\n",
        "\n",
        "        if not data:\n",
        "            print(\"Error: No data found in the table rows.\")\n",
        "            return None\n",
        "\n",
        "        df = pd.DataFrame(data, columns=headers_list)\n",
        "        return df\n",
        "\n",
        "    except WebDriverException as e:\n",
        "        print(f\"An error occurred with the WebDriver: {e}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred during Selenium scraping: {e}\")\n",
        "        return None\n",
        "    finally:\n",
        "        if driver:  # Ensure driver is closed even if an error occurs\n",
        "            driver.quit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91633d15"
      },
      "source": [
        "**Reasoning**:\n",
        "The `download_table` function has been updated with verbose ChromeDriver logging. Now, I will re-run the test with a single URL to verify if the logging provides insights into the 'Read timed out' error and if the table can be successfully scraped.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2d193b92"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "import random\n",
        "import os  # Import os for environment variable checks\n",
        "\n",
        "total_urls_to_download = len(shuffled_urls)\n",
        "\n",
        "df = pd.DataFrame()  # Initialized an empty DataFrame\n",
        "processed_count = 0\n",
        "\n",
        "# Test the download_table function with a single URL to verify\n",
        "url_to_test = shuffled_urls[0]\n",
        "print(f\"Testing download_table with URL: {url_to_test}\")\n",
        "df_test = download_table(url_to_test, selector)\n",
        "\n",
        "if df_test is not None:\n",
        "    print(\"Test download successful. Displaying first 5 rows of the test DataFrame:\")\n",
        "    print(df_test.head())\n",
        "else:\n",
        "    print(\"Test download failed. Please review the output above for errors.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "927624a6"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous execution indicated a WebDriver error, but the verbose log file was not inspected. To diagnose the 'Read timed out' error and the generic WebDriver error, I need to read the `chromedriver.log` file that was generated by the verbose logging to get more specific details.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3c513df2"
      },
      "outputs": [],
      "source": [
        "log_file_path = \"/tmp/chromedriver.log\"\n",
        "if os.path.exists(log_file_path):\n",
        "    print(f\"Content of {log_file_path}:\\n\")\n",
        "    with open(log_file_path, \"r\") as f:\n",
        "        print(f.read())\n",
        "else:\n",
        "    print(f\"ChromeDriver log file not found at {log_file_path}.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
