{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9a1c8f7",
   "metadata": {},
   "source": [
    "### Sharpe Strategy Worker Workflow\n",
    "\n",
    "This notebook executes a single backtest iteration for a given train/test data split. It is designed to be called by an orchestrator notebook (`py11_backtest_orchestrator.ipynb`) via `papermill`.\n",
    "\n",
    "**Workflow:**\n",
    "\n",
    "1.  **Receive Parameters:** Papermill injects paths for train/test data, Finviz data, the output location, and the benchmark ticker.\n",
    "2.  **Load Data:** Reads the data from the specified paths.\n",
    "3.  **Calculate Sharpe & Filter:** Calculates Sharpe ratios over multiple lookback periods within the training data and filters for tickers that consistently underperform the benchmark.\n",
    "4.  **Determine Weights:** Assigns equal weights to the selected underperforming tickers.\n",
    "5.  **Simulate & Compare:** Runs a simulation on the test data, comparing the portfolio's performance against the benchmark.\n",
    "6.  **Save Results:** Saves the daily portfolio and benchmark returns for the test period to the specified output path."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c58ec7",
   "metadata": {},
   "source": [
    "### Step 1: Setup and Parameters\n",
    "\n",
    "This cell imports necessary libraries and defines the parameters that will be injected by `papermill`. **Do not modify the `Parameters` section manually when running via the orchestrator.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86120229",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Papermill injects parameters here\n",
    "# Add parameter tag -> Open Command Palette (ctrl+ shift+ p) -> Add Cell Tag -> parameters \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Parameters ---\n",
    "# [REFACTOR] This cell is now tagged as the 'parameters' cell for papermill.\n",
    "# Default values are provided for interactive testing, but will be overwritten.\n",
    "returns_train_path = \"path/to/train_data.parquet\"\n",
    "returns_test_path = \"path/to/test_data.parquet\"\n",
    "finviz_data_path = \"path/to/finviz_data.parquet\"\n",
    "output_path = \"path/to/output.parquet\"\n",
    "benchmark_ticker = \"VGT\"\n",
    "\n",
    "# --- Verification (runs inside the notebook) ---\n",
    "print(\"--- Worker Notebook Execution Started ---\")\n",
    "print(f\"Train data path: {returns_train_path}\")\n",
    "print(f\"Test data path:  {returns_test_path}\")\n",
    "print(f\"Output path:     {output_path}\")\n",
    "print(f\"Benchmark:       {benchmark_ticker}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908bc2f1",
   "metadata": {},
   "source": [
    "### Step 2: Load Data\n",
    "\n",
    "Load the training and testing datasets using the paths provided by the orchestrator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af92227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string paths from papermill to Path objects\n",
    "returns_train_path = Path(returns_train_path)\n",
    "returns_test_path = Path(returns_test_path)\n",
    "finviz_data_path = Path(finviz_data_path)\n",
    "\n",
    "# Load data\n",
    "returns_train = pd.read_parquet(returns_train_path)\n",
    "returns_test = pd.read_parquet(returns_test_path)\n",
    "df_finviz = pd.read_parquet(finviz_data_path)\n",
    "\n",
    "print(f\"Loaded training data with shape: {returns_train.shape}\")\n",
    "print(f\"Loaded testing data with shape: {returns_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7218ab0",
   "metadata": {},
   "source": [
    "### Step 3: Calculate Sharpe Ratios and Filter Tickers\n",
    "\n",
    "Analyze the training data to select tickers for our portfolio. The strategy is to select assets that have consistently lower Sharpe ratios than the benchmark over multiple time horizons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6564b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sharpe(returns: pd.Series) -> float:\n",
    "    \"\"\"Calculates the Sharpe ratio for a returns series.\"\"\"\n",
    "    # Note: This is a simplified Sharpe ratio (risk-free rate = 0).\n",
    "    # For annualized Sharpe, adjustments would be needed.\n",
    "    if returns.std() == 0:\n",
    "        return 0.0\n",
    "    return returns.mean() / returns.std()\n",
    "\n",
    "# Define lookback periods\n",
    "lookbacks = [30, 60, 120, 240]\n",
    "# Dictionary of sharpe ratios, \n",
    "# key: days, \n",
    "# value: pd.series of tickers' sharpe ratios\n",
    "sharpe_ratios = {}\n",
    "\n",
    "# Calculate Sharpe for each lookback period\n",
    "for days in lookbacks:\n",
    "    if len(returns_train) >= days:\n",
    "        sharpe_ratios[days] = returns_train.iloc[-days:].apply(calculate_sharpe)\n",
    "\n",
    "# Find tickers that consistently underperform the benchmark\n",
    "if sharpe_ratios:\n",
    "    # Start with all tickers\n",
    "    candidate_tickers = returns_train.columns.drop('CASH')\n",
    "    \n",
    "    # Iteratively filter the list\n",
    "    for days, sr_series in sharpe_ratios.items():\n",
    "        # missing benchmark_ticker returns -np.inf\n",
    "        benchmark_sharpe = sr_series.get(benchmark_ticker, -np.inf)\n",
    "\n",
    "        # underperformers = sr_series[sr_series < benchmark_sharpe].index\n",
    "        #candidate_tickers = candidate_tickers.intersection(underperformers)        \n",
    "        \n",
    "        overperformers = sr_series[sr_series > benchmark_sharpe].index        \n",
    "        candidate_tickers = candidate_tickers.intersection(overperformers)\n",
    "    \n",
    "    selected_tickers = candidate_tickers.tolist()\n",
    "else:\n",
    "    selected_tickers = []\n",
    "\n",
    "print(f\"Found {len(selected_tickers)} tickers that consistently underperformed '{benchmark_ticker}'.\")\n",
    "print(f\"Selected tickers: {selected_tickers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e2aa46",
   "metadata": {},
   "source": [
    "### Step 4: Determine Portfolio Weights\n",
    "\n",
    "Allocate weights to the selected tickers. If no tickers are selected, the portfolio defaults to 100% cash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e885abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "if selected_tickers:\n",
    "    num_tickers = len(selected_tickers)\n",
    "    uniform_weight = 1.0 / num_tickers\n",
    "    selected_weights = np.full(num_tickers, uniform_weight)\n",
    "    print(f\"Assigned uniform weight of {uniform_weight:.4f} to {num_tickers} tickers.\")\n",
    "else:\n",
    "    # Default to a cash position if no tickers meet the criteria\n",
    "    selected_tickers = ['CASH']\n",
    "    selected_weights = np.array([1.0])\n",
    "    print(\"No tickers selected. Defaulting to 100% CASH position.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8687fdb7",
   "metadata": {},
   "source": [
    "### Step 5: Simulate Portfolio on Test Data\n",
    "\n",
    "Apply the calculated weights to the test data returns to simulate the portfolio's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa5f42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter test returns for selected tickers and calculate portfolio daily return\n",
    "portfolio_returns_test = returns_test[selected_tickers].dot(selected_weights)\n",
    "\n",
    "# Create a results DataFrame including the benchmark for comparison\n",
    "results_df = pd.DataFrame({\n",
    "    'Portfolio': portfolio_returns_test,\n",
    "    'Benchmark': returns_test[benchmark_ticker]\n",
    "})\n",
    "\n",
    "print(\"Simulation on test data complete.\")\n",
    "display(results_df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d10a1d",
   "metadata": {},
   "source": [
    "### Step 6: Save Results\n",
    "\n",
    "Save the resulting portfolio and benchmark daily returns to the specified output file. This file will be collected by the orchestrator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ad15a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [REFACTOR] Saving to a unique parquet file. The orchestrator will handle aggregation.\n",
    "output_path = Path(output_path)\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True) # Ensure directory exists\n",
    "\n",
    "results_df.to_parquet(output_path)\n",
    "\n",
    "print(f\"Successfully saved test period results to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134abe41",
   "metadata": {},
   "source": [
    "### Step 7: Verification and Analysis (Optional)\n",
    "\n",
    "This cell provides a quick visualization of the test period's performance. The final, aggregated analysis should be done in a separate notebook after the orchestrator finishes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bebc34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cumulative returns for plotting\n",
    "cumulative_results = (1 + results_df).cumprod()\n",
    "\n",
    "# Plot the results\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "cumulative_results.plot(ax=ax)\n",
    "\n",
    "ax.set_title(f'Portfolio vs. Benchmark ({benchmark_ticker}) Performance')\n",
    "ax.set_ylabel('Cumulative Return')\n",
    "ax.set_xlabel('Date')\n",
    "ax.legend(['Portfolio', 'Benchmark'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a46102",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
