{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96b80941",
   "metadata": {},
   "source": [
    "### Backtest Orchestrator Workflow\n",
    "\n",
    "This notebook automates a rolling-window backtest by iteratively executing a worker notebook (`_pm_worker_sharpe_strategy.ipynb`) for each time slice.\n",
    "\n",
    "**Workflow:**\n",
    "\n",
    "1.  **Setup:** Loads project configurations and strategy parameters.\n",
    "2.  **Load Data:** Reads the master adjusted close prices file.\n",
    "3.  **Prepare Data Chunks:** Converts prices to returns and uses a utility function to create rolling window data chunks for the backtest.\n",
    "4.  **Execute Backtests:** Loops through each data chunk, splitting it into train/test sets, saving them to a temporary directory, and running the worker notebook via `papermill` with the corresponding file paths.\n",
    "5.  **Aggregate Results:** After all workers complete, it collects their individual output files and combines them into a single, final portfolio returns file.\n",
    "6.  **Cleanup:** Deletes the temporary data files created during the process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e4995e",
   "metadata": {},
   "source": [
    "### Step 1: Setup and Configuration\n",
    "\n",
    "This cell contains all imports and configuration variables. It defines the project structure and parameters for the rolling backtest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f0a60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import papermill as pm\n",
    "import shutil\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# --- Project Path Configuration ---\n",
    "# [REFACTOR] Standardized path setup for consistency and portability.\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "ROOT_DIR = NOTEBOOK_DIR.parent\n",
    "DATA_DIR = ROOT_DIR / 'data'\n",
    "SRC_DIR = ROOT_DIR / 'src'\n",
    "TEMP_DIR = NOTEBOOK_DIR / 'temp_backtest_data'\n",
    "OUTPUT_DIR = NOTEBOOK_DIR / 'backtest_results'\n",
    "\n",
    "# --- Add src to Python path ---\n",
    "if str(SRC_DIR) not in sys.path:\n",
    "    sys.path.append(str(SRC_DIR))\n",
    "\n",
    "# --- Import Custom Modules ---\n",
    "# [REFACTOR] Moved imports here to follow the standard structure.\n",
    "import utils\n",
    "\n",
    "# --- Papermill Configuration ---\n",
    "WORKER_NOTEBOOK_NAME = \"_pm_worker_sharpe_strategy.ipynb\"\n",
    "WORKER_NOTEBOOK_PATH = NOTEBOOK_DIR / WORKER_NOTEBOOK_NAME\n",
    "AGGREGATED_RESULTS_FILENAME = \"final_portfolio_returns.parquet\"\n",
    "\n",
    "# --- Strategy & Backtest Parameters ---\n",
    "# [REFACTOR] All strategy parameters centralized for easy tuning.\n",
    "SLIDING_WINDOW_WIDTH = 300\n",
    "SLIDING_WINDOW_STEP = 30\n",
    "TRAIN_TEST_SPLIT_POINT = 270 # Defines how many rows of the window are for training\n",
    "BENCHMARK_TICKER = 'VGT'\n",
    "FINVIZ_DATA_FILENAME = '2025-08-01_df_finviz_merged_stocks_etfs.parquet' # Example filename\n",
    "\n",
    "# --- Verification ---\n",
    "print(\"--- Path Configuration ---\")\n",
    "print(f\"ROOT_DIR:                {ROOT_DIR}\")\n",
    "print(f\"NOTEBOOK_DIR:            {NOTEBOOK_DIR}\")\n",
    "print(f\"DATA_DIR:                {DATA_DIR}\")\n",
    "print(f\"SRC_DIR:                 {SRC_DIR}\")\n",
    "print(f\"TEMP_DIR:                {TEMP_DIR}\")\n",
    "print(f\"OUTPUT_DIR:              {OUTPUT_DIR}\")\n",
    "print(f\"WORKER_NOTEBOOK_PATH:    {WORKER_NOTEBOOK_PATH}\")\n",
    "assert all([ROOT_DIR.exists(), DATA_DIR.exists(), SRC_DIR.exists(), NOTEBOOK_DIR.exists()]), \"A key directory was not found!\"\n",
    "assert (DATA_DIR / FINVIZ_DATA_FILENAME).exists(), f\"Finviz data file not found: {FINVIZ_DATA_FILENAME}\"\n",
    "assert WORKER_NOTEBOOK_PATH.exists(), f\"Worker notebook not found at: {WORKER_NOTEBOOK_PATH}\"\n",
    "\n",
    "print(\"\\n--- Strategy Parameters ---\")\n",
    "print(f\"Window Width:  {SLIDING_WINDOW_WIDTH}, Step Size: {SLIDING_WINDOW_STEP}\")\n",
    "print(f\"Train/Test Split: {TRAIN_TEST_SPLIT_POINT} rows for training\")\n",
    "print(f\"Benchmark:     {BENCHMARK_TICKER}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6133f7",
   "metadata": {},
   "source": [
    "### Step 2: Load and Prepare Data\n",
    "\n",
    "Load the adjusted close prices and convert them to percentage returns, which form the basis of our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e4bba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load historical price data\n",
    "adj_close_path = DATA_DIR / 'df_adj_close.parquet'\n",
    "df_adj_close = pd.read_parquet(adj_close_path)\n",
    "\n",
    "# Calculate daily returns\n",
    "returns = df_adj_close.pct_change().dropna()\n",
    "\n",
    "# Add a risk-free 'CASH' asset with zero return\n",
    "returns['CASH'] = 0.0\n",
    "\n",
    "print(f\"Loaded returns data. Shape: {returns.shape}\")\n",
    "print(f\"Date range: {returns.index.min().strftime('%Y-%m-%d')} to {returns.index.max().strftime('%Y-%m-%d')}\")\n",
    "display(returns.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8ba9f6",
   "metadata": {},
   "source": [
    "### Step 3: Generate Rolling Window Chunks\n",
    "\n",
    "Using our utility function, slice the returns data into overlapping chunks that will be used for each iteration of the backtest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4312642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [REFACTOR] Logic moved to a clean, reusable function in src/utils.py\n",
    "rolling_chunks = utils.create_rolling_window_chunks(\n",
    "    returns_df=returns,\n",
    "    window_width=SLIDING_WINDOW_WIDTH,\n",
    "    step_size=SLIDING_WINDOW_STEP\n",
    ")\n",
    "\n",
    "print(f\"Successfully generated {len(rolling_chunks)} rolling window chunks.\")\n",
    "if rolling_chunks:\n",
    "    chunk = rolling_chunks[0]\n",
    "    print(f\"Shape of each chunk: {chunk.shape}\")\n",
    "    print(f\"First chunk date range: {chunk.index.min().strftime('%Y-%m-%d')} to {chunk.index.max().strftime('%Y-%m-%d')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300044ad",
   "metadata": {},
   "source": [
    "### Step 4: Execute Backtest Loop via Papermill\n",
    "\n",
    "Iterate through each data chunk, save the train/test splits to disk, and execute the worker notebook.\n",
    "\n",
    "**Note:** This cell may take a long time to run, depending on the number of chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad16612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temporary and output directories if they don't exist\n",
    "TEMP_DIR.mkdir(exist_ok=True)\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"--- Starting Papermill Execution Loop for {len(rolling_chunks)} chunks ---\")\n",
    "\n",
    "# For demonstration, we run only the first 3 chunks.\n",
    "# To run all, change to: for i, chunk in enumerate(rolling_chunks):\n",
    "for i, chunk in enumerate(rolling_chunks[:1]):\n",
    "    # Define train and test sets for this chunk\n",
    "    returns_train = chunk.iloc[:TRAIN_TEST_SPLIT_POINT]\n",
    "    returns_test = chunk.iloc[TRAIN_TEST_SPLIT_POINT:]\n",
    "\n",
    "    # Define temporary file paths for this iteration\n",
    "    train_file = TEMP_DIR / f\"returns_train_chunk_{i}.parquet\"\n",
    "    test_file = TEMP_DIR / f\"returns_test_chunk_{i}.parquet\"\n",
    "    result_file = OUTPUT_DIR / f\"result_chunk_{i}.parquet\"\n",
    "    output_notebook_path = OUTPUT_DIR / f\"output_notebook_chunk_{i}.ipynb\"\n",
    "\n",
    "    # Save data splits to disk\n",
    "    returns_train.to_parquet(train_file)\n",
    "    returns_test.to_parquet(test_file)\n",
    "\n",
    "    print(f\"\\nExecuting chunk {i+1}/{len(rolling_chunks)}...\")\n",
    "    print(f\"  Train Period: {returns_train.index.min().date()} to {returns_train.index.max().date()}\")\n",
    "    print(f\"  Test Period:  {returns_test.index.min().date()} to {returns_test.index.max().date()}\")\n",
    "\n",
    "    # [REFACTOR] All paths are now passed as parameters for a self-contained worker.\n",
    "    pm.execute_notebook(\n",
    "       input_path=WORKER_NOTEBOOK_PATH,\n",
    "       output_path=output_notebook_path,\n",
    "       parameters={\n",
    "           \"returns_train_path\": str(train_file),\n",
    "           \"returns_test_path\": str(test_file),\n",
    "           \"finviz_data_path\": str(DATA_DIR / FINVIZ_DATA_FILENAME),\n",
    "           \"output_path\": str(result_file),\n",
    "           \"benchmark_ticker\": BENCHMARK_TICKER,\n",
    "       },\n",
    "       kernel_name=\"python3\"\n",
    "    )\n",
    "\n",
    "print(\"\\n--- Papermill execution complete. ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de678cf6",
   "metadata": {},
   "source": [
    "### Step 5: Aggregate and Save Final Results\n",
    "\n",
    "Collect the individual result files from each worker run and combine them into a single, continuous time series of portfolio returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e839ca0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [REFACTOR] New aggregation step replaces the fragile 'append' logic.\n",
    "all_results = []\n",
    "for i in range(len(rolling_chunks[:1])): # Match the loop range from Step 4\n",
    "    result_file = OUTPUT_DIR / f\"result_chunk_{i}.parquet\"\n",
    "    if result_file.exists():\n",
    "        chunk_result = pd.read_parquet(result_file)\n",
    "        all_results.append(chunk_result)\n",
    "    else:\n",
    "        print(f\"Warning: Result file not found for chunk {i}: {result_file}\")\n",
    "\n",
    "if all_results:\n",
    "    final_portfolio_returns = pd.concat(all_results).sort_index()\n",
    "    # Remove any potential duplicate rows from window overlaps if necessary\n",
    "    final_portfolio_returns = final_portfolio_returns[~final_portfolio_returns.index.duplicated(keep='first')]\n",
    "\n",
    "    # Save the aggregated results\n",
    "    final_output_path = OUTPUT_DIR / AGGREGATED_RESULTS_FILENAME\n",
    "    final_portfolio_returns.to_parquet(final_output_path)\n",
    "\n",
    "    print(f\"Successfully aggregated {len(all_results)} result chunks.\")\n",
    "    print(f\"Final portfolio returns saved to: {final_output_path}\")\n",
    "    display(final_portfolio_returns.head())\n",
    "    display(final_portfolio_returns.tail())\n",
    "else:\n",
    "    print(\"No result files found to aggregate.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7349d916",
   "metadata": {},
   "source": [
    "### Step 6: Cleanup Temporary Files\n",
    "\n",
    "Remove the intermediate `temp_backtest_data` directory to keep the project folder clean. This step includes garbage collection and a short delay to ensure all file handles from the worker notebooks are released before deletion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647e9b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import gc\n",
    "import time\n",
    "\n",
    "# [REFACTOR] Added a more robust cleanup step to handle potential file locks on Windows.\n",
    "if TEMP_DIR.exists():\n",
    "    # Explicitly run garbage collection to help release file handles from papermill\n",
    "    print(\"Running garbage collection to release file handles...\")\n",
    "    gc.collect()\n",
    "\n",
    "    # Add a short delay to give the OS time to release file locks\n",
    "    print(\"Waiting 2 seconds before cleanup...\")\n",
    "    time.sleep(2)\n",
    "\n",
    "    try:\n",
    "        shutil.rmtree(TEMP_DIR)\n",
    "        print(f\"✅ Successfully removed temporary directory: {TEMP_DIR}\")\n",
    "    except OSError as e:\n",
    "        print(f\"❌ Error removing temporary directory {TEMP_DIR}: {e}\")\n",
    "        print(\"This may happen if a file is still open. Try restarting the kernel and running this cell again.\")\n",
    "else:\n",
    "    print(\"Temporary directory not found, no cleanup needed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c26b8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
