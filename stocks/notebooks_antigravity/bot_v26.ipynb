{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9ae2708",
   "metadata": {},
   "source": [
    "# Here is the refactored solution. I have separated the concerns into three distinct layers:\n",
    "1.  **The Data Contract:** explicit `dataclasses` defining exactly what goes in and comes out.\n",
    "2.  **The Engine:** A purely mathematical class (`AlphaEngine`) containing the logic, with no widget/plotting dependencies.\n",
    "3.  **The UI:** A cleaned-up dashboard function that simply sends inputs to the Engine and visualizes the Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc79d79",
   "metadata": {},
   "source": [
    "### Following is the reverse chronological fix log (most recent entry is at the top )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68b1a41",
   "metadata": {},
   "source": [
    "```\n",
    "To see the full dataframe of all tickers (both those that passed and those that failed) for a specific date, we need to capture a snapshot of the universe inside the `_get_eligible_universe` method.\n",
    "\n",
    "I have updated the **`AlphaEngine`** class below.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30d1f60",
   "metadata": {},
   "source": [
    "```\n",
    "To verify that the relative percentile logic is working, we can modify the `AlphaEngine` to report exactly **how the cutoff was calculated** for the specific start date.\n",
    "\n",
    "We want to see evidence that:\n",
    "1.  In earlier years (e.g., 2005), the volume cutoff is lower (e.g., $200k).\n",
    "2.  In later years (e.g., 2024), the volume cutoff is higher (e.g., $5M).\n",
    "\n",
    "Here is the updated `AlphaEngine` and `UI` code. I have added a **\"Audit Log\"** feature. When you run the tool, it will now print exactly what the Dollar Volume Threshold was for that specific day.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50de4e99",
   "metadata": {},
   "source": [
    "```\n",
    "The best way to solve this is to switch from a **Fixed Dollar Threshold** (e.g., \"$1 Million\") to a **Relative Percentile Threshold** (e.g., \"Top 50% of the market\").\n",
    "\n",
    "In 2004, a stock trading $200k might have been in the top 50% of liquid stocks. In 2024, that same $200k is illiquid garbage. Using a percentile automatically adjusts for inflation and market growth over time.\n",
    "\n",
    "Here is how to modify your code to support this.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e8a3b9",
   "metadata": {},
   "source": [
    "```\n",
    "To fix this, we need to pass the **actual** calculated start date (the trading day the engine \"snapped\" to) back from the `AlphaEngine` to the UI. Then, the UI can compare the *Requested Date* vs. the *Actual Date* and display the warning message if they differ.\n",
    "\n",
    "Here is the plan:\n",
    "1.  **Update `EngineOutput`**: Add a `start_date` field to the dataclass.\n",
    "2.  **Update `AlphaEngine.run`**: Populate this new field with `safe_start_date`.\n",
    "3.  **Update `plot_walk_forward_analyzer`**: Add logic to compare the user's input date with the engine's returned date and print the \"Info\" message if they are different.\n",
    "\n",
    "Here is the updated code (Sections C, D, and E have changed):\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb918f61",
   "metadata": {},
   "source": [
    "```\n",
    "I have updated the `AlphaEngine.run` method. specifically inside the `if inputs.mode == 'Manual List':` block. It now iterates through every manual ticker and performs two checks:\n",
    "1.  **Existence**: Is the ticker in the database?\n",
    "2.  **Availability**: Does the ticker have a valid price on the specific `Start Date`?\n",
    "\n",
    "If any ticker fails, it compiles a specific error message explaining why (e.g., \"No price data on start date\") and aborts the calculation immediately.  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b728a8",
   "metadata": {},
   "source": [
    "```\n",
    "The `snapshot_df` contains **every single feature** calculated by your `generate_features` function for that specific day, plus the new audit columns we added.\n",
    "\n",
    "Here is exactly what is inside that DataFrame:\n",
    "\n",
    "### 1. The Core Features (from `generate_features`)\n",
    "*   **`TR`**: True Range\n",
    "*   **`ATR`**: Average True Range\n",
    "*   **`ATRP`**: Average True Range Percent (Volatility)\n",
    "*   **`RollingStalePct`**: How often the price didn't move or volume was 0.\n",
    "*   **`RollMedDollarVol`**: Median Daily Dollar Volume (Liquidity).\n",
    "*   **`RollingSameVolCount`**: Data quality check for repeated volume numbers.\n",
    "\n",
    "### 2. The Audit Columns (Added during filtering)\n",
    "*   **`Calculated_Cutoff`**: The specific dollar amount required to pass on that day.\n",
    "*   **`Passed_Vol_Check`**: `True` if the ticker met the liquidity requirement.\n",
    "*   **`Passed_Final`**: `True` if it passed **all** checks (Liquidity + Stale + Quality).\n",
    "\n",
    "=========================================\n",
    "\n",
    "Here are the formulas translated directly into the Python `pandas` code used in your `generate_features` function.\n",
    "\n",
    "I have simplified the code slightly to assume a single ticker context (removing the `groupby` wrapper) so you can see the raw math clearly.\n",
    "\n",
    "### 1. True Range (TR)\n",
    "Calculates the maximum of the three price differences.\n",
    "\n",
    "prev_close = df_ohlcv['Adj Close'].shift(1)\n",
    "\n",
    "# The three components\n",
    "diff1 = df_ohlcv['Adj High'] - df_ohlcv['Adj Low']\n",
    "diff2 = (df_ohlcv['Adj High'] - prev_close).abs()\n",
    "diff3 = (df_ohlcv['Adj Low'] - prev_close).abs()\n",
    "\n",
    "# Taking the max of the three\n",
    "tr = pd.concat([diff1, diff2, diff3], axis=1).max(axis=1)\n",
    "\n",
    "### 2. Average True Range (ATR)\n",
    "Uses an Exponential Weighted Mean (EWM) with a specific alpha smoothing factor.\n",
    "\n",
    "# N = atr_period (e.g., 14)\n",
    "# alpha = 1 / N\n",
    "atr = tr.ewm(alpha=1/14, adjust=False).mean()\n",
    "\n",
    "### 3. ATR Percent (ATRP)\n",
    "Simple division to normalize volatility.\n",
    "\n",
    "atrp = atr / df_ohlcv['Adj Close']\n",
    "\n",
    "### 4. Rolling Stale Percentage\n",
    "Checks if volume is 0 OR if High equals Low (price didn't move), then averages that 1 or 0 signal over the window.\n",
    "\n",
    "# 1. Define the Stale Signal (1 for stale, 0 for active)\n",
    "is_stale = np.where(\n",
    "    (df_ohlcv['Volume'] == 0) | (df_ohlcv['Adj High'] == df_ohlcv['Adj Low']), \n",
    "    1,  \n",
    "    0\n",
    ")\n",
    "\n",
    "# 2. Calculate average over window (W=252)\n",
    "rolling_stale_pct = pd.Series(is_stale).rolling(window=252).mean()\n",
    "\n",
    "### 5. Rolling Median Dollar Volume\n",
    "Calculates raw dollar volume, then finds the median over the window.\n",
    "\n",
    "# 1. Calculate Daily Dollar Volume\n",
    "dollar_volume = df_ohlcv['Adj Close'] * df_ohlcv['Volume']\n",
    "\n",
    "# 2. Get Median over window (W=252)\n",
    "roll_med_dollar_vol = dollar_volume.rolling(window=252).median()\n",
    "\n",
    "### 6. Rolling Same Volume Count\n",
    "Checks if today's volume is exactly the same as yesterday's (a sign of bad data), then sums those occurrences.\n",
    "\n",
    "# 1. Check if Volume(t) - Volume(t-1) equals 0\n",
    "# .diff() calculates current row minus previous row\n",
    "has_same_volume = (df_ohlcv['Volume'].diff() == 0).astype(int)\n",
    "\n",
    "# 2. Sum the errors over window (W=252)\n",
    "rolling_same_vol_count = has_same_volume.rolling(window=252).sum()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1110f2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import ipywidgets as widgets\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Optional, Any\n",
    "from collections import Counter\n",
    "import pprint\n",
    "from datetime import datetime, date\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION A: CORE HELPER FUNCTIONS & FEATURE GENERATION\n",
    "# (Unchanged from previous version)\n",
    "# ==============================================================================\n",
    "# ... (Keep generate_features, calculate_gain, calculate_sharpe, \n",
    "#      calculate_sharpe_atr, calculate_buy_and_hold_performance as is) ...\n",
    "\n",
    "def generate_features(df_ohlcv: pd.DataFrame, atr_period: int = 14, quality_window: int = 252, quality_min_periods: int = 126) -> pd.DataFrame:\n",
    "    # (Same as before)\n",
    "    if not df_ohlcv.index.is_monotonic_increasing: df_ohlcv = df_ohlcv.sort_index()\n",
    "    grouped = df_ohlcv.groupby(level='Ticker')\n",
    "    prev_close = grouped['Adj Close'].shift(1)\n",
    "    tr = pd.concat([df_ohlcv['Adj High'] - df_ohlcv['Adj Low'], abs(df_ohlcv['Adj High'] - prev_close), abs(df_ohlcv['Adj Low'] - prev_close)], axis=1).max(axis=1, skipna=False)\n",
    "    atr = tr.groupby(level='Ticker').transform(lambda x: x.ewm(alpha=1/atr_period, adjust=False).mean())\n",
    "    atrp = (atr / df_ohlcv['Adj Close']).replace([np.inf, -np.inf], np.nan)\n",
    "    indicator_df = pd.DataFrame({'TR': tr, 'ATR': atr, 'ATRP': atrp})\n",
    "    quality_temp_df = pd.DataFrame({'IsStale': np.where((df_ohlcv['Volume'] == 0) | (df_ohlcv['Adj High'] == df_ohlcv['Adj Low']), 1, 0), 'DollarVolume': df_ohlcv['Adj Close'] * df_ohlcv['Volume'], 'HasSameVolume': (grouped['Volume'].diff() == 0).astype(int)}, index=df_ohlcv.index)\n",
    "    rolling_result = quality_temp_df.groupby(level='Ticker').rolling(window=quality_window, min_periods=quality_min_periods).agg({'IsStale': 'mean', 'DollarVolume': 'median', 'HasSameVolume': 'sum'}).rename(columns={'IsStale': 'RollingStalePct', 'DollarVolume': 'RollMedDollarVol', 'HasSameVolume': 'RollingSameVolCount'}).reset_index(level=0, drop=True)\n",
    "    return pd.concat([indicator_df, rolling_result], axis=1)\n",
    "\n",
    "def calculate_gain(price_series): \n",
    "    if price_series.dropna().shape[0] < 2: return np.nan\n",
    "    return (price_series.ffill().iloc[-1] / price_series.bfill().iloc[0]) - 1\n",
    "\n",
    "def calculate_sharpe(return_series):\n",
    "    if return_series.dropna().shape[0] < 2: return np.nan\n",
    "    std = return_series.std()\n",
    "    return (return_series.mean() / std * np.sqrt(252)) if std > 0 else 0.0\n",
    "\n",
    "def calculate_sharpe_atr(return_series, atrp_series):\n",
    "    if return_series.dropna().shape[0] < 2 or atrp_series.dropna().empty: return np.nan\n",
    "    mean_atrp = atrp_series.mean()\n",
    "    return (return_series.mean() / mean_atrp) if mean_atrp > 0 else 0.0\n",
    "\n",
    "def calculate_buy_and_hold_performance(df_close, features_df, tickers, start_date, end_date):\n",
    "    if not tickers: return pd.Series(dtype=float), pd.Series(dtype=float), pd.Series(dtype=float)\n",
    "    ticker_counts = Counter(tickers)\n",
    "    initial_weights = pd.Series({t: c / len(tickers) for t, c in ticker_counts.items()})\n",
    "    prices_raw = df_close[initial_weights.index.tolist()].loc[start_date:end_date]\n",
    "    if prices_raw.dropna(how='all').empty: return pd.Series(dtype=float), pd.Series(dtype=float), pd.Series(dtype=float)\n",
    "    prices_norm = prices_raw.div(prices_raw.bfill().iloc[0])\n",
    "    weighted_growth = prices_norm.mul(initial_weights, axis='columns')\n",
    "    value_series = weighted_growth.sum(axis=1)\n",
    "    return_series = value_series.pct_change()\n",
    "    full_idx = pd.MultiIndex.from_product([initial_weights.index.tolist(), return_series.index], names=['Ticker', 'Date'])\n",
    "    feat_subset = features_df.reindex(full_idx)['ATRP'].unstack(level='Ticker')\n",
    "    atrp_series = (weighted_growth.div(value_series, axis='index').align(feat_subset, join='inner', axis=1)[0] * weighted_growth.div(value_series, axis='index').align(feat_subset, join='inner', axis=1)[1]).sum(axis=1)\n",
    "    return value_series, return_series, atrp_series\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION B: METRIC REGISTRY\n",
    "# ==============================================================================\n",
    "\n",
    "def metric_price(d): return calculate_gain(d['calc_close'])\n",
    "def metric_sharpe(d): \n",
    "    r = d['daily_returns']\n",
    "    return (r.mean() / r.std() * np.sqrt(252)).replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "def metric_sharpe_atr(d):\n",
    "    return (d['daily_returns'].mean() / d['atrp']).replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "METRIC_REGISTRY = {\n",
    "    'Price': metric_price,\n",
    "    'Sharpe': metric_sharpe,\n",
    "    'Sharpe (ATR)': metric_sharpe_atr,\n",
    "}\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION C: DATA CONTRACTS (The API)\n",
    "# Updated EngineOutput to include actual start_date\n",
    "# ==============================================================================\n",
    "\n",
    "@dataclass\n",
    "class EngineInput:\n",
    "    mode: str\n",
    "    start_date: pd.Timestamp\n",
    "    calc_period: int\n",
    "    fwd_period: int\n",
    "    metric: str\n",
    "    benchmark_ticker: str\n",
    "    rank_start: int = 1\n",
    "    rank_end: int = 10\n",
    "    quality_thresholds: Dict[str, float] = field(default_factory=lambda: {'min_median_dollar_volume': 1_000_000, 'max_stale_pct': 0.05, 'max_same_vol_count': 10})\n",
    "    manual_tickers: List[str] = field(default_factory=list)\n",
    "    debug: bool = False\n",
    "\n",
    "@dataclass\n",
    "class EngineOutput:\n",
    "    portfolio_series: pd.Series\n",
    "    benchmark_series: pd.Series\n",
    "    normalized_plot_data: pd.DataFrame\n",
    "    tickers: List[str]\n",
    "    initial_weights: pd.Series\n",
    "    perf_metrics: Dict[str, float]\n",
    "    results_df: pd.DataFrame\n",
    "    start_date: pd.Timestamp # <--- NEW FIELD: The actual trading start date used\n",
    "    calc_end_date: pd.Timestamp\n",
    "    viz_end_date: pd.Timestamp\n",
    "    error_msg: Optional[str] = None\n",
    "    debug_data: Optional[Dict[str, Any]] = None\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION D: THE ALPHA ENGINE (The \"Brain\")\n",
    "# This version saves a sorted dataframe called `universe_snapshot` into the debug data. It adds columns showing exactly which tickers passed or failed the specific thresholds.\n",
    "# ==============================================================================\n",
    "\n",
    "class AlphaEngine:\n",
    "    def __init__(self, df_ohlcv: pd.DataFrame, master_ticker: str = 'SPY'):\n",
    "        print(\"--- ‚öôÔ∏è Initializing AlphaEngine ---\")\n",
    "        self.features_df = generate_features(df_ohlcv)\n",
    "        print(\"Optimizing data structures...\")\n",
    "        self.df_close = df_ohlcv['Adj Close'].unstack(level=0)\n",
    "        \n",
    "        if master_ticker not in self.df_close.columns:\n",
    "            master_ticker = self.df_close.columns[0]\n",
    "            print(f\"Warning: Master ticker not found. Using {master_ticker}\")\n",
    "            \n",
    "        self.trading_calendar = self.df_close[master_ticker].dropna().index.unique().sort_values()\n",
    "        print(\"‚úÖ AlphaEngine Ready.\")\n",
    "\n",
    "    def run(self, inputs: EngineInput) -> EngineOutput:\n",
    "        # --- A. Validate Dates ---\n",
    "        try:\n",
    "            start_idx = self.trading_calendar.searchsorted(inputs.start_date)\n",
    "            if start_idx < 0: start_idx = 0\n",
    "        except Exception:\n",
    "            return self._error_result(\"Invalid Start Date\")\n",
    "\n",
    "        desired_end_idx = start_idx + inputs.calc_period + inputs.fwd_period\n",
    "        if desired_end_idx >= len(self.trading_calendar):\n",
    "            return self._error_result(f\"Date range exceeds history.\")\n",
    "\n",
    "        safe_start_date = self.trading_calendar[start_idx]\n",
    "        safe_calc_end_date = self.trading_calendar[start_idx + inputs.calc_period]\n",
    "        safe_viz_end_date = self.trading_calendar[start_idx + inputs.calc_period + inputs.fwd_period]\n",
    "\n",
    "        # --- B. Select Tickers ---\n",
    "        tickers_to_trade = []\n",
    "        results_table = pd.DataFrame()\n",
    "        debug_dict = {}\n",
    "        audit_info = {} \n",
    "\n",
    "        if inputs.mode == 'Manual List':\n",
    "            validation_errors = []\n",
    "            valid_tickers = []\n",
    "            for t in inputs.manual_tickers:\n",
    "                if t not in self.df_close.columns:\n",
    "                    validation_errors.append(f\"‚ùå {t}: Ticker not found.\")\n",
    "                    continue\n",
    "                if pd.isna(self.df_close.at[safe_start_date, t]):\n",
    "                    validation_errors.append(f\"‚ö†Ô∏è {t}: No price data on start date.\")\n",
    "                    continue\n",
    "                valid_tickers.append(t)\n",
    "            \n",
    "            if validation_errors: return self._error_result(\"\\n\".join(validation_errors))\n",
    "            if not valid_tickers: return self._error_result(\"No valid tickers.\")\n",
    "            tickers_to_trade = valid_tickers\n",
    "            results_table = pd.DataFrame(index=valid_tickers)\n",
    "            \n",
    "        else: # Ranking Mode\n",
    "            eligible_tickers = self._get_eligible_universe(safe_start_date, inputs.quality_thresholds, audit_info)\n",
    "            debug_dict['audit_liquidity'] = audit_info \n",
    "            \n",
    "            if not eligible_tickers: return self._error_result(\"No tickers passed quality filters.\")\n",
    "            \n",
    "            calc_close = self.df_close.loc[safe_start_date:safe_calc_end_date, eligible_tickers]\n",
    "            idx_product = pd.MultiIndex.from_product([eligible_tickers, calc_close.index], names=['Ticker', 'Date'])\n",
    "            feat_slice = self.features_df.reindex(idx_product).dropna(how='all')\n",
    "            atrp_mean = feat_slice.groupby(level='Ticker')['ATRP'].mean()\n",
    "            \n",
    "            ingredients = { 'calc_close': calc_close, 'daily_returns': calc_close.pct_change(), 'atrp': atrp_mean }\n",
    "            if inputs.metric not in METRIC_REGISTRY: return self._error_result(f\"Metric '{inputs.metric}' not found.\")\n",
    "            metric_vals = METRIC_REGISTRY[inputs.metric](ingredients)\n",
    "            sorted_tickers = metric_vals.sort_values(ascending=False)\n",
    "            \n",
    "            start_r = max(0, inputs.rank_start - 1)\n",
    "            end_r = inputs.rank_end\n",
    "            tickers_to_trade = sorted_tickers.iloc[start_r:end_r].index.tolist()\n",
    "            if not tickers_to_trade: return self._error_result(\"No tickers generated from ranking.\")\n",
    "\n",
    "            results_table = pd.DataFrame({\n",
    "                'Rank': range(inputs.rank_start, inputs.rank_start + len(tickers_to_trade)),\n",
    "                'Ticker': tickers_to_trade,\n",
    "                'Metric Value': sorted_tickers.loc[tickers_to_trade].values\n",
    "            }).set_index('Ticker')\n",
    "\n",
    "        # --- C. Performance Calculations ---\n",
    "        p_val, p_ret, p_atrp = calculate_buy_and_hold_performance(self.df_close, self.features_df, tickers_to_trade, safe_start_date, safe_viz_end_date)\n",
    "        b_val, b_ret, b_atrp = calculate_buy_and_hold_performance(self.df_close, self.features_df, [inputs.benchmark_ticker], safe_start_date, safe_viz_end_date)\n",
    "\n",
    "        # --- D. Final Metrics ---\n",
    "        plot_data = self.df_close[list(set(tickers_to_trade))].loc[safe_start_date:safe_viz_end_date]\n",
    "        if not plot_data.empty: plot_data = plot_data / plot_data.bfill().iloc[0]\n",
    "        calc_end_ts = safe_calc_end_date\n",
    "        metrics = {}\n",
    "        get_gain = lambda s: (s.iloc[-1] / s.iloc[0]) - 1 if len(s) > 0 else 0\n",
    "\n",
    "        metrics['full_p_gain'] = get_gain(p_val)\n",
    "        metrics['calc_p_gain'] = get_gain(p_val.loc[:calc_end_ts])\n",
    "        metrics['fwd_p_gain'] = get_gain(p_val.loc[calc_end_ts:])\n",
    "        metrics['full_p_sharpe_atr'] = calculate_sharpe_atr(p_ret, p_atrp)\n",
    "        metrics['calc_p_sharpe_atr'] = calculate_sharpe_atr(p_ret.loc[:calc_end_ts], p_atrp.loc[p_ret.loc[:calc_end_ts].index])\n",
    "        metrics['fwd_p_sharpe_atr'] = calculate_sharpe_atr(p_ret.loc[calc_end_ts:].iloc[1:], p_atrp.loc[p_ret.loc[calc_end_ts:].iloc[1:].index])\n",
    "        \n",
    "        if not b_ret.empty:\n",
    "            metrics['full_b_gain'] = get_gain(b_val)\n",
    "            metrics['calc_b_gain'] = get_gain(b_val.loc[:calc_end_ts])\n",
    "            metrics['fwd_b_gain'] = get_gain(b_val.loc[calc_end_ts:])\n",
    "            metrics['full_b_sharpe_atr'] = calculate_sharpe_atr(b_ret, b_atrp)\n",
    "            metrics['calc_b_sharpe_atr'] = calculate_sharpe_atr(b_ret.loc[:calc_end_ts], b_atrp.loc[b_ret.loc[:calc_end_ts].index])\n",
    "            metrics['fwd_b_sharpe_atr'] = calculate_sharpe_atr(b_ret.loc[calc_end_ts:].iloc[1:], b_atrp.loc[b_ret.loc[calc_end_ts:].iloc[1:].index])\n",
    "\n",
    "        if not plot_data.empty: results_table['Fwd Gain'] = (plot_data.iloc[-1] / plot_data.loc[calc_end_ts]) - 1\n",
    "        ticker_counts = Counter(tickers_to_trade)\n",
    "        weights = pd.Series({t: c/len(tickers_to_trade) for t, c in ticker_counts.items()})\n",
    "\n",
    "        if inputs.debug:\n",
    "            trace_df = plot_data.copy()\n",
    "            trace_df.columns = [f'Norm_Price_{c}' for c in trace_df.columns]\n",
    "            trace_df['Norm_Price_Portfolio'] = p_val\n",
    "            if not b_val.empty: trace_df[f'Norm_Price_Benchmark_{inputs.benchmark_ticker}'] = b_val\n",
    "            debug_dict['portfolio_trace'] = trace_df\n",
    "\n",
    "        return EngineOutput(\n",
    "            portfolio_series=p_val, benchmark_series=b_val, normalized_plot_data=plot_data,\n",
    "            tickers=tickers_to_trade, initial_weights=weights, perf_metrics=metrics,\n",
    "            results_df=results_table, start_date=safe_start_date,\n",
    "            calc_end_date=safe_calc_end_date, viz_end_date=safe_viz_end_date, debug_data=debug_dict\n",
    "        )\n",
    "\n",
    "    # --- UPDATED: CAPTURE SNAPSHOT ---\n",
    "    def _get_eligible_universe(self, date_ts, thresholds, audit_container=None):\n",
    "        avail_dates = self.features_df.index.get_level_values('Date').unique().sort_values()\n",
    "        valid_dates = avail_dates[avail_dates <= date_ts]\n",
    "        if valid_dates.empty: return []\n",
    "        day_features = self.features_df.xs(valid_dates[-1], level='Date')\n",
    "\n",
    "        # 1. Determine Dynamic Cutoff\n",
    "        vol_cutoff = thresholds.get('min_median_dollar_volume', 0)\n",
    "        percentile_used = \"N/A\"\n",
    "        dynamic_val = 0\n",
    "        \n",
    "        if 'min_liquidity_percentile' in thresholds:\n",
    "            percentile_used = thresholds['min_liquidity_percentile']\n",
    "            dynamic_val = day_features['RollMedDollarVol'].quantile(percentile_used)\n",
    "            vol_cutoff = max(vol_cutoff, dynamic_val)\n",
    "\n",
    "        # 2. Logic Mask\n",
    "        mask = (\n",
    "            (day_features['RollMedDollarVol'] >= vol_cutoff) &\n",
    "            (day_features['RollingStalePct'] <= thresholds['max_stale_pct']) &\n",
    "            (day_features['RollingSameVolCount'] <= thresholds['max_same_vol_count'])\n",
    "        )\n",
    "\n",
    "        # 3. Capture Detailed Audit Snapshot\n",
    "        if audit_container is not None:\n",
    "            audit_container['date'] = valid_dates[-1]\n",
    "            audit_container['total_tickers_available'] = len(day_features)\n",
    "            audit_container['percentile_setting'] = percentile_used\n",
    "            audit_container['percentile_value_usd'] = dynamic_val\n",
    "            audit_container['final_cutoff_usd'] = vol_cutoff\n",
    "            audit_container['tickers_passed'] = mask.sum()\n",
    "            \n",
    "            # Save the DataFrame!\n",
    "            snapshot = day_features.copy()\n",
    "            snapshot['Calculated_Cutoff'] = vol_cutoff\n",
    "            snapshot['Passed_Vol_Check'] = snapshot['RollMedDollarVol'] >= vol_cutoff\n",
    "            snapshot['Passed_Final'] = mask\n",
    "            # Sort by volume so user can see the cutoff point easily\n",
    "            snapshot = snapshot.sort_values('RollMedDollarVol', ascending=False)\n",
    "            audit_container['universe_snapshot'] = snapshot\n",
    "\n",
    "        return day_features[mask].index.tolist()\n",
    "\n",
    "    def _error_result(self, msg):\n",
    "        return EngineOutput(pd.Series(dtype=float), pd.Series(dtype=float), pd.DataFrame(), [], pd.Series(dtype=float), {}, pd.DataFrame(), pd.Timestamp.min, pd.Timestamp.min, pd.Timestamp.min, msg)\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION E: THE UI (Visualization)\n",
    "# Update this function to read the audit data from the `debug_data` and print it nicely.\n",
    "# Updated print logic to detect date shift\n",
    "# Fixed EngineInput argument mapping\n",
    "# ==============================================================================\n",
    "\n",
    "def plot_walk_forward_analyzer(df_ohlcv, \n",
    "                               default_start_date='2020-01-01', \n",
    "                               default_calc_period=126, \n",
    "                               default_fwd_period=63,\n",
    "                               default_metric='Sharpe (ATR)', \n",
    "                               default_rank_start=1, \n",
    "                               default_rank_end=10,\n",
    "                               default_benchmark_ticker='SPY', \n",
    "                               master_calendar_ticker='SPY', \n",
    "                               quality_thresholds=None, \n",
    "                               debug=False):\n",
    "    \n",
    "    engine = AlphaEngine(df_ohlcv, master_ticker=master_calendar_ticker)\n",
    "    results_container = [None]\n",
    "    debug_container = [None]\n",
    "\n",
    "    # --- UPDATED DEFAULT SETTINGS WITH PERCENTILE ---\n",
    "    if quality_thresholds is None:\n",
    "        quality_thresholds = {\n",
    "            'min_median_dollar_volume': 100_000, # Hard floor\n",
    "            'min_liquidity_percentile': 0.50,    # Top 50%\n",
    "            'max_stale_pct': 0.05, \n",
    "            'max_same_vol_count': 10\n",
    "        }\n",
    "\n",
    "    # (Widget setup code remains the same...)\n",
    "    mode_selector = widgets.RadioButtons(options=['Ranking', 'Manual List'], value='Ranking', description='Portfolio Mode:', layout={'width': 'max-content'})\n",
    "    start_date_picker = widgets.DatePicker(description='Start Date:', value=pd.to_datetime(default_start_date))\n",
    "    calc_period_input = widgets.IntText(value=default_calc_period, description='Calc Period:')\n",
    "    fwd_period_input = widgets.IntText(value=default_fwd_period, description='Fwd Period:')\n",
    "    metric_dropdown = widgets.Dropdown(options=list(METRIC_REGISTRY.keys()), value=default_metric, description='Metric:')\n",
    "    rank_start_input = widgets.IntText(value=default_rank_start, description='Rank Start:')\n",
    "    rank_end_input = widgets.IntText(value=default_rank_end, description='Rank End:')\n",
    "    manual_tickers_input = widgets.Textarea(value='', placeholder='Enter tickers...', description='Manual Tickers:', layout={'width': '400px', 'height': '80px'})\n",
    "    benchmark_input = widgets.Text(value=default_benchmark_ticker, description='Benchmark:', placeholder='Enter Ticker')\n",
    "    update_button = widgets.Button(description=\"Update Chart\", button_style='primary')\n",
    "    ticker_list_output = widgets.Output()\n",
    "\n",
    "    ranking_controls = widgets.HBox([metric_dropdown, rank_start_input, rank_end_input])\n",
    "    manual_controls = widgets.HBox([manual_tickers_input])\n",
    "    date_controls = widgets.HBox([start_date_picker, calc_period_input, fwd_period_input])\n",
    "    ui = widgets.VBox([mode_selector, date_controls, ranking_controls, manual_controls, widgets.HBox([benchmark_input, update_button]), ticker_list_output], layout=widgets.Layout(margin='10px 0 20px 0'))\n",
    "    \n",
    "    def on_mode_change(c):\n",
    "        ranking_controls.layout.display = 'flex' if c['new'] == 'Ranking' else 'none'\n",
    "        manual_controls.layout.display = 'none' if c['new'] == 'Ranking' else 'flex'\n",
    "    mode_selector.observe(on_mode_change, names='value')\n",
    "    on_mode_change({'new': mode_selector.value})\n",
    "\n",
    "    fig = go.FigureWidget()\n",
    "    fig.update_layout(title='Walk-Forward Performance Analysis', height=600, template=\"plotly_white\", hovermode='x unified')\n",
    "    for i in range(50): fig.add_trace(go.Scatter(visible=False, line=dict(width=2)))\n",
    "    fig.add_trace(go.Scatter(name='Benchmark', visible=True, line=dict(color='black', width=3, dash='dash')))\n",
    "    fig.add_trace(go.Scatter(name='Group Portfolio', visible=True, line=dict(color='green', width=3)))\n",
    "\n",
    "    def update_plot(b):\n",
    "        ticker_list_output.clear_output()\n",
    "        manual_list = [t.strip().upper() for t in manual_tickers_input.value.split(',') if t.strip()]\n",
    "        start_date_raw = pd.to_datetime(start_date_picker.value)\n",
    "        \n",
    "        if start_date_raw < (engine.trading_calendar[0] - pd.Timedelta(days=7)):\n",
    "            with ticker_list_output: print(f\"‚ö†Ô∏è DATE WARNING: Start date {start_date_raw.date()} is too early.\"); return\n",
    "\n",
    "        inputs = EngineInput(\n",
    "            mode=mode_selector.value,\n",
    "            start_date=start_date_raw,\n",
    "            calc_period=calc_period_input.value,\n",
    "            fwd_period=fwd_period_input.value,\n",
    "            metric=metric_dropdown.value,\n",
    "            benchmark_ticker=benchmark_input.value.strip().upper(),\n",
    "            rank_start=rank_start_input.value,\n",
    "            rank_end=rank_end_input.value,\n",
    "            quality_thresholds=quality_thresholds,\n",
    "            manual_tickers=manual_list,\n",
    "            debug=debug\n",
    "        )\n",
    "        \n",
    "        with ticker_list_output:\n",
    "            res = engine.run(inputs)\n",
    "            results_container[0] = res\n",
    "            debug_container[0] = res.debug_data\n",
    "            if res.error_msg: print(res.error_msg); return\n",
    "\n",
    "            with fig.batch_update():\n",
    "                cols = res.normalized_plot_data.columns.tolist()\n",
    "                for i in range(50):\n",
    "                    if i < len(cols): fig.data[i].update(x=res.normalized_plot_data.index, y=res.normalized_plot_data[cols[i]], name=cols[i], visible=True)\n",
    "                    else: fig.data[i].visible = False\n",
    "                \n",
    "                fig.data[50].update(x=res.benchmark_series.index, y=res.benchmark_series.values, name=f\"Benchmark ({inputs.benchmark_ticker})\", visible=not res.benchmark_series.empty)\n",
    "                fig.data[51].update(x=res.portfolio_series.index, y=res.portfolio_series.values, visible=True)\n",
    "                fig.layout.shapes = [dict(type=\"line\", x0=res.calc_end_date, y0=0, x1=res.calc_end_date, y1=1, xref='x', yref='paper', line=dict(color=\"grey\", width=2, dash=\"dash\"))]\n",
    "\n",
    "            req_date = inputs.start_date.date()\n",
    "            act_date = res.start_date.date()\n",
    "            if req_date != act_date: print(f\"‚ÑπÔ∏è Info: Start date {req_date} is not a trading day. Snapping forward to {act_date}.\")\n",
    "            \n",
    "            # --- LIQUIDITY AUDIT PRINT ---\n",
    "            if inputs.mode == 'Ranking' and res.debug_data and 'audit_liquidity' in res.debug_data:\n",
    "                audit = res.debug_data['audit_liquidity']\n",
    "                if audit:\n",
    "                    pct_str = f\"{audit.get('percentile_setting', 0)*100:.0f}%\"\n",
    "                    cut_val = audit.get('final_cutoff_usd', 0)\n",
    "                    print(\"-\" * 60)\n",
    "                    print(f\"üîç LIQUIDITY CHECK ({act_date})\")\n",
    "                    print(f\"   Universe Size: {audit.get('total_tickers_available')} tickers\")\n",
    "                    print(f\"   Filtering: Top {pct_str} of Market\")\n",
    "                    print(f\"   Calculated Cutoff: ${cut_val:,.0f} / day\")\n",
    "                    print(f\"   Tickers Remaining: {audit.get('tickers_passed')}\")\n",
    "                    print(\"-\" * 60)\n",
    "            \n",
    "            print(f\"Analysis Period: {act_date} to {res.viz_end_date.date()}.\")\n",
    "            \n",
    "            if inputs.mode == 'Ranking': print(\"Ranked Tickers:\"); pprint.pprint(res.tickers)\n",
    "            else: print(\"Manual Portfolio Tickers:\"); pprint.pprint(res.tickers)\n",
    "            \n",
    "            m = res.perf_metrics\n",
    "            rows = [\n",
    "                {'Metric': 'Group Portfolio Gain', 'Full': m.get('full_p_gain'), 'Calc': m.get('calc_p_gain'), 'Fwd': m.get('fwd_p_gain')},\n",
    "                {'Metric': f'Benchmark ({inputs.benchmark_ticker}) Gain', 'Full': m.get('full_b_gain'), 'Calc': m.get('calc_b_gain'), 'Fwd': m.get('fwd_b_gain')},\n",
    "                {'Metric': '== Gain Delta', 'Full': m.get('full_p_gain',0)-m.get('full_b_gain',0), 'Calc': m.get('calc_p_gain',0)-m.get('calc_b_gain',0), 'Fwd': m.get('fwd_p_gain',0)-m.get('fwd_b_gain',0)},\n",
    "                {'Metric': 'Group Sharpe (ATR)', 'Full': m.get('full_p_sharpe_atr'), 'Calc': m.get('calc_p_sharpe_atr'), 'Fwd': m.get('fwd_p_sharpe_atr')},\n",
    "                {'Metric': f'Benchmark Sharpe (ATR)', 'Full': m.get('full_b_sharpe_atr'), 'Calc': m.get('calc_b_sharpe_atr'), 'Fwd': m.get('fwd_b_sharpe_atr')},\n",
    "                {'Metric': '== Sharpe Delta', 'Full': m.get('full_p_sharpe_atr',0)-m.get('full_b_sharpe_atr',0), 'Calc': m.get('calc_p_sharpe_atr',0)-m.get('calc_b_sharpe_atr',0), 'Fwd': m.get('fwd_p_sharpe_atr',0)-m.get('fwd_b_sharpe_atr',0)}\n",
    "            ]\n",
    "            display(pd.DataFrame(rows).set_index('Metric').style.format(\"{:+.2%}\", na_rep=\"N/A\"))\n",
    "\n",
    "    update_button.on_click(update_plot)\n",
    "    update_plot(None)\n",
    "    display(ui, fig)\n",
    "    return results_container, debug_container\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION F: UTILITIES\n",
    "# ==============================================================================\n",
    "\n",
    "def print_nested(d, indent=0, width=4):\n",
    "    \"\"\"Pretty-print any nested dict/list/tuple combination.\"\"\"\n",
    "    spacing = ' ' * indent\n",
    "    if isinstance(d, dict):\n",
    "        for k, v in d.items():\n",
    "            print(f'{spacing}{k}:')\n",
    "            print_nested(v, indent + width, width)\n",
    "    elif isinstance(d, (list, tuple)):\n",
    "        for item in d:\n",
    "            print_nested(item, indent, width)\n",
    "    else:\n",
    "        print(f'{spacing}{d}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "232740e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 9661318 entries, ('A', Timestamp('1999-11-18 00:00:00')) to ('ZWS', Timestamp('2025-12-02 00:00:00'))\n",
      "Data columns (total 5 columns):\n",
      " #   Column     Dtype  \n",
      "---  ------     -----  \n",
      " 0   Adj Open   float64\n",
      " 1   Adj High   float64\n",
      " 2   Adj Low    float64\n",
      " 3   Adj Close  float64\n",
      " 4   Volume     int64  \n",
      "dtypes: float64(4), int64(1)\n",
      "memory usage: 406.1+ MB\n",
      "df_ohlcv.info():\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Adj Open</th>\n",
       "      <th>Adj High</th>\n",
       "      <th>Adj Low</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ticker</th>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">A</th>\n",
       "      <th>1999-11-18</th>\n",
       "      <td>27.2452</td>\n",
       "      <td>29.9398</td>\n",
       "      <td>23.9518</td>\n",
       "      <td>26.3470</td>\n",
       "      <td>74716411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999-11-19</th>\n",
       "      <td>25.7108</td>\n",
       "      <td>25.7482</td>\n",
       "      <td>23.8396</td>\n",
       "      <td>24.1764</td>\n",
       "      <td>18198352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999-11-22</th>\n",
       "      <td>24.7378</td>\n",
       "      <td>26.3470</td>\n",
       "      <td>23.9893</td>\n",
       "      <td>26.3470</td>\n",
       "      <td>7857766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999-11-23</th>\n",
       "      <td>25.4488</td>\n",
       "      <td>26.1225</td>\n",
       "      <td>23.9518</td>\n",
       "      <td>23.9518</td>\n",
       "      <td>7138321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999-11-24</th>\n",
       "      <td>24.0267</td>\n",
       "      <td>25.1120</td>\n",
       "      <td>23.9518</td>\n",
       "      <td>24.5881</td>\n",
       "      <td>5785609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">ZWS</th>\n",
       "      <th>2025-11-25</th>\n",
       "      <td>47.2900</td>\n",
       "      <td>48.4800</td>\n",
       "      <td>47.1500</td>\n",
       "      <td>48.0200</td>\n",
       "      <td>592800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-11-26</th>\n",
       "      <td>47.5400</td>\n",
       "      <td>48.7000</td>\n",
       "      <td>47.3000</td>\n",
       "      <td>48.1300</td>\n",
       "      <td>1154100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-11-28</th>\n",
       "      <td>48.4600</td>\n",
       "      <td>48.4800</td>\n",
       "      <td>47.7000</td>\n",
       "      <td>47.7000</td>\n",
       "      <td>481400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-12-01</th>\n",
       "      <td>47.1700</td>\n",
       "      <td>48.1800</td>\n",
       "      <td>47.1500</td>\n",
       "      <td>47.7400</td>\n",
       "      <td>608100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-12-02</th>\n",
       "      <td>47.9800</td>\n",
       "      <td>48.3100</td>\n",
       "      <td>47.7050</td>\n",
       "      <td>47.8200</td>\n",
       "      <td>455240</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9661318 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Adj Open  Adj High  Adj Low  Adj Close    Volume\n",
       "Ticker Date                                                        \n",
       "A      1999-11-18   27.2452   29.9398  23.9518    26.3470  74716411\n",
       "       1999-11-19   25.7108   25.7482  23.8396    24.1764  18198352\n",
       "       1999-11-22   24.7378   26.3470  23.9893    26.3470   7857766\n",
       "       1999-11-23   25.4488   26.1225  23.9518    23.9518   7138321\n",
       "       1999-11-24   24.0267   25.1120  23.9518    24.5881   5785609\n",
       "...                     ...       ...      ...        ...       ...\n",
       "ZWS    2025-11-25   47.2900   48.4800  47.1500    48.0200    592800\n",
       "       2025-11-26   47.5400   48.7000  47.3000    48.1300   1154100\n",
       "       2025-11-28   48.4600   48.4800  47.7000    47.7000    481400\n",
       "       2025-12-01   47.1700   48.1800  47.1500    47.7400    608100\n",
       "       2025-12-02   47.9800   48.3100  47.7050    47.8200    455240\n",
       "\n",
       "[9661318 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_path = r'c:\\Users\\ping\\Files_win10\\python\\py311\\stocks\\data\\df_OHLCV_stocks_etfs.parquet'\n",
    "df_ohlcv = pd.read_parquet(data_path, engine='pyarrow')\n",
    "print(f'df_ohlcv.info():\\n{df_ohlcv.info()}')\n",
    "df_ohlcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6720574b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ‚öôÔ∏è Initializing AlphaEngine ---\n",
      "Optimizing data structures...\n",
      "‚úÖ AlphaEngine Ready.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77b7da8c865e48bdaa9f1f0c8c3d248e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(RadioButtons(description='Portfolio Mode:', layout=Layout(width='max-content'), options=('Ranki‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7feb63c47524440b73e4fd324fdbcc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'line': {'width': 2},\n",
       "              'name': 'JPST',\n",
       "              'type': 'scatter',\n",
       "              'uid': '3ddb194c-3f2c-4cf8-bc65-5041ecb01efc',\n",
       "              'visible': True,\n",
       "              'x': array([datetime.datetime(2025, 8, 13, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 14, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 15, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 18, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 19, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 20, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 21, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 22, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 25, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 26, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 27, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 28, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 29, 0, 0),\n",
       "                          datetime.datetime(2025, 9, 2, 0, 0),\n",
       "                          datetime.datetime(2025, 9, 3, 0, 0),\n",
       "                          datetime.datetime(2025, 9, 4, 0, 0)], dtype=object),\n",
       "              'y': array([1.        , 0.99980155, 1.00019645, 1.00019645, 1.0007898 , 1.00098826,\n",
       "                          1.0003949 , 1.0015796 , 1.00138316, 1.00177806, 1.00197451, 1.00197451,\n",
       "                          1.00236941, 1.00254781, 1.00294472, 1.00334163])},\n",
       "             {'line': {'width': 2},\n",
       "              'name': 'VLO',\n",
       "              'type': 'scatter',\n",
       "              'uid': 'd17e0f4d-e227-44c6-9f64-163d0923ccad',\n",
       "              'visible': True,\n",
       "              'x': array([datetime.datetime(2025, 8, 13, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 14, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 15, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 18, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 19, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 20, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 21, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 22, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 25, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 26, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 27, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 28, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 29, 0, 0),\n",
       "                          datetime.datetime(2025, 9, 2, 0, 0),\n",
       "                          datetime.datetime(2025, 9, 3, 0, 0),\n",
       "                          datetime.datetime(2025, 9, 4, 0, 0)], dtype=object),\n",
       "              'y': array([1.        , 0.9975757 , 1.00477468, 1.00484859, 1.01307494, 1.03202584,\n",
       "                          1.03040718, 1.06771769, 1.08049698, 1.07932179, 1.10737858, 1.11024635,\n",
       "                          1.11648447, 1.1331589 , 1.14123743, 1.14131134])},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '872b9827-fc92-42aa-bc20-4e0e250f9679', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': 'd648a03d-357e-41d6-bd61-a567b663d92b', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '90a79038-7d62-4787-bde0-a50a2faf498a', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': 'cb316058-b48a-46a9-a351-2b4c1d436bc5', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '84fad499-9772-4249-96c1-59a45f0073dc', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': 'a48d4a1a-b96f-44a4-a025-b4955c0a8c7b', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '592901fa-59fc-46bd-9fa7-1e2739846b87', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '4c257f7e-d392-4e0e-85a8-4bae23be183b', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': 'd89f42d6-f693-4fbd-8178-1194e378beaf', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': 'acf1c447-b4b7-4d91-8a2e-af2f45e68c27', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': 'f5d07f66-7d90-4acb-92a3-5e2c03a5bf5d', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '8c06c816-3f89-44b1-8b4e-911eb3c331fd', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': 'e808a6a1-5750-43b2-9011-584c34c7411c', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': 'f6fe85f9-8748-46a7-9103-d76595b4cf54', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': 'aefe0f8d-6ace-471f-9ecb-2c6ce16c9fe9', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': 'c46415aa-8eb7-46c5-9434-773b550f9f7a', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': 'c916d0e1-a478-438b-9b5b-aa89286e0eb9', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '8331eb1b-764d-4a62-a5b3-36c04b2c1561', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': 'bcf5fb20-9303-4218-8fd2-cbeecf01c9f9', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '3034adc2-4c54-4be9-8e1b-2f950d8fbe46', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '96347142-ea13-42c9-a20a-c09a6bd99be5', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '21e61f30-0005-4a02-8409-5f2e359140fe', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '439cf2b5-0287-4d03-bc40-8d6d99471df4', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '287dcff0-e0aa-4c12-a153-b9ad4211d887', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '272cf919-f65a-4851-9b9c-77f8ea16a503', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': 'b32f2c0e-f2a0-48e5-82af-307165aa273c', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': 'cf6999de-1757-4505-99a1-7637eba4e37e', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': 'a00b1e4b-837c-45c1-87a4-fd1f34d1307a', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': 'e24081aa-002c-43ba-a087-b5a673e402bc', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': 'ef942aea-b3a2-4098-b494-935c4d7b22d6', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '58cb4fdd-de50-4db4-b85c-02b2c7a8f3f1', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': 'dc40ae89-3b03-40c6-abd3-9f1b309d2c7c', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '3b426677-f0cf-4775-a59d-10e0d0c774aa', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '33b166e8-238f-485e-879a-36b13e2caa83', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '6d0b6ad8-9a32-420a-b502-cb6f94861113', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '8559b37a-22d8-4714-b38d-51e8d286fdee', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '4dca4dff-34cb-4652-af82-7c765085e21b', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '567312a3-944c-4adb-b16d-499654fd6c50', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '923f6b77-a2de-4fca-aa04-a8d513263b4f', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': 'f74732da-4543-4f8e-a2af-07a21b65df99', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '9784a080-bd54-4776-8836-24186250d1e9', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '16994df9-5ad7-46b7-b883-2aaec34b6010', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '6294d246-a845-46a3-a119-fe38de8e418a', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': 'bd761e16-3e94-40db-9c27-2dbc0f2c579c', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': 'b1556a23-cd08-4231-9901-c5817b96d522', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '862fbb81-213c-4770-be6b-363b05b1d772', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': 'a3b42ad1-7d48-47ab-ad5a-dc9f2936176f', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': 'f9f8a2df-b554-4aee-a68a-b90a59e2e5b3', 'visible': False},\n",
       "             {'line': {'color': 'black', 'dash': 'dash', 'width': 3},\n",
       "              'name': 'Benchmark (VOO)',\n",
       "              'type': 'scatter',\n",
       "              'uid': 'ddb67bf8-4931-4284-aa9d-75a5f869006d',\n",
       "              'visible': True,\n",
       "              'x': array([datetime.datetime(2025, 8, 13, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 14, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 15, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 18, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 19, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 20, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 21, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 22, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 25, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 26, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 27, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 28, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 29, 0, 0),\n",
       "                          datetime.datetime(2025, 9, 2, 0, 0),\n",
       "                          datetime.datetime(2025, 9, 3, 0, 0),\n",
       "                          datetime.datetime(2025, 9, 4, 0, 0)], dtype=object),\n",
       "              'y': array([1.        , 1.00001692, 0.99785841, 0.99750486, 0.99205615, 0.9894409 ,\n",
       "                          0.9855451 , 1.00062421, 0.99639177, 1.00037216, 1.00266599, 1.00620825,\n",
       "                          1.00040599, 0.99303391, 0.99811215, 1.00632666])},\n",
       "             {'line': {'color': 'green', 'width': 3},\n",
       "              'name': 'Group Portfolio',\n",
       "              'type': 'scatter',\n",
       "              'uid': '4f0493c0-c379-43d2-b981-dec7b0e1540a',\n",
       "              'visible': True,\n",
       "              'x': array([datetime.datetime(2025, 8, 13, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 14, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 15, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 18, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 19, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 20, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 21, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 22, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 25, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 26, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 27, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 28, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 29, 0, 0),\n",
       "                          datetime.datetime(2025, 9, 2, 0, 0),\n",
       "                          datetime.datetime(2025, 9, 3, 0, 0),\n",
       "                          datetime.datetime(2025, 9, 4, 0, 0)], dtype=object),\n",
       "              'y': array([1.        , 0.99868863, 1.00248556, 1.00252252, 1.00693237, 1.01650705,\n",
       "                          1.01540104, 1.03464865, 1.04094007, 1.04054992, 1.05467654, 1.05611043,\n",
       "                          1.05942694, 1.06785336, 1.07209107, 1.07232648])}],\n",
       "    'layout': {'height': 600,\n",
       "               'hovermode': 'x unified',\n",
       "               'shapes': [{'line': {'color': 'grey', 'dash': 'dash', 'width': 2},\n",
       "                           'type': 'line',\n",
       "                           'x0': Timestamp('2025-08-27 00:00:00'),\n",
       "                           'x1': Timestamp('2025-08-27 00:00:00'),\n",
       "                           'xref': 'x',\n",
       "                           'y0': 0,\n",
       "                           'y1': 1,\n",
       "                           'yref': 'paper'}],\n",
       "               'template': '...',\n",
       "               'title': {'text': 'Walk-Forward Performance Analysis'}}\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_container, debug_container = plot_walk_forward_analyzer(\n",
    "    df_ohlcv=df_ohlcv,\n",
    "    default_start_date='2025-08-13',\n",
    "    default_calc_period=10,\n",
    "    default_fwd_period=5,\n",
    "    default_metric='Sharpe (ATR)',\n",
    "    default_rank_start=15,\n",
    "    default_rank_end=16,\n",
    "    default_benchmark_ticker='VOO', \n",
    "    master_calendar_ticker='VOO',    \n",
    "    quality_thresholds = { \n",
    "        'min_median_dollar_volume': 100_000, # A low \"hard floor\" to filter absolute errors/garbage\n",
    "        # If min_liquidity_percentile is 0.8 (Top 20%), we want values > the 0.8 quantile.            \n",
    "        'min_liquidity_percentile': 0.50,    # Dynamic: Only keep the top 50% of stocks by volume\n",
    "        'max_stale_pct': 0.05, \n",
    "        'max_same_vol_count': 10\n",
    "    },\n",
    "    debug=True  # <-- Activate the new mode!\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d7e2a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = generate_features(df_ohlcv=df_ohlcv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e7267c",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tickers = ['SPY', 'AAPL', 'IWM', 'QQQ', 'META', 'EEM', 'BABA']\n",
    "my_tickers = ['NTES', 'LII',]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4206b269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ NTES exported to: ./export_csv/features_NTES.csv\n",
      "‚úÖ LII exported to: ./export_csv/features_LII.csv\n"
     ]
    }
   ],
   "source": [
    "for ticker in my_tickers:\n",
    "  if ticker in features_df.index.get_level_values('Ticker'):\n",
    "    ticker_features = features_df.loc[ticker]\n",
    "    ticker_features.to_csv(f'./export_csv/features_{ticker}.csv')\n",
    "    print(f\"‚úÖ {ticker} features exported to: ./export_csv/features_{ticker}.csv\")\n",
    "  else:\n",
    "    print(f\"‚ö†Ô∏è {ticker} not found in features_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719425da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ NTES exported to: ./export_csv/ohlcv_NTES.csv\n",
      "‚úÖ LII exported to: ./export_csv/ohlcv_LII.csv\n"
     ]
    }
   ],
   "source": [
    "for ticker in my_tickers:\n",
    "  if ticker in df_ohlcv.index.get_level_values('Ticker'):\n",
    "    ticker_features = df_ohlcv.loc[ticker]\n",
    "    ticker_features.to_csv(f'./export_csv/ohlcv_{ticker}.csv')\n",
    "    print(f\"‚úÖ {ticker} OHLCV exported to: ./export_csv/ohlcv_{ticker}.csv\")\n",
    "  else:\n",
    "    print(f\"‚ö†Ô∏è {ticker} not found in df_ohlcv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1e0c43a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TR</th>\n",
       "      <th>ATR</th>\n",
       "      <th>ATRP</th>\n",
       "      <th>RollingStalePct</th>\n",
       "      <th>RollMedDollarVol</th>\n",
       "      <th>RollingSameVolCount</th>\n",
       "      <th>Calculated_Cutoff</th>\n",
       "      <th>Passed_Vol_Check</th>\n",
       "      <th>Passed_Final</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ticker</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AAPL</th>\n",
       "      <td>1.1061</td>\n",
       "      <td>0.520297</td>\n",
       "      <td>0.017905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.348299e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.720650e+07</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            TR       ATR      ATRP  RollingStalePct  RollMedDollarVol  \\\n",
       "Ticker                                                                  \n",
       "AAPL    1.1061  0.520297  0.017905              0.0      5.348299e+09   \n",
       "\n",
       "        RollingSameVolCount  Calculated_Cutoff  Passed_Vol_Check  Passed_Final  \n",
       "Ticker                                                                          \n",
       "AAPL                    0.0       3.720650e+07              True          True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Assuming you have run the variables setup from the previous step\n",
    "snapshot_df = debug_container[0]['audit_liquidity']['universe_snapshot']\n",
    "\n",
    "if 'AAPL' in snapshot_df.index:\n",
    "    display(snapshot_df.loc[['AAPL']])\n",
    "else:\n",
    "    print(\"AAPL was not present in the data for this date.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32da9680",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Snapshot exported to: ./export_csv/snapshot_df.csv\n",
      "   Shape: (1226, 9)\n",
      "   Columns: ['TR', 'ATR', 'ATRP', 'RollingStalePct', 'RollMedDollarVol', 'RollingSameVolCount', 'Calculated_Cutoff', 'Passed_Vol_Check', 'Passed_Final']\n"
     ]
    }
   ],
   "source": [
    "snapshot_df.to_csv('./export_csv/snapshot_df.csv')\n",
    "print(f\"‚úÖ Snapshot exported to: ./export_csv/snapshot_df.csv\")\n",
    "print(f\"   Shape: {snapshot_df.shape}\")\n",
    "print(f\"   Columns: {list(snapshot_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7f5e1e53",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÖ Date: 2015-04-28\n",
      "üí∞ Calculated Cutoff: $37,206,497\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_df6b0\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_df6b0_level0_col0\" class=\"col_heading level0 col0\" >TR</th>\n",
       "      <th id=\"T_df6b0_level0_col1\" class=\"col_heading level0 col1\" >ATR</th>\n",
       "      <th id=\"T_df6b0_level0_col2\" class=\"col_heading level0 col2\" >ATRP</th>\n",
       "      <th id=\"T_df6b0_level0_col3\" class=\"col_heading level0 col3\" >RollingStalePct</th>\n",
       "      <th id=\"T_df6b0_level0_col4\" class=\"col_heading level0 col4\" >RollMedDollarVol</th>\n",
       "      <th id=\"T_df6b0_level0_col5\" class=\"col_heading level0 col5\" >RollingSameVolCount</th>\n",
       "      <th id=\"T_df6b0_level0_col6\" class=\"col_heading level0 col6\" >Calculated_Cutoff</th>\n",
       "      <th id=\"T_df6b0_level0_col7\" class=\"col_heading level0 col7\" >Passed_Vol_Check</th>\n",
       "      <th id=\"T_df6b0_level0_col8\" class=\"col_heading level0 col8\" >Passed_Final</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Ticker</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "      <th class=\"blank col4\" >&nbsp;</th>\n",
       "      <th class=\"blank col5\" >&nbsp;</th>\n",
       "      <th class=\"blank col6\" >&nbsp;</th>\n",
       "      <th class=\"blank col7\" >&nbsp;</th>\n",
       "      <th class=\"blank col8\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_df6b0_level0_row0\" class=\"row_heading level0 row0\" >NCLH</th>\n",
       "      <td id=\"T_df6b0_row0_col0\" class=\"data row0 col0\" >1.590000</td>\n",
       "      <td id=\"T_df6b0_row0_col1\" class=\"data row0 col1\" >1.195890</td>\n",
       "      <td id=\"T_df6b0_row0_col2\" class=\"data row0 col2\" >0.023837</td>\n",
       "      <td id=\"T_df6b0_row0_col3\" class=\"data row0 col3\" >0.0%</td>\n",
       "      <td id=\"T_df6b0_row0_col4\" class=\"data row0 col4\" >$37,587,368</td>\n",
       "      <td id=\"T_df6b0_row0_col5\" class=\"data row0 col5\" >0.000000</td>\n",
       "      <td id=\"T_df6b0_row0_col6\" class=\"data row0 col6\" >$37,206,497</td>\n",
       "      <td id=\"T_df6b0_row0_col7\" class=\"data row0 col7\" >True</td>\n",
       "      <td id=\"T_df6b0_row0_col8\" class=\"data row0 col8\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_df6b0_level0_row1\" class=\"row_heading level0 row1\" >TER</th>\n",
       "      <td id=\"T_df6b0_row1_col0\" class=\"data row1 col0\" >0.559200</td>\n",
       "      <td id=\"T_df6b0_row1_col1\" class=\"data row1 col1\" >0.396619</td>\n",
       "      <td id=\"T_df6b0_row1_col2\" class=\"data row1 col2\" >0.022918</td>\n",
       "      <td id=\"T_df6b0_row1_col3\" class=\"data row1 col3\" >0.0%</td>\n",
       "      <td id=\"T_df6b0_row1_col4\" class=\"data row1 col4\" >$37,569,939</td>\n",
       "      <td id=\"T_df6b0_row1_col5\" class=\"data row1 col5\" >0.000000</td>\n",
       "      <td id=\"T_df6b0_row1_col6\" class=\"data row1 col6\" >$37,206,497</td>\n",
       "      <td id=\"T_df6b0_row1_col7\" class=\"data row1 col7\" >True</td>\n",
       "      <td id=\"T_df6b0_row1_col8\" class=\"data row1 col8\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_df6b0_level0_row2\" class=\"row_heading level0 row2\" >AER</th>\n",
       "      <td id=\"T_df6b0_row2_col0\" class=\"data row2 col0\" >0.787600</td>\n",
       "      <td id=\"T_df6b0_row2_col1\" class=\"data row2 col1\" >0.750605</td>\n",
       "      <td id=\"T_df6b0_row2_col2\" class=\"data row2 col2\" >0.016253</td>\n",
       "      <td id=\"T_df6b0_row2_col3\" class=\"data row2 col3\" >0.0%</td>\n",
       "      <td id=\"T_df6b0_row2_col4\" class=\"data row2 col4\" >$37,551,067</td>\n",
       "      <td id=\"T_df6b0_row2_col5\" class=\"data row2 col5\" >0.000000</td>\n",
       "      <td id=\"T_df6b0_row2_col6\" class=\"data row2 col6\" >$37,206,497</td>\n",
       "      <td id=\"T_df6b0_row2_col7\" class=\"data row2 col7\" >True</td>\n",
       "      <td id=\"T_df6b0_row2_col8\" class=\"data row2 col8\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_df6b0_level0_row3\" class=\"row_heading level0 row3\" >EVRG</th>\n",
       "      <td id=\"T_df6b0_row3_col0\" class=\"data row3 col0\" >0.421200</td>\n",
       "      <td id=\"T_df6b0_row3_col1\" class=\"data row3 col1\" >0.439924</td>\n",
       "      <td id=\"T_df6b0_row3_col2\" class=\"data row3 col2\" >0.016716</td>\n",
       "      <td id=\"T_df6b0_row3_col3\" class=\"data row3 col3\" >0.0%</td>\n",
       "      <td id=\"T_df6b0_row3_col4\" class=\"data row3 col4\" >$37,542,028</td>\n",
       "      <td id=\"T_df6b0_row3_col5\" class=\"data row3 col5\" >0.000000</td>\n",
       "      <td id=\"T_df6b0_row3_col6\" class=\"data row3 col6\" >$37,206,497</td>\n",
       "      <td id=\"T_df6b0_row3_col7\" class=\"data row3 col7\" >True</td>\n",
       "      <td id=\"T_df6b0_row3_col8\" class=\"data row3 col8\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_df6b0_level0_row4\" class=\"row_heading level0 row4\" >VRSK</th>\n",
       "      <td id=\"T_df6b0_row4_col0\" class=\"data row4 col0\" >0.632100</td>\n",
       "      <td id=\"T_df6b0_row4_col1\" class=\"data row4 col1\" >0.866085</td>\n",
       "      <td id=\"T_df6b0_row4_col2\" class=\"data row4 col2\" >0.012449</td>\n",
       "      <td id=\"T_df6b0_row4_col3\" class=\"data row4 col3\" >0.0%</td>\n",
       "      <td id=\"T_df6b0_row4_col4\" class=\"data row4 col4\" >$37,535,055</td>\n",
       "      <td id=\"T_df6b0_row4_col5\" class=\"data row4 col5\" >0.000000</td>\n",
       "      <td id=\"T_df6b0_row4_col6\" class=\"data row4 col6\" >$37,206,497</td>\n",
       "      <td id=\"T_df6b0_row4_col7\" class=\"data row4 col7\" >True</td>\n",
       "      <td id=\"T_df6b0_row4_col8\" class=\"data row4 col8\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_df6b0_level0_row5\" class=\"row_heading level0 row5\" >NTES</th>\n",
       "      <td id=\"T_df6b0_row5_col0\" class=\"data row5 col0\" >0.475900</td>\n",
       "      <td id=\"T_df6b0_row5_col1\" class=\"data row5 col1\" >0.658619</td>\n",
       "      <td id=\"T_df6b0_row5_col2\" class=\"data row5 col2\" >0.031880</td>\n",
       "      <td id=\"T_df6b0_row5_col3\" class=\"data row5 col3\" >0.0%</td>\n",
       "      <td id=\"T_df6b0_row5_col4\" class=\"data row5 col4\" >$37,206,497</td>\n",
       "      <td id=\"T_df6b0_row5_col5\" class=\"data row5 col5\" >0.000000</td>\n",
       "      <td id=\"T_df6b0_row5_col6\" class=\"data row5 col6\" >$37,206,497</td>\n",
       "      <td id=\"T_df6b0_row5_col7\" class=\"data row5 col7\" >True</td>\n",
       "      <td id=\"T_df6b0_row5_col8\" class=\"data row5 col8\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_df6b0_level0_row6\" class=\"row_heading level0 row6\" >LII</th>\n",
       "      <td id=\"T_df6b0_row6_col0\" class=\"data row6 col0\" >1.659600</td>\n",
       "      <td id=\"T_df6b0_row6_col1\" class=\"data row6 col1\" >1.712063</td>\n",
       "      <td id=\"T_df6b0_row6_col2\" class=\"data row6 col2\" >0.018024</td>\n",
       "      <td id=\"T_df6b0_row6_col3\" class=\"data row6 col3\" >0.0%</td>\n",
       "      <td id=\"T_df6b0_row6_col4\" class=\"data row6 col4\" >$37,107,899</td>\n",
       "      <td id=\"T_df6b0_row6_col5\" class=\"data row6 col5\" >0.000000</td>\n",
       "      <td id=\"T_df6b0_row6_col6\" class=\"data row6 col6\" >$37,206,497</td>\n",
       "      <td id=\"T_df6b0_row6_col7\" class=\"data row6 col7\" >False</td>\n",
       "      <td id=\"T_df6b0_row6_col8\" class=\"data row6 col8\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_df6b0_level0_row7\" class=\"row_heading level0 row7\" >ATI</th>\n",
       "      <td id=\"T_df6b0_row7_col0\" class=\"data row7 col0\" >0.750900</td>\n",
       "      <td id=\"T_df6b0_row7_col1\" class=\"data row7 col1\" >1.041575</td>\n",
       "      <td id=\"T_df6b0_row7_col2\" class=\"data row7 col2\" >0.031199</td>\n",
       "      <td id=\"T_df6b0_row7_col3\" class=\"data row7 col3\" >0.0%</td>\n",
       "      <td id=\"T_df6b0_row7_col4\" class=\"data row7 col4\" >$36,741,036</td>\n",
       "      <td id=\"T_df6b0_row7_col5\" class=\"data row7 col5\" >0.000000</td>\n",
       "      <td id=\"T_df6b0_row7_col6\" class=\"data row7 col6\" >$37,206,497</td>\n",
       "      <td id=\"T_df6b0_row7_col7\" class=\"data row7 col7\" >False</td>\n",
       "      <td id=\"T_df6b0_row7_col8\" class=\"data row7 col8\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_df6b0_level0_row8\" class=\"row_heading level0 row8\" >BMO</th>\n",
       "      <td id=\"T_df6b0_row8_col0\" class=\"data row8 col0\" >0.374300</td>\n",
       "      <td id=\"T_df6b0_row8_col1\" class=\"data row8 col1\" >0.620426</td>\n",
       "      <td id=\"T_df6b0_row8_col2\" class=\"data row8 col2\" >0.014351</td>\n",
       "      <td id=\"T_df6b0_row8_col3\" class=\"data row8 col3\" >0.0%</td>\n",
       "      <td id=\"T_df6b0_row8_col4\" class=\"data row8 col4\" >$36,597,179</td>\n",
       "      <td id=\"T_df6b0_row8_col5\" class=\"data row8 col5\" >0.000000</td>\n",
       "      <td id=\"T_df6b0_row8_col6\" class=\"data row8 col6\" >$37,206,497</td>\n",
       "      <td id=\"T_df6b0_row8_col7\" class=\"data row8 col7\" >False</td>\n",
       "      <td id=\"T_df6b0_row8_col8\" class=\"data row8 col8\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_df6b0_level0_row9\" class=\"row_heading level0 row9\" >MKC</th>\n",
       "      <td id=\"T_df6b0_row9_col0\" class=\"data row9 col0\" >0.312700</td>\n",
       "      <td id=\"T_df6b0_row9_col1\" class=\"data row9 col1\" >0.400849</td>\n",
       "      <td id=\"T_df6b0_row9_col2\" class=\"data row9 col2\" >0.012820</td>\n",
       "      <td id=\"T_df6b0_row9_col3\" class=\"data row9 col3\" >0.0%</td>\n",
       "      <td id=\"T_df6b0_row9_col4\" class=\"data row9 col4\" >$36,228,620</td>\n",
       "      <td id=\"T_df6b0_row9_col5\" class=\"data row9 col5\" >0.000000</td>\n",
       "      <td id=\"T_df6b0_row9_col6\" class=\"data row9 col6\" >$37,206,497</td>\n",
       "      <td id=\"T_df6b0_row9_col7\" class=\"data row9 col7\" >False</td>\n",
       "      <td id=\"T_df6b0_row9_col8\" class=\"data row9 col8\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_df6b0_level0_row10\" class=\"row_heading level0 row10\" >BF-B</th>\n",
       "      <td id=\"T_df6b0_row10_col0\" class=\"data row10 col0\" >0.467800</td>\n",
       "      <td id=\"T_df6b0_row10_col1\" class=\"data row10 col1\" >0.428298</td>\n",
       "      <td id=\"T_df6b0_row10_col2\" class=\"data row10 col2\" >0.013897</td>\n",
       "      <td id=\"T_df6b0_row10_col3\" class=\"data row10 col3\" >0.0%</td>\n",
       "      <td id=\"T_df6b0_row10_col4\" class=\"data row10 col4\" >$36,226,284</td>\n",
       "      <td id=\"T_df6b0_row10_col5\" class=\"data row10 col5\" >0.000000</td>\n",
       "      <td id=\"T_df6b0_row10_col6\" class=\"data row10 col6\" >$37,206,497</td>\n",
       "      <td id=\"T_df6b0_row10_col7\" class=\"data row10 col7\" >False</td>\n",
       "      <td id=\"T_df6b0_row10_col8\" class=\"data row10 col8\" >False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x20884fd5a10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. Access the data inside the container list\n",
    "current_debug_data = debug_container[0]\n",
    "\n",
    "# 2. Check if the audit data exists (it is created only in 'Ranking' mode)\n",
    "if current_debug_data and 'audit_liquidity' in current_debug_data:\n",
    "    audit = current_debug_data['audit_liquidity']\n",
    "    snapshot_df = audit['universe_snapshot']\n",
    "    \n",
    "    print(f\"üìÖ Date: {audit['date'].date()}\")\n",
    "    print(f\"üí∞ Calculated Cutoff: ${audit['final_cutoff_usd']:,.0f}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "# 3. View the tickers right around the cutoff point\n",
    "# Find the index where 'Passed_Vol_Check' switches from True to False\n",
    "    try:\n",
    "        # Get the integer location (iloc) of the last True value\n",
    "        last_pass_iloc = np.where(snapshot_df['Passed_Vol_Check'])[0][-1]\n",
    "        \n",
    "        # Show 5 rows before and 5 rows after the cutoff\n",
    "        start = max(0, last_pass_iloc - 5)\n",
    "        end = min(len(snapshot_df), last_pass_iloc + 6)\n",
    "        \n",
    "        display(snapshot_df.iloc[start:end].style.format({\n",
    "            'RollMedDollarVol': '${:,.0f}',\n",
    "            'Calculated_Cutoff': '${:,.0f}',\n",
    "            'RollingStalePct': '{:.1%}'\n",
    "        }))\n",
    "    except IndexError:\n",
    "        print(\"Could not determine cutoff boundary (maybe all passed or all failed).\")\n",
    "        display(snapshot_df.head())\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No audit data found. Make sure you are in 'Ranking' mode and have clicked 'Update Chart'.\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b6b19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(snapshot_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "551b239f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EngineOutput(portfolio_series=Date\n",
      "2015-04-28    1.000000\n",
      "2015-04-29    1.015713\n",
      "2015-04-30    1.050600\n",
      "2015-05-01    1.077869\n",
      "2015-05-04    1.078302\n",
      "2015-05-05    1.063868\n",
      "2015-05-06    1.073178\n",
      "2015-05-07    1.086883\n",
      "2015-05-08    1.099645\n",
      "2015-05-11    1.099924\n",
      "2015-05-12    1.092934\n",
      "2015-05-13    1.101673\n",
      "2015-05-14    1.113253\n",
      "2015-05-15    1.117366\n",
      "2015-05-18    1.118461\n",
      "2015-05-19    1.119714\n",
      "dtype: float64, benchmark_series=Date\n",
      "2015-04-28    1.000000\n",
      "2015-04-29    0.995925\n",
      "2015-04-30    0.986015\n",
      "2015-05-01    0.996334\n",
      "2015-05-04    0.999430\n",
      "2015-05-05    0.987768\n",
      "2015-05-06    0.983798\n",
      "2015-05-07    0.987768\n",
      "2015-05-08    1.000725\n",
      "2015-05-11    0.995925\n",
      "2015-05-12    0.993032\n",
      "2015-05-13    0.993342\n",
      "2015-05-14    1.003716\n",
      "2015-05-15    1.004744\n",
      "2015-05-18    1.007841\n",
      "2015-05-19    1.007327\n",
      "dtype: float64, normalized_plot_data=Ticker           VMC       MLM      MDLZ       LKQ\n",
      "Date                                              \n",
      "2015-04-28  1.000000  1.000000  1.000000  1.000000\n",
      "2015-04-29  0.999759  1.001462  1.051630  1.009999\n",
      "2015-04-30  1.029246  1.048120  1.042661  1.082371\n",
      "2015-05-01  1.063064  1.086182  1.050272  1.111958\n",
      "2015-05-04  1.060297  1.090296  1.049457  1.113156\n",
      "2015-05-05  1.029004  1.079207  1.041302  1.105959\n",
      "2015-05-06  1.051270  1.095441  1.044837  1.101162\n",
      "2015-05-07  1.077145  1.106387  1.059238  1.104761\n",
      "2015-05-08  1.088819  1.138057  1.071739  1.099964\n",
      "2015-05-11  1.098689  1.129899  1.068748  1.102360\n",
      "2015-05-12  1.085932  1.107641  1.068204  1.109959\n",
      "2015-05-13  1.093633  1.131298  1.068204  1.113557\n",
      "2015-05-14  1.115899  1.136443  1.088313  1.112359\n",
      "2015-05-15  1.107114  1.141212  1.099185  1.121953\n",
      "2015-05-18  1.109400  1.141141  1.086954  1.136348\n",
      "2015-05-19  1.111325  1.133431  1.093751  1.140348, tickers=['LKQ', 'MLM', 'MDLZ', 'VMC'], initial_weights=LKQ     0.25\n",
      "MLM     0.25\n",
      "MDLZ    0.25\n",
      "VMC     0.25\n",
      "dtype: float64, perf_metrics={'full_p_gain': 0.11971351101912475, 'calc_p_gain': 0.09293385494501005, 'fwd_p_gain': 0.024502540526994743, 'full_p_sharpe_atr': 0.39833851694465405, 'calc_p_sharpe_atr': 0.46820493409120156, 'fwd_p_sharpe_atr': 0.2566350817884936, 'full_b_gain': 0.007326801352640411, 'calc_b_gain': -0.0069675837039056, 'fwd_b_gain': 0.014394681202716919, 'full_b_sharpe_atr': 0.05407359003487235, 'calc_b_sharpe_atr': -0.06988985031193463, 'fwd_b_sharpe_atr': 0.3127671531446315}, results_df=        Rank  Metric Value  Fwd Gain\n",
      "Ticker                              \n",
      "LKQ        1      0.524672  0.027379\n",
      "MLM        2      0.466553  0.023283\n",
      "MDLZ       3      0.456371  0.023915\n",
      "VMC        4      0.440323  0.023384, start_date=Timestamp('2015-04-28 00:00:00'), calc_end_date=Timestamp('2015-05-12 00:00:00'), viz_end_date=Timestamp('2015-05-19 00:00:00'), error_msg=None, debug_data={'audit_liquidity': {'date': Timestamp('2015-04-28 00:00:00'), 'total_tickers_available': 1226, 'percentile_setting': 0.5, 'percentile_value_usd': 37206497.0155, 'final_cutoff_usd': 37206497.0155, 'tickers_passed': 605, 'universe_snapshot':              TR       ATR      ATRP  RollingStalePct  RollMedDollarVol  \\\n",
      "Ticker                                                                   \n",
      "SPY     1.81500  1.604518  0.009071              0.0      1.886723e+10   \n",
      "AAPL    1.10610  0.520297  0.017905              0.0      5.348299e+09   \n",
      "IWM     1.84700  1.280112  0.011738              0.0      3.867770e+09   \n",
      "QQQ     1.33100  1.133654  0.011149              0.0      3.066815e+09   \n",
      "META    1.66980  1.605122  0.020018              0.0      2.275138e+09   \n",
      "...         ...       ...       ...              ...               ...   \n",
      "PDBC    0.00463  0.099226  0.010278              NaN               NaN   \n",
      "QRVO    4.86600  3.038847  0.044781              NaN               NaN   \n",
      "QSR     1.97350  1.002236  0.031966              NaN               NaN   \n",
      "SMMT    0.39000  0.684552  0.066526              NaN               NaN   \n",
      "TOTL    0.05790  0.075702  0.002268              NaN               NaN   \n",
      "\n",
      "        RollingSameVolCount  Calculated_Cutoff  Passed_Vol_Check  Passed_Final  \n",
      "Ticker                                                                          \n",
      "SPY                     0.0       3.720650e+07              True          True  \n",
      "AAPL                    0.0       3.720650e+07              True          True  \n",
      "IWM                     0.0       3.720650e+07              True          True  \n",
      "QQQ                     0.0       3.720650e+07              True          True  \n",
      "META                    0.0       3.720650e+07              True          True  \n",
      "...                     ...                ...               ...           ...  \n",
      "PDBC                    NaN       3.720650e+07             False         False  \n",
      "QRVO                    NaN       3.720650e+07             False         False  \n",
      "QSR                     NaN       3.720650e+07             False         False  \n",
      "SMMT                    NaN       3.720650e+07             False         False  \n",
      "TOTL                    NaN       3.720650e+07             False         False  \n",
      "\n",
      "[1226 rows x 9 columns]}, 'portfolio_trace':             Norm_Price_VMC  Norm_Price_MLM  Norm_Price_MDLZ  Norm_Price_LKQ  \\\n",
      "Date                                                                          \n",
      "2015-04-28        1.000000        1.000000         1.000000        1.000000   \n",
      "2015-04-29        0.999759        1.001462         1.051630        1.009999   \n",
      "2015-04-30        1.029246        1.048120         1.042661        1.082371   \n",
      "2015-05-01        1.063064        1.086182         1.050272        1.111958   \n",
      "2015-05-04        1.060297        1.090296         1.049457        1.113156   \n",
      "2015-05-05        1.029004        1.079207         1.041302        1.105959   \n",
      "2015-05-06        1.051270        1.095441         1.044837        1.101162   \n",
      "2015-05-07        1.077145        1.106387         1.059238        1.104761   \n",
      "2015-05-08        1.088819        1.138057         1.071739        1.099964   \n",
      "2015-05-11        1.098689        1.129899         1.068748        1.102360   \n",
      "2015-05-12        1.085932        1.107641         1.068204        1.109959   \n",
      "2015-05-13        1.093633        1.131298         1.068204        1.113557   \n",
      "2015-05-14        1.115899        1.136443         1.088313        1.112359   \n",
      "2015-05-15        1.107114        1.141212         1.099185        1.121953   \n",
      "2015-05-18        1.109400        1.141141         1.086954        1.136348   \n",
      "2015-05-19        1.111325        1.133431         1.093751        1.140348   \n",
      "\n",
      "            Norm_Price_Portfolio  Norm_Price_Benchmark_VOO  \n",
      "Date                                                        \n",
      "2015-04-28              1.000000                  1.000000  \n",
      "2015-04-29              1.015713                  0.995925  \n",
      "2015-04-30              1.050600                  0.986015  \n",
      "2015-05-01              1.077869                  0.996334  \n",
      "2015-05-04              1.078302                  0.999430  \n",
      "2015-05-05              1.063868                  0.987768  \n",
      "2015-05-06              1.073178                  0.983798  \n",
      "2015-05-07              1.086883                  0.987768  \n",
      "2015-05-08              1.099645                  1.000725  \n",
      "2015-05-11              1.099924                  0.995925  \n",
      "2015-05-12              1.092934                  0.993032  \n",
      "2015-05-13              1.101673                  0.993342  \n",
      "2015-05-14              1.113253                  1.003716  \n",
      "2015-05-15              1.117366                  1.004744  \n",
      "2015-05-18              1.118461                  1.007841  \n",
      "2015-05-19              1.119714                  1.007327  })\n"
     ]
    }
   ],
   "source": [
    "print_nested(results_container)\n",
    "# print('='*20)\n",
    "# print_nested(debug_container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d210b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "================================  \n",
    "================================  \n",
    "================================  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05cafb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa5a8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_container, debug_container = plot_walk_forward_analyzer(\n",
    "    df_ohlcv=df_ohlcv,\n",
    "    default_start_date='2025-08-13',\n",
    "    default_calc_period=10,\n",
    "    default_fwd_period=5,\n",
    "    default_metric='Sharpe (ATR)',\n",
    "    default_rank_start=15,\n",
    "    default_rank_end=16,\n",
    "    default_benchmark_ticker='VOO', \n",
    "    master_calendar_ticker='VOO',    \n",
    "    quality_thresholds = { \n",
    "        'min_median_dollar_volume': 100_000, # A low \"hard floor\" to filter absolute errors/garbage\n",
    "        # If min_liquidity_percentile is 0.8 (Top 20%), we want values > the 0.8 quantile.            \n",
    "        'min_liquidity_percentile': 0.50,    # Dynamic: Only keep the top 50% of stocks by volume\n",
    "        'max_stale_pct': 0.05, \n",
    "        'max_same_vol_count': 10\n",
    "    },\n",
    "    debug=True  # <-- Activate the new mode!\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65032321",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4963462",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_container, debug_container = plot_walk_forward_analyzer(\n",
    "    df_ohlcv=df_ohlcv,\n",
    "    default_start_date='2025-08-13',\n",
    "    default_calc_period=10,\n",
    "    default_fwd_period=5,\n",
    "    default_metric='Sharpe (ATR)',\n",
    "    default_rank_start=15,\n",
    "    default_rank_end=16,\n",
    "    default_benchmark_ticker='VOO', \n",
    "    master_calendar_ticker='VOO',\n",
    "    quality_thresholds = {\n",
    "        'min_median_dollar_volume': 100_000, # A low \"hard floor\" to filter absolute errors/garbage\n",
    "        # If min_liquidity_percentile is 0.8 (Top 20%), we want values > the 0.8 quantile.            \n",
    "        'min_liquidity_percentile': 0.50,    # Dynamic: Only keep the top 50% of stocks by volume\n",
    "        'max_stale_pct': 0.05, \n",
    "        'max_same_vol_count': 10\n",
    "    }\n",
    "\n",
    "    debug=True  # <-- Activate the new mode!\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287bfc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_nested(results_container)\n",
    "print('='*20)\n",
    "print_nested(debug_container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1276510d",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = generate_features(\n",
    "    df_ohlcv=df_ohlcv,\n",
    "    atr_period=14,\n",
    "    quality_window=252,\n",
    "    quality_min_periods=123,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a53bcb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d374b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12267f03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969da7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.loc['GBTC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119c3254",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b963c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_container, debug_container = plot_walk_forward_analyzer(\n",
    "    df_ohlcv=df_ohlcv,\n",
    "    default_start_date='2025-08-13',\n",
    "    default_calc_period=10,\n",
    "    default_fwd_period=5,\n",
    "    default_metric='Sharpe (ATR)',\n",
    "    default_rank_start=15,\n",
    "    default_rank_end=16,\n",
    "    default_benchmark_ticker='VOO', \n",
    "    master_calendar_ticker='VOO',\n",
    "    quality_thresholds={ 'min_median_dollar_volume': 10_000_000, \n",
    "                         'max_stale_pct': 0.05, \n",
    "                         'max_same_vol_count': 1 },\n",
    "    debug=True  # <-- Activate the new mode!\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d084c783",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_container, debug_container = plot_walk_forward_analyzer(\n",
    "    df_ohlcv=df_ohlcv,\n",
    "    default_start_date='2025-08-13',\n",
    "    default_calc_period=10,\n",
    "    default_fwd_period=5,\n",
    "    default_metric='Sharpe (ATR)',\n",
    "    default_rank_start=15,\n",
    "    default_rank_end=16,\n",
    "    default_benchmark_ticker='VOO', \n",
    "    master_calendar_ticker='VOO',\n",
    "    quality_thresholds={ 'min_median_dollar_volume': 10_000_000, \n",
    "                         'max_stale_pct': 0.05, \n",
    "                         'max_same_vol_count': 1 },\n",
    "    debug=True  # <-- Activate the new mode!\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5269f794",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_nested(results_container)\n",
    "print('='*20)\n",
    "print_nested(debug_container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa50c7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c5b49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_container, debug_container = plot_walk_forward_analyzer(\n",
    "    df_ohlcv=df_ohlcv,\n",
    "    default_start_date='2025-08-13',\n",
    "    default_calc_period=10,\n",
    "    default_fwd_period=5,\n",
    "    default_metric='Sharpe (ATR)',\n",
    "    default_rank_start=15,\n",
    "    default_rank_end=16,\n",
    "    default_benchmark_ticker='VOO', \n",
    "    master_calendar_ticker='VOO',\n",
    "    quality_thresholds={ 'min_median_dollar_volume': 10_000_000, \n",
    "                         'max_stale_pct': 0.05, \n",
    "                         'max_same_vol_count': 1 },\n",
    "    debug=True  # <-- Activate the new mode!\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf477beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_container, debug_container = plot_walk_forward_analyzer(\n",
    "    df_ohlcv=df_ohlcv,\n",
    "    default_start_date='2025-08-13',\n",
    "    default_calc_period=10,\n",
    "    default_fwd_period=5,\n",
    "    default_metric='Sharpe (ATR)',\n",
    "    default_rank_start=15,\n",
    "    default_rank_end=16,\n",
    "    default_benchmark_ticker='VOO', \n",
    "    master_calendar_ticker='VOO',\n",
    "    quality_thresholds={ 'min_median_dollar_volume': 10_000_000, \n",
    "                         'max_stale_pct': 0.05, \n",
    "                         'max_same_vol_count': 1 },\n",
    "    debug=True  # <-- Activate the new mode!\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d918af",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3e8fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(debug_container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7954c2e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634cea7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_container, debug_container = plot_walk_forward_analyzer(\n",
    "    df_ohlcv=df_ohlcv,\n",
    "    default_start_date='2025-08-13',\n",
    "    default_calc_period=10,\n",
    "    default_fwd_period=5,\n",
    "    default_metric='Sharpe (ATR)',\n",
    "    default_rank_start=15,\n",
    "    default_rank_end=16,\n",
    "    default_benchmark_ticker='VOO', \n",
    "    master_calendar_ticker='VOO',\n",
    "    quality_thresholds={ 'min_median_dollar_volume': 10_000_000, \n",
    "                         'max_stale_pct': 0.05, \n",
    "                         'max_same_vol_count': 1 },\n",
    "    debug=True  # <-- Activate the new mode!\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440818e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_container, debug_container = plot_walk_forward_analyzer(\n",
    "    df_ohlcv=df_ohlcv,\n",
    "    default_start_date='2025-08-13',\n",
    "    default_calc_period=10,\n",
    "    default_fwd_period=5,\n",
    "    default_metric='Sharpe (ATR)',\n",
    "    default_rank_start=15,\n",
    "    default_rank_end=16,\n",
    "    default_benchmark_ticker='VOO', \n",
    "    master_calendar_ticker='VOO',\n",
    "    quality_thresholds={ 'min_median_dollar_volume': 10_000_000, \n",
    "                         'max_stale_pct': 0.05, \n",
    "                         'max_same_vol_count': 1 },\n",
    "    debug=True  # <-- Activate the new mode!\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e75f562",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_container, debug_container = plot_walk_forward_analyzer(\n",
    "    df_ohlcv=df_ohlcv,\n",
    "    default_start_date='2025-08-13',\n",
    "    default_calc_period=10,\n",
    "    default_fwd_period=5,\n",
    "    default_metric='Sharpe (ATR)',\n",
    "    default_rank_start=15,\n",
    "    default_rank_end=16,\n",
    "    default_benchmark_ticker='VOO', \n",
    "    master_calendar_ticker='VOO',\n",
    "    quality_thresholds={ 'min_median_dollar_volume': 10_000_000, \n",
    "                         'max_stale_pct': 0.05, \n",
    "                         'max_same_vol_count': 1 },\n",
    "    debug=True  # <-- Activate the new mode!\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fb1c2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa313b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_container, debug_container = plot_walk_forward_analyzer(\n",
    "    df_ohlcv=df_ohlcv,\n",
    "    default_start_date='2025-08-13',\n",
    "    default_calc_period=10,\n",
    "    default_fwd_period=5,\n",
    "    default_metric='Sharpe (ATR)',\n",
    "    default_rank_start=15,\n",
    "    default_rank_end=16,\n",
    "    default_benchmark_ticker='VOO', \n",
    "    master_calendar_ticker='VOO',\n",
    "    quality_thresholds={ 'min_median_dollar_volume': 10_000_000, \n",
    "                         'max_stale_pct': 0.05, \n",
    "                         'max_same_vol_count': 1 },\n",
    "    debug=True  # <-- Activate the new mode!\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba67970",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_container, debug_container = plot_walk_forward_analyzer(\n",
    "    df_ohlcv=df_ohlcv,\n",
    "    default_start_date='2025-08-13',\n",
    "    default_calc_period=10,\n",
    "    default_fwd_period=5,\n",
    "    default_metric='Sharpe (ATR)',\n",
    "    default_rank_start=15,\n",
    "    default_rank_end=16,\n",
    "    default_benchmark_ticker='VOO', \n",
    "    master_calendar_ticker='VOO',\n",
    "    quality_thresholds={ 'min_median_dollar_volume': 10_000_000, \n",
    "                         'max_stale_pct': 0.05, \n",
    "                         'max_same_vol_count': 1 },\n",
    "    debug=True  # <-- Activate the new mode!\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ba4327",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b362b9f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92af7980",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c2218be",
   "metadata": {},
   "source": [
    "### 1. The Data Contracts (Standardized Inputs/Outputs)\n",
    "\n",
    "First, we define the \"language\" the Engine and UI will speak. This eliminates ambiguity about dictionary keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b4a4ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba9b4b6f",
   "metadata": {},
   "source": [
    "### 2. The Engine (Calculation Logic)\n",
    "\n",
    "This class encapsulates the heavy lifting. It holds the data and executes the math. It replaces the loose functions `run_walk_forward_step` and `run_manual_portfolio_step`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684327bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaEngine:\n",
    "    def __init__(self, df_ohlcv: pd.DataFrame, master_ticker: str = 'SPY'):\n",
    "        \"\"\"\n",
    "        Initialize the engine with data once. \n",
    "        Pre-calculates unstacked dataframes for speed.\n",
    "        \"\"\"\n",
    "        # 1. Generate Features\n",
    "        self.features_df = generate_features(df_ohlcv)\n",
    "        \n",
    "        # 2. Unstack Data for Vectorized Ops\n",
    "        self.df_close = df_ohlcv['Adj Close'].unstack(level=0)\n",
    "        self.df_high = df_ohlcv['Adj High'].unstack(level=0)\n",
    "        self.df_low = df_ohlcv['Adj Low'].unstack(level=0)\n",
    "        \n",
    "        # 3. Setup Calendar\n",
    "        if master_ticker not in self.df_close.columns:\n",
    "            # Fallback to the first column if master ticker missing\n",
    "            master_ticker = self.df_close.columns[0]\n",
    "        self.trading_calendar = self.df_close[master_ticker].dropna().index.unique().sort_values()\n",
    "\n",
    "    def run(self, inputs: EngineInput) -> EngineOutput:\n",
    "        \"\"\"The single entry point for the UI or Bot.\"\"\"\n",
    "        \n",
    "        # 1. Date Validation\n",
    "        try:\n",
    "            start_idx = self.trading_calendar.get_loc(inputs.start_date)\n",
    "        except KeyError:\n",
    "             # Find nearest date if exact match missing\n",
    "            start_idx = self.trading_calendar.searchsorted(inputs.start_date)\n",
    "        \n",
    "        # Check bounds\n",
    "        desired_end_idx = start_idx + inputs.calc_period + inputs.fwd_period\n",
    "        if desired_end_idx >= len(self.trading_calendar):\n",
    "            return EngineOutput(\n",
    "                portfolio_series=pd.Series(), benchmark_series=pd.Series(),\n",
    "                normalized_plot_data=pd.DataFrame(), tickers=[], weights=pd.Series(),\n",
    "                performance_metrics={}, ranking_table=pd.DataFrame(),\n",
    "                calc_end_date=pd.Timestamp.min, viz_end_date=pd.Timestamp.min,\n",
    "                error_msg=\"Date range exceeds available history.\"\n",
    "            )\n",
    "\n",
    "        # 2. Define Dates\n",
    "        safe_start_date = self.trading_calendar[start_idx]\n",
    "        safe_calc_end_date = self.trading_calendar[start_idx + inputs.calc_period]\n",
    "        safe_viz_end_date = self.trading_calendar[start_idx + inputs.calc_period + inputs.fwd_period]\n",
    "\n",
    "        # 3. Select Tickers (Ranking vs Manual)\n",
    "        tickers_to_trade = []\n",
    "        ranking_table = pd.DataFrame()\n",
    "\n",
    "        if inputs.mode == 'Manual':\n",
    "            valid_tickers = [t for t in inputs.manual_tickers if t in self.df_close.columns]\n",
    "            if not valid_tickers:\n",
    "                return self._create_error_output(\"No valid tickers found in manual list.\")\n",
    "            tickers_to_trade = valid_tickers\n",
    "            # Create a dummy table for the UI\n",
    "            ranking_table = pd.DataFrame(index=tickers_to_trade)\n",
    "            ranking_table['Selection'] = 'Manual'\n",
    "            \n",
    "        else: # Ranking Mode\n",
    "            # Reuse existing logic for ranking\n",
    "            # (Simplified for brevity, assumes metric calculation functions exist globally)\n",
    "            calc_slice = self.df_close.loc[safe_start_date:safe_calc_end_date]\n",
    "            \n",
    "            # --- METRIC CALCULATION HOOK ---\n",
    "            # Using the Registry defined in your original code\n",
    "            if inputs.metric not in METRIC_REGISTRY:\n",
    "                return self._create_error_output(f\"Unknown metric: {inputs.metric}\")\n",
    "            \n",
    "            # Prepare metric ingredients\n",
    "            daily_returns = calc_slice.pct_change()\n",
    "            idx_slice = pd.MultiIndex.from_product([calc_slice.columns, calc_slice.index])\n",
    "            feat_slice = self.features_df.loc[self.features_df.index.intersection(idx_slice)]\n",
    "            atrp_mean = feat_slice.groupby(level='Ticker')['ATRP'].mean()\n",
    "            \n",
    "            ingredients = {\n",
    "                'calc_close': calc_slice, \n",
    "                'daily_returns': daily_returns, \n",
    "                'atrp': atrp_mean\n",
    "            }\n",
    "            \n",
    "            metric_series = METRIC_REGISTRY[inputs.metric](ingredients)\n",
    "            sorted_tickers = metric_series.sort_values(ascending=False)\n",
    "            \n",
    "            # Select Top N\n",
    "            start_r = max(0, inputs.rank_start - 1)\n",
    "            end_r = inputs.rank_end\n",
    "            tickers_to_trade = sorted_tickers.iloc[start_r:end_r].index.tolist()\n",
    "            \n",
    "            ranking_table = pd.DataFrame({\n",
    "                'Metric': inputs.metric,\n",
    "                'Value': sorted_tickers.loc[tickers_to_trade]\n",
    "            })\n",
    "\n",
    "        # 4. Calculate Portfolio Performance\n",
    "        # Use the existing helper function\n",
    "        port_val, port_ret, port_atrp = calculate_buy_and_hold_performance(\n",
    "            self.df_close, self.features_df, tickers_to_trade, \n",
    "            safe_start_date, safe_viz_end_date\n",
    "        )\n",
    "\n",
    "        # 5. Calculate Benchmark Performance\n",
    "        bm_val, bm_ret, bm_atrp = calculate_buy_and_hold_performance(\n",
    "            self.df_close, self.features_df, [inputs.benchmark_ticker], \n",
    "            safe_start_date, safe_viz_end_date\n",
    "        )\n",
    "\n",
    "        # 6. Compute Scalar Metrics (The \"Perf Data\")\n",
    "        # Split into calc/fwd periods\n",
    "        actual_calc_end_ts = safe_calc_end_date\n",
    "        \n",
    "        calc_slice_p = port_ret.loc[:actual_calc_end_ts]\n",
    "        fwd_slice_p = port_ret.loc[actual_calc_end_ts:].iloc[1:]\n",
    "        \n",
    "        metrics = {\n",
    "            'calc_p_gain': calculate_gain(port_val.loc[:actual_calc_end_ts]),\n",
    "            'fwd_p_gain': calculate_gain(port_val.loc[actual_calc_end_ts:]),\n",
    "            'calc_p_sharpe': calculate_sharpe(calc_slice_p),\n",
    "            'fwd_p_sharpe': calculate_sharpe(fwd_slice_p),\n",
    "        }\n",
    "        \n",
    "        # Add benchmark comparison if valid\n",
    "        if not bm_val.empty:\n",
    "            metrics['calc_b_gain'] = calculate_gain(bm_val.loc[:actual_calc_end_ts])\n",
    "            metrics['fwd_b_gain'] = calculate_gain(bm_val.loc[actual_calc_end_ts:])\n",
    "            metrics['alpha_gain'] = metrics['fwd_p_gain'] - metrics['fwd_b_gain']\n",
    "\n",
    "        # 7. Prepare Plot Data\n",
    "        plot_data = self.df_close[tickers_to_trade].loc[safe_start_date:safe_viz_end_date]\n",
    "        # Normalize to start at 1.0\n",
    "        plot_data = plot_data / plot_data.bfill().iloc[0]\n",
    "\n",
    "        return EngineOutput(\n",
    "            portfolio_series=port_val,\n",
    "            benchmark_series=bm_val,\n",
    "            normalized_plot_data=plot_data,\n",
    "            tickers=tickers_to_trade,\n",
    "            weights=pd.Series(1/len(tickers_to_trade), index=tickers_to_trade), # Simplified\n",
    "            performance_metrics=metrics,\n",
    "            ranking_table=ranking_table,\n",
    "            calc_end_date=actual_calc_end_ts,\n",
    "            viz_end_date=safe_viz_end_date\n",
    "        )\n",
    "\n",
    "    def _create_error_output(self, msg):\n",
    "        return EngineOutput(\n",
    "            portfolio_series=pd.Series(), benchmark_series=pd.Series(),\n",
    "            normalized_plot_data=pd.DataFrame(), tickers=[], weights=pd.Series(),\n",
    "            performance_metrics={}, ranking_table=pd.DataFrame(),\n",
    "            calc_end_date=pd.Timestamp.min, viz_end_date=pd.Timestamp.min,\n",
    "            error_msg=msg\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bb8fba",
   "metadata": {},
   "source": [
    "### 3. The UI (Rewritten Function)\n",
    "\n",
    "The UI now calls `engine.run()` instead of doing the math itself. This separates the logic completely. The UI is now just a \"View\" layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c30fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_walk_forward_analyzer(df_ohlcv, \n",
    "                               default_start_date='2020-01-01', \n",
    "                               master_ticker='SPY',\n",
    "                               debug=False): # Simplified args for readability\n",
    "    \n",
    "    # 1. Initialize The Engine\n",
    "    print(\"Initializing Alpha Engine...\")\n",
    "    engine = AlphaEngine(df_ohlcv, master_ticker=master_ticker)\n",
    "    \n",
    "    # --- WIDGET DEFINITIONS (Standard) ---\n",
    "    start_date_picker = widgets.DatePicker(description='Start Date:', value=pd.to_datetime(default_start_date))\n",
    "    calc_period_input = widgets.IntText(value=126, description='Calc Days:')\n",
    "    fwd_period_input = widgets.IntText(value=63, description='Fwd Days:')\n",
    "    \n",
    "    metric_dropdown = widgets.Dropdown(options=list(METRIC_REGISTRY.keys()), value='Sharpe (ATR)', description='Metric:')\n",
    "    rank_start_input = widgets.IntText(value=1, description='Rank Start:')\n",
    "    rank_end_input = widgets.IntText(value=10, description='Rank End:')\n",
    "    \n",
    "    benchmark_input = widgets.Text(value='SPY', description='Benchmark:')\n",
    "    mode_selector = widgets.RadioButtons(options=['Ranking', 'Manual'], value='Ranking', description='Mode:')\n",
    "    manual_tickers_input = widgets.Textarea(placeholder='AAPL, MSFT...', description='Manual:')\n",
    "    \n",
    "    update_button = widgets.Button(description=\"Run Analysis\", button_style='primary')\n",
    "    output_area = widgets.Output()\n",
    "    \n",
    "    # Plotly Figure\n",
    "    fig = go.FigureWidget()\n",
    "    fig.layout.height = 600\n",
    "    fig.layout.title = \"Walk-Forward Analysis\"\n",
    "    \n",
    "    # --- UI LOGIC (The Glue) ---\n",
    "    \n",
    "    def on_update_click(b):\n",
    "        output_area.clear_output()\n",
    "        \n",
    "        # 1. Build the Input Contract\n",
    "        # This is where we bridge UI Widgets -> Engine Data\n",
    "        manual_list = [t.strip().upper() for t in manual_tickers_input.value.split(',') if t.strip()]\n",
    "        \n",
    "        inputs = EngineInput(\n",
    "            mode=mode_selector.value,\n",
    "            start_date=pd.to_datetime(start_date_picker.value),\n",
    "            calc_period=calc_period_input.value,\n",
    "            fwd_period=fwd_period_input.value,\n",
    "            benchmark_ticker=benchmark_input.value.strip().upper(),\n",
    "            metric=metric_dropdown.value,\n",
    "            rank_start=rank_start_input.value,\n",
    "            rank_end=rank_end_input.value,\n",
    "            manual_tickers=manual_list\n",
    "        )\n",
    "        \n",
    "        with output_area:\n",
    "            # 2. Call the Engine\n",
    "            print(\"Running calculation...\")\n",
    "            result = engine.run(inputs)\n",
    "            \n",
    "            # 3. Handle Errors\n",
    "            if result.error_msg:\n",
    "                print(f\"‚ùå Error: {result.error_msg}\")\n",
    "                return\n",
    "\n",
    "            # 4. Update Visuals (Plotly)\n",
    "            fig.data = [] # Clear traces\n",
    "            \n",
    "            # Plot Components\n",
    "            for ticker in result.tickers:\n",
    "                if ticker in result.normalized_plot_data:\n",
    "                    series = result.normalized_plot_data[ticker]\n",
    "                    fig.add_trace(go.Scatter(x=series.index, y=series.values, name=ticker, opacity=0.3))\n",
    "            \n",
    "            # Plot Portfolio & Benchmark\n",
    "            fig.add_trace(go.Scatter(x=result.portfolio_series.index, y=result.portfolio_series.values, \n",
    "                                     name='Portfolio', line=dict(color='green', width=3)))\n",
    "            \n",
    "            if not result.benchmark_series.empty:\n",
    "                fig.add_trace(go.Scatter(x=result.benchmark_series.index, y=result.benchmark_series.values, \n",
    "                                         name='Benchmark', line=dict(color='black', dash='dash')))\n",
    "\n",
    "            # Add Vertical Line for Calc/Fwd split\n",
    "            fig.layout.shapes = []\n",
    "            fig.add_vline(x=result.calc_end_date.timestamp() * 1000, line_dash=\"dash\", line_color=\"gray\")\n",
    "\n",
    "            # 5. Update Text Reports\n",
    "            print(f\"Analysis Period: {inputs.start_date.date()} -> {result.viz_end_date.date()}\")\n",
    "            print(\"\\n--- Performance Metrics ---\")\n",
    "            \n",
    "            # Format simple table\n",
    "            metrics_df = pd.DataFrame([\n",
    "                {'Metric': 'Calculated Gain', 'Value': result.performance_metrics.get('calc_p_gain', 0)},\n",
    "                {'Metric': 'Forward Gain', 'Value': result.performance_metrics.get('fwd_p_gain', 0)},\n",
    "                {'Metric': 'Forward Alpha', 'Value': result.performance_metrics.get('alpha_gain', 0)},\n",
    "                {'Metric': 'Fwd Sharpe', 'Value': result.performance_metrics.get('fwd_p_sharpe', 0)},\n",
    "            ])\n",
    "            display(metrics_df.style.format({'Value': '{:.2%}'}))\n",
    "            \n",
    "            print(\"\\n--- Portfolio Composition ---\")\n",
    "            display(result.ranking_table.head(15))\n",
    "\n",
    "    update_button.on_click(on_update_click)\n",
    "    \n",
    "    # --- LAYOUT ---\n",
    "    controls = widgets.VBox([\n",
    "        widgets.HBox([mode_selector, start_date_picker]),\n",
    "        widgets.HBox([calc_period_input, fwd_period_input]),\n",
    "        widgets.HBox([metric_dropdown, rank_start_input, rank_end_input]),\n",
    "        widgets.HBox([manual_tickers_input]),\n",
    "        widgets.HBox([benchmark_input, update_button])\n",
    "    ])\n",
    "    \n",
    "    display(controls, output_area, fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6083e240",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079f3a97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f500c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be96a818",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c92c3cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd892c11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ae617f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d277651",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c982c2cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f6bb5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66c0740",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f866eef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99d5b8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c330ac7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# GOLDEN COPY - Manual Ticker List for Group Portfolio Sharpe (ATR)\n",
    "\n",
    "# Date: 2025-10-19\n",
    "# Version: Verified Manual Ticker List with duplicate tickers (i.e.RCL, RCL, STIP)\n",
    "#  for Group Portfolio Sharpe (ATR) and Benchmark Sharpe (ATR)  \n",
    "\n",
    "# Date: 2025-10-17\n",
    "# Version: Verified Buy-and-Hold Portfolio and Benchmark Sharpe (ATR)  \n",
    "\n",
    "# ==============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import ipywidgets as widgets\n",
    "import time\n",
    "import pprint\n",
    "import os # Make sure os is imported for the export function later\n",
    "import re\n",
    "\n",
    "from datetime import datetime, date\n",
    "from IPython.display import display, Markdown\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "from itertools import product\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "pd.set_option('display.max_rows', 30)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.width', 3000)\n",
    "\n",
    "\n",
    "# --- REFACTORING PHASE 1 CODE: Feature Generation Engine ---\n",
    "\n",
    "def generate_features(df_ohlcv: pd.DataFrame, \n",
    "                      atr_period: int = 14, \n",
    "                      quality_window: int = 252, \n",
    "                      quality_min_periods: int = 126) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generates a comprehensive DataFrame of derived features from raw OHLCV data.\n",
    "\n",
    "    This function performs all heavy, window-based calculations upfront to be used\n",
    "    by downstream analysis functions. It calculates:\n",
    "    1. Technical Indicators: True Range (TR), ATR, and ATRP.\n",
    "    2. Data Quality Metrics: Rolling stale percentage, median dollar volume, etc.\n",
    "\n",
    "    Args:\n",
    "        df_ohlcv: The primary DataFrame with a (Ticker, Date) MultiIndex and\n",
    "                  columns for 'Adj High', 'Adj Low', 'Adj Close', 'Volume'.\n",
    "        atr_period: The lookback period for the ATR's Exponential Moving Average.\n",
    "        quality_window: The rolling window size for data quality metrics.\n",
    "        quality_min_periods: The minimum number of observations required to have\n",
    "                             a valid quality metric.\n",
    "\n",
    "    Returns:\n",
    "        A new DataFrame with the same (Ticker, Date) MultiIndex containing all\n",
    "        calculated feature columns.\n",
    "    \"\"\"\n",
    "    print(\"--- Starting Feature Generation ---\")\n",
    "    \n",
    "    # Ensure the DataFrame is sorted for correct window and shift operations\n",
    "    # FIX: Replaced is_lexsorted() with the current pandas attribute\n",
    "    if not df_ohlcv.index.is_monotonic_increasing:\n",
    "        print(\"Sorting index for calculation accuracy...\")\n",
    "        df_ohlcv = df_ohlcv.sort_index()\n",
    "\n",
    "    # --- 1. Technical Indicator Calculation (TR, ATR, ATRP) ---\n",
    "    print(f\"Calculating technical indicators (ATR Period: {atr_period})...\")\n",
    "    \n",
    "    # Group by ticker to handle each security independently\n",
    "    grouped = df_ohlcv.groupby(level='Ticker')\n",
    "    \n",
    "    # Get the previous day's close required for True Range\n",
    "    prev_close = grouped['Adj Close'].shift(1)\n",
    "    \n",
    "    # Calculate the three components of True Range\n",
    "    high_low = df_ohlcv['Adj High'] - df_ohlcv['Adj Low']\n",
    "    high_prev_close = abs(df_ohlcv['Adj High'] - prev_close)\n",
    "    low_prev_close = abs(df_ohlcv['Adj Low'] - prev_close)\n",
    "    \n",
    "    # Combine the components to get the final TR\n",
    "    tr = pd.concat([high_low, high_prev_close, low_prev_close], axis=1).max(axis=1, skipna=False)\n",
    "    \n",
    "    # # Calculate the ATR using an Exponential Moving Average\n",
    "    # --- FIX IS HERE ---\n",
    "    # Use .transform() to apply the EWM function. \n",
    "    # This guarantees the resulting Series has the exact same index as 'tr',\n",
    "    # preventing the index alignment error during the subsequent division.\n",
    "    atr = tr.groupby(level='Ticker').transform(\n",
    "        lambda x: x.ewm(alpha=1/atr_period, adjust=False).mean()\n",
    "    )\n",
    "\n",
    "    # --- CHANGE 1: Removed .fillna(0) ---\n",
    "    # ATRP will now be NaN on the first day, consistent with TR and ATR.\n",
    "    atrp = (atr / df_ohlcv['Adj Close']).replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    indicator_df = pd.DataFrame({\n",
    "        'TR': tr,\n",
    "        'ATR': atr,\n",
    "        'ATRP': atrp\n",
    "    })\n",
    "    \n",
    "    # --- 2. Data Quality Metric Calculation ---\n",
    "    print(f\"Calculating data quality metrics (Window: {quality_window} days)...\")\n",
    "    \n",
    "    # Create intermediate flags needed for quality calculations\n",
    "    is_stale = np.where((df_ohlcv['Volume'] == 0) | (df_ohlcv['Adj High'] == df_ohlcv['Adj Low']), 1, 0)\n",
    "    dollar_volume = df_ohlcv['Adj Close'] * df_ohlcv['Volume']\n",
    "    has_same_volume = (grouped['Volume'].diff() == 0).astype(int)\n",
    "    \n",
    "    # Combine flags into a temporary DataFrame for rolling calculations\n",
    "    quality_temp_df = pd.DataFrame({\n",
    "        'IsStale': is_stale,\n",
    "        'DollarVolume': dollar_volume,\n",
    "        'HasSameVolume': has_same_volume\n",
    "    }, index=df_ohlcv.index) # Explicitly set index to be safe\n",
    "    \n",
    "    # Perform the rolling calculations on the grouped data\n",
    "    # --- FIX IS HERE ---\n",
    "    # We switch to the older, more compatible dictionary-based aggregation method.\n",
    "    # This syntax is understood by nearly all versions of pandas.\n",
    "    rolling_result = quality_temp_df.groupby(level='Ticker').rolling(\n",
    "        window=quality_window,\n",
    "        min_periods=quality_min_periods\n",
    "    ).agg({\n",
    "        'IsStale': 'mean',\n",
    "        'DollarVolume': 'median',\n",
    "        'HasSameVolume': 'sum'\n",
    "    })\n",
    "    \n",
    "    # The dictionary syntax produces columns with the original names ('IsStale', etc.).\n",
    "    # We now explicitly rename them to our desired final names.\n",
    "    rolling_result = rolling_result.rename(columns={\n",
    "        'IsStale': 'RollingStalePct',\n",
    "        'DollarVolume': 'RollMedDollarVol', # <-- RENAMED HERE\n",
    "        'HasSameVolume': 'RollingSameVolCount'\n",
    "    })\n",
    "\n",
    "    # The index after a grouped rolling operation is hierarchical.\n",
    "    # We remove the outermost 'Ticker' level to restore the original index structure.\n",
    "    rolling_quality = rolling_result.reset_index(level=0, drop=True)\n",
    "\n",
    "    # --- 3. Combine All Features ---\n",
    "    print(\"Combining all feature sets...\")\n",
    "    features_df = pd.concat([indicator_df, rolling_quality], axis=1)\n",
    "    \n",
    "    print(\"‚úÖ Feature generation complete.\")\n",
    "    return features_df\n",
    "\n",
    "\n",
    "def test_features_df(features_df: pd.DataFrame, \n",
    "                     df_ohlcv: pd.DataFrame, \n",
    "                     test_ticker: str = 'AAPL',\n",
    "                     spot_check_date: str = '2020-03-20'):\n",
    "    \"\"\"\n",
    "    Runs a suite of tests to verify the correctness of the generated features_df.\n",
    "\n",
    "    Args:\n",
    "        features_df: The generated DataFrame from the generate_features function.\n",
    "        df_ohlcv: The original source OHLCV DataFrame.\n",
    "        test_ticker: A common, liquid ticker to use for specific value checks.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Running Verification Suite for features_df (Test Ticker: {test_ticker}) ---\")\n",
    "    \n",
    "    # --- Test 1: Structural Integrity ---\n",
    "    print(\"\\n[Test 1: Structural Integrity]\")\n",
    "    assert features_df.index.equals(df_ohlcv.index), \"FAIL: Index does not match original df_ohlcv.\"\n",
    "    print(\"  ‚úÖ PASS: Index matches original df_ohlcv.\")\n",
    "    \n",
    "    expected_cols = ['TR', 'ATR', 'ATRP', 'RollingStalePct', 'RollMedDollarVol', 'RollingSameVolCount']\n",
    "    assert all(col in features_df.columns for col in expected_cols), \"FAIL: Missing one or more expected columns.\"\n",
    "    print(\"  ‚úÖ PASS: All expected feature columns are present.\")\n",
    "    print(f\"  - DataFrame Info:\")\n",
    "    features_df.info(verbose=False, memory_usage='deep')\n",
    "\n",
    "\n",
    "    # --- Test 2: ATR Calculation Logic ---\n",
    "    print(\"\\n[Test 2: ATR Logic Verification]\")\n",
    "    ticker_features = features_df.loc[test_ticker]\n",
    "    \n",
    "    # Test 2a: First TR value should be NaN (since prev_close is NaN)\n",
    "    first_tr = ticker_features['TR'].iloc[0]\n",
    "    assert pd.isna(first_tr), f\"FAIL: First TR value for {test_ticker} should be NaN, but got {first_tr}.\"\n",
    "    print(f\"  ‚úÖ PASS: First TR value for {test_ticker} is NaN as expected.\")\n",
    "\n",
    "    # Test 2b: The first valid ATR should equal the first valid TR (EWM cold start behavior)\n",
    "    first_valid_tr_val = ticker_features['TR'].dropna().iloc[0]\n",
    "    first_valid_atr_val = ticker_features['ATR'].dropna().iloc[0]\n",
    "    assert np.isclose(first_valid_tr_val, first_valid_atr_val), \\\n",
    "        f\"FAIL: First valid ATR ({first_valid_atr_val}) should equal first valid TR ({first_valid_tr_val}).\"\n",
    "    print(\"  ‚úÖ PASS: First valid ATR correctly seeded with first valid TR.\")\n",
    "\n",
    "\n",
    "    # --- Test 3: Rolling Quality Metrics Logic ---\n",
    "    print(\"\\n[Test 3: Rolling Quality Metrics Logic Verification]\")\n",
    "    quality_min_periods = 126 # Should match the parameter used in generation\n",
    "    \n",
    "    # Test 3a: Check for leading NaNs\n",
    "    first_valid_quality_idx = ticker_features['RollingStalePct'].first_valid_index()\n",
    "    if first_valid_quality_idx is None:\n",
    "        print(f\"  - INFO: No valid quality metrics found for {test_ticker} (likely too little data). Skipping test.\")\n",
    "    else:\n",
    "        position_of_first_valid = ticker_features.index.get_loc(first_valid_quality_idx)\n",
    "        assert position_of_first_valid == quality_min_periods - 1, \\\n",
    "            f\"FAIL: First valid quality metric should appear at index {quality_min_periods - 1}, but appeared at {position_of_first_valid}.\"\n",
    "        print(f\"  ‚úÖ PASS: Leading NaNs are present for the first {quality_min_periods - 1} periods as expected.\")\n",
    "\n",
    "\n",
    "    # --- Test 4: Spot Check Against Manual Calculation ---\n",
    "    print(\"\\n[Test 4: Spot Check vs. Manual Calculation]\")\n",
    "    # # Choose a specific date for a manual calculation\n",
    "    # spot_check_date = '2020-03-20' # A volatile day for a good test\n",
    "    \n",
    "    # Manual TR Calculation\n",
    "    today_data = df_ohlcv.loc[(test_ticker, spot_check_date)]\n",
    "    yesterday_data = df_ohlcv.loc[(test_ticker, pd.to_datetime(spot_check_date) - pd.Timedelta(days=1))] # simple lookback for test\n",
    "    \n",
    "    manual_h_l = today_data['Adj High'] - today_data['Adj Low']\n",
    "    manual_h_pc = abs(today_data['Adj High'] - yesterday_data['Adj Close'])\n",
    "    manual_l_pc = abs(today_data['Adj Low'] - yesterday_data['Adj Close'])\n",
    "    manual_tr = max(manual_h_l, manual_h_pc, manual_l_pc)\n",
    "    \n",
    "    code_tr = ticker_features.loc[spot_check_date]['TR']\n",
    "    \n",
    "    assert np.isclose(manual_tr, code_tr), f\"FAIL: Manual TR ({manual_tr:.4f}) does not match code TR ({code_tr:.4f}) on {spot_check_date}.\"\n",
    "    print(f\"  ‚úÖ PASS: Manually calculated TR on {spot_check_date} matches code's TR.\")\n",
    "    \n",
    "    print(\"\\n--- ‚úÖ All Verification Tests Passed ---\")\n",
    "\n",
    "\n",
    "def export_ticker_data(ticker_to_export: str, \n",
    "                         df_ohlcv: pd.DataFrame, \n",
    "                         features_df: pd.DataFrame, \n",
    "                         output_dir: str = 'export_csv'):\n",
    "    \"\"\"\n",
    "    Exports the raw OHLCV data and the corresponding calculated features for a \n",
    "    single ticker to two separate CSV files.\n",
    "\n",
    "    This function is designed for easy manual verification of data and calculations.\n",
    "    It will create the output directory if it does not exist.\n",
    "\n",
    "    Args:\n",
    "        ticker_to_export: The ticker symbol to export (e.g., 'AAPL').\n",
    "        df_ohlcv: The main DataFrame containing the raw OHLCV data with a \n",
    "                  (Ticker, Date) MultiIndex.\n",
    "        features_df: The DataFrame containing the calculated features with a \n",
    "                     (Ticker, Date) MultiIndex.\n",
    "        output_dir: The directory where the CSV files will be saved. \n",
    "                    Defaults to 'export_csv'.\n",
    "    \"\"\"\n",
    "    print(f\"--- Attempting to export data for ticker: {ticker_to_export} ---\")\n",
    "    \n",
    "    # --- 1. Ensure the output directory exists ---\n",
    "    try:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        print(f\"Output directory '{output_dir}' is ready.\")\n",
    "    except OSError as e:\n",
    "        print(f\"Error: Could not create directory '{output_dir}'. Reason: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- 2. Isolate the data for the specified ticker ---\n",
    "    try:\n",
    "        # Use .loc to select all rows for the given ticker from the MultiIndex\n",
    "        ticker_ohlcv = df_ohlcv.loc[ticker_to_export]\n",
    "        ticker_features = features_df.loc[ticker_to_export]\n",
    "        \n",
    "        if ticker_ohlcv.empty:\n",
    "            print(f\"Warning: No OHLCV data found for ticker '{ticker_to_export}'. Cannot export.\")\n",
    "            return\n",
    "            \n",
    "        print(f\"Found {len(ticker_ohlcv)} rows of data for '{ticker_to_export}'.\")\n",
    "        \n",
    "    except KeyError:\n",
    "        print(f\"Error: Ticker '{ticker_to_export}' not found in one or both of the DataFrames. Please check the symbol.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while accessing data: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- 3. Construct file paths and export to CSV ---\n",
    "    try:\n",
    "        # Define the full path for each output file\n",
    "        ohlcv_filename = f\"{ticker_to_export}_ohlcv.csv\"\n",
    "        features_filename = f\"{ticker_to_export}_features.csv\"\n",
    "        \n",
    "        ohlcv_filepath = os.path.join(output_dir, ohlcv_filename)\n",
    "        features_filepath = os.path.join(output_dir, features_filename)\n",
    "        \n",
    "        # Export the DataFrames to CSV. The index (Date) will be included.\n",
    "        ticker_ohlcv.to_csv(ohlcv_filepath)\n",
    "        ticker_features.to_csv(features_filepath)\n",
    "        \n",
    "        print(\"\\n‚úÖ Export successful!\")\n",
    "        print(f\"   - Raw OHLCV data saved to: {ohlcv_filepath}\")\n",
    "        print(f\"   - Calculated features saved to: {features_filepath}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: Failed to write data to CSV files. Reason: {e}\")\n",
    "\n",
    "\n",
    "def create_synthetic_ticker_data(\n",
    "    ticker_name: str = 'SYNTH', \n",
    "    num_days: int = 50,\n",
    "    num_zero_volume_days: int = 5,\n",
    "    num_flat_price_days: int = 3\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Creates a synthetic OHLCV DataFrame with predictable patterns and randomly injected\n",
    "    stale data conditions for robust testing.\n",
    "\n",
    "    Args:\n",
    "        ticker_name: The name for the synthetic ticker.\n",
    "        num_days: The total number of days for the ticker's history.\n",
    "        num_zero_volume_days: The number of random days to set Volume to 0.\n",
    "        num_flat_price_days: The number of random days to set High == Low.\n",
    "\n",
    "    Returns:\n",
    "        A pandas DataFrame with a (Ticker, Date) MultiIndex.\n",
    "    \"\"\"\n",
    "    print(f\"--- Creating synthetic data for '{ticker_name}' with {num_days} days ---\")\n",
    "    \n",
    "    # 1. Create a base DataFrame with \"normal\" data\n",
    "    dates = pd.to_datetime(pd.date_range(start='2023-01-01', periods=num_days, freq='B'))\n",
    "    data = {\n",
    "        'Adj Open': 100.0, 'Adj High': 102.0, 'Adj Low': 98.0,\n",
    "        'Adj Close': 100.0, 'Volume': 1_000_000\n",
    "    }\n",
    "    df = pd.DataFrame(data, index=dates)\n",
    "    df['Adj Close'] = df['Adj Close'] + np.random.randn(num_days) * 0.5 # Add some noise\n",
    "\n",
    "    # 2. Define a \"protected\" window for specific verification tests.\n",
    "    # The `verify_synthetic_ticker_features` function depends on this exact window.\n",
    "    # We will not inject random stale days here.\n",
    "    protected_start_idx, protected_end_idx = 10, 20\n",
    "    \n",
    "    # 3. Inject random \"stale\" days OUTSIDE the protected window\n",
    "    available_indices = df.index.drop(df.index[protected_start_idx:protected_end_idx])\n",
    "    \n",
    "    # Inject zero-volume days\n",
    "    if num_zero_volume_days > 0:\n",
    "        if len(available_indices) < num_zero_volume_days:\n",
    "            raise ValueError(\"Not enough available days to inject zero-volume days.\")\n",
    "        zero_vol_dates = np.random.choice(available_indices, num_zero_volume_days, replace=False)\n",
    "        df.loc[zero_vol_dates, 'Volume'] = 0\n",
    "        print(f\"  - Injected {num_zero_volume_days} random zero-volume 'stale' days.\")\n",
    "        # Update available indices to avoid overlap\n",
    "        available_indices = available_indices.drop(zero_vol_dates)\n",
    "\n",
    "    # Inject flat-price days (High == Low)\n",
    "    if num_flat_price_days > 0:\n",
    "        if len(available_indices) < num_flat_price_days:\n",
    "            raise ValueError(\"Not enough available days to inject flat-price days.\")\n",
    "        flat_price_dates = np.random.choice(available_indices, num_flat_price_days, replace=False)\n",
    "        # Set High and Low to be the same as the Close price for that day\n",
    "        df.loc[flat_price_dates, 'Adj High'] = df.loc[flat_price_dates, 'Adj Close']\n",
    "        df.loc[flat_price_dates, 'Adj Low'] = df.loc[flat_price_dates, 'Adj Close']\n",
    "        print(f\"  - Injected {num_flat_price_days} random flat-price 'stale' days.\")\n",
    "\n",
    "    # 4. Inject the specific, hand-crafted patterns inside the protected window for verification\n",
    "    print(\"  - Injecting specific patterns for programmatic verification...\")\n",
    "    # Pattern for RollingStalePct: 2 stale days in 10 (20%)\n",
    "    df.iloc[10, df.columns.get_loc('Volume')] = 0  # Stale day (zero volume)\n",
    "    df.iloc[11, df.columns.get_loc('Adj High')] = 99.0 # Stale day (High == Low)\n",
    "    df.iloc[11, df.columns.get_loc('Adj Low')] = 99.0\n",
    "    \n",
    "    # Pattern for RollingMedianVolume\n",
    "    for i in range(10):\n",
    "        df.iloc[10 + i, df.columns.get_loc('Adj Close')] = 100.0 # Standardize price for easy median calc\n",
    "        df.iloc[10 + i, df.columns.get_loc('Volume')] = (i + 1) * 10000\n",
    "\n",
    "    # Pattern for RollingSameVolCount\n",
    "    df.iloc[15, df.columns.get_loc('Volume')] = 77777\n",
    "    df.iloc[16, df.columns.get_loc('Volume')] = 77777\n",
    "    df.iloc[17, df.columns.get_loc('Volume')] = 77777\n",
    "    \n",
    "    # 5. Set the MultiIndex\n",
    "    df['Ticker'] = ticker_name\n",
    "    df = df.set_index(['Ticker', df.index])\n",
    "    df.index.names = ['Ticker', 'Date']\n",
    "    \n",
    "    print(\"‚úÖ Synthetic data created successfully.\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def verify_synthetic_ticker_features(features_df: pd.DataFrame, \n",
    "                                       ticker_name: str = 'SYNTH',\n",
    "                                       quality_window: int = 10):\n",
    "    \"\"\"\n",
    "    Verifies the quality metric calculations on the features_df generated from\n",
    "    the synthetic ticker data.\n",
    "\n",
    "    Args:\n",
    "        features_df: The DataFrame of calculated features.\n",
    "        ticker_name: The name of the synthetic ticker.\n",
    "        quality_window: The rolling window used, which must match the window\n",
    "                        of the synthetic data pattern.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Running Verification on Synthetic Ticker '{ticker_name}' ---\")\n",
    "    \n",
    "    # --- Expected values based on our synthetic data design ---\n",
    "    EXPECTED_STALE_PCT = 0.20  # 2 stale days out of 10\n",
    "    EXPECTED_MEDIAN_DOLLAR_VOL = 5_500_000.0 # median of (10k..100k) * price of 100\n",
    "    EXPECTED_SAME_VOL_COUNT = 2.0 # Three consecutive days gives two 'diff() == 0' events\n",
    "\n",
    "    try:\n",
    "        # Isolate the features for our synthetic ticker\n",
    "        ticker_features = features_df.loc[ticker_name]\n",
    "        \n",
    "        # The first valid calculation will be on the last day of our 10-day window.\n",
    "        # The window starts at index 10 and has a length of 10, so it ends at index 19.\n",
    "        verification_date = ticker_features.index[19]\n",
    "        \n",
    "        print(f\"Verifying calculations on date: {verification_date.date()}\")\n",
    "        \n",
    "        # Get the calculated values from the DataFrame\n",
    "        calculated_values = ticker_features.loc[verification_date]\n",
    "        stale_pct = calculated_values['RollingStalePct']\n",
    "        # --- CHANGE 2 (continued): Accessing the renamed column ---\n",
    "        median_vol = calculated_values['RollMedDollarVol'] \n",
    "        same_vol_count = calculated_values['RollingSameVolCount']\n",
    "        \n",
    "        # --- Perform Assertions ---\n",
    "        print(\"\\n[Test 1: RollingStalePct]\")\n",
    "        assert np.isclose(stale_pct, EXPECTED_STALE_PCT), f\"FAIL: Expected {EXPECTED_STALE_PCT}, Got {stale_pct}\"\n",
    "        print(f\"  ‚úÖ PASS: Expected {EXPECTED_STALE_PCT}, Got {stale_pct}\")\n",
    "\n",
    "        print(\"\\n[Test 2: RollMedDollarVol]\")\n",
    "        assert np.isclose(median_vol, EXPECTED_MEDIAN_DOLLAR_VOL), f\"FAIL: Expected {EXPECTED_MEDIAN_DOLLAR_VOL}, Got {median_vol}\"\n",
    "        print(f\"  ‚úÖ PASS: Expected {EXPECTED_MEDIAN_DOLLAR_VOL}, Got {median_vol}\")\n",
    "\n",
    "        print(\"\\n[Test 3: RollingSameVolCount]\")\n",
    "        assert np.isclose(same_vol_count, EXPECTED_SAME_VOL_COUNT), f\"FAIL: Expected {EXPECTED_SAME_VOL_COUNT}, Got {same_vol_count}\"\n",
    "        print(f\"  ‚úÖ PASS: Expected {EXPECTED_SAME_VOL_COUNT}, Got {same_vol_count}\")\n",
    "\n",
    "        print(\"\\n--- ‚úÖ All Synthetic Data Verification Tests Passed ---\")\n",
    "\n",
    "    except KeyError:\n",
    "        print(f\"FAIL: Ticker '{ticker_name}' not found in features_df.\")\n",
    "    except IndexError:\n",
    "        print(\"FAIL: Not enough data in features_df to run verification. Check num_days.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during verification: {e}\")\n",
    "\n",
    "\n",
    "# --- A. HELPER FUNCTIONS ---\n",
    "\n",
    "def calculate_gain(price_series: pd.Series):\n",
    "    \"\"\"Calculates the total gain over a series of prices.\"\"\"\n",
    "    # Ensure there are at least two data points to calculate a gain\n",
    "    if price_series.dropna().shape[0] < 2: return np.nan\n",
    "    # Use forward-fill for the end price and back-fill for the start price\n",
    "    # to handle potential NaNs at the beginning or end of the series.\n",
    "    return (price_series.ffill().iloc[-1] / price_series.bfill().iloc[0]) - 1\n",
    "\n",
    "\n",
    "def calculate_sharpe(return_series: pd.Series):\n",
    "    \"\"\"Calculates the annualized Sharpe ratio from a series of daily returns.\"\"\"\n",
    "    # Ensure there are at least two returns to calculate a standard deviation\n",
    "    if return_series.dropna().shape[0] < 2: return np.nan\n",
    "    std_dev = return_series.std()\n",
    "    # Avoid division by zero if returns are constant\n",
    "    if std_dev > 0 and std_dev != np.inf:\n",
    "        return (return_series.mean() / std_dev) * np.sqrt(252)\n",
    "    return np.nan\n",
    "\n",
    "\n",
    "def calculate_sharpe_atr(return_series: pd.Series, atrp_series: pd.Series):\n",
    "    \"\"\"Calculates a Sharpe-like ratio using mean ATRP as the denominator.\"\"\"\n",
    "    # Ensure there are returns and that ATRP data is valid\n",
    "    if return_series.dropna().shape[0] < 2 or atrp_series.dropna().empty:\n",
    "        return np.nan\n",
    "        \n",
    "    mean_return = return_series.mean()\n",
    "    mean_atrp = atrp_series.mean()\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    if mean_atrp > 0 and mean_atrp != np.inf:\n",
    "        return mean_return / mean_atrp\n",
    "        \n",
    "    return np.nan\n",
    "\n",
    "\n",
    "def print_nested(d, indent=0, width=4):\n",
    "    \"\"\"Pretty-print any nested dict/list/tuple combination.\"\"\"\n",
    "    spacing = ' ' * indent\n",
    "    if isinstance(d, dict):\n",
    "        for k, v in d.items():\n",
    "            print(f'{spacing}{k}:')\n",
    "            print_nested(v, indent + width, width)\n",
    "    elif isinstance(d, (list, tuple)):\n",
    "        for item in d:\n",
    "            print_nested(item, indent, width)\n",
    "    else:\n",
    "        print(f'{spacing}{d}')\n",
    "\n",
    "\n",
    "# --- B. MODULAR METRIC CALCULATION ENGINE ---\n",
    "\n",
    "def calculate_price_metric(metric_data: dict) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Calculates the 'Price' metric (total gain) over the calculation period.\n",
    "\n",
    "    Args:\n",
    "        metric_data: A dictionary containing pre-calculated data Series.\n",
    "                     Requires 'calc_close' (pd.DataFrame): The close prices for the calc period.\n",
    "\n",
    "    Returns:\n",
    "        A pandas Series of the calculated metric values, indexed by Ticker.\n",
    "    \"\"\"\n",
    "    calc_close = metric_data['calc_close']\n",
    "    \n",
    "    # Ensure there are at least two data points to calculate a gain\n",
    "    if len(calc_close) < 2:\n",
    "        return pd.Series(dtype='float64', index=calc_close.columns)\n",
    "\n",
    "    first_prices = calc_close.bfill().iloc[0]\n",
    "    last_prices = calc_close.ffill().iloc[-1]\n",
    "    \n",
    "    # The division of two Series aligns by index (Ticker), which is what we want.\n",
    "    price_metric = last_prices / first_prices\n",
    "    \n",
    "    return price_metric.dropna()\n",
    "\n",
    "\n",
    "def calculate_sharpe_metric(metric_data: dict) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Calculates the annualized 'Sharpe' ratio metric over the calculation period.\n",
    "\n",
    "    Args:\n",
    "        metric_data: A dictionary containing pre-calculated data Series.\n",
    "                     Requires 'daily_returns' (pd.DataFrame): Daily returns for the calc period.\n",
    "\n",
    "    Returns:\n",
    "        A pandas Series of the calculated metric values, indexed by Ticker.\n",
    "    \"\"\"\n",
    "    daily_returns = metric_data['daily_returns']\n",
    "    \n",
    "    # Ensure there's enough data to calculate standard deviation\n",
    "    if len(daily_returns.dropna()) < 2:\n",
    "        return pd.Series(dtype='float64', index=daily_returns.columns)\n",
    "    \n",
    "    mean_returns = daily_returns.mean()\n",
    "    std_returns = daily_returns.std()\n",
    "    \n",
    "    # Standard annualized Sharpe Ratio calculation. Avoid division by zero.\n",
    "    # We replace resulting NaNs/infs with 0 to handle cases of zero volatility.\n",
    "    sharpe_ratio = (mean_returns / std_returns * np.sqrt(252))\n",
    "    \n",
    "    return sharpe_ratio.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "    \n",
    "def calculate_sharpe_atr_metric(metric_data: dict) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Calculates the 'Sharpe (ATR)' metric over the calculation period.\n",
    "\n",
    "    Args:\n",
    "        metric_data: A dictionary containing pre-calculated data Series.\n",
    "                     Requires 'daily_returns' (pd.DataFrame): Daily returns for the calc period.\n",
    "                     Requires 'atrp' (pd.Series): Mean ATRP for each ticker over the period.\n",
    "\n",
    "    Returns:\n",
    "        A pandas Series of the calculated metric values, indexed by Ticker.\n",
    "    \"\"\"\n",
    "    daily_returns = metric_data['daily_returns']\n",
    "    atrp = metric_data['atrp']\n",
    "    \n",
    "    mean_returns = daily_returns.mean()\n",
    "    \n",
    "    # ATRP-based Sharpe. Avoid division by zero.\n",
    "    # We replace resulting NaNs/infs with 0.\n",
    "    sharpe_atr = mean_returns / atrp\n",
    "    \n",
    "    return sharpe_atr.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "\n",
    "def calculate_buy_and_hold_performance(df_close: pd.DataFrame,\n",
    "                                       features_df: pd.DataFrame,\n",
    "                                       tickers: list,\n",
    "                                       start_date: pd.Timestamp,\n",
    "                                       end_date: pd.Timestamp):\n",
    "    \"\"\"\n",
    "    Calculates performance for a buy-and-hold portfolio with specified initial weights.\n",
    "\n",
    "    [UPGRADED to handle duplicate tickers in the input list as initial weights.\n",
    "     e.g., ['A', 'A', 'B'] means an initial 2/3 weight in A and 1/3 in B.]\n",
    "\n",
    "    Args:\n",
    "        df_close (pd.DataFrame): DataFrame of adjusted close prices.\n",
    "        features_df (pd.DataFrame): DataFrame with 'ATRP' and a ('Ticker', 'Date') MultiIndex.\n",
    "        tickers (list): A list of ticker symbols. Duplicates indicate higher initial weight.\n",
    "        start_date (pd.Timestamp): The starting date for the calculation.\n",
    "        end_date (pd.Timestamp): The ending date for the calculation.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (portfolio_value_series, portfolio_return_series, portfolio_atrp_series)\n",
    "    \"\"\"\n",
    "    if not tickers or tickers is None:\n",
    "        empty_series = pd.Series(dtype='float64')\n",
    "        return empty_series, empty_series, empty_series\n",
    "\n",
    "    # --- NEW LOGIC: Determine initial weights from the input list ---\n",
    "    # 1. Count occurrences of each ticker\n",
    "    ticker_counts = Counter(tickers)\n",
    "    total_parts = len(tickers)\n",
    "\n",
    "    # 2. Create a Series of initial weights (e.g., {'A': 0.66, 'B': 0.33})\n",
    "    initial_weights = pd.Series({ticker: count / total_parts for ticker, count in ticker_counts.items()})\n",
    "    \n",
    "    # 3. Get the unique tickers needed for data selection\n",
    "    unique_tickers = initial_weights.index.tolist()\n",
    "    # --- END OF NEW LOGIC ---\n",
    "\n",
    "    # 4. Calculate Portfolio Value and Return Series\n",
    "    prices_raw = df_close[unique_tickers].loc[start_date:end_date]\n",
    "    if prices_raw.dropna(how='all').empty:\n",
    "        empty_series = pd.Series(dtype='float64')\n",
    "        return empty_series, empty_series, empty_series\n",
    "\n",
    "    # Normalize prices of each unique stock (growth of $1 in each)\n",
    "    prices_norm = prices_raw.div(prices_raw.bfill().iloc[0])\n",
    "\n",
    "    # Multiply the growth of each stock by its *initial* portfolio weight\n",
    "    weighted_growth = prices_norm.mul(initial_weights, axis='columns')\n",
    "\n",
    "    # The total portfolio value is the sum of the weighted growth of its components\n",
    "    value_series = weighted_growth.sum(axis=1)\n",
    "    return_series = value_series.pct_change()\n",
    "\n",
    "    # 5. Calculate Value-Weighted Portfolio ATRP Series\n",
    "    full_period_index = pd.MultiIndex.from_product(\n",
    "        [unique_tickers, return_series.index],\n",
    "        names=['Ticker', 'Date']\n",
    "    )\n",
    "    portfolio_atrp_features = features_df.loc[features_df.index.intersection(full_period_index)]\n",
    "    portfolio_atrp_daily_unstacked = portfolio_atrp_features['ATRP'].unstack(level='Ticker')\n",
    "\n",
    "    # The drifting weight of each component is its value (from weighted_growth)\n",
    "    # divided by the total portfolio value for that day.\n",
    "    drifting_weights = weighted_growth.div(value_series, axis='index')\n",
    "\n",
    "    aligned_weights, aligned_atrp = drifting_weights.align(portfolio_atrp_daily_unstacked, join='inner', axis=1)\n",
    "    \n",
    "    atrp_series = (aligned_weights * aligned_atrp).sum(axis=1)\n",
    "\n",
    "    return value_series, return_series, atrp_series\n",
    "\n",
    "\n",
    "\n",
    "# The single source of truth for all available ranking metrics.\n",
    "# Maps the user-facing name to the calculation function.\n",
    "METRIC_REGISTRY = {\n",
    "    'Price': calculate_price_metric,\n",
    "    'Sharpe': calculate_sharpe_metric,\n",
    "    'Sharpe (ATR)': calculate_sharpe_atr_metric,\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Metric Registry Initialized with:\", list(METRIC_REGISTRY.keys()))\n",
    "\n",
    "\n",
    "# --- B. THE CORE CALCULATION ENGINE ---\n",
    "\n",
    "def run_walk_forward_step(df_close_full, df_high_full, df_low_full,\n",
    "                          master_trading_days,\n",
    "                          start_date, calc_period, fwd_period,\n",
    "                          metric, rank_start, rank_end, benchmark_ticker,\n",
    "                          features_df,\n",
    "                          debug=False):\n",
    "    \"\"\"\n",
    "    Runs a single step of the walk-forward analysis with a strict, pre-emptive\n",
    "    check to ensure the full period is available.\n",
    "    \"\"\"\n",
    "    debug_data = {} if debug else None\n",
    "\n",
    "    # 1. Determine exact date ranges with a NEW pre-emptive check\n",
    "    try:\n",
    "        start_idx = master_trading_days.get_loc(start_date)\n",
    "    except KeyError:\n",
    "        return ({'error': f\"Start date {start_date.date()} is not a valid trading day.\"}, None)\n",
    "\n",
    "    # +++ THIS IS THE NEW PRE-EMPTIVE CHECK LOGIC +++\n",
    "    # Calculate the desired end index without clamping first.\n",
    "    desired_viz_end_idx = start_idx + calc_period + fwd_period\n",
    "    \n",
    "    # Check if the desired end index is out of bounds.\n",
    "    if desired_viz_end_idx >= len(master_trading_days):\n",
    "        last_available_date = master_trading_days[-1].date()\n",
    "        required_days = calc_period + fwd_period\n",
    "        available_days = len(master_trading_days) - start_idx\n",
    "        error_msg = (f\"Not enough data for the full requested period. \"\n",
    "                     f\"Required: {required_days} days, Available: {available_days} days until {last_available_date}.\")\n",
    "        return ({'error': error_msg}, None)\n",
    "    # --- END OF NEW CHECK ---\n",
    "\n",
    "    # If the check passes, we know the full period is available.\n",
    "    # The 'min' calls are now just a redundant safety measure.\n",
    "    calc_end_idx = min(start_idx + calc_period, len(master_trading_days) - 1)\n",
    "    viz_end_idx = min(calc_end_idx + fwd_period, len(master_trading_days) - 1)\n",
    "\n",
    "    safe_start_date = master_trading_days[start_idx]\n",
    "    safe_calc_end_date = master_trading_days[calc_end_idx]\n",
    "    safe_viz_end_date = master_trading_days[viz_end_idx]\n",
    "    \n",
    "    if safe_start_date >= safe_calc_end_date:\n",
    "        return ({'error': \"Invalid date range (calc period has zero or negative length).\"}, None)\n",
    "\n",
    "    # (The rest of the function remains completely unchanged...)\n",
    "    # 2. Slice data for the calculation period\n",
    "    calc_close_raw = df_close_full.loc[safe_start_date:safe_calc_end_date]\n",
    "    calc_close = calc_close_raw.dropna(axis=1, how='all')\n",
    "    if calc_close.shape[1] == 0 or len(calc_close) < 2:\n",
    "        return ({'error': \"Not enough data in calc period.\"}, None)\n",
    "\n",
    "    # 3. Calculate INTERMEDIATE data required for the ranking metrics\n",
    "    daily_returns = calc_close.bfill().ffill().pct_change()\n",
    "    valid_tickers = calc_close.columns\n",
    "    calc_period_index = pd.MultiIndex.from_product([valid_tickers, calc_close.index], names=['Ticker', 'Date'])\n",
    "    features_in_period = features_df.loc[features_df.index.intersection(calc_period_index)]\n",
    "    atrp = features_in_period.groupby(level='Ticker')['ATRP'].mean()\n",
    "\n",
    "    # 4. Calculate all ranking metrics by iterating through the METRIC_REGISTRY\n",
    "    metric_ingredients = { 'calc_close': calc_close, 'daily_returns': daily_returns, 'atrp': atrp, }\n",
    "    metric_values = {}\n",
    "    for name, func in METRIC_REGISTRY.items():\n",
    "        metric_values[name] = func(metric_ingredients)\n",
    "    if metric not in metric_values or metric_values[metric].empty:\n",
    "        return ({'error': f\"Metric '{metric}' could not be calculated or resulted in no valid tickers.\"}, None)\n",
    "\n",
    "    # 5. Rank tickers and select the portfolio\n",
    "    sorted_tickers = metric_values[metric].sort_values(ascending=False)\n",
    "    tickers_to_display = sorted_tickers.index[rank_start-1:rank_end].tolist()\n",
    "    if not tickers_to_display:\n",
    "        return ({'error': \"No tickers found for the selected rank.\"}, None)\n",
    "\n",
    "#################################\n",
    "    # 6. Calculate Portfolio & Benchmark Performance\n",
    "    # +++ THIS ENTIRE SECTION IS REFACTORED +++\n",
    "    actual_calc_end_ts = calc_close.index.max()\n",
    "\n",
    "    # +++ FIX: RE-INTRODUCE THE `normalized_plot_data` CALCULATION HERE +++\n",
    "    # This is needed for the debug trace and the final plot output.\n",
    "    normalized_plot_data = df_close_full[tickers_to_display].loc[safe_start_date:safe_viz_end_date]\n",
    "    normalized_plot_data = normalized_plot_data.div(normalized_plot_data.bfill().iloc[0])\n",
    "    # --- END OF FIX ---\n",
    "\n",
    "    # Use the new central function for the portfolio\n",
    "    portfolio_series, portfolio_return_series, portfolio_atrp_series = \\\n",
    "        calculate_buy_and_hold_performance(df_close_full, features_df, tickers_to_display,\n",
    "                                           safe_start_date, safe_viz_end_date)\n",
    "\n",
    "    # Use the new central function for the benchmark (a portfolio of one)\n",
    "    if benchmark_ticker in df_close_full.columns:\n",
    "        benchmark_price_series, benchmark_return_series, benchmark_atrp_series = \\\n",
    "            calculate_buy_and_hold_performance(df_close_full, features_df, [benchmark_ticker],\n",
    "                                               safe_start_date, safe_viz_end_date)\n",
    "    else:\n",
    "        benchmark_price_series = pd.Series(dtype='float64')\n",
    "        benchmark_return_series = pd.Series(dtype='float64')\n",
    "        benchmark_atrp_series = pd.Series(dtype='float64')\n",
    "\n",
    "    # Now, split the generated series into calc and fwd periods\n",
    "    # This logic remains, but it's much cleaner as it operates on the final series\n",
    "    try:\n",
    "        boundary_loc = portfolio_return_series.index.get_loc(actual_calc_end_ts)\n",
    "        calc_portfolio_returns = portfolio_return_series.iloc[:boundary_loc + 1]\n",
    "        fwd_portfolio_returns = portfolio_return_series.iloc[boundary_loc + 1:]\n",
    "        calc_portfolio_atrp = portfolio_atrp_series.iloc[:boundary_loc + 1]\n",
    "        fwd_portfolio_atrp = portfolio_atrp_series.iloc[boundary_loc + 1:]\n",
    "        \n",
    "        if not benchmark_return_series.empty:\n",
    "            bm_boundary_loc = benchmark_return_series.index.get_loc(actual_calc_end_ts)\n",
    "            calc_benchmark_returns = benchmark_return_series.iloc[:bm_boundary_loc + 1]\n",
    "            fwd_benchmark_returns = benchmark_return_series.iloc[bm_boundary_loc + 1:]\n",
    "            calc_benchmark_atrp = benchmark_atrp_series.iloc[:bm_boundary_loc + 1]\n",
    "            fwd_benchmark_atrp = benchmark_atrp_series.iloc[bm_boundary_loc + 1:]\n",
    "        else:\n",
    "            calc_benchmark_returns, fwd_benchmark_returns = pd.Series(dtype='float64'), pd.Series(dtype='float64')\n",
    "            calc_benchmark_atrp, fwd_benchmark_atrp = pd.Series(dtype='float64'), pd.Series(dtype='float64')\n",
    "\n",
    "    except (KeyError, IndexError): # Fallback for edge cases\n",
    "        calc_portfolio_returns = portfolio_return_series.loc[:actual_calc_end_ts]\n",
    "        fwd_portfolio_returns = portfolio_return_series.loc[actual_calc_end_ts:].iloc[1:]\n",
    "        calc_portfolio_atrp = portfolio_atrp_series.loc[:actual_calc_end_ts]\n",
    "        fwd_portfolio_atrp = portfolio_atrp_series.loc[actual_calc_end_ts:].iloc[1:]\n",
    "        \n",
    "        if not benchmark_return_series.empty:\n",
    "            calc_benchmark_returns = benchmark_return_series.loc[:actual_calc_end_ts]\n",
    "            fwd_benchmark_returns = benchmark_return_series.loc[actual_calc_end_ts:].iloc[1:]\n",
    "            calc_benchmark_atrp = benchmark_atrp_series.loc[:actual_calc_end_ts]\n",
    "            fwd_benchmark_atrp = benchmark_atrp_series.loc[actual_calc_end_ts:].iloc[1:]\n",
    "        else:\n",
    "            calc_benchmark_returns, fwd_benchmark_returns = pd.Series(dtype='float64'), pd.Series(dtype='float64')\n",
    "            calc_benchmark_atrp, fwd_benchmark_atrp = pd.Series(dtype='float64'), pd.Series(dtype='float64')\n",
    "\n",
    "\n",
    "###################\n",
    "\n",
    "    perf_data = {}\n",
    "    perf_data['calc_p_gain'] = calculate_gain(portfolio_series.loc[:actual_calc_end_ts])\n",
    "    perf_data['fwd_p_gain'] = calculate_gain(portfolio_series.loc[actual_calc_end_ts:])\n",
    "    perf_data['full_p_gain'] = calculate_gain(portfolio_series)\n",
    "    perf_data['calc_p_sharpe'] = calculate_sharpe(calc_portfolio_returns)\n",
    "    perf_data['fwd_p_sharpe'] = calculate_sharpe(fwd_portfolio_returns)\n",
    "    perf_data['full_p_sharpe'] = calculate_sharpe(portfolio_return_series)\n",
    "    perf_data['calc_b_gain'] = calculate_gain(benchmark_price_series.loc[safe_start_date:actual_calc_end_ts]) if benchmark_price_series is not None else np.nan\n",
    "    perf_data['fwd_b_gain'] = calculate_gain(benchmark_price_series.loc[actual_calc_end_ts:safe_viz_end_date]) if benchmark_price_series is not None else np.nan\n",
    "    perf_data['full_b_gain'] = calculate_gain(benchmark_price_series.loc[safe_start_date:safe_viz_end_date]) if benchmark_price_series is not None else np.nan\n",
    "    perf_data['calc_b_sharpe'] = calculate_sharpe(calc_benchmark_returns)\n",
    "    perf_data['fwd_b_sharpe'] = calculate_sharpe(fwd_benchmark_returns)\n",
    "    perf_data['full_b_sharpe'] = calculate_sharpe(benchmark_return_series)\n",
    "    perf_data['calc_p_sharpe_atr'] = calculate_sharpe_atr(calc_portfolio_returns, calc_portfolio_atrp)\n",
    "    perf_data['fwd_p_sharpe_atr'] = calculate_sharpe_atr(fwd_portfolio_returns, fwd_portfolio_atrp)\n",
    "    perf_data['full_p_sharpe_atr'] = calculate_sharpe_atr(portfolio_return_series, portfolio_atrp_series)\n",
    "    perf_data['calc_b_sharpe_atr'] = calculate_sharpe_atr(calc_benchmark_returns, calc_benchmark_atrp)\n",
    "    perf_data['fwd_b_sharpe_atr'] = calculate_sharpe_atr(fwd_benchmark_returns, fwd_benchmark_atrp)\n",
    "    perf_data['full_b_sharpe_atr'] = calculate_sharpe_atr(benchmark_return_series, benchmark_atrp_series)\n",
    "    if debug:\n",
    "        df_ranking_base = pd.DataFrame({'MeanDailyReturn': daily_returns.mean(),'StdDevDailyReturn': daily_returns.std(),'MeanATRP': atrp})\n",
    "        df_metrics = pd.DataFrame(metric_values)\n",
    "        df_metrics.columns = [f'Metric_{col}' for col in df_metrics.columns]\n",
    "        df_ranking = df_ranking_base.join(df_metrics, how='left')\n",
    "        df_ranking.index.name = 'Ticker'\n",
    "        debug_data['ranking_metrics'] = df_ranking.sort_values(f'Metric_{metric}', ascending=False)\n",
    "    calc_end_prices = calc_close.ffill().iloc[-1]\n",
    "    fwd_close_slice = df_close_full.loc[actual_calc_end_ts:safe_viz_end_date]\n",
    "    viz_end_prices = fwd_close_slice.ffill().iloc[-1] if not fwd_close_slice.empty and len(fwd_close_slice) >= 2 else calc_end_prices\n",
    "    calc_gains = (calc_end_prices / calc_close.bfill().iloc[0]) - 1\n",
    "    fwd_gains = (viz_end_prices / calc_end_prices) - 1\n",
    "    results_df = pd.DataFrame({'Rank': range(rank_start, rank_start + len(tickers_to_display)), 'Metric': metric, 'MetricValue': sorted_tickers.loc[tickers_to_display].values, 'CalcPrice': calc_end_prices.loc[tickers_to_display], 'CalcGain': calc_gains.loc[tickers_to_display], 'FwdGain': fwd_gains.loc[tickers_to_display]}, index=pd.Index(tickers_to_display, name='Ticker'))\n",
    "    if benchmark_price_series is not None and benchmark_ticker in calc_close.columns:\n",
    "        benchmark_df_row = pd.DataFrame({'Rank': np.nan, 'Metric': metric, 'MetricValue': metric_values[metric].get(benchmark_ticker, np.nan), 'CalcPrice': calc_end_prices[benchmark_ticker], 'CalcGain': calc_gains[benchmark_ticker], 'FwdGain': fwd_gains[benchmark_ticker]}, index=pd.Index([f\"{benchmark_ticker} (BM)\"], name='Ticker'))\n",
    "        results_df = pd.concat([results_df, benchmark_df_row])\n",
    "    if debug:\n",
    "        df_trace = normalized_plot_data.copy()\n",
    "        df_trace.columns = [f'Norm_Price_{c}' for c in df_trace.columns]\n",
    "        df_trace['Norm_Price_Portfolio'] = portfolio_series\n",
    "        if benchmark_price_series is not None and not benchmark_price_series.loc[safe_start_date:safe_viz_end_date].dropna().empty:\n",
    "            norm_bm = benchmark_price_series.loc[safe_start_date:safe_viz_end_date] / benchmark_price_series.loc[safe_start_date:].bfill().iloc[0]\n",
    "            df_trace[f'Norm_Price_Benchmark_{benchmark_ticker}'] = norm_bm\n",
    "        for col in df_trace.columns:\n",
    "            if 'Norm_Price' in col:\n",
    "                df_trace[col.replace('Norm_Price', 'Return')] = df_trace[col].pct_change()\n",
    "        debug_data['portfolio_trace'] = df_trace\n",
    "    final_results = {\n",
    "        'tickers_to_display': tickers_to_display, 'normalized_plot_data': normalized_plot_data,\n",
    "        'portfolio_series': portfolio_series, 'benchmark_price_series': benchmark_price_series,\n",
    "        'performance_data': perf_data, 'results_df': results_df, 'actual_calc_end_ts': actual_calc_end_ts,\n",
    "        'safe_start_date': safe_start_date, 'safe_viz_end_date': safe_viz_end_date,\n",
    "        'error': None\n",
    "    }\n",
    "    return (final_results, debug_data)\n",
    "\n",
    "\n",
    "# --- C. DYNAMIC DATA QUALITY FILTER FUNCTIONS ---\n",
    "\n",
    "def get_eligible_universe(features_df, filter_date, thresholds):\n",
    "    \"\"\"Filters the universe of tickers based on quality metrics for a given date.\"\"\"\n",
    "    filter_date_ts = pd.to_datetime(filter_date)\n",
    "    # The index is now the comprehensive features_df\n",
    "    date_index = features_df.index.get_level_values('Date').unique().sort_values()\n",
    "    \n",
    "    if filter_date_ts < date_index[0]:\n",
    "        print(f\"Warning: Filter date {filter_date_ts.date()} is before the earliest data point. Returning empty universe.\")\n",
    "        return []\n",
    "        \n",
    "    # Find the most recent date with quality data on or before the filter date\n",
    "    valid_prior_dates = date_index[date_index <= filter_date_ts]\n",
    "    if valid_prior_dates.empty:\n",
    "        print(f\"Warning: No available data found on or before {filter_date_ts.date()}. Returning empty universe.\")\n",
    "        return []\n",
    "        \n",
    "    actual_date_to_use = valid_prior_dates[-1]\n",
    "    if actual_date_to_use.date() != filter_date_ts.date():\n",
    "        print(f\"‚ÑπÔ∏è Info: Filter date {filter_date_ts.date()} not found. Using previous available date {actual_date_to_use.date()}.\")\n",
    "\n",
    "    metrics_on_date = features_df.xs(actual_date_to_use, level='Date')\n",
    "    \n",
    "    # Apply filters using the new column names from features_df\n",
    "    mask = ((metrics_on_date['RollMedDollarVol'] >= thresholds['min_median_dollar_volume']) & # <-- RENAMED\n",
    "            (metrics_on_date['RollingStalePct'] <= thresholds['max_stale_pct']) &\n",
    "            (metrics_on_date['RollingSameVolCount'] <= thresholds['max_same_vol_count']))\n",
    "            \n",
    "    eligible_tickers = metrics_on_date[mask].index.tolist()\n",
    "    all_tickers = metrics_on_date.index.tolist()\n",
    "    print(f\"Dynamic Filter ({filter_date_ts.date()}): Kept {len(eligible_tickers)} of {len(all_tickers)} tickers.\")\n",
    "    return eligible_tickers\n",
    "\n",
    "\n",
    "# --- D. INTERACTIVE ANALYSIS & BACKTESTING TOOLS ---\n",
    "\n",
    "def plot_walk_forward_analyzer(df_ohlcv,\n",
    "                               default_start_date=None, default_calc_period=126, default_fwd_period=63,\n",
    "                               default_metric='Sharpe (ATR)', default_rank_start=1, default_rank_end=10,\n",
    "                               default_benchmark_ticker='SPY', master_calendar_ticker='SPY',\n",
    "                               quality_thresholds={'min_median_dollar_volume': 1_000_000, 'max_stale_pct': 0.05, 'max_same_vol_count': 10},\n",
    "                               debug=False):\n",
    "    # --- Initial Setup and Widget Creation (unchanged) ---\n",
    "    print(\"Initializing Walk-Forward Analyzer...\")\n",
    "    # ... (all setup code is identical)\n",
    "    if not isinstance(df_ohlcv.index, pd.MultiIndex): raise ValueError(\"Input DataFrame must have a (Ticker, Date) MultiIndex.\")\n",
    "    df_ohlcv = df_ohlcv.sort_index()\n",
    "    if master_calendar_ticker not in df_ohlcv.index.get_level_values(0):\n",
    "        raise ValueError(f\"Master calendar ticker '{master_calendar_ticker}' not found in DataFrame.\")\n",
    "    master_trading_days = df_ohlcv.loc[master_calendar_ticker].index.unique().sort_values()\n",
    "    print(f\"Master trading day calendar created from '{master_calendar_ticker}' ({len(master_trading_days)} days).\")\n",
    "    print(\"--- Generating all features upfront... ---\")\n",
    "    features_df = generate_features(df_ohlcv)\n",
    "    print(\"Pre-processing data (unstacking)...\")\n",
    "    df_close_full = df_ohlcv['Adj Close'].unstack(level=0)\n",
    "    df_high_full = df_ohlcv['Adj High'].unstack(level=0)\n",
    "    df_low_full = df_ohlcv['Adj Low'].unstack(level=0)\n",
    "\n",
    "    # --- Widget Creation (existing widgets) ---\n",
    "    start_date_picker = widgets.DatePicker(description='Start Date:', value=pd.to_datetime(default_start_date), disabled=False)\n",
    "    calc_period_input = widgets.IntText(value=default_calc_period, description='Calc Period (days):')\n",
    "    fwd_period_input = widgets.IntText(value=default_fwd_period, description='Fwd Period (days):')\n",
    "    if default_metric not in METRIC_REGISTRY:\n",
    "        default_metric = list(METRIC_REGISTRY.keys())[0]\n",
    "    metric_dropdown = widgets.Dropdown(options=list(METRIC_REGISTRY.keys()), value=default_metric, description='Metric:')\n",
    "    rank_start_input = widgets.IntText(value=default_rank_start, description='Rank Start:')\n",
    "    rank_end_input = widgets.IntText(value=default_rank_end, description='Rank End:')\n",
    "    benchmark_ticker_input = widgets.Text(value=default_benchmark_ticker, description='Benchmark:', placeholder='Enter Ticker')\n",
    "    update_button = widgets.Button(description=\"Update Chart\", button_style='primary')\n",
    "    mode_selector = widgets.RadioButtons(options=['Ranking', 'Manual List'], value='Ranking', description='Portfolio Mode:', layout={'width': 'max-content'})\n",
    "    manual_tickers_input = widgets.Textarea(value='', placeholder='Enter tickers, comma-separated...\\ne.g., AAPL, AAPL, MSFT, GOOG', description='Manual Tickers:', layout={'width': '400px', 'height': '80px'})\n",
    "    \n",
    "    # --- Layout and Observer Setup (unchanged) ---\n",
    "    ranking_controls_box = widgets.HBox([metric_dropdown, rank_start_input, rank_end_input])\n",
    "    manual_controls_box = widgets.HBox([manual_tickers_input])\n",
    "    def on_mode_change(change):\n",
    "        if change['new'] == 'Ranking':\n",
    "            ranking_controls_box.layout.display = 'flex'; manual_controls_box.layout.display = 'none'\n",
    "        else:\n",
    "            ranking_controls_box.layout.display = 'none'; manual_controls_box.layout.display = 'flex'\n",
    "    mode_selector.observe(on_mode_change, names='value')\n",
    "    \n",
    "    # --- Plot and Output Setup ---\n",
    "    ticker_list_output = widgets.Output()\n",
    "    results_container, debug_data_container = [None], [None]\n",
    "    fig = go.FigureWidget()\n",
    "    max_traces = 50\n",
    "    for i in range(max_traces): fig.add_trace(go.Scatter(x=[None], y=[None], mode='lines', name=f'placeholder_{i}', visible=False, showlegend=False))\n",
    "    # Note the names here match the update logic below\n",
    "    fig.add_trace(go.Scatter(x=[None], y=[None], mode='lines', name='Benchmark', visible=True, showlegend=True, line=dict(color='black', width=3, dash='dash')))\n",
    "    fig.add_trace(go.Scatter(x=[None], y=[None], mode='lines', name='Group Portfolio', visible=True, showlegend=True, line=dict(color='green', width=3)))\n",
    "\n",
    "    def update_plot(button_click):\n",
    "        ticker_list_output.clear_output()\n",
    "        # --- 1. Common Parameter Gathering and Validation (unchanged) ---\n",
    "        start_date_raw = pd.to_datetime(start_date_picker.value)\n",
    "        start_date_idx = master_trading_days.searchsorted(start_date_raw)\n",
    "        actual_start_date = master_trading_days[start_date_idx]\n",
    "\n",
    "        # ### NEW WARNING LOGIC STARTS HERE ###\n",
    "        # Check if the requested date is significantly before the Master Calendar's first day\n",
    "        first_available_date = master_trading_days[0]\n",
    "        \n",
    "        # We assume if the difference is more than 7 days, it's a user error, not a holiday offset\n",
    "        if start_date_raw < (first_available_date - pd.Timedelta(days=7)):\n",
    "            with ticker_list_output:\n",
    "                print(f\"‚ö†Ô∏è DATE WARNING: The requested start date ({start_date_raw.date()}) is unavailable.\")\n",
    "                print(f\"   The Master Calendar Ticker '{master_calendar_ticker}' only starts trading on {first_available_date.date()}.\")\n",
    "                print(f\"   >> System auto-adjusted the start date to: {actual_start_date.date()}\")\n",
    "                print(f\"   >> To fix: Use a Master Ticker with longer history (e.g., 'SPY' starts 1993).\")\n",
    "                print(\"-\" * 60)\n",
    "        # ### NEW WARNING LOGIC ENDS HERE ###\n",
    "\n",
    "        calc_period = calc_period_input.value; fwd_period = fwd_period_input.value\n",
    "        benchmark_ticker = benchmark_ticker_input.value.strip().upper()\n",
    "        \n",
    "        # --- 2. Mode-Specific Logic to Get Results (unchanged) ---\n",
    "        selected_mode = mode_selector.value\n",
    "        results, debug_output = None, None\n",
    "        if selected_mode == 'Ranking':\n",
    "            metric = metric_dropdown.value\n",
    "            rank_start = rank_start_input.value; rank_end = rank_end_input.value\n",
    "            eligible_tickers = get_eligible_universe(features_df, actual_start_date, quality_thresholds)\n",
    "            results, debug_output = run_walk_forward_step(df_close_full, df_high_full, df_low_full, master_trading_days, actual_start_date, calc_period, fwd_period, metric, rank_start, rank_end, benchmark_ticker, features_df=features_df, debug=debug)\n",
    "        elif selected_mode == 'Manual List':\n",
    "            raw_text = manual_tickers_input.value\n",
    "            manual_tickers = [t.strip().upper() for t in raw_text.split(',') if t.strip()]\n",
    "            results, debug_output = run_manual_portfolio_step(df_close_full, features_df, manual_tickers, master_trading_days, actual_start_date, calc_period, fwd_period, benchmark_ticker, debug=debug)\n",
    "        \n",
    "        # --- 3. Common Results Processing and Plotting ---\n",
    "        if results.get('error'):\n",
    "            with ticker_list_output: print(f\"Error: {results['error']}\"); return\n",
    "        results_container[0] = results; debug_data_container[0] = debug_output\n",
    "        \n",
    "        with fig.batch_update():\n",
    "            for i in range(max_traces):\n",
    "                trace = fig.data[i]\n",
    "                if i < len(results['tickers_to_display']):\n",
    "                    ticker = results['tickers_to_display'][i]; plot_data_series = results['normalized_plot_data'][ticker]\n",
    "                    trace.x, trace.y, trace.name, trace.visible, trace.showlegend = plot_data_series.index, plot_data_series.values, ticker, True, True\n",
    "                else: trace.visible, trace.showlegend = False, False\n",
    "            \n",
    "            ### RESTORED ### --- Benchmark and Portfolio Trace Logic ---\n",
    "            benchmark_trace = fig.data[max_traces]\n",
    "            # Use the pre-normalized benchmark series if available\n",
    "            if results['benchmark_price_series'] is not None and not results['benchmark_price_series'].dropna().empty:\n",
    "                benchmark_series = results['benchmark_price_series']\n",
    "                benchmark_trace.x, benchmark_trace.y, benchmark_trace.name, benchmark_trace.visible = benchmark_series.index, benchmark_series.values, f\"Benchmark ({benchmark_ticker})\", True\n",
    "            else:\n",
    "                benchmark_trace.visible = False\n",
    "\n",
    "            portfolio_trace = fig.data[max_traces + 1]\n",
    "            portfolio_trace.x, portfolio_trace.y, portfolio_trace.name, portfolio_trace.visible = results['portfolio_series'].index, results['portfolio_series'].values, 'Group Portfolio', True\n",
    "            \n",
    "            fig.layout.shapes = []; fig.add_shape(type=\"line\", x0=results['actual_calc_end_ts'], y0=0, x1=results['actual_calc_end_ts'], y1=1, xref='x', yref='paper', line=dict(color=\"grey\", width=2, dash=\"dash\"))\n",
    "            ### END RESTORED ###\n",
    "            \n",
    "        with ticker_list_output:\n",
    "            print(f\"Analysis Period: {results['safe_start_date'].date()} to {results['safe_viz_end_date'].date()}.\")\n",
    "            if selected_mode == 'Ranking':\n",
    "                print(\"Ranked Tickers:\"); pprint.pprint(results['tickers_to_display'])\n",
    "            else:\n",
    "                print(\"Manual Portfolio (Unique Tickers):\"); pprint.pprint(results['tickers_to_display'])\n",
    "                print(\"\\n--- Portfolio Composition ---\"); display(results['results_df'].style.format({'InitialWeight': '{:.2%}', 'FwdGain': '{:+.2%}'}))\n",
    "\n",
    "            ### RESTORED ### --- Performance Summary Table Logic ---\n",
    "            p = results['performance_data']\n",
    "            rows = []\n",
    "            rows.append({'Metric': 'Group Portfolio Gain', 'Full': p['full_p_gain'], 'Calc': p['calc_p_gain'], 'Fwd': p['fwd_p_gain']})\n",
    "            if not np.isnan(p.get('full_b_gain')):\n",
    "                rows.append({'Metric': f'Benchmark ({benchmark_ticker}) Gain', 'Full': p['full_b_gain'], 'Calc': p['calc_b_gain'], 'Fwd': p['fwd_b_gain']})\n",
    "                rows.append({'Metric': '== Gain Delta (vs Bm)', 'Full': p['full_p_gain'] - p['full_b_gain'], 'Calc': p['calc_p_gain'] - p['calc_b_gain'], 'Fwd': p['fwd_p_gain'] - p['fwd_b_gain']})\n",
    "            rows.append({'Metric': 'Group Portfolio Sharpe (ATR)', 'Full': p['full_p_sharpe_atr'], 'Calc': p['calc_p_sharpe_atr'], 'Fwd': p['fwd_p_sharpe_atr']})\n",
    "            if not np.isnan(p.get('full_b_sharpe_atr')):\n",
    "                rows.append({'Metric': f'Benchmark ({benchmark_ticker}) Sharpe (ATR)', 'Full': p['full_b_sharpe_atr'], 'Calc': p['calc_b_sharpe_atr'], 'Fwd': p['fwd_b_sharpe_atr']})\n",
    "                rows.append({'Metric': '== Sharpe (ATR) Delta (vs Bm)', 'Full': p['full_p_sharpe_atr'] - p['full_b_sharpe_atr'], 'Calc': p['calc_p_sharpe_atr'] - p['calc_b_sharpe_atr'], 'Fwd': p['fwd_p_sharpe_atr'] - p['fwd_b_sharpe_atr']})\n",
    "            \n",
    "            report_df = pd.DataFrame(rows).set_index('Metric')\n",
    "            gain_rows = [row for row in report_df.index if 'Gain' in row]\n",
    "            sharpe_rows = [row for row in report_df.index if 'Sharpe' in row]\n",
    "            \n",
    "            styled_df = report_df.style.format('{:+.2%}', na_rep='N/A', subset=(gain_rows, report_df.columns)).format('{:+.4f}', na_rep='N/A', subset=(sharpe_rows, report_df.columns)).set_properties(**{'text-align': 'right', 'width': '100px'}).set_table_styles([{'selector': 'th.col_heading', 'props': [('text-align', 'right')]}, {'selector': 'th.row_heading', 'props': [('text-align', 'left')]}])\n",
    "            \n",
    "            print(\"\\n--- Strategy Performance Summary ---\")\n",
    "            display(styled_df)\n",
    "            ### END RESTORED ###\n",
    "\n",
    "    # --- Final UI Assembly and Initial Call (unchanged) ---\n",
    "    mode_selector_row = widgets.HBox([mode_selector], layout=widgets.Layout(margin='0 0 10px 125px'))\n",
    "    controls_row1 = widgets.HBox([start_date_picker, calc_period_input, fwd_period_input])\n",
    "    common_controls_row = widgets.HBox([benchmark_ticker_input, update_button])\n",
    "    ui_container = widgets.VBox([mode_selector_row, controls_row1, ranking_controls_box, manual_controls_box, common_controls_row, ticker_list_output], layout=widgets.Layout(margin='10px 0 20px 0'))\n",
    "    fig.update_layout(title_text='Walk-Forward Performance Analysis', xaxis_title='Date', yaxis_title='Normalized Price', hovermode='x unified', height=600)\n",
    "    fig.add_hline(y=1, line_width=1, line_dash=\"dash\", line_color=\"grey\")\n",
    "    update_button.on_click(update_plot)\n",
    "    \n",
    "    # ### MODIFIED ### Assemble the final UI container with the new widgets\n",
    "    # The benchmark and update button are now in their own row for better layout.\n",
    "    common_controls_row = widgets.HBox([benchmark_ticker_input, update_button])\n",
    "    \n",
    "    ### MODIFIED ### - Add a wrapper HBox with a left margin for alignment\n",
    "    # This pushes the radio buttons to the right to align with the input fields below it.\n",
    "    # You can adjust the '95px' value for pixel-perfect alignment in your environment.\n",
    "    mode_selector_row = widgets.HBox(\n",
    "        [mode_selector],\n",
    "        layout=widgets.Layout(margin='0 0 10px 95px') # top, right, bottom, left\n",
    "    )\n",
    "\n",
    "    ui_container = widgets.VBox([\n",
    "        mode_selector_row, # Use the new aligned row\n",
    "        controls_row1,\n",
    "        ranking_controls_box, # Will be shown/hidden by the observer\n",
    "        manual_controls_box,  # Will be shown/hidden by the observer\n",
    "        common_controls_row,\n",
    "        ticker_list_output\n",
    "    ], layout=widgets.Layout(margin='10px 0 20px 0'))\n",
    "    \n",
    "    # ### NEW ### Set the initial visibility of the controls\n",
    "    on_mode_change({'new': mode_selector.value})\n",
    "    display(ui_container, fig)\n",
    "    update_plot(None)\n",
    "    \n",
    "    return (results_container, debug_data_container)\n",
    "\n",
    "\n",
    "def run_full_backtest(df_ohlcv, strategy_params, quality_thresholds):\n",
    "    \"\"\"Runs a full backtest of a strategy over a specified date range.\"\"\"\n",
    "    print(f\"--- Running Full Forensic Backtest for Strategy: {strategy_params['metric']} (Top {strategy_params['rank_start']}-{strategy_params['rank_end']}) ---\")\n",
    "    \n",
    "    # (No changes to the initial setup part...)\n",
    "    start_date, end_date = pd.to_datetime(strategy_params['start_date']), pd.to_datetime(strategy_params['end_date'])\n",
    "    calc_period, fwd_period = strategy_params['calc_period'], strategy_params['fwd_period']\n",
    "    metric, rank_start, rank_end = strategy_params['metric'], strategy_params['rank_start'], strategy_params['rank_end']\n",
    "    benchmark_ticker = strategy_params['benchmark_ticker']\n",
    "    master_calendar_ticker = strategy_params.get('master_calendar_ticker', 'VOO')\n",
    "    if master_calendar_ticker not in df_ohlcv.index.get_level_values(0):\n",
    "        raise ValueError(f\"Master calendar ticker '{master_calendar_ticker}' not found in DataFrame.\")\n",
    "    master_trading_days = df_ohlcv.loc[master_calendar_ticker].index.unique().sort_values()\n",
    "    start_idx = master_trading_days.searchsorted(start_date)\n",
    "    end_idx = master_trading_days.searchsorted(end_date, side='right')\n",
    "    print(\"--- Generating all features upfront... ---\")\n",
    "    features_df = generate_features(df_ohlcv)\n",
    "    df_close_full = df_ohlcv['Adj Close'].unstack(level=0); df_high_full = df_ohlcv['Adj High'].unstack(level=0); df_low_full = df_ohlcv['Adj Low'].unstack(level=0)\n",
    "    \n",
    "    # Loop through all periods in the backtest range\n",
    "    step_indices = range(start_idx, end_idx, fwd_period)\n",
    "    all_fwd_gains, period_by_period_debug = [], {}\n",
    "\n",
    "    print(f\"Simulating {len(step_indices)} periods from {master_trading_days[step_indices[0]].date()} to {master_trading_days[step_indices[-1]].date()}...\")\n",
    "    for current_idx in tqdm(step_indices, desc=\"Backtest Progress\"):\n",
    "        step_date = master_trading_days[current_idx]\n",
    "        eligible_tickers = get_eligible_universe(features_df, step_date, quality_thresholds)\n",
    "        if not eligible_tickers: continue\n",
    "        df_close_step = df_close_full[eligible_tickers]; df_high_step = df_high_full[eligible_tickers]; df_low_step = df_low_full[eligible_tickers]\n",
    "        \n",
    "        # +++ UPDATE THE CALL HERE +++\n",
    "        results, debug_output = run_walk_forward_step(\n",
    "            df_close_step, df_high_step, df_low_step, master_trading_days,\n",
    "            step_date, calc_period, fwd_period,\n",
    "            metric, rank_start, rank_end, benchmark_ticker,\n",
    "            features_df=features_df,  # Pass the features_df\n",
    "            debug=True\n",
    "        )\n",
    "        \n",
    "        # (The rest of the function remains unchanged...)\n",
    "        if results['error'] is None:\n",
    "            fwd_series = results['portfolio_series'].loc[results['actual_calc_end_ts']:]\n",
    "            all_fwd_gains.append(fwd_series.pct_change().dropna())\n",
    "            period_by_period_debug[step_date.date().isoformat()] = debug_output\n",
    "            \n",
    "    if not all_fwd_gains:\n",
    "        print(\"Error: No valid periods were simulated.\"); return None\n",
    "\n",
    "    strategy_returns = pd.concat(all_fwd_gains); strategy_equity_curve = (1 + strategy_returns).cumprod()\n",
    "    benchmark_returns = df_close_full[benchmark_ticker].pct_change().loc[strategy_equity_curve.index]; benchmark_equity_curve = (1 + benchmark_returns).cumprod()\n",
    "    cumulative_equity_df = pd.DataFrame({'Strategy_Equity': strategy_equity_curve, 'Benchmark_Equity': benchmark_equity_curve})\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=cumulative_equity_df.index, y=cumulative_equity_df['Strategy_Equity'], name='Strategy', line=dict(color='green')))\n",
    "    fig.add_trace(go.Scatter(x=cumulative_equity_df.index, y=cumulative_equity_df['Benchmark_Equity'], name=f'Benchmark ({benchmark_ticker})', line=dict(color='black', dash='dash')))\n",
    "    fig.update_layout(title=f\"Cumulative Performance: '{metric}' Strategy (Top {rank_start}-{rank_end})\", xaxis_title=\"Date\", yaxis_title=\"Cumulative Growth\")\n",
    "    fig.show()\n",
    "    final_backtest_results = {'cumulative_performance': cumulative_equity_df, 'period_by_period_debug': period_by_period_debug}\n",
    "    print(\"\\n‚úÖ Full backtest complete. Results object is ready for forensic analysis.\")\n",
    "    return final_backtest_results\n",
    "\n",
    "\n",
    "def run_manual_portfolio_step(df_close_full, features_df,\n",
    "                              manual_tickers,\n",
    "                              master_trading_days,\n",
    "                              start_date, calc_period, fwd_period,\n",
    "                              benchmark_ticker,\n",
    "                              debug=False):\n",
    "    \"\"\"\n",
    "    [CORRECTED to calculate the full, comprehensive set of performance metrics,\n",
    "     including standard Sharpe, to match the output of run_walk_forward_step.]\n",
    "\n",
    "     Runs a performance analysis for a manually specified portfolio of tickers.\n",
    "\n",
    "    Bypasses all ranking and filtering logic, directly calculating performance\n",
    "    for the provided list. The output format is identical to run_walk_forward_step\n",
    "    for seamless UI integration.\n",
    "\n",
    "    Args:\n",
    "        df_close_full (pd.DataFrame): DataFrame of all close prices.\n",
    "        features_df (pd.DataFrame): DataFrame with all features (for ATRP).\n",
    "        manual_tickers (list): The user-provided list of tickers, duplicates allowed.\n",
    "        master_trading_days (pd.DatetimeIndex): The calendar of trading days.\n",
    "        start_date (pd.Timestamp): The desired start date for the analysis.\n",
    "        calc_period (int): The number of days in the calculation period.\n",
    "        fwd_period (int): The number of days in the forward-test period.\n",
    "        benchmark_ticker (str): The ticker symbol for the benchmark.\n",
    "        debug (bool): Flag to generate detailed debug data.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (results_dict, debug_dict) in the same format as run_walk_forward_step.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Sections 1-4 (Date calculation, Ticker validation, Core performance calls) are unchanged ---\n",
    "    debug_data = {} if debug else None\n",
    "    try: start_idx = master_trading_days.get_loc(start_date)\n",
    "    except KeyError: return ({'error': f\"Start date {start_date.date()} is not a valid trading day.\"}, None)\n",
    "    desired_viz_end_idx = start_idx + calc_period + fwd_period\n",
    "    if desired_viz_end_idx >= len(master_trading_days): return ({'error': \"Not enough data for the full requested period.\"}, None)\n",
    "    calc_end_idx = start_idx + calc_period\n",
    "    viz_end_idx = calc_end_idx + fwd_period\n",
    "    safe_start_date, safe_calc_end_date, safe_viz_end_date = master_trading_days[start_idx], master_trading_days[calc_end_idx], master_trading_days[viz_end_idx]\n",
    "    if not manual_tickers: return ({'error': \"The manual ticker list cannot be empty.\"}, None)\n",
    "    unique_tickers_in_list = sorted(list(set(manual_tickers)))\n",
    "    valid_tickers_in_data = [t for t in unique_tickers_in_list if t in df_close_full.columns]\n",
    "    if not valid_tickers_in_data: return ({'error': \"None of the provided tickers were found in the dataset.\"}, None)\n",
    "    tickers_to_display = [t for t in manual_tickers if t in valid_tickers_in_data]\n",
    "    portfolio_series, portfolio_return_series, portfolio_atrp_series = calculate_buy_and_hold_performance(df_close_full, features_df, tickers_to_display, safe_start_date, safe_viz_end_date)\n",
    "    if benchmark_ticker and benchmark_ticker in df_close_full.columns:\n",
    "        benchmark_price_series, benchmark_return_series, benchmark_atrp_series = calculate_buy_and_hold_performance(df_close_full, features_df, [benchmark_ticker], safe_start_date, safe_viz_end_date)\n",
    "    else:\n",
    "        benchmark_price_series, benchmark_return_series, benchmark_atrp_series = (pd.Series(dtype='float64'),)*3\n",
    "    actual_calc_end_ts = safe_calc_end_date\n",
    "    calc_portfolio_returns = portfolio_return_series.loc[:actual_calc_end_ts]; fwd_portfolio_returns = portfolio_return_series.loc[actual_calc_end_ts:].iloc[1:]\n",
    "    calc_portfolio_atrp = portfolio_atrp_series.loc[:actual_calc_end_ts]; fwd_portfolio_atrp = portfolio_atrp_series.loc[actual_calc_end_ts:].iloc[1:]\n",
    "\n",
    "    # --- 5. Calculate final performance metrics ---\n",
    "    perf_data = {}\n",
    "    if not benchmark_price_series.empty:\n",
    "        calc_benchmark_returns = benchmark_return_series.loc[:actual_calc_end_ts]; fwd_benchmark_returns = benchmark_return_series.loc[actual_calc_end_ts:].iloc[1:]\n",
    "        calc_benchmark_atrp = benchmark_atrp_series.loc[:actual_calc_end_ts]; fwd_benchmark_atrp = benchmark_atrp_series.loc[actual_calc_end_ts:].iloc[1:]\n",
    "    else:\n",
    "        calc_benchmark_returns, fwd_benchmark_returns = pd.Series(dtype='float64'), pd.Series(dtype='float64')\n",
    "        calc_benchmark_atrp, fwd_benchmark_atrp = pd.Series(dtype='float64'), pd.Series(dtype='float64')\n",
    "    perf_data['calc_p_gain'] = calculate_gain(portfolio_series.loc[:actual_calc_end_ts])\n",
    "    perf_data['fwd_p_gain'] = calculate_gain(portfolio_series.loc[actual_calc_end_ts:])\n",
    "    perf_data['full_p_gain'] = calculate_gain(portfolio_series)\n",
    "    perf_data['calc_p_sharpe'] = calculate_sharpe(calc_portfolio_returns)\n",
    "    perf_data['fwd_p_sharpe'] = calculate_sharpe(fwd_portfolio_returns)\n",
    "    perf_data['full_p_sharpe'] = calculate_sharpe(portfolio_return_series)\n",
    "    perf_data['calc_b_gain'] = calculate_gain(benchmark_price_series.loc[:actual_calc_end_ts]) if not benchmark_price_series.empty else np.nan\n",
    "    perf_data['fwd_b_gain'] = calculate_gain(benchmark_price_series.loc[actual_calc_end_ts:]) if not benchmark_price_series.empty else np.nan\n",
    "    perf_data['full_b_gain'] = calculate_gain(benchmark_price_series) if not benchmark_price_series.empty else np.nan\n",
    "    perf_data['calc_b_sharpe'] = calculate_sharpe(calc_benchmark_returns)\n",
    "    perf_data['fwd_b_sharpe'] = calculate_sharpe(fwd_benchmark_returns)\n",
    "    perf_data['full_b_sharpe'] = calculate_sharpe(benchmark_return_series)\n",
    "    perf_data['calc_p_sharpe_atr'] = calculate_sharpe_atr(calc_portfolio_returns, calc_portfolio_atrp)\n",
    "    perf_data['fwd_p_sharpe_atr'] = calculate_sharpe_atr(fwd_portfolio_returns, fwd_portfolio_atrp)\n",
    "    perf_data['full_p_sharpe_atr'] = calculate_sharpe_atr(portfolio_return_series, portfolio_atrp_series)\n",
    "    perf_data['calc_b_sharpe_atr'] = calculate_sharpe_atr(calc_benchmark_returns, calc_benchmark_atrp)\n",
    "    perf_data['fwd_b_sharpe_atr'] = calculate_sharpe_atr(fwd_benchmark_returns, fwd_benchmark_atrp)\n",
    "    perf_data['full_b_sharpe_atr'] = calculate_sharpe_atr(benchmark_return_series, benchmark_atrp_series)\n",
    "\n",
    "    # --- Sections 6-8 (results_df, debug data, final dict assembly) ---\n",
    "    from collections import Counter\n",
    "    ticker_counts = Counter(tickers_to_display); total_parts = len(tickers_to_display)\n",
    "    start_prices = df_close_full.loc[actual_calc_end_ts, valid_tickers_in_data]; end_prices = df_close_full.loc[safe_viz_end_date, valid_tickers_in_data]\n",
    "    fwd_gains = (end_prices / start_prices) - 1\n",
    "    results_data = [{'InitialWeight': ticker_counts[ticker] / total_parts, 'FwdGain': fwd_gains.get(ticker, np.nan)} for ticker in valid_tickers_in_data]\n",
    "    results_df = pd.DataFrame(results_data, index=pd.Index(valid_tickers_in_data, name='Ticker'))\n",
    "    normalized_plot_data = df_close_full[valid_tickers_in_data].loc[safe_start_date:safe_viz_end_date]\n",
    "    normalized_plot_data = normalized_plot_data.div(normalized_plot_data.bfill().iloc[0])\n",
    "\n",
    "    # === START: MISSING CODE BLOCK ==============================================\n",
    "    if debug:\n",
    "        # Create the portfolio trace for debugging, similar to the original function\n",
    "        df_trace = normalized_plot_data.copy()\n",
    "        df_trace.columns = [f'Norm_Price_{c}' for c in df_trace.columns]\n",
    "        df_trace['Norm_Price_Portfolio'] = portfolio_series\n",
    "        if benchmark_price_series is not None and not benchmark_price_series.empty:\n",
    "            df_trace[f'Norm_Price_Benchmark_{benchmark_ticker}'] = benchmark_price_series\n",
    "        for col in df_trace.columns:\n",
    "            if 'Norm_Price' in col:\n",
    "                df_trace[col.replace('Norm_Price', 'Return')] = df_trace[col].pct_change()\n",
    "        debug_data['portfolio_trace'] = df_trace\n",
    "        # 'ranking_metrics' is not applicable in this mode\n",
    "        debug_data['ranking_metrics'] = pd.DataFrame() \n",
    "    # === END: MISSING CODE BLOCK ================================================\n",
    "\n",
    "    final_results = {\n",
    "        'tickers_to_display': valid_tickers_in_data,\n",
    "        'normalized_plot_data': normalized_plot_data,\n",
    "        'portfolio_series': portfolio_series,\n",
    "        'benchmark_price_series': benchmark_price_series,\n",
    "        'performance_data': perf_data,\n",
    "        'results_df': results_df,\n",
    "        'actual_calc_end_ts': actual_calc_end_ts,\n",
    "        'safe_start_date': safe_start_date,\n",
    "        'safe_viz_end_date': safe_viz_end_date,\n",
    "        'error': None\n",
    "    }\n",
    "    return (final_results, debug_data)\n",
    "\n",
    "\n",
    "# --- E. VERIFICATION TOOLS (User Requested) ---\n",
    "\n",
    "def verify_group_tickers_walk_forward_calculation(df_ohlcv, tickers_to_verify, benchmark_ticker,\n",
    "                                                  start_date, calc_period, fwd_period,\n",
    "                                                  master_calendar_ticker='VOO', export_csv=False):\n",
    "    \"\"\"Verifies portfolio and benchmark performance and optionally exports the data.\"\"\"\n",
    "    display(Markdown(f\"## Verification Report for Portfolio vs. Benchmark\"))\n",
    "    display(Markdown(f\"**Portfolio Tickers:** `{tickers_to_verify}`\\n**Benchmark Ticker:** `{benchmark_ticker}`\"))\n",
    "    \n",
    "    # 1. Setup trading day calendar and determine exact period dates\n",
    "    if master_calendar_ticker not in df_ohlcv.index.get_level_values(0):\n",
    "        raise ValueError(f\"Master calendar ticker '{master_calendar_ticker}' not found in DataFrame.\")\n",
    "    master_trading_days = df_ohlcv.loc[master_calendar_ticker].index.unique().sort_values()\n",
    "\n",
    "    start_date_raw = pd.to_datetime(start_date)\n",
    "    start_idx = master_trading_days.searchsorted(start_date_raw)\n",
    "    if start_idx >= len(master_trading_days):\n",
    "        print(f\"Error: Start date {start_date_raw.date()} is after the last available trading day.\"); return\n",
    "    actual_start_date = master_trading_days[start_idx]\n",
    "    \n",
    "    calc_end_idx = min(start_idx + calc_period, len(master_trading_days) - 1)\n",
    "    fwd_end_idx = min(calc_end_idx + fwd_period, len(master_trading_days) - 1)\n",
    "    \n",
    "    actual_calc_end_date = master_trading_days[calc_end_idx]\n",
    "    actual_fwd_end_date = master_trading_days[fwd_end_idx]\n",
    "    \n",
    "    display(Markdown(f\"**Analysis Start:** `{actual_start_date.date()}` (Selected: `{start_date_raw.date()}`)\\n\"\n",
    "                    f\"**Calc End:** `{actual_calc_end_date.date()}` ({calc_period} trading days)\\n\"\n",
    "                    f\"**Fwd End:** `{actual_fwd_end_date.date()}` ({fwd_period} trading days)\"))\n",
    "\n",
    "    # 2. Recreate the portfolio and benchmark series from scratch\n",
    "    df_close_full = df_ohlcv['Adj Close'].unstack(level=0)\n",
    "    portfolio_prices_raw_slice = df_close_full[tickers_to_verify].loc[actual_start_date:actual_fwd_end_date]\n",
    "    portfolio_value_series = portfolio_prices_raw_slice.div(portfolio_prices_raw_slice.bfill().iloc[0]).mean(axis=1)\n",
    "    benchmark_price_series = df_close_full.get(benchmark_ticker)\n",
    "    \n",
    "    # 3. Optionally export the underlying daily data to a CSV for external checking\n",
    "    if export_csv:\n",
    "        export_df = pd.DataFrame({\n",
    "            'Portfolio_Normalized_Price': portfolio_value_series,\n",
    "            'Portfolio_Daily_Return': portfolio_value_series.pct_change()\n",
    "        })\n",
    "        if benchmark_price_series is not None:\n",
    "            norm_bm = benchmark_price_series.loc[actual_start_date:actual_fwd_end_date]\n",
    "            norm_bm = norm_bm / norm_bm.bfill().iloc[0]\n",
    "            export_df['Benchmark_Normalized_Price'] = norm_bm\n",
    "            export_df['Benchmark_Daily_Return'] = norm_bm.pct_change()\n",
    "\n",
    "        output_dir = 'export_csv'\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        tickers_str = '_'.join(tickers_to_verify)\n",
    "        filename = f\"verify_group_{actual_start_date.date()}_{tickers_str}.csv\"\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "        export_df.to_csv(filepath)\n",
    "        print(f\"\\n‚úÖ Data exported to: {filepath}\")\n",
    "\n",
    "    # 4. Define a helper to print detailed calculation steps\n",
    "    def print_verification_steps(title, price_series):\n",
    "        display(Markdown(f\"#### Verification for: `{title}`\"))\n",
    "        if price_series.dropna().shape[0] < 2: print(\"  - Not enough data points.\"); return {'gain': np.nan, 'sharpe': np.nan}\n",
    "        start_price, end_price = price_series.bfill().iloc[0], price_series.ffill().iloc[-1]\n",
    "        gain = (end_price / start_price) - 1\n",
    "        print(f\"Start Value ({price_series.first_valid_index().date()}): {start_price:,.4f}\\nEnd Value   ({price_series.last_valid_index().date()}): {end_price:,.4f}\\nGain = {gain:.2%}\")\n",
    "        returns = price_series.pct_change()\n",
    "        sharpe = calculate_sharpe(returns)\n",
    "        print(f\"Mean Daily Return: {returns.mean():.6f}\\nStd Dev: {returns.std():.6f}\\nSharpe = {sharpe:.2f}\")\n",
    "        return {'gain': gain, 'sharpe': sharpe}\n",
    "\n",
    "    # 5. Run verification for each period\n",
    "    display(Markdown(\"### A. Calculation Period\"))\n",
    "    perf_calc_p = print_verification_steps(\"Group Portfolio\", portfolio_value_series.loc[actual_start_date:actual_calc_end_date])\n",
    "    if benchmark_price_series is not None:\n",
    "        perf_calc_b = print_verification_steps(f\"Benchmark\", benchmark_price_series.loc[actual_start_date:actual_calc_end_date])\n",
    "    \n",
    "    display(Markdown(\"### B. Forward Period\"))\n",
    "    perf_fwd_p = print_verification_steps(\"Group Portfolio\", portfolio_value_series.loc[actual_calc_end_date:actual_fwd_end_date])\n",
    "    if benchmark_price_series is not None:\n",
    "        perf_fwd_b = print_verification_steps(f\"Benchmark\", benchmark_price_series.loc[actual_calc_end_date:actual_fwd_end_date])\n",
    "\n",
    "\n",
    "def verify_ticker_ranking_metrics(df_ohlcv, ticker, start_date, calc_period,\n",
    "                                  master_calendar_ticker='VOO', export_csv=False):\n",
    "    \"\"\"Verifies ranking metrics for a single ticker and optionally exports the data.\"\"\"\n",
    "    display(Markdown(f\"## Verification Report for Ticker Ranking: `{ticker}`\"))\n",
    "    \n",
    "    # 1. Setup trading day calendar and determine exact period dates\n",
    "    if master_calendar_ticker not in df_ohlcv.index.get_level_values(0):\n",
    "        raise ValueError(f\"Master calendar ticker '{master_calendar_ticker}' not found in DataFrame.\")\n",
    "    master_trading_days = df_ohlcv.loc[master_calendar_ticker].index.unique().sort_values()\n",
    "\n",
    "    start_date_raw = pd.to_datetime(start_date)\n",
    "    start_idx = master_trading_days.searchsorted(start_date_raw)\n",
    "    if start_idx >= len(master_trading_days):\n",
    "        print(f\"Error: Start date {start_date_raw.date()} is after the last available trading day.\"); return\n",
    "    actual_start_date = master_trading_days[start_idx]\n",
    "    \n",
    "    calc_end_idx = min(start_idx + calc_period, len(master_trading_days) - 1)\n",
    "    actual_calc_end_date = master_trading_days[calc_end_idx]\n",
    "\n",
    "    # 2. Extract and prepare the raw data for the specific ticker and period\n",
    "    df_ticker = df_ohlcv.loc[ticker].sort_index()\n",
    "    calc_df = df_ticker.loc[actual_start_date:actual_calc_end_date].copy()\n",
    "    if calc_df.empty or len(calc_df) < 2: \n",
    "        print(\"No data or not enough data in calc period.\"); return\n",
    "\n",
    "    display(Markdown(\"### A. Calculation Period (for Ranking Metrics)\"))\n",
    "    display(Markdown(f\"**Period Start:** `{actual_start_date.date()}`\\n\"\n",
    "                    f\"**Period End:** `{actual_calc_end_date.date()}`\\n\"\n",
    "                    f\"**Total Trading Days:** `{len(calc_df)}` (Requested: `{calc_period}`)\"))\n",
    "    \n",
    "    display(Markdown(\"#### Detailed Metric Calculation Data\"))\n",
    "    \n",
    "    # 3. Calculate all intermediate metrics as new columns for full transparency\n",
    "    vdf = calc_df[['Adj High', 'Adj Low', 'Adj Close']].copy()\n",
    "    vdf['Daily_Return'] = vdf['Adj Close'].pct_change()\n",
    "    \n",
    "    # Corrected True Range (TR) calculation for a single ticker (Series)\n",
    "    tr_df = pd.DataFrame({\n",
    "        'h_l': vdf['Adj High'] - vdf['Adj Low'],\n",
    "        'h_cp': abs(vdf['Adj High'] - vdf['Adj Close'].shift(1)),\n",
    "        'l_cp': abs(vdf['Adj Low'] - vdf['Adj Close'].shift(1))\n",
    "    })\n",
    "    vdf['TR'] = tr_df.max(axis=1)\n",
    "    \n",
    "    vdf['ATR_14'] = vdf['TR'].ewm(alpha=1/14, adjust=False).mean()\n",
    "    vdf['ATRP'] = vdf['ATR_14'] / vdf['Adj Close']\n",
    "    \n",
    "    print(\"--- Start of Calculation Period ---\")\n",
    "    display(vdf.head())\n",
    "    print(\"\\n--- End of Calculation Period ---\")\n",
    "    display(vdf.tail())\n",
    "\n",
    "    # 4. Optionally export this detailed breakdown to CSV\n",
    "    if export_csv:\n",
    "        output_dir = 'export_csv'\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        filename = f\"verify_ticker_{actual_start_date.date()}_{ticker}.csv\"\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "        vdf.to_csv(filepath)\n",
    "        print(f\"\\n‚úÖ Data exported to: {filepath}\")\n",
    "    \n",
    "    # 5. Print final metric calculations with formulas\n",
    "    display(Markdown(\"#### `MetricValue` Verification Summary:\"))\n",
    "    \n",
    "    calc_start_price = vdf['Adj Close'].bfill().iloc[0]\n",
    "    calc_end_price = vdf['Adj Close'].ffill().iloc[-1]\n",
    "    price_metric = (calc_end_price / calc_start_price)\n",
    "    print(f\"1. Price Metric: (Last Price / First Price) = ({calc_end_price:.2f} / {calc_start_price:.2f}) = {price_metric:.4f}\")\n",
    "    \n",
    "    daily_returns = vdf['Daily_Return'].dropna()\n",
    "    sharpe_ratio = calculate_sharpe(daily_returns)\n",
    "    print(f\"2. Sharpe Metric: (Mean Daily Return / Std Dev) * sqrt(252) = {sharpe_ratio:.4f}\")\n",
    "\n",
    "    atrp_mean = vdf['ATRP'].mean()\n",
    "    mean_daily_return = vdf['Daily_Return'].mean()\n",
    "    sharpe_atr = (mean_daily_return / atrp_mean) if atrp_mean > 0 else 0\n",
    "    print(f\"3. Sharpe (ATR) Metric: (Mean Daily Return / Mean ATRP) = ({mean_daily_return:.6f} / {atrp_mean:.6f}) = {sharpe_atr:.4f}\")\n",
    "\n",
    "\n",
    "def verify_sharpe_atr_calculation_checked(df_ohlcv, features_df, tickers_to_verify, benchmark_ticker,\n",
    "                                    start_date, calc_period, fwd_period,\n",
    "                                    master_calendar_ticker='VOO', debug=False):\n",
    "    \"\"\"\n",
    "    Verifies the Sharpe (ATR) calculations for a portfolio and benchmark.\n",
    "\n",
    "    [MODIFIED to use the centralized performance function and trace the new value-weighted ATRP logic]\n",
    "    \"\"\"\n",
    "    # Assuming display and Markdown are imported from IPython.display\n",
    "    from IPython.display import display, Markdown\n",
    "    \n",
    "    display(Markdown(f\"## Verification Report for Sharpe (ATR) Calculation\"))\n",
    "    display(Markdown(f\"**Portfolio Tickers:** `{tickers_to_verify}`\\n**Benchmark Ticker:** `{benchmark_ticker}`\"))\n",
    "\n",
    "    # --- 1. Determine Exact Period Dates (No changes) ---\n",
    "    master_trading_days = df_ohlcv.loc[master_calendar_ticker].index.unique().sort_values()\n",
    "    start_date_raw = pd.to_datetime(start_date)\n",
    "    start_idx = master_trading_days.searchsorted(start_date_raw)\n",
    "    actual_start_date = master_trading_days[start_idx]\n",
    "    calc_end_idx = min(start_idx + calc_period, len(master_trading_days) - 1)\n",
    "    fwd_end_idx = min(calc_end_idx + fwd_period, len(master_trading_days) - 1)\n",
    "    actual_calc_end_date = master_trading_days[calc_end_idx]\n",
    "    actual_fwd_end_date = master_trading_days[fwd_end_idx]\n",
    "    \n",
    "    display(Markdown(f\"**Full Period:** `{actual_start_date.date()}` to `{actual_fwd_end_date.date()}`\\n\"\n",
    "                    f\"**Calc End Date:** `{actual_calc_end_date.date()}`\"))\n",
    "\n",
    "    # --- 2. Recreate Portfolio & Benchmark Series using the Centralized Function ---\n",
    "    df_close_full = df_ohlcv['Adj Close'].unstack(level=0)\n",
    "\n",
    "    # Use the central function for both portfolio and benchmark\n",
    "    portfolio_value_series, portfolio_return_series, portfolio_atrp_series = \\\n",
    "        calculate_buy_and_hold_performance(df_close_full, features_df, tickers_to_verify,\n",
    "                                           actual_start_date, actual_fwd_end_date)\n",
    "    \n",
    "    _, benchmark_return_series, benchmark_atrp_series = \\\n",
    "        calculate_buy_and_hold_performance(df_close_full, features_df, [benchmark_ticker],\n",
    "                                           actual_start_date, actual_fwd_end_date)\n",
    "\n",
    "    # --- 3. DETAILED DEBUG TRACE ---\n",
    "    # We re-calculate intermediate steps here ONLY for the purpose of detailed printing.\n",
    "    # The final series used in the analysis below come from the trusted central function.\n",
    "    if debug:\n",
    "        display(Markdown(\"---\"))\n",
    "        # --- RETURN TRACE (largely unchanged) ---\n",
    "        display(Markdown(\"### üêõ Detailed Portfolio Return Calculation Trace\"))\n",
    "        portfolio_prices_raw = df_close_full[tickers_to_verify].loc[actual_start_date:actual_fwd_end_date]\n",
    "        normalization_base = portfolio_prices_raw.bfill().iloc[0]\n",
    "        portfolio_prices_norm = portfolio_prices_raw.div(normalization_base)\n",
    "        \n",
    "        print(\"\\n[STEP 1] Raw Adjusted Close prices.\")\n",
    "        display(portfolio_prices_raw)\n",
    "        print(\"\\n[STEP 2] Normalization base (first valid row of prices).\")\n",
    "        display(pd.DataFrame(normalization_base).T)\n",
    "        print(\"\\n[STEP 2a] Normalized prices (each stock's value from an initial $1 investment).\")\n",
    "        display(portfolio_prices_norm)\n",
    "        print(\"\\n[STEP 3] Averaged normalized portfolio value series.\")\n",
    "        display(pd.DataFrame(portfolio_value_series, columns=['N_portf_value']))\n",
    "        print(\"\\n[STEP 4] Final portfolio daily return series (pct_change of Step 3).\")\n",
    "        display(pd.DataFrame(portfolio_return_series, columns=['N_portf_rtn']))\n",
    "        \n",
    "        # +++ NEW: DETAILED ATRP CALCULATION TRACE +++\n",
    "        display(Markdown(\"### üêõ Detailed Portfolio ATRP Calculation Trace (Value-Weighted)\"))\n",
    "        p_idx = pd.MultiIndex.from_product([tickers_to_verify, portfolio_return_series.index])\n",
    "        p_atrp_df = features_df.loc[features_df.index.intersection(p_idx)]['ATRP'].unstack(level=0)\n",
    "        \n",
    "        daily_total_value = portfolio_prices_norm.sum(axis=1)\n",
    "        daily_weights = portfolio_prices_norm.div(daily_total_value, axis=0)\n",
    "\n",
    "        print(\"\\n[STEP 5] Individual component ATRP values for each stock.\")\n",
    "        print(\"This is the raw risk input for each component.\")\n",
    "        display(p_atrp_df)\n",
    "\n",
    "        print(\"\\n[STEP 6] Drifting daily portfolio weights (based on market value).\")\n",
    "        print(\"Note how weights start near equal and drift over time. This is the core of the buy-and-hold logic.\")\n",
    "        display(daily_weights)\n",
    "        \n",
    "        print(\"\\n[STEP 7] Final value-weighted portfolio ATRP series.\")\n",
    "        print(\"Result of (Weights * Component_ATRPs) summed each day.\")\n",
    "        display(pd.DataFrame(portfolio_atrp_series, columns=['value_weighted_atrp']))\n",
    "        display(Markdown(\"---\"))\n",
    "    # +++ END OF DEBUG BLOCK +++\n",
    "\n",
    "    # --- 4. Define a Helper to Print Detailed Calculation Steps (Unchanged) ---\n",
    "    def _calculate_and_print_metrics(period_name, returns, atrps):\n",
    "        display(Markdown(f\"#### {period_name}\"))\n",
    "        if returns.dropna().empty or atrps.dropna().empty:\n",
    "            print(\"  - Not enough data to calculate.\")\n",
    "            return np.nan\n",
    "        mean_return = returns.mean()\n",
    "        mean_atrp = atrps.mean()\n",
    "        sharpe_atr = mean_return / mean_atrp if mean_atrp > 0 else np.nan\n",
    "        if debug:\n",
    "            # ... (debug printouts for mean calculation) ...\n",
    "            pass\n",
    "        print(f\"  - Mean Daily Return: {mean_return:,.6f}\")\n",
    "        print(f\"  - Mean Daily ATRP:  {mean_atrp:,.6f}\")\n",
    "        print(f\"  - Sharpe (ATR) = (Mean Return / Mean ATRP) = {sharpe_atr:,.4f}\")\n",
    "        return sharpe_atr\n",
    "\n",
    "    # --- 5. Run Verification for Portfolio (Unchanged) ---\n",
    "    display(Markdown(\"### A. Group Portfolio Verification\"))\n",
    "    _calculate_and_print_metrics(\"Full Period\", portfolio_return_series, portfolio_atrp_series)\n",
    "    _calculate_and_print_metrics(\"Calculation Period\", portfolio_return_series.loc[:actual_calc_end_date], portfolio_atrp_series.loc[:actual_calc_end_date])\n",
    "    _calculate_and_print_metrics(\"Forward Period\", portfolio_return_series.loc[actual_calc_end_date:].iloc[1:], portfolio_atrp_series.loc[actual_calc_end_date:].iloc[1:])\n",
    "\n",
    "    # --- 6. Run Verification for Benchmark (Unchanged) ---\n",
    "    display(Markdown(f\"### B. Benchmark ({benchmark_ticker}) Verification\"))\n",
    "    _calculate_and_print_metrics(\"Full Period\", benchmark_return_series, benchmark_atrp_series)\n",
    "    _calculate_and_print_metrics(\"Calculation Period\", benchmark_return_series.loc[:actual_calc_end_date], benchmark_atrp_series.loc[:actual_calc_end_date])\n",
    "    _calculate_and_print_metrics(\"Forward Period\", benchmark_return_series.loc[actual_calc_end_date:].iloc[1:], benchmark_atrp_series.loc[actual_calc_end_date:].iloc[1:])\n",
    "\n",
    "\n",
    "# --- F. AUTOMATION SCRIPT - STRATEGY SEARCH ---\n",
    "\n",
    "def run_strategy_search(df_ohlcv, config):\n",
    "    \"\"\"\n",
    "    Runs the main backtesting loop with checkpointing to be resumable.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # --- 1. SETUP & LOAD PROGRESS ---\n",
    "    print(\"--- Phase 1: Pre-processing and Loading Progress ---\")\n",
    "\n",
    "    # +++ ADD THIS BLOCK +++\n",
    "    # Pre-calculate all features for the entire dataset ONCE.\n",
    "    print(\"--- Generating all features upfront... ---\")\n",
    "    features_df = generate_features(df_ohlcv)\n",
    "\n",
    "    # --- DELETE THIS LINE ---\n",
    "    # quality_metrics_df = calculate_rolling_quality_metrics(df_ohlcv, window=252)\n",
    "    \n",
    "    print(\"Unstacking data for performance...\")\n",
    "    df_close_full = df_ohlcv['Adj Close'].unstack(level=0)\n",
    "    df_high_full = df_ohlcv['Adj High'].unstack(level=0)\n",
    "    df_low_full = df_ohlcv['Adj Low'].unstack(level=0)\n",
    "    \n",
    "    master_calendar_ticker = config['master_calendar_ticker']\n",
    "    master_trading_days = df_ohlcv.loc[master_calendar_ticker].index.unique().sort_values()\n",
    "    print(f\"Master trading day calendar created from '{master_calendar_ticker}' ({len(master_trading_days)} days).\")\n",
    "\n",
    "    results_path = config['results_output_path']\n",
    "    completed_params = set()\n",
    "    \n",
    "    if os.path.exists(results_path):\n",
    "        print(f\"Found existing results file. Loading progress from: {results_path}\")\n",
    "        df_progress = pd.read_csv(results_path)\n",
    "        for _, row in df_progress.iterrows():\n",
    "            param_key = (\n",
    "                row['calc_period'], row['fwd_period'], row['metric'],\n",
    "                (row['rank_start'], row['rank_end'])\n",
    "            )\n",
    "            completed_params.add(param_key)\n",
    "        print(f\"Found {len(completed_params)} completed parameter sets to skip.\")\n",
    "    else:\n",
    "        print(\"No existing results file found. Starting a new run.\")\n",
    "\n",
    "    print(\"‚úÖ Pre-processing complete.\\n\")\n",
    "\n",
    "    # --- 2. SETUP THE MAIN LOOP (No changes here) ---\n",
    "    print(\"--- Phase 2: Setting up Simulation Loops ---\")\n",
    "    param_combinations = list(product(\n",
    "        config['calc_periods'], config['fwd_periods'],\n",
    "        config['metrics'], config['rank_slices']\n",
    "    ))\n",
    "    search_start_date = pd.to_datetime(config['search_start_date'])\n",
    "    search_end_date = pd.to_datetime(config['search_end_date'])\n",
    "    start_idx = master_trading_days.searchsorted(search_start_date, side='left')\n",
    "    end_idx = master_trading_days.searchsorted(search_end_date, side='right')\n",
    "    step_dates_map = {}\n",
    "    print(\"Pre-calculating rebalancing schedules for each holding period...\")\n",
    "    for fwd_period in sorted(config['fwd_periods']):\n",
    "        step_indices = range(start_idx, end_idx, fwd_period)\n",
    "        step_dates_map[fwd_period] = master_trading_days[step_indices]\n",
    "        print(f\"  - Holding Period {fwd_period} days: {len(step_dates_map[fwd_period])} rebalances\")\n",
    "    print(f\"Found {len(param_combinations)} total parameter sets to simulate.\")\n",
    "    print(\"‚úÖ Setup complete. Starting main loop...\\n\")\n",
    "\n",
    "    # --- 3. RUN THE MAIN LOOP ---\n",
    "    print(\"--- Phase 3: Running Simulations ---\")\n",
    "    pbar = tqdm(param_combinations, desc=\"Parameter Sets\")\n",
    "    \n",
    "    for params in pbar:\n",
    "        calc_period, fwd_period, metric, rank_slice = params\n",
    "        rank_start, rank_end = rank_slice\n",
    "        \n",
    "        param_key = (calc_period, fwd_period, metric, rank_slice)\n",
    "        if param_key in completed_params:\n",
    "            pbar.set_description(f\"Skipping {param_key}\")\n",
    "            continue\n",
    "\n",
    "        pbar.set_description(f\"Running {param_key}\")\n",
    "        \n",
    "        current_params_results = []\n",
    "        \n",
    "        current_step_dates = step_dates_map[fwd_period]\n",
    "        for step_date in current_step_dates:\n",
    "            # +++ UPDATE THIS CALL +++\n",
    "            eligible_tickers = get_eligible_universe(\n",
    "                features_df, filter_date=step_date, thresholds=config['quality_thresholds']\n",
    "            )\n",
    "            if not eligible_tickers: continue\n",
    "            \n",
    "            df_close_step = df_close_full[eligible_tickers]\n",
    "            df_high_step = df_high_full[eligible_tickers]\n",
    "            df_low_step = df_low_full[eligible_tickers]\n",
    "\n",
    "            # +++ UPDATE THIS CALL +++\n",
    "            step_result, _ = run_walk_forward_step(\n",
    "                df_close_full=df_close_step, df_high_full=df_high_step, df_low_full=df_low_step,\n",
    "                master_trading_days=master_trading_days, start_date=step_date,\n",
    "                calc_period=calc_period, fwd_period=fwd_period,\n",
    "                metric=metric, rank_start=rank_start, rank_end=rank_end,\n",
    "                benchmark_ticker=config['benchmark_ticker'],\n",
    "                features_df=features_df, # Pass the features_df\n",
    "                debug=False\n",
    "            )\n",
    "            \n",
    "            if step_result['error'] is None:\n",
    "                p = step_result['performance_data']\n",
    "                log_entry = {\n",
    "                    'step_date': step_date.date(), 'calc_period': calc_period,\n",
    "                    'fwd_period': fwd_period, 'metric': metric,\n",
    "                    'rank_start': rank_start, 'rank_end': rank_end,\n",
    "                    'num_universe': len(eligible_tickers),\n",
    "                    'num_portfolio': len(step_result['tickers_to_display']),\n",
    "                    'fwd_p_gain': p['fwd_p_gain'], 'fwd_b_gain': p['fwd_b_gain'],\n",
    "                    'fwd_gain_delta': p['fwd_p_gain'] - p['fwd_b_gain'] if not np.isnan(p['fwd_b_gain']) else np.nan,\n",
    "                    'fwd_p_sharpe': p['fwd_p_sharpe'],\n",
    "                }\n",
    "                current_params_results.append(log_entry)\n",
    "        \n",
    "        # --- CHECKPOINTING (No changes here) ---\n",
    "        if current_params_results:\n",
    "            df_to_append = pd.DataFrame(current_params_results)\n",
    "            df_to_append.to_csv(\n",
    "                results_path,\n",
    "                mode='a',\n",
    "                header=not os.path.exists(results_path),\n",
    "                index=False\n",
    "            )\n",
    "            completed_params.add(param_key)\n",
    "\n",
    "    print(\"‚úÖ Main loop finished.\\n\")\n",
    "    \n",
    "    # --- 4. RETURN FINAL DATAFRAME (No changes here) ---\n",
    "    print(\"--- Phase 4: Loading Final Results ---\")\n",
    "    if os.path.exists(results_path):\n",
    "        final_df = pd.read_csv(results_path)\n",
    "        end_time = time.time()\n",
    "        print(f\"‚úÖ Process complete. Total execution time: {time.time() - start_time:.2f} seconds.\")\n",
    "        return final_df\n",
    "    else:\n",
    "        print(\"Warning: No results were generated.\")\n",
    "        return None    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73c4b65",
   "metadata": {},
   "source": [
    "### CALCULATION VERIFIED \"Group Portfolio Sharpe (ATR) calculation for RCL, RCL, STIP\"\n",
    "### TODO: run_manual_portfolio_step  \n",
    "- debug is not returning data\n",
    "- merge with older version to get back comments\n",
    "- move ticker textbox input to the right\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0151d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_container, debug_container = plot_walk_forward_analyzer(\n",
    "    df_ohlcv=df_ohlcv,\n",
    "    default_start_date='2025-08-13',\n",
    "    default_calc_period=10,\n",
    "    default_fwd_period=5,\n",
    "    default_metric='Sharpe (ATR)',\n",
    "    default_rank_start=15,\n",
    "    default_rank_end=16,\n",
    "    default_benchmark_ticker='VOO', \n",
    "    master_calendar_ticker='VOO',\n",
    "    quality_thresholds={ 'min_median_dollar_volume': 10_000_000, \n",
    "                         'max_stale_pct': 0.05, \n",
    "                         'max_same_vol_count': 1 },\n",
    "    debug=True  # <-- Activate the new mode!\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676bca01",
   "metadata": {},
   "source": [
    "#####################################################    \n",
    "#####################################################    \n",
    "#####################################################    \n",
    "#####################################################    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf4c6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3320468b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_nested(results_container)\n",
    "print(f'\\n{\"=\" * 20}\\n')\n",
    "print_nested(debug_container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f365ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_group_tickers_walk_forward_calculation(df_ohlcv=df_ohlcv,\n",
    "                                              tickers_to_verify=['B', 'NVDA', 'B'],\n",
    "                                              benchmark_ticker='VOO',\n",
    "                                              start_date='2025-11-05',\n",
    "                                              calc_period=10,\n",
    "                                              fwd_period=5,\n",
    "                                              master_calendar_ticker='VOO',\n",
    "                                              export_csv=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2ea73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_ticker_ranking_metrics(df_ohlcv, \n",
    "                              ticker='B',\n",
    "                              start_date='2025-11-05',\n",
    "                              calc_period=10,\n",
    "                              master_calendar_ticker='VOO', \n",
    "                              export_csv=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62b8048",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.loc['NVDA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836c2b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = generate_features(df_ohlcv=df_ohlcv, \n",
    "                                atr_period=14, \n",
    "                                quality_window=252, \n",
    "                                quality_min_periods=126)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cc9b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.loc['B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b1c05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_sharpe_atr_calculation_checked(df_ohlcv=df_ohlcv, \n",
    "                                      features_df=features_df, \n",
    "                                      tickers_to_verify=['B', 'NVDA', 'B'],\n",
    "                                      benchmark_ticker='VOO',\n",
    "                                      start_date='2025-11-05',\n",
    "                                      calc_period=10,\n",
    "                                      fwd_period=5,\n",
    "                                      master_calendar_ticker='VOO', \n",
    "                                      debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c5f2f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93b73a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f0327f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce80892c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421e1a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_manual_portfolio_step(df_close_full, features_df,\n",
    "                              manual_tickers,\n",
    "                              master_trading_days,\n",
    "                              start_date, calc_period, fwd_period,\n",
    "                              benchmark_ticker,\n",
    "                              debug=False):\n",
    "    \"\"\"\n",
    "    Runs a performance analysis for a manually specified portfolio of tickers.\n",
    "\n",
    "    Bypasses all ranking and filtering logic, directly calculating performance\n",
    "    for the provided list. The output format is identical to run_walk_forward_step\n",
    "    for seamless UI integration.\n",
    "\n",
    "    Args:\n",
    "        df_close_full (pd.DataFrame): DataFrame of all close prices.\n",
    "        features_df (pd.DataFrame): DataFrame with all features (for ATRP).\n",
    "        manual_tickers (list): The user-provided list of tickers, duplicates allowed.\n",
    "        master_trading_days (pd.DatetimeIndex): The calendar of trading days.\n",
    "        start_date (pd.Timestamp): The desired start date for the analysis.\n",
    "        calc_period (int): The number of days in the calculation period.\n",
    "        fwd_period (int): The number of days in the forward-test period.\n",
    "        benchmark_ticker (str): The ticker symbol for the benchmark.\n",
    "        debug (bool): Flag to generate detailed debug data.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (results_dict, debug_dict) in the same format as run_walk_forward_step.\n",
    "    \"\"\"\n",
    "    debug_data = {} if debug else None\n",
    "\n",
    "    # 1. Determine exact date ranges (identical logic to run_walk_forward_step)\n",
    "    try:\n",
    "        start_idx = master_trading_days.get_loc(start_date)\n",
    "    except KeyError:\n",
    "        return ({'error': f\"Start date {start_date.date()} is not a valid trading day.\"}, None)\n",
    "\n",
    "    desired_viz_end_idx = start_idx + calc_period + fwd_period\n",
    "    if desired_viz_end_idx >= len(master_trading_days):\n",
    "        return ({'error': \"Not enough data for the full requested period.\"}, None)\n",
    "\n",
    "    calc_end_idx = start_idx + calc_period\n",
    "    viz_end_idx = calc_end_idx + fwd_period\n",
    "    safe_start_date = master_trading_days[start_idx]\n",
    "    safe_calc_end_date = master_trading_days[calc_end_idx]\n",
    "    safe_viz_end_date = master_trading_days[viz_end_idx]\n",
    "\n",
    "    # 2. Validate and process the manual ticker list\n",
    "    if not manual_tickers:\n",
    "        return ({'error': \"The manual ticker list cannot be empty.\"}, None)\n",
    "        \n",
    "    unique_tickers_in_list = sorted(list(set(manual_tickers)))\n",
    "    valid_tickers_in_data = [t for t in unique_tickers_in_list if t in df_close_full.columns]\n",
    "    \n",
    "    if not valid_tickers_in_data:\n",
    "        return ({'error': \"None of the provided tickers were found in the dataset.\"}, None)\n",
    "\n",
    "    # Filter the manual list to only include tickers that exist in our data\n",
    "    tickers_to_display = [t for t in manual_tickers if t in valid_tickers_in_data]\n",
    "\n",
    "    # 3. Calculate Portfolio & Benchmark Performance using the central function\n",
    "    portfolio_series, portfolio_return_series, portfolio_atrp_series = \\\n",
    "        calculate_buy_and_hold_performance(df_close_full, features_df, tickers_to_display,\n",
    "                                           safe_start_date, safe_viz_end_date)\n",
    "\n",
    "    if benchmark_ticker and benchmark_ticker in df_close_full.columns:\n",
    "        benchmark_price_series, benchmark_return_series, benchmark_atrp_series = \\\n",
    "            calculate_buy_and_hold_performance(df_close_full, features_df, [benchmark_ticker],\n",
    "                                               safe_start_date, safe_viz_end_date)\n",
    "    else:\n",
    "        benchmark_price_series, benchmark_return_series, benchmark_atrp_series = (pd.Series(dtype='float64'),)*3\n",
    "\n",
    "    # 4. Split series into calc and fwd periods\n",
    "    actual_calc_end_ts = safe_calc_end_date\n",
    "    # This block is identical to the one in run_walk_forward_step\n",
    "    calc_portfolio_returns = portfolio_return_series.loc[:actual_calc_end_ts]\n",
    "    fwd_portfolio_returns = portfolio_return_series.loc[actual_calc_end_ts:].iloc[1:]\n",
    "    calc_portfolio_atrp = portfolio_atrp_series.loc[:actual_calc_end_ts]\n",
    "    fwd_portfolio_atrp = portfolio_atrp_series.loc[actual_calc_end_ts:].iloc[1:]\n",
    "    # ... and so on for benchmark series\n",
    "    if not benchmark_return_series.empty:\n",
    "        calc_benchmark_returns = benchmark_return_series.loc[:actual_calc_end_ts]\n",
    "        fwd_benchmark_returns = benchmark_return_series.loc[actual_calc_end_ts:].iloc[1:]\n",
    "        calc_benchmark_atrp = benchmark_atrp_series.loc[:actual_calc_end_ts]\n",
    "        fwd_benchmark_atrp = benchmark_atrp_series.loc[actual_calc_end_ts:].iloc[1:]\n",
    "    else:\n",
    "        calc_benchmark_returns, fwd_benchmark_returns = pd.Series(dtype='float64'), pd.Series(dtype='float64')\n",
    "        calc_benchmark_atrp, fwd_benchmark_atrp = pd.Series(dtype='float64'), pd.Series(dtype='float64')\n",
    "\n",
    "    # 5. Calculate final performance metrics\n",
    "    perf_data = {}\n",
    "    # This block is also identical\n",
    "    perf_data['calc_p_gain'] = calculate_gain(portfolio_series.loc[:actual_calc_end_ts])\n",
    "    perf_data['fwd_p_gain'] = calculate_gain(portfolio_series.loc[actual_calc_end_ts:])\n",
    "    perf_data['full_p_gain'] = calculate_gain(portfolio_series)\n",
    "    # ... (all other perf_data calculations for portfolio and benchmark) ...\n",
    "    perf_data['full_p_sharpe_atr'] = calculate_sharpe_atr(portfolio_return_series, portfolio_atrp_series)\n",
    "    perf_data['full_b_sharpe_atr'] = calculate_sharpe_atr(benchmark_return_series, benchmark_atrp_series)\n",
    "    \n",
    "    # 6. Prepare `results_df` (simplified for manual mode)\n",
    "    from collections import Counter\n",
    "    ticker_counts = Counter(tickers_to_display)\n",
    "    total_parts = len(tickers_to_display)\n",
    "    \n",
    "    # Calculate individual forward gains for the results table\n",
    "    start_prices = df_close_full.loc[actual_calc_end_ts, valid_tickers_in_data]\n",
    "    end_prices = df_close_full.loc[safe_viz_end_date, valid_tickers_in_data]\n",
    "    fwd_gains = (end_prices / start_prices) - 1\n",
    "\n",
    "    results_data = []\n",
    "    for ticker in valid_tickers_in_data:\n",
    "        results_data.append({\n",
    "            'InitialWeight': ticker_counts[ticker] / total_parts,\n",
    "            'FwdGain': fwd_gains.get(ticker, np.nan)\n",
    "        })\n",
    "    results_df = pd.DataFrame(results_data, index=pd.Index(valid_tickers_in_data, name='Ticker'))\n",
    "    \n",
    "    # 7. Prepare other outputs for consistency\n",
    "    normalized_plot_data = df_close_full[valid_tickers_in_data].loc[safe_start_date:safe_viz_end_date]\n",
    "    normalized_plot_data = normalized_plot_data.div(normalized_plot_data.bfill().iloc[0])\n",
    "\n",
    "    if debug:\n",
    "        # Create the portfolio trace for debugging, similar to the original function\n",
    "        df_trace = normalized_plot_data.copy()\n",
    "        df_trace.columns = [f'Norm_Price_{c}' for c in df_trace.columns]\n",
    "        df_trace['Norm_Price_Portfolio'] = portfolio_series\n",
    "        if benchmark_price_series is not None and not benchmark_price_series.empty:\n",
    "            df_trace[f'Norm_Price_Benchmark_{benchmark_ticker}'] = benchmark_price_series\n",
    "        for col in df_trace.columns:\n",
    "            if 'Norm_Price' in col:\n",
    "                df_trace[col.replace('Norm_Price', 'Return')] = df_trace[col].pct_change()\n",
    "        debug_data['portfolio_trace'] = df_trace\n",
    "        # 'ranking_metrics' is not applicable in this mode\n",
    "        debug_data['ranking_metrics'] = pd.DataFrame() \n",
    "\n",
    "    # 8. Assemble the final results dictionary\n",
    "    final_results = {\n",
    "        'tickers_to_display': valid_tickers_in_data, # Use unique tickers for plotting\n",
    "        'normalized_plot_data': normalized_plot_data,\n",
    "        'portfolio_series': portfolio_series,\n",
    "        'benchmark_price_series': benchmark_price_series,\n",
    "        'performance_data': perf_data,\n",
    "        'results_df': results_df,\n",
    "        'actual_calc_end_ts': actual_calc_end_ts,\n",
    "        'safe_start_date': safe_start_date,\n",
    "        'safe_viz_end_date': safe_viz_end_date,\n",
    "        'error': None\n",
    "    }\n",
    "    return (final_results, debug_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3caacef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. GENERATE SYNTHETIC TEST DATA ---\n",
    "print(\"Generating synthetic test data...\")\n",
    "# Master calendar: ~2 years of daily data\n",
    "master_trading_days = pd.to_datetime(pd.date_range(start='2022-01-01', end='2023-12-31', freq='D'))\n",
    "num_days = len(master_trading_days)\n",
    "tickers_to_generate = ['AAPL', 'MSFT', 'GOOG', 'NVDA', 'VOO']\n",
    "\n",
    "# Generate predictable price series\n",
    "np.random.seed(42)\n",
    "close_data = {\n",
    "    'AAPL': 150 + np.linspace(0, 50, num_days) + np.random.randn(num_days) * 2, # Steady winner\n",
    "    'MSFT': 300 + np.linspace(0, 70, num_days) + np.random.randn(num_days) * 4, # Volatile winner\n",
    "    'GOOG': 140 - np.linspace(0, 40, num_days) + np.random.randn(num_days) * 3, # Loser\n",
    "    'NVDA': 200 + np.linspace(0, 300, num_days) + np.random.randn(num_days) * 8, # Big winner\n",
    "    'VOO':  400 + np.linspace(0, 50, num_days) + np.random.randn(num_days) * 1.5 # Benchmark\n",
    "}\n",
    "df_close_full = pd.DataFrame(close_data, index=master_trading_days).round(2)\n",
    "\n",
    "# Generate simple, constant ATRP features\n",
    "feature_list = []\n",
    "atrp_map = {'AAPL': 0.015, 'MSFT': 0.02, 'GOOG': 0.018, 'NVDA': 0.03, 'VOO': 0.01}\n",
    "for ticker in tickers_to_generate:\n",
    "    ticker_features = pd.DataFrame({\n",
    "        'Date': master_trading_days,\n",
    "        'Ticker': ticker,\n",
    "        'ATRP': atrp_map[ticker]\n",
    "    })\n",
    "    feature_list.append(ticker_features)\n",
    "features_df = pd.concat(feature_list).set_index(['Ticker', 'Date'])\n",
    "print(\"Test data generation complete.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c20e29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. EXECUTE THE TEST ---\n",
    "display(Markdown(\"### Testing `run_manual_portfolio_step`\"))\n",
    "\n",
    "# Define a manual portfolio. Note the duplicates and the invalid ticker.\n",
    "manual_list = ['NVDA', 'NVDA', 'AAPL', 'GOOG', 'FAKE_TICKER']\n",
    "start_date_str = '2022-05-01'\n",
    "\n",
    "print(f\"Manual Ticker List: {manual_list}\")\n",
    "print(f\"Start Date: {start_date_str}, Calc Period: 126 days, Fwd Period: 63 days\")\n",
    "\n",
    "results, debug_info = run_manual_portfolio_step(\n",
    "    df_close_full=df_close_full,\n",
    "    features_df=features_df,\n",
    "    manual_tickers=manual_list,\n",
    "    master_trading_days=master_trading_days,\n",
    "    start_date=pd.to_datetime(start_date_str),\n",
    "    calc_period=126,\n",
    "    fwd_period=63,\n",
    "    benchmark_ticker='VOO',\n",
    "    debug=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53bcad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. VERIFY AND DISPLAY RESULTS ---\n",
    "if results.get('error'):\n",
    "    print(f\"\\nERROR: {results['error']}\")\n",
    "else:\n",
    "    display(Markdown(\"---\"))\n",
    "    display(Markdown(\"#### Output Verification\"))\n",
    "    print(\"Function executed successfully. Verifying output structure...\")\n",
    "    print(f\"Result Keys: {list(results.keys())}\")\n",
    "    \n",
    "    display(Markdown(\"#### Results DataFrame (`results_df`)\"))\n",
    "    print(\"This table shows the initial weights and forward gain for each valid ticker.\")\n",
    "    display(results['results_df'].style.format({'InitialWeight': '{:.2%}', 'FwdGain': '{:+.2%}'}))\n",
    "\n",
    "    display(Markdown(\"#### Performance Data (`performance_data`)\"))\n",
    "    display(pd.DataFrame.from_dict(results['performance_data'], orient='index', columns=['Value']))\n",
    "    \n",
    "    display(Markdown(\"#### Portfolio Series (first 5 rows)\"))\n",
    "    display(results['portfolio_series'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa034d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.to_csv('./export_csv/features_df.csv')\n",
    "df_close_full.to_csv('./export_csv/df_close_full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d92b18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', 300):\n",
    "    print_nested(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea35196",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', 300):\n",
    "    print_nested(debug_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac24e4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f7f2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_close_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17abc969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have df_close_full, features_df, master_trading_days loaded\n",
    "# and the necessary helper functions defined.\n",
    "\n",
    "manual_list = ['AAPL', 'AAPL', 'MSFT', 'GOOG', 'INVALID_TICKER']\n",
    "results, debug_info = run_manual_portfolio_step(\n",
    "    df_close_full=df_close_full,\n",
    "    features_df=features_df,\n",
    "    manual_tickers=manual_list,\n",
    "    master_trading_days=master_trading_days,\n",
    "    start_date=pd.to_datetime('2022-01-01'),\n",
    "    calc_period=252,\n",
    "    fwd_period=63,\n",
    "    benchmark_ticker='VOO',\n",
    "    debug=True\n",
    ")\n",
    "\n",
    "# Check the output structure\n",
    "print(results.keys())\n",
    "display(results['results_df'])\n",
    "display(results['performance_data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a665263",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a682e1fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899b9cf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdaa9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test_suite():\n",
    "    \"\"\"\n",
    "    Creates sample data and tests the calculate_buy_and_hold_performance function.\n",
    "    \"\"\"\n",
    "    # 1. --- Create Sample Data ---\n",
    "    dates = pd.to_datetime(['2023-01-01', '2023-01-02', '2023-01-03', '2023-01-04'])\n",
    "    \n",
    "    # MSFT doubles in price, GOOG goes up 50%\n",
    "    close_data = {\n",
    "        'MSFT': [100.0, 120.0, 160.0, 200.0],\n",
    "        'GOOG': [200.0, 220.0, 260.0, 300.0]\n",
    "    }\n",
    "    df_close = pd.DataFrame(close_data, index=dates)\n",
    "\n",
    "    # Simple, constant ATRP values for predictability\n",
    "    features_data = {\n",
    "        'Date':     list(dates) * 2,\n",
    "        'Ticker':   ['MSFT'] * 4 + ['GOOG'] * 4,\n",
    "        'ATRP':     [0.01] * 4   + [0.02] * 4,\n",
    "    }\n",
    "    features_df = pd.DataFrame(features_data).set_index(['Ticker', 'Date'])\n",
    "\n",
    "    start_date = dates[0]\n",
    "    end_date = dates[-1]\n",
    "\n",
    "    print(\"--- Test Setup ---\")\n",
    "    print(\"Close Prices:\")\n",
    "    display(df_close)\n",
    "    print(\"\\nFeature ATRP:\")\n",
    "    display(features_df)\n",
    "    print(\"-\" * 20)\n",
    "\n",
    "    # 2. --- Test Case 1: Equal Weights (should behave like the old function) ---\n",
    "    print(\"\\n--- Test Case 1: Equal Weights ['MSFT', 'GOOG'] ---\")\n",
    "    tickers_equal = ['MSFT', 'GOOG']\n",
    "    \n",
    "    val_eq, ret_eq, atrp_eq = calculate_buy_and_hold_performance(\n",
    "        df_close, features_df, tickers_equal, start_date, end_date\n",
    "    )\n",
    "    \n",
    "    print(\"\\nCalculated Portfolio Value Series:\")\n",
    "    display(pd.DataFrame(val_eq, columns=['Value']))\n",
    "    \n",
    "    # Manual verification:\n",
    "    # Day 1: (120/100)*0.5 + (220/200)*0.5 = 1.2*0.5 + 1.1*0.5 = 0.6 + 0.55 = 1.15\n",
    "    # Day 3: (200/100)*0.5 + (300/200)*0.5 = 2.0*0.5 + 1.5*0.5 = 1.0 + 0.75 = 1.75\n",
    "    # The code should match this.\n",
    "    \n",
    "    print(\"\\nCalculated Portfolio ATRP Series:\")\n",
    "    display(pd.DataFrame(atrp_eq, columns=['ATRP']))\n",
    "    # On day 1, MSFT value is 0.6, GOOG is 0.55. Total is 1.15.\n",
    "    # MSFT weight = 0.6/1.15 = 0.5217. GOOG weight = 0.55/1.15 = 0.4783\n",
    "    # ATRP = 0.5217*0.01 + 0.4783*0.02 = 0.005217 + 0.009566 = 0.014783\n",
    "    \n",
    "    # 3. --- Test Case 2: Unequal Weights (verifies the new logic) ---\n",
    "    print(\"\\n--- Test Case 2: Unequal Weights ['MSFT', 'MSFT', 'MSFT', 'GOOG'] ---\")\n",
    "    print(\"Initial Weights: 75% MSFT, 25% GOOG\")\n",
    "    tickers_unequal = ['MSFT', 'MSFT', 'MSFT', 'GOOG']\n",
    "    \n",
    "    val_uneq, ret_uneq, atrp_uneq = calculate_buy_and_hold_performance(\n",
    "        df_close, features_df, tickers_unequal, start_date, end_date\n",
    "    )\n",
    "    \n",
    "    print(\"\\nCalculated Portfolio Value Series:\")\n",
    "    display(pd.DataFrame(val_uneq, columns=['Value']))\n",
    "\n",
    "    # Manual verification:\n",
    "    # Day 1: (120/100)*0.75 + (220/200)*0.25 = 1.2*0.75 + 1.1*0.25 = 0.9 + 0.275 = 1.175\n",
    "    # Day 3: (200/100)*0.75 + (300/200)*0.25 = 2.0*0.75 + 1.5*0.25 = 1.5 + 0.375 = 1.875\n",
    "    # The code should match this.\n",
    "    \n",
    "    print(\"\\nCalculated Portfolio ATRP Series:\")\n",
    "    display(pd.DataFrame(atrp_uneq, columns=['ATRP']))\n",
    "    # On day 1, MSFT value is 0.9, GOOG is 0.275. Total is 1.175.\n",
    "    # MSFT weight = 0.9/1.175 = 0.766. GOOG weight = 0.275/1.175 = 0.234\n",
    "    # ATRP = 0.766*0.01 + 0.234*0.02 = 0.00766 + 0.00468 = 0.01234\n",
    "\n",
    "    print(\"\\nCalculated Portfolio Return Series:\")\n",
    "    display(pd.DataFrame(ret_uneq, columns=['Return']))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6e9814",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_test_suite()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcf37d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415fecc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df530ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bef440",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82af6d28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c1701e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_container, debug_container = plot_walk_forward_analyzer(\n",
    "    df_ohlcv=df_ohlcv,\n",
    "    default_start_date='2018-10-03',\n",
    "    default_calc_period=252,\n",
    "    default_fwd_period=63,\n",
    "    default_metric='Sharpe (ATR)',\n",
    "    default_rank_start=1,\n",
    "    default_rank_end=5,\n",
    "    default_benchmark_ticker='VOO',\n",
    "    quality_thresholds={ 'min_median_dollar_volume': 10_000_000, \n",
    "                         'max_stale_pct': 0.05, \n",
    "                         'max_same_vol_count': 1 },\n",
    "    debug=True  # <-- Activate the new mode!\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fea8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume 'df_ohlcv' is your loaded dataset\n",
    "# You might need to regenerate features_df if it's not in your notebook's memory\n",
    "print(\"--- Regenerating features for verification ---\")\n",
    "features_df = generate_features(df_ohlcv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca56efc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers_to_verify = ['STIP', 'RCL']\n",
    "start_date = '2025-08-13'\n",
    "end_date = '2025-09-04'\n",
    "benchmark_ticker = 'VOO'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2046ea22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now call the verification function with the exact parameters from the UI\n",
    "verify_sharpe_atr_calculation_checked(\n",
    "    df_ohlcv=df_ohlcv,\n",
    "    features_df=features_df,\n",
    "    tickers_to_verify=tickers_to_verify, # <-- From the UI output\n",
    "    benchmark_ticker=benchmark_ticker,\n",
    "    start_date=start_date,\n",
    "    calc_period=10,\n",
    "    fwd_period=5,\n",
    "    debug=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89540b88",
   "metadata": {},
   "source": [
    "##########################################   \n",
    "##########################################   \n",
    "##########################################   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108a523c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13650d09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cf510b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ==============================================================================\n",
    "# # GOLDEN COPY - COMPLETE PROJECT CODE (All Fixes Included)\n",
    "# # Version: Verified Portfolio and Benchmark Sharpe (ATR)  \n",
    "# # Date: 2025-10-15\n",
    "# # ==============================================================================\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import plotly.graph_objects as go\n",
    "# import ipywidgets as widgets\n",
    "# import time\n",
    "# import pprint\n",
    "# import os # Make sure os is imported for the export function later\n",
    "# import re\n",
    "\n",
    "# from datetime import datetime, date\n",
    "# from IPython.display import display, Markdown\n",
    "# from tqdm.auto import tqdm\n",
    "# from pathlib import Path\n",
    "# from itertools import product\n",
    "\n",
    "\n",
    "# pd.set_option('display.max_rows', 30)\n",
    "# pd.set_option('display.max_columns', 50)\n",
    "# pd.set_option('display.width', 3000)\n",
    "\n",
    "\n",
    "# # --- REFACTORING PHASE 1 CODE: Feature Generation Engine ---\n",
    "\n",
    "# def generate_features(df_ohlcv: pd.DataFrame, \n",
    "#                       atr_period: int = 14, \n",
    "#                       quality_window: int = 252, \n",
    "#                       quality_min_periods: int = 126) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     Generates a comprehensive DataFrame of derived features from raw OHLCV data.\n",
    "\n",
    "#     This function performs all heavy, window-based calculations upfront to be used\n",
    "#     by downstream analysis functions. It calculates:\n",
    "#     1. Technical Indicators: True Range (TR), ATR, and ATRP.\n",
    "#     2. Data Quality Metrics: Rolling stale percentage, median dollar volume, etc.\n",
    "\n",
    "#     Args:\n",
    "#         df_ohlcv: The primary DataFrame with a (Ticker, Date) MultiIndex and\n",
    "#                   columns for 'Adj High', 'Adj Low', 'Adj Close', 'Volume'.\n",
    "#         atr_period: The lookback period for the ATR's Exponential Moving Average.\n",
    "#         quality_window: The rolling window size for data quality metrics.\n",
    "#         quality_min_periods: The minimum number of observations required to have\n",
    "#                              a valid quality metric.\n",
    "\n",
    "#     Returns:\n",
    "#         A new DataFrame with the same (Ticker, Date) MultiIndex containing all\n",
    "#         calculated feature columns.\n",
    "#     \"\"\"\n",
    "#     print(\"--- Starting Feature Generation ---\")\n",
    "    \n",
    "#     # Ensure the DataFrame is sorted for correct window and shift operations\n",
    "#     # FIX: Replaced is_lexsorted() with the current pandas attribute\n",
    "#     if not df_ohlcv.index.is_monotonic_increasing:\n",
    "#         print(\"Sorting index for calculation accuracy...\")\n",
    "#         df_ohlcv = df_ohlcv.sort_index()\n",
    "\n",
    "#     # --- 1. Technical Indicator Calculation (TR, ATR, ATRP) ---\n",
    "#     print(f\"Calculating technical indicators (ATR Period: {atr_period})...\")\n",
    "    \n",
    "#     # Group by ticker to handle each security independently\n",
    "#     grouped = df_ohlcv.groupby(level='Ticker')\n",
    "    \n",
    "#     # Get the previous day's close required for True Range\n",
    "#     prev_close = grouped['Adj Close'].shift(1)\n",
    "    \n",
    "#     # Calculate the three components of True Range\n",
    "#     high_low = df_ohlcv['Adj High'] - df_ohlcv['Adj Low']\n",
    "#     high_prev_close = abs(df_ohlcv['Adj High'] - prev_close)\n",
    "#     low_prev_close = abs(df_ohlcv['Adj Low'] - prev_close)\n",
    "    \n",
    "#     # Combine the components to get the final TR\n",
    "#     tr = pd.concat([high_low, high_prev_close, low_prev_close], axis=1).max(axis=1, skipna=False)\n",
    "    \n",
    "#     # # Calculate the ATR using an Exponential Moving Average\n",
    "#     # --- FIX IS HERE ---\n",
    "#     # Use .transform() to apply the EWM function. \n",
    "#     # This guarantees the resulting Series has the exact same index as 'tr',\n",
    "#     # preventing the index alignment error during the subsequent division.\n",
    "#     atr = tr.groupby(level='Ticker').transform(\n",
    "#         lambda x: x.ewm(alpha=1/atr_period, adjust=False).mean()\n",
    "#     )\n",
    "\n",
    "#     # --- CHANGE 1: Removed .fillna(0) ---\n",
    "#     # ATRP will now be NaN on the first day, consistent with TR and ATR.\n",
    "#     atrp = (atr / df_ohlcv['Adj Close']).replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "#     indicator_df = pd.DataFrame({\n",
    "#         'TR': tr,\n",
    "#         'ATR': atr,\n",
    "#         'ATRP': atrp\n",
    "#     })\n",
    "    \n",
    "#     # --- 2. Data Quality Metric Calculation ---\n",
    "#     print(f\"Calculating data quality metrics (Window: {quality_window} days)...\")\n",
    "    \n",
    "#     # Create intermediate flags needed for quality calculations\n",
    "#     is_stale = np.where((df_ohlcv['Volume'] == 0) | (df_ohlcv['Adj High'] == df_ohlcv['Adj Low']), 1, 0)\n",
    "#     dollar_volume = df_ohlcv['Adj Close'] * df_ohlcv['Volume']\n",
    "#     has_same_volume = (grouped['Volume'].diff() == 0).astype(int)\n",
    "    \n",
    "#     # Combine flags into a temporary DataFrame for rolling calculations\n",
    "#     quality_temp_df = pd.DataFrame({\n",
    "#         'IsStale': is_stale,\n",
    "#         'DollarVolume': dollar_volume,\n",
    "#         'HasSameVolume': has_same_volume\n",
    "#     }, index=df_ohlcv.index) # Explicitly set index to be safe\n",
    "    \n",
    "#     # Perform the rolling calculations on the grouped data\n",
    "#     # --- FIX IS HERE ---\n",
    "#     # We switch to the older, more compatible dictionary-based aggregation method.\n",
    "#     # This syntax is understood by nearly all versions of pandas.\n",
    "#     rolling_result = quality_temp_df.groupby(level='Ticker').rolling(\n",
    "#         window=quality_window,\n",
    "#         min_periods=quality_min_periods\n",
    "#     ).agg({\n",
    "#         'IsStale': 'mean',\n",
    "#         'DollarVolume': 'median',\n",
    "#         'HasSameVolume': 'sum'\n",
    "#     })\n",
    "    \n",
    "#     # The dictionary syntax produces columns with the original names ('IsStale', etc.).\n",
    "#     # We now explicitly rename them to our desired final names.\n",
    "#     rolling_result = rolling_result.rename(columns={\n",
    "#         'IsStale': 'RollingStalePct',\n",
    "#         'DollarVolume': 'RollMedDollarVol', # <-- RENAMED HERE\n",
    "#         'HasSameVolume': 'RollingSameVolCount'\n",
    "#     })\n",
    "\n",
    "#     # The index after a grouped rolling operation is hierarchical.\n",
    "#     # We remove the outermost 'Ticker' level to restore the original index structure.\n",
    "#     rolling_quality = rolling_result.reset_index(level=0, drop=True)\n",
    "\n",
    "#     # --- 3. Combine All Features ---\n",
    "#     print(\"Combining all feature sets...\")\n",
    "#     features_df = pd.concat([indicator_df, rolling_quality], axis=1)\n",
    "    \n",
    "#     print(\"‚úÖ Feature generation complete.\")\n",
    "#     return features_df\n",
    "\n",
    "# def test_features_df(features_df: pd.DataFrame, \n",
    "#                      df_ohlcv: pd.DataFrame, \n",
    "#                      test_ticker: str = 'AAPL',\n",
    "#                      spot_check_date: str = '2020-03-20'):\n",
    "#     \"\"\"\n",
    "#     Runs a suite of tests to verify the correctness of the generated features_df.\n",
    "\n",
    "#     Args:\n",
    "#         features_df: The generated DataFrame from the generate_features function.\n",
    "#         df_ohlcv: The original source OHLCV DataFrame.\n",
    "#         test_ticker: A common, liquid ticker to use for specific value checks.\n",
    "#     \"\"\"\n",
    "#     print(f\"\\n--- Running Verification Suite for features_df (Test Ticker: {test_ticker}) ---\")\n",
    "    \n",
    "#     # --- Test 1: Structural Integrity ---\n",
    "#     print(\"\\n[Test 1: Structural Integrity]\")\n",
    "#     assert features_df.index.equals(df_ohlcv.index), \"FAIL: Index does not match original df_ohlcv.\"\n",
    "#     print(\"  ‚úÖ PASS: Index matches original df_ohlcv.\")\n",
    "    \n",
    "#     expected_cols = ['TR', 'ATR', 'ATRP', 'RollingStalePct', 'RollMedDollarVol', 'RollingSameVolCount']\n",
    "#     assert all(col in features_df.columns for col in expected_cols), \"FAIL: Missing one or more expected columns.\"\n",
    "#     print(\"  ‚úÖ PASS: All expected feature columns are present.\")\n",
    "#     print(f\"  - DataFrame Info:\")\n",
    "#     features_df.info(verbose=False, memory_usage='deep')\n",
    "\n",
    "\n",
    "#     # --- Test 2: ATR Calculation Logic ---\n",
    "#     print(\"\\n[Test 2: ATR Logic Verification]\")\n",
    "#     ticker_features = features_df.loc[test_ticker]\n",
    "    \n",
    "#     # Test 2a: First TR value should be NaN (since prev_close is NaN)\n",
    "#     first_tr = ticker_features['TR'].iloc[0]\n",
    "#     assert pd.isna(first_tr), f\"FAIL: First TR value for {test_ticker} should be NaN, but got {first_tr}.\"\n",
    "#     print(f\"  ‚úÖ PASS: First TR value for {test_ticker} is NaN as expected.\")\n",
    "\n",
    "#     # Test 2b: The first valid ATR should equal the first valid TR (EWM cold start behavior)\n",
    "#     first_valid_tr_val = ticker_features['TR'].dropna().iloc[0]\n",
    "#     first_valid_atr_val = ticker_features['ATR'].dropna().iloc[0]\n",
    "#     assert np.isclose(first_valid_tr_val, first_valid_atr_val), \\\n",
    "#         f\"FAIL: First valid ATR ({first_valid_atr_val}) should equal first valid TR ({first_valid_tr_val}).\"\n",
    "#     print(\"  ‚úÖ PASS: First valid ATR correctly seeded with first valid TR.\")\n",
    "\n",
    "\n",
    "#     # --- Test 3: Rolling Quality Metrics Logic ---\n",
    "#     print(\"\\n[Test 3: Rolling Quality Metrics Logic Verification]\")\n",
    "#     quality_min_periods = 126 # Should match the parameter used in generation\n",
    "    \n",
    "#     # Test 3a: Check for leading NaNs\n",
    "#     first_valid_quality_idx = ticker_features['RollingStalePct'].first_valid_index()\n",
    "#     if first_valid_quality_idx is None:\n",
    "#         print(f\"  - INFO: No valid quality metrics found for {test_ticker} (likely too little data). Skipping test.\")\n",
    "#     else:\n",
    "#         position_of_first_valid = ticker_features.index.get_loc(first_valid_quality_idx)\n",
    "#         assert position_of_first_valid == quality_min_periods - 1, \\\n",
    "#             f\"FAIL: First valid quality metric should appear at index {quality_min_periods - 1}, but appeared at {position_of_first_valid}.\"\n",
    "#         print(f\"  ‚úÖ PASS: Leading NaNs are present for the first {quality_min_periods - 1} periods as expected.\")\n",
    "\n",
    "\n",
    "#     # --- Test 4: Spot Check Against Manual Calculation ---\n",
    "#     print(\"\\n[Test 4: Spot Check vs. Manual Calculation]\")\n",
    "#     # # Choose a specific date for a manual calculation\n",
    "#     # spot_check_date = '2020-03-20' # A volatile day for a good test\n",
    "    \n",
    "#     # Manual TR Calculation\n",
    "#     today_data = df_ohlcv.loc[(test_ticker, spot_check_date)]\n",
    "#     yesterday_data = df_ohlcv.loc[(test_ticker, pd.to_datetime(spot_check_date) - pd.Timedelta(days=1))] # simple lookback for test\n",
    "    \n",
    "#     manual_h_l = today_data['Adj High'] - today_data['Adj Low']\n",
    "#     manual_h_pc = abs(today_data['Adj High'] - yesterday_data['Adj Close'])\n",
    "#     manual_l_pc = abs(today_data['Adj Low'] - yesterday_data['Adj Close'])\n",
    "#     manual_tr = max(manual_h_l, manual_h_pc, manual_l_pc)\n",
    "    \n",
    "#     code_tr = ticker_features.loc[spot_check_date]['TR']\n",
    "    \n",
    "#     assert np.isclose(manual_tr, code_tr), f\"FAIL: Manual TR ({manual_tr:.4f}) does not match code TR ({code_tr:.4f}) on {spot_check_date}.\"\n",
    "#     print(f\"  ‚úÖ PASS: Manually calculated TR on {spot_check_date} matches code's TR.\")\n",
    "    \n",
    "#     print(\"\\n--- ‚úÖ All Verification Tests Passed ---\")\n",
    "\n",
    "# def export_ticker_data(ticker_to_export: str, \n",
    "#                          df_ohlcv: pd.DataFrame, \n",
    "#                          features_df: pd.DataFrame, \n",
    "#                          output_dir: str = 'export_csv'):\n",
    "#     \"\"\"\n",
    "#     Exports the raw OHLCV data and the corresponding calculated features for a \n",
    "#     single ticker to two separate CSV files.\n",
    "\n",
    "#     This function is designed for easy manual verification of data and calculations.\n",
    "#     It will create the output directory if it does not exist.\n",
    "\n",
    "#     Args:\n",
    "#         ticker_to_export: The ticker symbol to export (e.g., 'AAPL').\n",
    "#         df_ohlcv: The main DataFrame containing the raw OHLCV data with a \n",
    "#                   (Ticker, Date) MultiIndex.\n",
    "#         features_df: The DataFrame containing the calculated features with a \n",
    "#                      (Ticker, Date) MultiIndex.\n",
    "#         output_dir: The directory where the CSV files will be saved. \n",
    "#                     Defaults to 'export_csv'.\n",
    "#     \"\"\"\n",
    "#     print(f\"--- Attempting to export data for ticker: {ticker_to_export} ---\")\n",
    "    \n",
    "#     # --- 1. Ensure the output directory exists ---\n",
    "#     try:\n",
    "#         os.makedirs(output_dir, exist_ok=True)\n",
    "#         print(f\"Output directory '{output_dir}' is ready.\")\n",
    "#     except OSError as e:\n",
    "#         print(f\"Error: Could not create directory '{output_dir}'. Reason: {e}\")\n",
    "#         return\n",
    "\n",
    "#     # --- 2. Isolate the data for the specified ticker ---\n",
    "#     try:\n",
    "#         # Use .loc to select all rows for the given ticker from the MultiIndex\n",
    "#         ticker_ohlcv = df_ohlcv.loc[ticker_to_export]\n",
    "#         ticker_features = features_df.loc[ticker_to_export]\n",
    "        \n",
    "#         if ticker_ohlcv.empty:\n",
    "#             print(f\"Warning: No OHLCV data found for ticker '{ticker_to_export}'. Cannot export.\")\n",
    "#             return\n",
    "            \n",
    "#         print(f\"Found {len(ticker_ohlcv)} rows of data for '{ticker_to_export}'.\")\n",
    "        \n",
    "#     except KeyError:\n",
    "#         print(f\"Error: Ticker '{ticker_to_export}' not found in one or both of the DataFrames. Please check the symbol.\")\n",
    "#         return\n",
    "#     except Exception as e:\n",
    "#         print(f\"An unexpected error occurred while accessing data: {e}\")\n",
    "#         return\n",
    "\n",
    "#     # --- 3. Construct file paths and export to CSV ---\n",
    "#     try:\n",
    "#         # Define the full path for each output file\n",
    "#         ohlcv_filename = f\"{ticker_to_export}_ohlcv.csv\"\n",
    "#         features_filename = f\"{ticker_to_export}_features.csv\"\n",
    "        \n",
    "#         ohlcv_filepath = os.path.join(output_dir, ohlcv_filename)\n",
    "#         features_filepath = os.path.join(output_dir, features_filename)\n",
    "        \n",
    "#         # Export the DataFrames to CSV. The index (Date) will be included.\n",
    "#         ticker_ohlcv.to_csv(ohlcv_filepath)\n",
    "#         ticker_features.to_csv(features_filepath)\n",
    "        \n",
    "#         print(\"\\n‚úÖ Export successful!\")\n",
    "#         print(f\"   - Raw OHLCV data saved to: {ohlcv_filepath}\")\n",
    "#         print(f\"   - Calculated features saved to: {features_filepath}\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error: Failed to write data to CSV files. Reason: {e}\")\n",
    "\n",
    "# def create_synthetic_ticker_data(\n",
    "#     ticker_name: str = 'SYNTH', \n",
    "#     num_days: int = 50,\n",
    "#     num_zero_volume_days: int = 5,\n",
    "#     num_flat_price_days: int = 3\n",
    "# ) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     Creates a synthetic OHLCV DataFrame with predictable patterns and randomly injected\n",
    "#     stale data conditions for robust testing.\n",
    "\n",
    "#     Args:\n",
    "#         ticker_name: The name for the synthetic ticker.\n",
    "#         num_days: The total number of days for the ticker's history.\n",
    "#         num_zero_volume_days: The number of random days to set Volume to 0.\n",
    "#         num_flat_price_days: The number of random days to set High == Low.\n",
    "\n",
    "#     Returns:\n",
    "#         A pandas DataFrame with a (Ticker, Date) MultiIndex.\n",
    "#     \"\"\"\n",
    "#     print(f\"--- Creating synthetic data for '{ticker_name}' with {num_days} days ---\")\n",
    "    \n",
    "#     # 1. Create a base DataFrame with \"normal\" data\n",
    "#     dates = pd.to_datetime(pd.date_range(start='2023-01-01', periods=num_days, freq='B'))\n",
    "#     data = {\n",
    "#         'Adj Open': 100.0, 'Adj High': 102.0, 'Adj Low': 98.0,\n",
    "#         'Adj Close': 100.0, 'Volume': 1_000_000\n",
    "#     }\n",
    "#     df = pd.DataFrame(data, index=dates)\n",
    "#     df['Adj Close'] = df['Adj Close'] + np.random.randn(num_days) * 0.5 # Add some noise\n",
    "\n",
    "#     # 2. Define a \"protected\" window for specific verification tests.\n",
    "#     # The `verify_synthetic_ticker_features` function depends on this exact window.\n",
    "#     # We will not inject random stale days here.\n",
    "#     protected_start_idx, protected_end_idx = 10, 20\n",
    "    \n",
    "#     # 3. Inject random \"stale\" days OUTSIDE the protected window\n",
    "#     available_indices = df.index.drop(df.index[protected_start_idx:protected_end_idx])\n",
    "    \n",
    "#     # Inject zero-volume days\n",
    "#     if num_zero_volume_days > 0:\n",
    "#         if len(available_indices) < num_zero_volume_days:\n",
    "#             raise ValueError(\"Not enough available days to inject zero-volume days.\")\n",
    "#         zero_vol_dates = np.random.choice(available_indices, num_zero_volume_days, replace=False)\n",
    "#         df.loc[zero_vol_dates, 'Volume'] = 0\n",
    "#         print(f\"  - Injected {num_zero_volume_days} random zero-volume 'stale' days.\")\n",
    "#         # Update available indices to avoid overlap\n",
    "#         available_indices = available_indices.drop(zero_vol_dates)\n",
    "\n",
    "#     # Inject flat-price days (High == Low)\n",
    "#     if num_flat_price_days > 0:\n",
    "#         if len(available_indices) < num_flat_price_days:\n",
    "#             raise ValueError(\"Not enough available days to inject flat-price days.\")\n",
    "#         flat_price_dates = np.random.choice(available_indices, num_flat_price_days, replace=False)\n",
    "#         # Set High and Low to be the same as the Close price for that day\n",
    "#         df.loc[flat_price_dates, 'Adj High'] = df.loc[flat_price_dates, 'Adj Close']\n",
    "#         df.loc[flat_price_dates, 'Adj Low'] = df.loc[flat_price_dates, 'Adj Close']\n",
    "#         print(f\"  - Injected {num_flat_price_days} random flat-price 'stale' days.\")\n",
    "\n",
    "#     # 4. Inject the specific, hand-crafted patterns inside the protected window for verification\n",
    "#     print(\"  - Injecting specific patterns for programmatic verification...\")\n",
    "#     # Pattern for RollingStalePct: 2 stale days in 10 (20%)\n",
    "#     df.iloc[10, df.columns.get_loc('Volume')] = 0  # Stale day (zero volume)\n",
    "#     df.iloc[11, df.columns.get_loc('Adj High')] = 99.0 # Stale day (High == Low)\n",
    "#     df.iloc[11, df.columns.get_loc('Adj Low')] = 99.0\n",
    "    \n",
    "#     # Pattern for RollingMedianVolume\n",
    "#     for i in range(10):\n",
    "#         df.iloc[10 + i, df.columns.get_loc('Adj Close')] = 100.0 # Standardize price for easy median calc\n",
    "#         df.iloc[10 + i, df.columns.get_loc('Volume')] = (i + 1) * 10000\n",
    "\n",
    "#     # Pattern for RollingSameVolCount\n",
    "#     df.iloc[15, df.columns.get_loc('Volume')] = 77777\n",
    "#     df.iloc[16, df.columns.get_loc('Volume')] = 77777\n",
    "#     df.iloc[17, df.columns.get_loc('Volume')] = 77777\n",
    "    \n",
    "#     # 5. Set the MultiIndex\n",
    "#     df['Ticker'] = ticker_name\n",
    "#     df = df.set_index(['Ticker', df.index])\n",
    "#     df.index.names = ['Ticker', 'Date']\n",
    "    \n",
    "#     print(\"‚úÖ Synthetic data created successfully.\")\n",
    "#     return df\n",
    "\n",
    "# def verify_synthetic_ticker_features(features_df: pd.DataFrame, \n",
    "#                                        ticker_name: str = 'SYNTH',\n",
    "#                                        quality_window: int = 10):\n",
    "#     \"\"\"\n",
    "#     Verifies the quality metric calculations on the features_df generated from\n",
    "#     the synthetic ticker data.\n",
    "\n",
    "#     Args:\n",
    "#         features_df: The DataFrame of calculated features.\n",
    "#         ticker_name: The name of the synthetic ticker.\n",
    "#         quality_window: The rolling window used, which must match the window\n",
    "#                         of the synthetic data pattern.\n",
    "#     \"\"\"\n",
    "#     print(f\"\\n--- Running Verification on Synthetic Ticker '{ticker_name}' ---\")\n",
    "    \n",
    "#     # --- Expected values based on our synthetic data design ---\n",
    "#     EXPECTED_STALE_PCT = 0.20  # 2 stale days out of 10\n",
    "#     EXPECTED_MEDIAN_DOLLAR_VOL = 5_500_000.0 # median of (10k..100k) * price of 100\n",
    "#     EXPECTED_SAME_VOL_COUNT = 2.0 # Three consecutive days gives two 'diff() == 0' events\n",
    "\n",
    "#     try:\n",
    "#         # Isolate the features for our synthetic ticker\n",
    "#         ticker_features = features_df.loc[ticker_name]\n",
    "        \n",
    "#         # The first valid calculation will be on the last day of our 10-day window.\n",
    "#         # The window starts at index 10 and has a length of 10, so it ends at index 19.\n",
    "#         verification_date = ticker_features.index[19]\n",
    "        \n",
    "#         print(f\"Verifying calculations on date: {verification_date.date()}\")\n",
    "        \n",
    "#         # Get the calculated values from the DataFrame\n",
    "#         calculated_values = ticker_features.loc[verification_date]\n",
    "#         stale_pct = calculated_values['RollingStalePct']\n",
    "#         # --- CHANGE 2 (continued): Accessing the renamed column ---\n",
    "#         median_vol = calculated_values['RollMedDollarVol'] \n",
    "#         same_vol_count = calculated_values['RollingSameVolCount']\n",
    "        \n",
    "#         # --- Perform Assertions ---\n",
    "#         print(\"\\n[Test 1: RollingStalePct]\")\n",
    "#         assert np.isclose(stale_pct, EXPECTED_STALE_PCT), f\"FAIL: Expected {EXPECTED_STALE_PCT}, Got {stale_pct}\"\n",
    "#         print(f\"  ‚úÖ PASS: Expected {EXPECTED_STALE_PCT}, Got {stale_pct}\")\n",
    "\n",
    "#         print(\"\\n[Test 2: RollMedDollarVol]\")\n",
    "#         assert np.isclose(median_vol, EXPECTED_MEDIAN_DOLLAR_VOL), f\"FAIL: Expected {EXPECTED_MEDIAN_DOLLAR_VOL}, Got {median_vol}\"\n",
    "#         print(f\"  ‚úÖ PASS: Expected {EXPECTED_MEDIAN_DOLLAR_VOL}, Got {median_vol}\")\n",
    "\n",
    "#         print(\"\\n[Test 3: RollingSameVolCount]\")\n",
    "#         assert np.isclose(same_vol_count, EXPECTED_SAME_VOL_COUNT), f\"FAIL: Expected {EXPECTED_SAME_VOL_COUNT}, Got {same_vol_count}\"\n",
    "#         print(f\"  ‚úÖ PASS: Expected {EXPECTED_SAME_VOL_COUNT}, Got {same_vol_count}\")\n",
    "\n",
    "#         print(\"\\n--- ‚úÖ All Synthetic Data Verification Tests Passed ---\")\n",
    "\n",
    "#     except KeyError:\n",
    "#         print(f\"FAIL: Ticker '{ticker_name}' not found in features_df.\")\n",
    "#     except IndexError:\n",
    "#         print(\"FAIL: Not enough data in features_df to run verification. Check num_days.\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"An unexpected error occurred during verification: {e}\")\n",
    "\n",
    "# # --- A. HELPER FUNCTIONS ---\n",
    "\n",
    "# def calculate_gain(price_series: pd.Series):\n",
    "#     \"\"\"Calculates the total gain over a series of prices.\"\"\"\n",
    "#     # Ensure there are at least two data points to calculate a gain\n",
    "#     if price_series.dropna().shape[0] < 2: return np.nan\n",
    "#     # Use forward-fill for the end price and back-fill for the start price\n",
    "#     # to handle potential NaNs at the beginning or end of the series.\n",
    "#     return (price_series.ffill().iloc[-1] / price_series.bfill().iloc[0]) - 1\n",
    "\n",
    "# def calculate_sharpe(return_series: pd.Series):\n",
    "#     \"\"\"Calculates the annualized Sharpe ratio from a series of daily returns.\"\"\"\n",
    "#     # Ensure there are at least two returns to calculate a standard deviation\n",
    "#     if return_series.dropna().shape[0] < 2: return np.nan\n",
    "#     std_dev = return_series.std()\n",
    "#     # Avoid division by zero if returns are constant\n",
    "#     if std_dev > 0 and std_dev != np.inf:\n",
    "#         return (return_series.mean() / std_dev) * np.sqrt(252)\n",
    "#     return np.nan\n",
    "\n",
    "# def calculate_sharpe_atr(return_series: pd.Series, atrp_series: pd.Series):\n",
    "#     \"\"\"Calculates a Sharpe-like ratio using mean ATRP as the denominator.\"\"\"\n",
    "#     # Ensure there are returns and that ATRP data is valid\n",
    "#     if return_series.dropna().shape[0] < 2 or atrp_series.dropna().empty:\n",
    "#         return np.nan\n",
    "        \n",
    "#     mean_return = return_series.mean()\n",
    "#     mean_atrp = atrp_series.mean()\n",
    "    \n",
    "#     # Avoid division by zero\n",
    "#     if mean_atrp > 0 and mean_atrp != np.inf:\n",
    "#         return mean_return / mean_atrp\n",
    "        \n",
    "#     return np.nan\n",
    "\n",
    "# def print_nested(d, indent=0, width=4):\n",
    "#     \"\"\"Pretty-print any nested dict/list/tuple combination.\"\"\"\n",
    "#     spacing = ' ' * indent\n",
    "#     if isinstance(d, dict):\n",
    "#         for k, v in d.items():\n",
    "#             print(f'{spacing}{k}:')\n",
    "#             print_nested(v, indent + width, width)\n",
    "#     elif isinstance(d, (list, tuple)):\n",
    "#         for item in d:\n",
    "#             print_nested(item, indent, width)\n",
    "#     else:\n",
    "#         print(f'{spacing}{d}')\n",
    "\n",
    "# # --- B. MODULAR METRIC CALCULATION ENGINE ---\n",
    "\n",
    "# def calculate_price_metric(metric_data: dict) -> pd.Series:\n",
    "#     \"\"\"\n",
    "#     Calculates the 'Price' metric (total gain) over the calculation period.\n",
    "\n",
    "#     Args:\n",
    "#         metric_data: A dictionary containing pre-calculated data Series.\n",
    "#                      Requires 'calc_close' (pd.DataFrame): The close prices for the calc period.\n",
    "\n",
    "#     Returns:\n",
    "#         A pandas Series of the calculated metric values, indexed by Ticker.\n",
    "#     \"\"\"\n",
    "#     calc_close = metric_data['calc_close']\n",
    "    \n",
    "#     # Ensure there are at least two data points to calculate a gain\n",
    "#     if len(calc_close) < 2:\n",
    "#         return pd.Series(dtype='float64', index=calc_close.columns)\n",
    "\n",
    "#     first_prices = calc_close.bfill().iloc[0]\n",
    "#     last_prices = calc_close.ffill().iloc[-1]\n",
    "    \n",
    "#     # The division of two Series aligns by index (Ticker), which is what we want.\n",
    "#     price_metric = last_prices / first_prices\n",
    "    \n",
    "#     return price_metric.dropna()\n",
    "\n",
    "# def calculate_sharpe_metric(metric_data: dict) -> pd.Series:\n",
    "#     \"\"\"\n",
    "#     Calculates the annualized 'Sharpe' ratio metric over the calculation period.\n",
    "\n",
    "#     Args:\n",
    "#         metric_data: A dictionary containing pre-calculated data Series.\n",
    "#                      Requires 'daily_returns' (pd.DataFrame): Daily returns for the calc period.\n",
    "\n",
    "#     Returns:\n",
    "#         A pandas Series of the calculated metric values, indexed by Ticker.\n",
    "#     \"\"\"\n",
    "#     daily_returns = metric_data['daily_returns']\n",
    "    \n",
    "#     # Ensure there's enough data to calculate standard deviation\n",
    "#     if len(daily_returns.dropna()) < 2:\n",
    "#         return pd.Series(dtype='float64', index=daily_returns.columns)\n",
    "    \n",
    "#     mean_returns = daily_returns.mean()\n",
    "#     std_returns = daily_returns.std()\n",
    "    \n",
    "#     # Standard annualized Sharpe Ratio calculation. Avoid division by zero.\n",
    "#     # We replace resulting NaNs/infs with 0 to handle cases of zero volatility.\n",
    "#     sharpe_ratio = (mean_returns / std_returns * np.sqrt(252))\n",
    "    \n",
    "#     return sharpe_ratio.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "    \n",
    "# def calculate_sharpe_atr_metric(metric_data: dict) -> pd.Series:\n",
    "#     \"\"\"\n",
    "#     Calculates the 'Sharpe (ATR)' metric over the calculation period.\n",
    "\n",
    "#     Args:\n",
    "#         metric_data: A dictionary containing pre-calculated data Series.\n",
    "#                      Requires 'daily_returns' (pd.DataFrame): Daily returns for the calc period.\n",
    "#                      Requires 'atrp' (pd.Series): Mean ATRP for each ticker over the period.\n",
    "\n",
    "#     Returns:\n",
    "#         A pandas Series of the calculated metric values, indexed by Ticker.\n",
    "#     \"\"\"\n",
    "#     daily_returns = metric_data['daily_returns']\n",
    "#     atrp = metric_data['atrp']\n",
    "    \n",
    "#     mean_returns = daily_returns.mean()\n",
    "    \n",
    "#     # ATRP-based Sharpe. Avoid division by zero.\n",
    "#     # We replace resulting NaNs/infs with 0.\n",
    "#     sharpe_atr = mean_returns / atrp\n",
    "    \n",
    "#     return sharpe_atr.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "# # The single source of truth for all available ranking metrics.\n",
    "# # Maps the user-facing name to the calculation function.\n",
    "# METRIC_REGISTRY = {\n",
    "#     'Price': calculate_price_metric,\n",
    "#     'Sharpe': calculate_sharpe_metric,\n",
    "#     'Sharpe (ATR)': calculate_sharpe_atr_metric,\n",
    "# }\n",
    "\n",
    "# print(\"‚úÖ Metric Registry Initialized with:\", list(METRIC_REGISTRY.keys()))\n",
    "\n",
    "# # --- B. THE CORE CALCULATION ENGINE ---\n",
    "\n",
    "# def run_walk_forward_step(df_close_full, df_high_full, df_low_full,\n",
    "#                           master_trading_days,\n",
    "#                           start_date, calc_period, fwd_period,\n",
    "#                           metric, rank_start, rank_end, benchmark_ticker,\n",
    "#                           features_df,\n",
    "#                           debug=False):\n",
    "#     \"\"\"\n",
    "#     Runs a single step of the walk-forward analysis with a strict, pre-emptive\n",
    "#     check to ensure the full period is available.\n",
    "#     \"\"\"\n",
    "#     debug_data = {} if debug else None\n",
    "\n",
    "#     # 1. Determine exact date ranges with a NEW pre-emptive check\n",
    "#     try:\n",
    "#         start_idx = master_trading_days.get_loc(start_date)\n",
    "#     except KeyError:\n",
    "#         return ({'error': f\"Start date {start_date.date()} is not a valid trading day.\"}, None)\n",
    "\n",
    "#     # +++ THIS IS THE NEW PRE-EMPTIVE CHECK LOGIC +++\n",
    "#     # Calculate the desired end index without clamping first.\n",
    "#     desired_viz_end_idx = start_idx + calc_period + fwd_period\n",
    "    \n",
    "#     # Check if the desired end index is out of bounds.\n",
    "#     if desired_viz_end_idx >= len(master_trading_days):\n",
    "#         last_available_date = master_trading_days[-1].date()\n",
    "#         required_days = calc_period + fwd_period\n",
    "#         available_days = len(master_trading_days) - start_idx\n",
    "#         error_msg = (f\"Not enough data for the full requested period. \"\n",
    "#                      f\"Required: {required_days} days, Available: {available_days} days until {last_available_date}.\")\n",
    "#         return ({'error': error_msg}, None)\n",
    "#     # --- END OF NEW CHECK ---\n",
    "\n",
    "#     # If the check passes, we know the full period is available.\n",
    "#     # The 'min' calls are now just a redundant safety measure.\n",
    "#     calc_end_idx = min(start_idx + calc_period, len(master_trading_days) - 1)\n",
    "#     viz_end_idx = min(calc_end_idx + fwd_period, len(master_trading_days) - 1)\n",
    "\n",
    "#     safe_start_date = master_trading_days[start_idx]\n",
    "#     safe_calc_end_date = master_trading_days[calc_end_idx]\n",
    "#     safe_viz_end_date = master_trading_days[viz_end_idx]\n",
    "    \n",
    "#     if safe_start_date >= safe_calc_end_date:\n",
    "#         return ({'error': \"Invalid date range (calc period has zero or negative length).\"}, None)\n",
    "\n",
    "#     # (The rest of the function remains completely unchanged...)\n",
    "#     # 2. Slice data for the calculation period\n",
    "#     calc_close_raw = df_close_full.loc[safe_start_date:safe_calc_end_date]\n",
    "#     calc_close = calc_close_raw.dropna(axis=1, how='all')\n",
    "#     if calc_close.shape[1] == 0 or len(calc_close) < 2:\n",
    "#         return ({'error': \"Not enough data in calc period.\"}, None)\n",
    "\n",
    "#     # 3. Calculate INTERMEDIATE data required for the ranking metrics\n",
    "#     daily_returns = calc_close.bfill().ffill().pct_change()\n",
    "#     valid_tickers = calc_close.columns\n",
    "#     calc_period_index = pd.MultiIndex.from_product([valid_tickers, calc_close.index], names=['Ticker', 'Date'])\n",
    "#     features_in_period = features_df.loc[features_df.index.intersection(calc_period_index)]\n",
    "#     atrp = features_in_period.groupby(level='Ticker')['ATRP'].mean()\n",
    "\n",
    "#     # 4. Calculate all ranking metrics by iterating through the METRIC_REGISTRY\n",
    "#     metric_ingredients = { 'calc_close': calc_close, 'daily_returns': daily_returns, 'atrp': atrp, }\n",
    "#     metric_values = {}\n",
    "#     for name, func in METRIC_REGISTRY.items():\n",
    "#         metric_values[name] = func(metric_ingredients)\n",
    "#     if metric not in metric_values or metric_values[metric].empty:\n",
    "#         return ({'error': f\"Metric '{metric}' could not be calculated or resulted in no valid tickers.\"}, None)\n",
    "\n",
    "#     # 5. Rank tickers and select the portfolio\n",
    "#     sorted_tickers = metric_values[metric].sort_values(ascending=False)\n",
    "#     tickers_to_display = sorted_tickers.index[rank_start-1:rank_end].tolist()\n",
    "#     if not tickers_to_display:\n",
    "#         return ({'error': \"No tickers found for the selected rank.\"}, None)\n",
    "\n",
    "#     # 6. Calculate Portfolio & Benchmark Performance\n",
    "#     normalized_plot_data = df_close_full[tickers_to_display].loc[safe_start_date:safe_viz_end_date]\n",
    "#     normalized_plot_data = normalized_plot_data.div(normalized_plot_data.bfill().iloc[0])\n",
    "#     actual_calc_end_ts = calc_close.index.max()\n",
    "#     portfolio_series = normalized_plot_data.mean(axis=1)\n",
    "#     portfolio_return_series = portfolio_series.pct_change()\n",
    "#     benchmark_price_series = df_close_full.get(benchmark_ticker)\n",
    "#     benchmark_return_series = benchmark_price_series.loc[safe_start_date:safe_viz_end_date].bfill().ffill().pct_change() if benchmark_price_series is not None else pd.Series(dtype='float64')\n",
    "#     try:\n",
    "#         boundary_loc = portfolio_return_series.index.get_loc(actual_calc_end_ts)\n",
    "#         calc_portfolio_returns = portfolio_return_series.iloc[:boundary_loc + 1]\n",
    "#         fwd_portfolio_returns = portfolio_return_series.iloc[boundary_loc + 1:]\n",
    "#         if benchmark_price_series is not None:\n",
    "#             bm_boundary_loc = benchmark_return_series.index.get_loc(actual_calc_end_ts)\n",
    "#             calc_benchmark_returns = benchmark_return_series.iloc[:bm_boundary_loc + 1]\n",
    "#             fwd_benchmark_returns = benchmark_return_series.iloc[bm_boundary_loc + 1:]\n",
    "#         else:\n",
    "#             calc_benchmark_returns, fwd_benchmark_returns = pd.Series(dtype='float64'), pd.Series(dtype='float64')\n",
    "#     except (KeyError, IndexError):\n",
    "#         calc_portfolio_returns = portfolio_return_series.loc[:actual_calc_end_ts]\n",
    "#         fwd_portfolio_returns = portfolio_return_series.loc[actual_calc_end_ts:].iloc[1:]\n",
    "#         if benchmark_price_series is not None:\n",
    "#             calc_benchmark_returns = benchmark_return_series.loc[:actual_calc_end_ts]\n",
    "#             fwd_benchmark_returns = benchmark_return_series.loc[actual_calc_end_ts:].iloc[1:]\n",
    "#         else:\n",
    "#             calc_benchmark_returns, fwd_benchmark_returns = pd.Series(dtype='float64'), pd.Series(dtype='float64')\n",
    "#     full_period_index = pd.MultiIndex.from_product([tickers_to_display, portfolio_return_series.index], names=['Ticker', 'Date'])\n",
    "#     portfolio_atrp_features = features_df.loc[features_df.index.intersection(full_period_index)]\n",
    "#     portfolio_atrp_daily_unstacked = portfolio_atrp_features['ATRP'].unstack(level='Ticker')\n",
    "#     portfolio_atrp_series = portfolio_atrp_daily_unstacked.mean(axis=1)\n",
    "#     if benchmark_ticker in df_close_full.columns:\n",
    "#         benchmark_atrp_series = features_df.xs(benchmark_ticker, level='Ticker')['ATRP'].loc[safe_start_date:safe_viz_end_date]\n",
    "#     else:\n",
    "#         benchmark_atrp_series = pd.Series(dtype='float64')\n",
    "#     calc_portfolio_atrp = portfolio_atrp_series.loc[:actual_calc_end_ts]\n",
    "#     fwd_portfolio_atrp = portfolio_atrp_series.loc[actual_calc_end_ts:].iloc[1:]\n",
    "#     calc_benchmark_atrp = benchmark_atrp_series.loc[:actual_calc_end_ts]\n",
    "#     fwd_benchmark_atrp = benchmark_atrp_series.loc[actual_calc_end_ts:].iloc[1:]\n",
    "#     perf_data = {}\n",
    "#     perf_data['calc_p_gain'] = calculate_gain(portfolio_series.loc[:actual_calc_end_ts])\n",
    "#     perf_data['fwd_p_gain'] = calculate_gain(portfolio_series.loc[actual_calc_end_ts:])\n",
    "#     perf_data['full_p_gain'] = calculate_gain(portfolio_series)\n",
    "#     perf_data['calc_p_sharpe'] = calculate_sharpe(calc_portfolio_returns)\n",
    "#     perf_data['fwd_p_sharpe'] = calculate_sharpe(fwd_portfolio_returns)\n",
    "#     perf_data['full_p_sharpe'] = calculate_sharpe(portfolio_return_series)\n",
    "#     perf_data['calc_b_gain'] = calculate_gain(benchmark_price_series.loc[safe_start_date:actual_calc_end_ts]) if benchmark_price_series is not None else np.nan\n",
    "#     perf_data['fwd_b_gain'] = calculate_gain(benchmark_price_series.loc[actual_calc_end_ts:safe_viz_end_date]) if benchmark_price_series is not None else np.nan\n",
    "#     perf_data['full_b_gain'] = calculate_gain(benchmark_price_series.loc[safe_start_date:safe_viz_end_date]) if benchmark_price_series is not None else np.nan\n",
    "#     perf_data['calc_b_sharpe'] = calculate_sharpe(calc_benchmark_returns)\n",
    "#     perf_data['fwd_b_sharpe'] = calculate_sharpe(fwd_benchmark_returns)\n",
    "#     perf_data['full_b_sharpe'] = calculate_sharpe(benchmark_return_series)\n",
    "#     perf_data['calc_p_sharpe_atr'] = calculate_sharpe_atr(calc_portfolio_returns, calc_portfolio_atrp)\n",
    "#     perf_data['fwd_p_sharpe_atr'] = calculate_sharpe_atr(fwd_portfolio_returns, fwd_portfolio_atrp)\n",
    "#     perf_data['full_p_sharpe_atr'] = calculate_sharpe_atr(portfolio_return_series, portfolio_atrp_series)\n",
    "#     perf_data['calc_b_sharpe_atr'] = calculate_sharpe_atr(calc_benchmark_returns, calc_benchmark_atrp)\n",
    "#     perf_data['fwd_b_sharpe_atr'] = calculate_sharpe_atr(fwd_benchmark_returns, fwd_benchmark_atrp)\n",
    "#     perf_data['full_b_sharpe_atr'] = calculate_sharpe_atr(benchmark_return_series, benchmark_atrp_series)\n",
    "#     if debug:\n",
    "#         df_ranking_base = pd.DataFrame({'MeanDailyReturn': daily_returns.mean(),'StdDevDailyReturn': daily_returns.std(),'MeanATRP': atrp})\n",
    "#         df_metrics = pd.DataFrame(metric_values)\n",
    "#         df_metrics.columns = [f'Metric_{col}' for col in df_metrics.columns]\n",
    "#         df_ranking = df_ranking_base.join(df_metrics, how='left')\n",
    "#         df_ranking.index.name = 'Ticker'\n",
    "#         debug_data['ranking_metrics'] = df_ranking.sort_values(f'Metric_{metric}', ascending=False)\n",
    "#     calc_end_prices = calc_close.ffill().iloc[-1]\n",
    "#     fwd_close_slice = df_close_full.loc[actual_calc_end_ts:safe_viz_end_date]\n",
    "#     viz_end_prices = fwd_close_slice.ffill().iloc[-1] if not fwd_close_slice.empty and len(fwd_close_slice) >= 2 else calc_end_prices\n",
    "#     calc_gains = (calc_end_prices / calc_close.bfill().iloc[0]) - 1\n",
    "#     fwd_gains = (viz_end_prices / calc_end_prices) - 1\n",
    "#     results_df = pd.DataFrame({'Rank': range(rank_start, rank_start + len(tickers_to_display)), 'Metric': metric, 'MetricValue': sorted_tickers.loc[tickers_to_display].values, 'CalcPrice': calc_end_prices.loc[tickers_to_display], 'CalcGain': calc_gains.loc[tickers_to_display], 'FwdGain': fwd_gains.loc[tickers_to_display]}, index=pd.Index(tickers_to_display, name='Ticker'))\n",
    "#     if benchmark_price_series is not None and benchmark_ticker in calc_close.columns:\n",
    "#         benchmark_df_row = pd.DataFrame({'Rank': np.nan, 'Metric': metric, 'MetricValue': metric_values[metric].get(benchmark_ticker, np.nan), 'CalcPrice': calc_end_prices[benchmark_ticker], 'CalcGain': calc_gains[benchmark_ticker], 'FwdGain': fwd_gains[benchmark_ticker]}, index=pd.Index([f\"{benchmark_ticker} (BM)\"], name='Ticker'))\n",
    "#         results_df = pd.concat([results_df, benchmark_df_row])\n",
    "#     if debug:\n",
    "#         df_trace = normalized_plot_data.copy()\n",
    "#         df_trace.columns = [f'Norm_Price_{c}' for c in df_trace.columns]\n",
    "#         df_trace['Norm_Price_Portfolio'] = portfolio_series\n",
    "#         if benchmark_price_series is not None and not benchmark_price_series.loc[safe_start_date:safe_viz_end_date].dropna().empty:\n",
    "#             norm_bm = benchmark_price_series.loc[safe_start_date:safe_viz_end_date] / benchmark_price_series.loc[safe_start_date:].bfill().iloc[0]\n",
    "#             df_trace[f'Norm_Price_Benchmark_{benchmark_ticker}'] = norm_bm\n",
    "#         for col in df_trace.columns:\n",
    "#             if 'Norm_Price' in col:\n",
    "#                 df_trace[col.replace('Norm_Price', 'Return')] = df_trace[col].pct_change()\n",
    "#         debug_data['portfolio_trace'] = df_trace\n",
    "#     final_results = {\n",
    "#         'tickers_to_display': tickers_to_display, 'normalized_plot_data': normalized_plot_data,\n",
    "#         'portfolio_series': portfolio_series, 'benchmark_price_series': benchmark_price_series,\n",
    "#         'performance_data': perf_data, 'results_df': results_df, 'actual_calc_end_ts': actual_calc_end_ts,\n",
    "#         'safe_start_date': safe_start_date, 'safe_viz_end_date': safe_viz_end_date,\n",
    "#         'error': None\n",
    "#     }\n",
    "#     return (final_results, debug_data)\n",
    "\n",
    "# # --- C. DYNAMIC DATA QUALITY FILTER FUNCTIONS ---\n",
    "\n",
    "# def get_eligible_universe(features_df, filter_date, thresholds):\n",
    "#     \"\"\"Filters the universe of tickers based on quality metrics for a given date.\"\"\"\n",
    "#     filter_date_ts = pd.to_datetime(filter_date)\n",
    "#     # The index is now the comprehensive features_df\n",
    "#     date_index = features_df.index.get_level_values('Date').unique().sort_values()\n",
    "    \n",
    "#     if filter_date_ts < date_index[0]:\n",
    "#         print(f\"Warning: Filter date {filter_date_ts.date()} is before the earliest data point. Returning empty universe.\")\n",
    "#         return []\n",
    "        \n",
    "#     # Find the most recent date with quality data on or before the filter date\n",
    "#     valid_prior_dates = date_index[date_index <= filter_date_ts]\n",
    "#     if valid_prior_dates.empty:\n",
    "#         print(f\"Warning: No available data found on or before {filter_date_ts.date()}. Returning empty universe.\")\n",
    "#         return []\n",
    "        \n",
    "#     actual_date_to_use = valid_prior_dates[-1]\n",
    "#     if actual_date_to_use.date() != filter_date_ts.date():\n",
    "#         print(f\"‚ÑπÔ∏è Info: Filter date {filter_date_ts.date()} not found. Using previous available date {actual_date_to_use.date()}.\")\n",
    "\n",
    "#     metrics_on_date = features_df.xs(actual_date_to_use, level='Date')\n",
    "    \n",
    "#     # Apply filters using the new column names from features_df\n",
    "#     mask = ((metrics_on_date['RollMedDollarVol'] >= thresholds['min_median_dollar_volume']) & # <-- RENAMED\n",
    "#             (metrics_on_date['RollingStalePct'] <= thresholds['max_stale_pct']) &\n",
    "#             (metrics_on_date['RollingSameVolCount'] <= thresholds['max_same_vol_count']))\n",
    "            \n",
    "#     eligible_tickers = metrics_on_date[mask].index.tolist()\n",
    "#     all_tickers = metrics_on_date.index.tolist()\n",
    "#     print(f\"Dynamic Filter ({filter_date_ts.date()}): Kept {len(eligible_tickers)} of {len(all_tickers)} tickers.\")\n",
    "#     return eligible_tickers\n",
    "\n",
    "# # --- D. INTERACTIVE ANALYSIS & BACKTESTING TOOLS ---\n",
    "\n",
    "# def plot_walk_forward_analyzer(df_ohlcv,\n",
    "#                                default_start_date=None, default_calc_period=126, default_fwd_period=63,\n",
    "#                                default_metric='Sharpe (ATR)', default_rank_start=1, default_rank_end=10,\n",
    "#                                default_benchmark_ticker='VOO', master_calendar_ticker='VOO',\n",
    "#                                quality_thresholds={'min_median_dollar_volume': 1_000_000, 'max_stale_pct': 0.05, 'max_same_vol_count': 10},\n",
    "#                                debug=False):\n",
    "#     # (No changes to the initial setup part of this function...)\n",
    "#     print(\"Initializing Walk-Forward Analyzer (using Trading Day Logic)...\")\n",
    "#     if not isinstance(df_ohlcv.index, pd.MultiIndex): raise ValueError(\"Input DataFrame must have a (Ticker, Date) MultiIndex.\")\n",
    "#     df_ohlcv = df_ohlcv.sort_index()\n",
    "#     if master_calendar_ticker not in df_ohlcv.index.get_level_values(0):\n",
    "#         raise ValueError(f\"Master calendar ticker '{master_calendar_ticker}' not found in DataFrame.\")\n",
    "#     master_trading_days = df_ohlcv.loc[master_calendar_ticker].index.unique().sort_values()\n",
    "#     print(f\"Master trading day calendar created from '{master_calendar_ticker}' ({len(master_trading_days)} days).\")\n",
    "#     print(\"--- Generating all features upfront... ---\")\n",
    "#     features_df = generate_features(df_ohlcv)\n",
    "#     print(\"Pre-processing data (unstacking)...\")\n",
    "#     df_close_full = df_ohlcv['Adj Close'].unstack(level=0)\n",
    "#     df_high_full = df_ohlcv['Adj High'].unstack(level=0)\n",
    "#     df_low_full = df_ohlcv['Adj Low'].unstack(level=0)\n",
    "#     start_date_picker = widgets.DatePicker(description='Start Date:', value=pd.to_datetime(default_start_date), disabled=False)\n",
    "#     calc_period_input = widgets.IntText(value=default_calc_period, description='Calc Period (days):')\n",
    "#     fwd_period_input = widgets.IntText(value=default_fwd_period, description='Fwd Period (days):')\n",
    "#     if default_metric not in METRIC_REGISTRY:\n",
    "#         fallback_metric = list(METRIC_REGISTRY.keys())[0]\n",
    "#         print(f\"‚ö†Ô∏è Warning: Default metric '{default_metric}' not in registry. Using '{fallback_metric}'.\")\n",
    "#         default_metric = fallback_metric\n",
    "#     metric_dropdown = widgets.Dropdown(options=list(METRIC_REGISTRY.keys()), value=default_metric, description='Metric:')\n",
    "#     rank_start_input = widgets.IntText(value=default_rank_start, description='Rank Start:')\n",
    "#     rank_end_input = widgets.IntText(value=default_rank_end, description='Rank End:')\n",
    "#     benchmark_ticker_input = widgets.Text(value=default_benchmark_ticker, description='Benchmark:', placeholder='Enter Ticker')\n",
    "#     update_button = widgets.Button(description=\"Update Chart\", button_style='primary')\n",
    "#     ticker_list_output = widgets.Output()\n",
    "#     results_container, debug_data_container = [None], [None]\n",
    "#     fig = go.FigureWidget()\n",
    "#     max_traces = 50\n",
    "#     for i in range(max_traces): fig.add_trace(go.Scatter(x=[None], y=[None], mode='lines', name=f'placeholder_{i}', visible=False, showlegend=False))\n",
    "#     fig.add_trace(go.Scatter(x=[None], y=[None], mode='lines', name='Benchmark', visible=True, showlegend=True, line=dict(color='black', width=3, dash='dash')))\n",
    "#     fig.add_trace(go.Scatter(x=[None], y=[None], mode='lines', name='Group Portfolio', visible=True, showlegend=True, line=dict(color='green', width=3)))\n",
    "\n",
    "#     def update_plot(button_click):\n",
    "#         ticker_list_output.clear_output()\n",
    "#         start_date_raw = pd.to_datetime(start_date_picker.value)\n",
    "#         start_date_idx = master_trading_days.searchsorted(start_date_raw)\n",
    "#         if start_date_idx >= len(master_trading_days):\n",
    "#             with ticker_list_output: print(f\"Error: Start date is after the last available trading day.\"); return\n",
    "#         actual_start_date = master_trading_days[start_date_idx]\n",
    "#         with ticker_list_output:\n",
    "#             if start_date_raw.date() != actual_start_date.date():\n",
    "#                 print(f\"‚ÑπÔ∏è Info: Start date {start_date_raw.date()} is not a trading day. Snapping forward to {actual_start_date.date()}.\")\n",
    "#         calc_period = calc_period_input.value; fwd_period = fwd_period_input.value; metric = metric_dropdown.value\n",
    "#         rank_start = rank_start_input.value; rank_end = rank_end_input.value; benchmark_ticker = benchmark_ticker_input.value.strip().upper()\n",
    "#         if rank_start > rank_end:\n",
    "#             with ticker_list_output: print(\"Error: 'Rank Start' must be <= 'Rank End'.\"); return\n",
    "#         if rank_start < 1 or calc_period < 2 or fwd_period < 1:\n",
    "#             with ticker_list_output: print(\"Error: Ranks must be >= 1, Calc Period >= 2, Fwd Period >= 1.\"); return\n",
    "#         required_days = calc_period + fwd_period\n",
    "#         if start_date_idx + required_days > len(master_trading_days):\n",
    "#             available_days = len(master_trading_days) - start_date_idx; last_available_date = master_trading_days[-1].date()\n",
    "#             with ticker_list_output:\n",
    "#                 print(f\"Error: Not enough data for the requested period.\\n  Start Date: {actual_start_date.date()}\\n  Required Days: {calc_period} (calc) + {fwd_period} (fwd) = {required_days}\\n  Available Days from Start: {available_days} (until {last_available_date})\\n  Please shorten the 'Calc Period' / 'Fwd Period' or choose an earlier 'Start Date'.\")\n",
    "#             return\n",
    "#         eligible_tickers = get_eligible_universe(features_df, actual_start_date, quality_thresholds)\n",
    "#         if not eligible_tickers:\n",
    "#             with ticker_list_output: print(f\"Error: No eligible tickers found on {actual_start_date.date()} with the current quality filters.\"); return\n",
    "#         df_close_step = df_close_full[eligible_tickers]; df_high_step = df_high_full[eligible_tickers]; df_low_step = df_low_full[eligible_tickers]\n",
    "#         results, debug_output = run_walk_forward_step(df_close_full, df_high_full, df_low_full, master_trading_days, actual_start_date, calc_period, fwd_period, metric, rank_start, rank_end, benchmark_ticker, features_df=features_df, debug=debug)\n",
    "#         if results.get('error'):\n",
    "#             with ticker_list_output: print(f\"Error: {results['error']}\"); return\n",
    "#         period_dates = {'calc_period_start': results['safe_start_date'], 'calc_period_end': results['actual_calc_end_ts'], 'forward_period_start': results['actual_calc_end_ts'], 'forward_period_end': results['safe_viz_end_date']}\n",
    "#         run_parameters = {'calc_period': calc_period, 'fwd_period': fwd_period, 'rank_metric': metric, 'rank_start': rank_start, 'rank_end': rank_end, 'benchmark_ticker': benchmark_ticker}\n",
    "#         results.update(period_dates); results.update(run_parameters)\n",
    "#         if debug_output is not None and isinstance(debug_output, dict):\n",
    "#             debug_output.update(period_dates); debug_output.update(run_parameters)\n",
    "#         with fig.batch_update():\n",
    "#             for i in range(max_traces):\n",
    "#                 trace = fig.data[i]\n",
    "#                 if i < len(results['tickers_to_display']):\n",
    "#                     ticker = results['tickers_to_display'][i]; plot_data_series = results['normalized_plot_data'][ticker]\n",
    "#                     trace.x, trace.y, trace.name, trace.visible, trace.showlegend = plot_data_series.index, plot_data_series.values, ticker, True, True\n",
    "#                 else: trace.visible, trace.showlegend = False, False\n",
    "#             benchmark_trace = fig.data[max_traces]\n",
    "#             if results['benchmark_price_series'] is not None and not results['benchmark_price_series'].loc[results['safe_start_date']:results['safe_viz_end_date']].dropna().empty:\n",
    "#                 normalized_benchmark = results['benchmark_price_series'].loc[results['safe_start_date']:results['safe_viz_end_date']] / results['benchmark_price_series'].loc[results['safe_start_date']:].bfill().iloc[0]\n",
    "#                 benchmark_trace.x, benchmark_trace.y, benchmark_trace.name, benchmark_trace.visible = normalized_benchmark.index, normalized_benchmark.values, f\"Benchmark ({benchmark_ticker})\", True\n",
    "#             else: benchmark_trace.visible = False\n",
    "#             portfolio_trace = fig.data[max_traces + 1]\n",
    "#             portfolio_trace.x, portfolio_trace.y, portfolio_trace.name, portfolio_trace.visible = results['portfolio_series'].index, results['portfolio_series'], 'Group Portfolio', True\n",
    "#             fig.layout.shapes = []; fig.add_shape(type=\"line\", x0=results['actual_calc_end_ts'], y0=0, x1=results['actual_calc_end_ts'], y1=1, xref='x', yref='paper', line=dict(color=\"grey\", width=2, dash=\"dash\"))\n",
    "#         results_container[0] = results; debug_data_container[0] = debug_output\n",
    "#         with ticker_list_output:\n",
    "#             print(f\"Analysis Period: {results['safe_start_date'].date()} to {results['safe_viz_end_date'].date()}.\")\n",
    "#             pprint.pprint(results['tickers_to_display'])\n",
    "#             p = results['performance_data']\n",
    "            \n",
    "#             # --- START OF MODIFIED BLOCK ---\n",
    "#             rows = []\n",
    "#             # Gain Metrics\n",
    "#             rows.append({'Metric': 'Group Portfolio Gain', 'Full': p['full_p_gain'], 'Calc': p['calc_p_gain'], 'Fwd': p['fwd_p_gain']})\n",
    "#             if not np.isnan(p.get('full_b_gain')):\n",
    "#                 rows.append({'Metric': f'Benchmark ({benchmark_ticker}) Gain', 'Full': p['full_b_gain'], 'Calc': p['calc_b_gain'], 'Fwd': p['fwd_b_gain']})\n",
    "#                 rows.append({'Metric': '== Gain Delta (vs Bm)', 'Full': p['full_p_gain'] - p['full_b_gain'], 'Calc': p['calc_p_gain'] - p['calc_b_gain'], 'Fwd': p['fwd_p_gain'] - p['fwd_b_gain']})\n",
    "            \n",
    "#             # Standard Sharpe Metrics\n",
    "#             rows.append({'Metric': 'Group Portfolio Sharpe', 'Full': p['full_p_sharpe'], 'Calc': p['calc_p_sharpe'], 'Fwd': p['fwd_p_sharpe']})\n",
    "#             if not np.isnan(p.get('full_b_sharpe')):\n",
    "#                 rows.append({'Metric': f'Benchmark ({benchmark_ticker}) Sharpe', 'Full': p['full_b_sharpe'], 'Calc': p['calc_b_sharpe'], 'Fwd': p['fwd_b_sharpe']})\n",
    "#                 rows.append({'Metric': '== Sharpe Delta (vs Bm)', 'Full': p['full_p_sharpe'] - p['full_b_sharpe'], 'Calc': p['calc_p_sharpe'] - p['calc_b_sharpe'], 'Fwd': p['fwd_p_sharpe'] - p['fwd_b_sharpe']})\n",
    "\n",
    "#             # Sharpe (ATR) Metrics\n",
    "#             rows.append({'Metric': 'Group Portfolio Sharpe (ATR)', 'Full': p['full_p_sharpe_atr'], 'Calc': p['calc_p_sharpe_atr'], 'Fwd': p['fwd_p_sharpe_atr']})\n",
    "#             if not np.isnan(p.get('full_b_sharpe_atr')):\n",
    "#                 rows.append({'Metric': f'Benchmark ({benchmark_ticker}) Sharpe (ATR)', 'Full': p['full_b_sharpe_atr'], 'Calc': p['calc_b_sharpe_atr'], 'Fwd': p['fwd_b_sharpe_atr']})\n",
    "#                 rows.append({'Metric': '== Sharpe (ATR) Delta (vs Bm)', 'Full': p['full_p_sharpe_atr'] - p['full_b_sharpe_atr'], 'Calc': p['calc_p_sharpe_atr'] - p['calc_b_sharpe_atr'], 'Fwd': p['fwd_p_sharpe_atr'] - p['fwd_b_sharpe_atr']})\n",
    "\n",
    "#             report_df = pd.DataFrame(rows).set_index('Metric')\n",
    "#             gain_rows = [row for row in report_df.index if 'Gain' in row]\n",
    "#             sharpe_rows = [row for row in report_df.index if 'Sharpe' in row] # This now correctly includes both types of Sharpe\n",
    "#             # --- END OF MODIFIED BLOCK ---\n",
    "            \n",
    "#             styled_df = report_df.style.format('{:+.2%}', na_rep='N/A', subset=(gain_rows, report_df.columns)).format('{:+.4f}', na_rep='N/A', subset=(sharpe_rows, report_df.columns)).set_properties(**{'text-align': 'right', 'width': '100px'}).set_table_styles([{'selector': 'th.col_heading', 'props': [('text-align', 'right')]}, {'selector': 'th.row_heading', 'props': [('text-align', 'left')]}])\n",
    "#             print(\"\\n--- Strategy Performance Summary ---\")\n",
    "#             display(styled_df)\n",
    "#     fig.update_layout(title_text='Walk-Forward Performance Analysis', xaxis_title='Date', yaxis_title='Normalized Price (Start = 1)', hovermode='x unified', legend_title_text='Tickers (Ranked)', height=600, margin=dict(t=50))\n",
    "#     fig.add_hline(y=1, line_width=1, line_dash=\"dash\", line_color=\"grey\")\n",
    "#     update_button.on_click(update_plot)\n",
    "#     controls_row1 = widgets.HBox([start_date_picker, calc_period_input, fwd_period_input])\n",
    "#     controls_row2 = widgets.HBox([metric_dropdown, rank_start_input, rank_end_input, benchmark_ticker_input, update_button])\n",
    "#     ui_container = widgets.VBox([controls_row1, controls_row2, ticker_list_output], layout=widgets.Layout(margin='10px 0 20px 0'))\n",
    "#     display(ui_container, fig)\n",
    "#     update_plot(None)\n",
    "#     return (results_container, debug_data_container)\n",
    "\n",
    "# def run_full_backtest(df_ohlcv, strategy_params, quality_thresholds):\n",
    "#     \"\"\"Runs a full backtest of a strategy over a specified date range.\"\"\"\n",
    "#     print(f\"--- Running Full Forensic Backtest for Strategy: {strategy_params['metric']} (Top {strategy_params['rank_start']}-{strategy_params['rank_end']}) ---\")\n",
    "    \n",
    "#     # (No changes to the initial setup part...)\n",
    "#     start_date, end_date = pd.to_datetime(strategy_params['start_date']), pd.to_datetime(strategy_params['end_date'])\n",
    "#     calc_period, fwd_period = strategy_params['calc_period'], strategy_params['fwd_period']\n",
    "#     metric, rank_start, rank_end = strategy_params['metric'], strategy_params['rank_start'], strategy_params['rank_end']\n",
    "#     benchmark_ticker = strategy_params['benchmark_ticker']\n",
    "#     master_calendar_ticker = strategy_params.get('master_calendar_ticker', 'VOO')\n",
    "#     if master_calendar_ticker not in df_ohlcv.index.get_level_values(0):\n",
    "#         raise ValueError(f\"Master calendar ticker '{master_calendar_ticker}' not found in DataFrame.\")\n",
    "#     master_trading_days = df_ohlcv.loc[master_calendar_ticker].index.unique().sort_values()\n",
    "#     start_idx = master_trading_days.searchsorted(start_date)\n",
    "#     end_idx = master_trading_days.searchsorted(end_date, side='right')\n",
    "#     print(\"--- Generating all features upfront... ---\")\n",
    "#     features_df = generate_features(df_ohlcv)\n",
    "#     df_close_full = df_ohlcv['Adj Close'].unstack(level=0); df_high_full = df_ohlcv['Adj High'].unstack(level=0); df_low_full = df_ohlcv['Adj Low'].unstack(level=0)\n",
    "    \n",
    "#     # Loop through all periods in the backtest range\n",
    "#     step_indices = range(start_idx, end_idx, fwd_period)\n",
    "#     all_fwd_gains, period_by_period_debug = [], {}\n",
    "\n",
    "#     print(f\"Simulating {len(step_indices)} periods from {master_trading_days[step_indices[0]].date()} to {master_trading_days[step_indices[-1]].date()}...\")\n",
    "#     for current_idx in tqdm(step_indices, desc=\"Backtest Progress\"):\n",
    "#         step_date = master_trading_days[current_idx]\n",
    "#         eligible_tickers = get_eligible_universe(features_df, step_date, quality_thresholds)\n",
    "#         if not eligible_tickers: continue\n",
    "#         df_close_step = df_close_full[eligible_tickers]; df_high_step = df_high_full[eligible_tickers]; df_low_step = df_low_full[eligible_tickers]\n",
    "        \n",
    "#         # +++ UPDATE THE CALL HERE +++\n",
    "#         results, debug_output = run_walk_forward_step(\n",
    "#             df_close_step, df_high_step, df_low_step, master_trading_days,\n",
    "#             step_date, calc_period, fwd_period,\n",
    "#             metric, rank_start, rank_end, benchmark_ticker,\n",
    "#             features_df=features_df,  # Pass the features_df\n",
    "#             debug=True\n",
    "#         )\n",
    "        \n",
    "#         # (The rest of the function remains unchanged...)\n",
    "#         if results['error'] is None:\n",
    "#             fwd_series = results['portfolio_series'].loc[results['actual_calc_end_ts']:]\n",
    "#             all_fwd_gains.append(fwd_series.pct_change().dropna())\n",
    "#             period_by_period_debug[step_date.date().isoformat()] = debug_output\n",
    "            \n",
    "#     if not all_fwd_gains:\n",
    "#         print(\"Error: No valid periods were simulated.\"); return None\n",
    "\n",
    "#     strategy_returns = pd.concat(all_fwd_gains); strategy_equity_curve = (1 + strategy_returns).cumprod()\n",
    "#     benchmark_returns = df_close_full[benchmark_ticker].pct_change().loc[strategy_equity_curve.index]; benchmark_equity_curve = (1 + benchmark_returns).cumprod()\n",
    "#     cumulative_equity_df = pd.DataFrame({'Strategy_Equity': strategy_equity_curve, 'Benchmark_Equity': benchmark_equity_curve})\n",
    "#     fig = go.Figure()\n",
    "#     fig.add_trace(go.Scatter(x=cumulative_equity_df.index, y=cumulative_equity_df['Strategy_Equity'], name='Strategy', line=dict(color='green')))\n",
    "#     fig.add_trace(go.Scatter(x=cumulative_equity_df.index, y=cumulative_equity_df['Benchmark_Equity'], name=f'Benchmark ({benchmark_ticker})', line=dict(color='black', dash='dash')))\n",
    "#     fig.update_layout(title=f\"Cumulative Performance: '{metric}' Strategy (Top {rank_start}-{rank_end})\", xaxis_title=\"Date\", yaxis_title=\"Cumulative Growth\")\n",
    "#     fig.show()\n",
    "#     final_backtest_results = {'cumulative_performance': cumulative_equity_df, 'period_by_period_debug': period_by_period_debug}\n",
    "#     print(\"\\n‚úÖ Full backtest complete. Results object is ready for forensic analysis.\")\n",
    "#     return final_backtest_results\n",
    "\n",
    "# # --- E. VERIFICATION TOOLS (User Requested) ---\n",
    "\n",
    "# def verify_group_tickers_walk_forward_calculation(df_ohlcv, tickers_to_verify, benchmark_ticker,\n",
    "#                                                   start_date, calc_period, fwd_period,\n",
    "#                                                   master_calendar_ticker='VOO', export_csv=False):\n",
    "#     \"\"\"Verifies portfolio and benchmark performance and optionally exports the data.\"\"\"\n",
    "#     display(Markdown(f\"## Verification Report for Portfolio vs. Benchmark\"))\n",
    "#     display(Markdown(f\"**Portfolio Tickers:** `{tickers_to_verify}`\\n**Benchmark Ticker:** `{benchmark_ticker}`\"))\n",
    "    \n",
    "#     # 1. Setup trading day calendar and determine exact period dates\n",
    "#     if master_calendar_ticker not in df_ohlcv.index.get_level_values(0):\n",
    "#         raise ValueError(f\"Master calendar ticker '{master_calendar_ticker}' not found in DataFrame.\")\n",
    "#     master_trading_days = df_ohlcv.loc[master_calendar_ticker].index.unique().sort_values()\n",
    "\n",
    "#     start_date_raw = pd.to_datetime(start_date)\n",
    "#     start_idx = master_trading_days.searchsorted(start_date_raw)\n",
    "#     if start_idx >= len(master_trading_days):\n",
    "#         print(f\"Error: Start date {start_date_raw.date()} is after the last available trading day.\"); return\n",
    "#     actual_start_date = master_trading_days[start_idx]\n",
    "    \n",
    "#     calc_end_idx = min(start_idx + calc_period, len(master_trading_days) - 1)\n",
    "#     fwd_end_idx = min(calc_end_idx + fwd_period, len(master_trading_days) - 1)\n",
    "    \n",
    "#     actual_calc_end_date = master_trading_days[calc_end_idx]\n",
    "#     actual_fwd_end_date = master_trading_days[fwd_end_idx]\n",
    "    \n",
    "#     display(Markdown(f\"**Analysis Start:** `{actual_start_date.date()}` (Selected: `{start_date_raw.date()}`)\\n\"\n",
    "#                     f\"**Calc End:** `{actual_calc_end_date.date()}` ({calc_period} trading days)\\n\"\n",
    "#                     f\"**Fwd End:** `{actual_fwd_end_date.date()}` ({fwd_period} trading days)\"))\n",
    "\n",
    "#     # 2. Recreate the portfolio and benchmark series from scratch\n",
    "#     df_close_full = df_ohlcv['Adj Close'].unstack(level=0)\n",
    "#     portfolio_prices_raw_slice = df_close_full[tickers_to_verify].loc[actual_start_date:actual_fwd_end_date]\n",
    "#     portfolio_value_series = portfolio_prices_raw_slice.div(portfolio_prices_raw_slice.bfill().iloc[0]).mean(axis=1)\n",
    "#     benchmark_price_series = df_close_full.get(benchmark_ticker)\n",
    "    \n",
    "#     # 3. Optionally export the underlying daily data to a CSV for external checking\n",
    "#     if export_csv:\n",
    "#         export_df = pd.DataFrame({\n",
    "#             'Portfolio_Normalized_Price': portfolio_value_series,\n",
    "#             'Portfolio_Daily_Return': portfolio_value_series.pct_change()\n",
    "#         })\n",
    "#         if benchmark_price_series is not None:\n",
    "#             norm_bm = benchmark_price_series.loc[actual_start_date:actual_fwd_end_date]\n",
    "#             norm_bm = norm_bm / norm_bm.bfill().iloc[0]\n",
    "#             export_df['Benchmark_Normalized_Price'] = norm_bm\n",
    "#             export_df['Benchmark_Daily_Return'] = norm_bm.pct_change()\n",
    "\n",
    "#         output_dir = 'export_csv'\n",
    "#         os.makedirs(output_dir, exist_ok=True)\n",
    "#         tickers_str = '_'.join(tickers_to_verify)\n",
    "#         filename = f\"verify_group_{actual_start_date.date()}_{tickers_str}.csv\"\n",
    "#         filepath = os.path.join(output_dir, filename)\n",
    "#         export_df.to_csv(filepath)\n",
    "#         print(f\"\\n‚úÖ Data exported to: {filepath}\")\n",
    "\n",
    "#     # 4. Define a helper to print detailed calculation steps\n",
    "#     def print_verification_steps(title, price_series):\n",
    "#         display(Markdown(f\"#### Verification for: `{title}`\"))\n",
    "#         if price_series.dropna().shape[0] < 2: print(\"  - Not enough data points.\"); return {'gain': np.nan, 'sharpe': np.nan}\n",
    "#         start_price, end_price = price_series.bfill().iloc[0], price_series.ffill().iloc[-1]\n",
    "#         gain = (end_price / start_price) - 1\n",
    "#         print(f\"Start Value ({price_series.first_valid_index().date()}): {start_price:,.4f}\\nEnd Value   ({price_series.last_valid_index().date()}): {end_price:,.4f}\\nGain = {gain:.2%}\")\n",
    "#         returns = price_series.pct_change()\n",
    "#         sharpe = calculate_sharpe(returns)\n",
    "#         print(f\"Mean Daily Return: {returns.mean():.6f}\\nStd Dev: {returns.std():.6f}\\nSharpe = {sharpe:.2f}\")\n",
    "#         return {'gain': gain, 'sharpe': sharpe}\n",
    "\n",
    "#     # 5. Run verification for each period\n",
    "#     display(Markdown(\"### A. Calculation Period\"))\n",
    "#     perf_calc_p = print_verification_steps(\"Group Portfolio\", portfolio_value_series.loc[actual_start_date:actual_calc_end_date])\n",
    "#     if benchmark_price_series is not None:\n",
    "#         perf_calc_b = print_verification_steps(f\"Benchmark\", benchmark_price_series.loc[actual_start_date:actual_calc_end_date])\n",
    "    \n",
    "#     display(Markdown(\"### B. Forward Period\"))\n",
    "#     perf_fwd_p = print_verification_steps(\"Group Portfolio\", portfolio_value_series.loc[actual_calc_end_date:actual_fwd_end_date])\n",
    "#     if benchmark_price_series is not None:\n",
    "#         perf_fwd_b = print_verification_steps(f\"Benchmark\", benchmark_price_series.loc[actual_calc_end_date:actual_fwd_end_date])\n",
    "\n",
    "# def verify_ticker_ranking_metrics(df_ohlcv, ticker, start_date, calc_period,\n",
    "#                                   master_calendar_ticker='VOO', export_csv=False):\n",
    "#     \"\"\"Verifies ranking metrics for a single ticker and optionally exports the data.\"\"\"\n",
    "#     display(Markdown(f\"## Verification Report for Ticker Ranking: `{ticker}`\"))\n",
    "    \n",
    "#     # 1. Setup trading day calendar and determine exact period dates\n",
    "#     if master_calendar_ticker not in df_ohlcv.index.get_level_values(0):\n",
    "#         raise ValueError(f\"Master calendar ticker '{master_calendar_ticker}' not found in DataFrame.\")\n",
    "#     master_trading_days = df_ohlcv.loc[master_calendar_ticker].index.unique().sort_values()\n",
    "\n",
    "#     start_date_raw = pd.to_datetime(start_date)\n",
    "#     start_idx = master_trading_days.searchsorted(start_date_raw)\n",
    "#     if start_idx >= len(master_trading_days):\n",
    "#         print(f\"Error: Start date {start_date_raw.date()} is after the last available trading day.\"); return\n",
    "#     actual_start_date = master_trading_days[start_idx]\n",
    "    \n",
    "#     calc_end_idx = min(start_idx + calc_period, len(master_trading_days) - 1)\n",
    "#     actual_calc_end_date = master_trading_days[calc_end_idx]\n",
    "\n",
    "#     # 2. Extract and prepare the raw data for the specific ticker and period\n",
    "#     df_ticker = df_ohlcv.loc[ticker].sort_index()\n",
    "#     calc_df = df_ticker.loc[actual_start_date:actual_calc_end_date].copy()\n",
    "#     if calc_df.empty or len(calc_df) < 2: \n",
    "#         print(\"No data or not enough data in calc period.\"); return\n",
    "\n",
    "#     display(Markdown(\"### A. Calculation Period (for Ranking Metrics)\"))\n",
    "#     display(Markdown(f\"**Period Start:** `{actual_start_date.date()}`\\n\"\n",
    "#                     f\"**Period End:** `{actual_calc_end_date.date()}`\\n\"\n",
    "#                     f\"**Total Trading Days:** `{len(calc_df)}` (Requested: `{calc_period}`)\"))\n",
    "    \n",
    "#     display(Markdown(\"#### Detailed Metric Calculation Data\"))\n",
    "    \n",
    "#     # 3. Calculate all intermediate metrics as new columns for full transparency\n",
    "#     vdf = calc_df[['Adj High', 'Adj Low', 'Adj Close']].copy()\n",
    "#     vdf['Daily_Return'] = vdf['Adj Close'].pct_change()\n",
    "    \n",
    "#     # Corrected True Range (TR) calculation for a single ticker (Series)\n",
    "#     tr_df = pd.DataFrame({\n",
    "#         'h_l': vdf['Adj High'] - vdf['Adj Low'],\n",
    "#         'h_cp': abs(vdf['Adj High'] - vdf['Adj Close'].shift(1)),\n",
    "#         'l_cp': abs(vdf['Adj Low'] - vdf['Adj Close'].shift(1))\n",
    "#     })\n",
    "#     vdf['TR'] = tr_df.max(axis=1)\n",
    "    \n",
    "#     vdf['ATR_14'] = vdf['TR'].ewm(alpha=1/14, adjust=False).mean()\n",
    "#     vdf['ATRP'] = vdf['ATR_14'] / vdf['Adj Close']\n",
    "    \n",
    "#     print(\"--- Start of Calculation Period ---\")\n",
    "#     display(vdf.head())\n",
    "#     print(\"\\n--- End of Calculation Period ---\")\n",
    "#     display(vdf.tail())\n",
    "\n",
    "#     # 4. Optionally export this detailed breakdown to CSV\n",
    "#     if export_csv:\n",
    "#         output_dir = 'export_csv'\n",
    "#         os.makedirs(output_dir, exist_ok=True)\n",
    "#         filename = f\"verify_ticker_{actual_start_date.date()}_{ticker}.csv\"\n",
    "#         filepath = os.path.join(output_dir, filename)\n",
    "#         vdf.to_csv(filepath)\n",
    "#         print(f\"\\n‚úÖ Data exported to: {filepath}\")\n",
    "    \n",
    "#     # 5. Print final metric calculations with formulas\n",
    "#     display(Markdown(\"#### `MetricValue` Verification Summary:\"))\n",
    "    \n",
    "#     calc_start_price = vdf['Adj Close'].bfill().iloc[0]\n",
    "#     calc_end_price = vdf['Adj Close'].ffill().iloc[-1]\n",
    "#     price_metric = (calc_end_price / calc_start_price)\n",
    "#     print(f\"1. Price Metric: (Last Price / First Price) = ({calc_end_price:.2f} / {calc_start_price:.2f}) = {price_metric:.4f}\")\n",
    "    \n",
    "#     daily_returns = vdf['Daily_Return'].dropna()\n",
    "#     sharpe_ratio = calculate_sharpe(daily_returns)\n",
    "#     print(f\"2. Sharpe Metric: (Mean Daily Return / Std Dev) * sqrt(252) = {sharpe_ratio:.4f}\")\n",
    "\n",
    "#     atrp_mean = vdf['ATRP'].mean()\n",
    "#     mean_daily_return = vdf['Daily_Return'].mean()\n",
    "#     sharpe_atr = (mean_daily_return / atrp_mean) if atrp_mean > 0 else 0\n",
    "#     print(f\"3. Sharpe (ATR) Metric: (Mean Daily Return / Mean ATRP) = ({mean_daily_return:.6f} / {atrp_mean:.6f}) = {sharpe_atr:.4f}\")\n",
    "\n",
    "\n",
    "# def verify_sharpe_atr_calculation_checked(df_ohlcv, features_df, tickers_to_verify, benchmark_ticker,\n",
    "#                                     start_date, calc_period, fwd_period,\n",
    "#                                     master_calendar_ticker='VOO', debug=False):\n",
    "#     \"\"\"\n",
    "#     Verifies the Sharpe (ATR) calculations for a portfolio and benchmark.\n",
    "\n",
    "#     This function transparently recalculates the key components for Sharpe (ATR)\n",
    "#     and can optionally export the underlying source data for manual inspection.\n",
    "#     \"\"\"\n",
    "#     display(Markdown(f\"## Verification Report for Sharpe (ATR) Calculation\"))\n",
    "#     display(Markdown(f\"**Portfolio Tickers:** `{tickers_to_verify}`\\n**Benchmark Ticker:** `{benchmark_ticker}`\"))\n",
    "\n",
    "#     # --- 1. Determine Exact Period Dates (No changes here) ---\n",
    "#     master_trading_days = df_ohlcv.loc[master_calendar_ticker].index.unique().sort_values()\n",
    "#     start_date_raw = pd.to_datetime(start_date)\n",
    "#     start_idx = master_trading_days.searchsorted(start_date_raw)\n",
    "#     actual_start_date = master_trading_days[start_idx]\n",
    "#     calc_end_idx = min(start_idx + calc_period, len(master_trading_days) - 1)\n",
    "#     fwd_end_idx = min(calc_end_idx + fwd_period, len(master_trading_days) - 1)\n",
    "#     actual_calc_end_date = master_trading_days[calc_end_idx]\n",
    "#     actual_fwd_end_date = master_trading_days[fwd_end_idx]\n",
    "    \n",
    "#     display(Markdown(f\"**Full Period:** `{actual_start_date.date()}` to `{actual_fwd_end_date.date()}`\\n\"\n",
    "#                     f\"**Calc End Date:** `{actual_calc_end_date.date()}`\"))\n",
    "\n",
    "#     # Original debug export block (can be kept or removed)\n",
    "#     if debug:\n",
    "#         # ... (original export code remains here) ...\n",
    "#         pass # Assuming original block is here\n",
    "\n",
    "#     # --- 2. Recreate Portfolio & Benchmark Series from Scratch ---\n",
    "#     df_close_full = df_ohlcv['Adj Close'].unstack(level=0)\n",
    "#     portfolio_prices_raw = df_close_full[tickers_to_verify].loc[actual_start_date:actual_fwd_end_date]\n",
    "#     portfolio_prices_norm = portfolio_prices_raw.div(portfolio_prices_raw.bfill().iloc[0])\n",
    "#     portfolio_value_series = portfolio_prices_norm.mean(axis=1)\n",
    "#     portfolio_return_series = portfolio_value_series.pct_change()\n",
    "#     p_idx = pd.MultiIndex.from_product([tickers_to_verify, portfolio_return_series.index])\n",
    "#     p_atrp_df = features_df.loc[features_df.index.intersection(p_idx)]['ATRP'].unstack(level=0)\n",
    "#     portfolio_atrp_series = p_atrp_df.mean(axis=1)\n",
    "\n",
    "# ###############################    \n",
    "#     # benchmark_return_series = df_close_full[benchmark_ticker].pct_change().loc[actual_start_date:actual_fwd_end_date]\n",
    "\n",
    "#     # 1. First, slice the raw prices for the desired date range.\n",
    "#     benchmark_prices_raw = df_close_full[benchmark_ticker].loc[actual_start_date:actual_fwd_end_date]\n",
    "#     # 2. Then, calculate the percentage change on the sliced data.\n",
    "#     benchmark_return_series = benchmark_prices_raw.pct_change()\n",
    "\n",
    "# ###############################    \n",
    "#     benchmark_atrp_series = features_df.xs(benchmark_ticker, level='Ticker')['ATRP'].loc[actual_start_date:actual_fwd_end_date]\n",
    "\n",
    "\n",
    "#     # +++ NEW: DETAILED RETURN CALCULATION TRACE (PORTFOLIO) +++\n",
    "#     if debug:\n",
    "#         display(Markdown(\"---\"))\n",
    "#         display(Markdown(\"### üêõ Detailed Portfolio Return Calculation Trace\"))\n",
    "\n",
    "#         # Step 1: Show the raw prices being used\n",
    "#         print(\"\\n[STEP 1] Raw Adjusted Close prices used for calculation.\")\n",
    "#         print(\"Compare these values with your 'Adj Close' columns.\")\n",
    "#         display(portfolio_prices_raw)\n",
    "\n",
    "#         # Step 2: Show the normalization base and the result\n",
    "#         normalization_base = portfolio_prices_raw.bfill().iloc[0]\n",
    "#         print(\"\\n[STEP 2] Normalization base (first row of prices).\")\n",
    "#         print(\"Each column in Step 1 is divided by this corresponding value.\")\n",
    "#         display(pd.DataFrame(normalization_base).T)\n",
    "\n",
    "#         print(\"\\n[STEP 2a] Normalized prices (Result of division).\")\n",
    "#         print(\"Compare these values with your 'N Close' columns.\")\n",
    "#         display(portfolio_prices_norm)\n",
    "\n",
    "#         # Step 3: Show the averaged portfolio value series\n",
    "#         print(\"\\n[STEP 3] Averaged normalized portfolio value series.\")\n",
    "#         print(\"This is the row-by-row average of the table in Step 2a.\")\n",
    "#         print(\"Compare this series with your 'N_portf' column.\")\n",
    "#         display(pd.DataFrame(portfolio_value_series, columns=['N_portf']))\n",
    "\n",
    "#         # Step 4: Show the final portfolio return series\n",
    "#         print(\"\\n[STEP 4] Final portfolio daily return series (pct_change).\")\n",
    "#         print(\"This is the percentage change of the series in Step 3.\")\n",
    "#         print(\"Compare this series with your 'N_portf_rtn' column.\")\n",
    "#         display(pd.DataFrame(portfolio_return_series, columns=['N_portf_rtn']))\n",
    "#         display(Markdown(\"---\"))\n",
    "#     # +++ END OF NEW DEBUG BLOCK +++\n",
    "\n",
    "\n",
    "#     # --- 3. Define a Helper to Print Detailed Calculation Steps ---\n",
    "#     # MODIFIED to include more debug details inside\n",
    "#     def _calculate_and_print_metrics(period_name, returns, atrps):\n",
    "#         display(Markdown(f\"#### {period_name}\"))\n",
    "#         if returns.dropna().empty or atrps.dropna().empty:\n",
    "#             print(\"  - Not enough data to calculate.\")\n",
    "#             return np.nan\n",
    "\n",
    "#         # Standard calculations\n",
    "#         mean_return = returns.mean()\n",
    "#         mean_atrp = atrps.mean()\n",
    "#         sharpe_atr = mean_return / mean_atrp if mean_atrp > 0 else np.nan\n",
    "\n",
    "#         # +++ ADDED: Detailed Mean Calculation Breakdown +++\n",
    "#         if debug:\n",
    "#             valid_returns = returns.dropna()\n",
    "#             num_returns = valid_returns.count()\n",
    "#             sum_returns = valid_returns.sum()\n",
    "#             manual_mean = sum_returns / num_returns if num_returns > 0 else 0\n",
    "#             print(f\"  - (Debug) Number of valid daily returns: {num_returns}\")\n",
    "#             print(f\"  - (Debug) Sum of all daily returns:      {sum_returns:,.8f}\")\n",
    "#             print(f\"  - (Debug) Manually calculated mean:      {manual_mean:,.8f} (Sum / Count)\")\n",
    "#         # +++ END OF ADDED DETAIL +++\n",
    "\n",
    "#         print(f\"  - Mean Daily Return: {mean_return:,.6f}\")\n",
    "#         print(f\"  - Mean Daily ATRP:  {mean_atrp:,.6f}\")\n",
    "#         print(f\"  - Sharpe (ATR) = (Mean Return / Mean ATRP) = {sharpe_atr:,.4f}\")\n",
    "#         return sharpe_atr\n",
    "\n",
    "#     # --- 4. Run Verification for Portfolio (No changes here) ---\n",
    "#     display(Markdown(\"### A. Group Portfolio Verification\"))\n",
    "#     _calculate_and_print_metrics(\"Full Period\", portfolio_return_series, portfolio_atrp_series)\n",
    "#     _calculate_and_print_metrics(\"Calculation Period\", portfolio_return_series.loc[:actual_calc_end_date], portfolio_atrp_series.loc[:actual_calc_end_date])\n",
    "#     _calculate_and_print_metrics(\"Forward Period\", portfolio_return_series.loc[actual_calc_end_date:].iloc[1:], portfolio_atrp_series.loc[actual_calc_end_date:].iloc[1:])\n",
    "\n",
    "#     # --- 5. Run Verification for Benchmark (No changes here) ---\n",
    "#     display(Markdown(f\"### B. Benchmark ({benchmark_ticker}) Verification\"))\n",
    "#     _calculate_and_print_metrics(\"Full Period\", benchmark_return_series, benchmark_atrp_series)\n",
    "#     _calculate_and_print_metrics(\"Calculation Period\", benchmark_return_series.loc[:actual_calc_end_date], benchmark_atrp_series.loc[:actual_calc_end_date])\n",
    "#     _calculate_and_print_metrics(\"Forward Period\", benchmark_return_series.loc[actual_calc_end_date:].iloc[1:], benchmark_atrp_series.loc[actual_calc_end_date:].iloc[1:])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # --- F. AUTOMATION SCRIPT - STRATEGY SEARCH ---\n",
    "\n",
    "# def run_strategy_search(df_ohlcv, config):\n",
    "#     \"\"\"\n",
    "#     Runs the main backtesting loop with checkpointing to be resumable.\n",
    "#     \"\"\"\n",
    "#     start_time = time.time()\n",
    "    \n",
    "#     # --- 1. SETUP & LOAD PROGRESS ---\n",
    "#     print(\"--- Phase 1: Pre-processing and Loading Progress ---\")\n",
    "\n",
    "#     # +++ ADD THIS BLOCK +++\n",
    "#     # Pre-calculate all features for the entire dataset ONCE.\n",
    "#     print(\"--- Generating all features upfront... ---\")\n",
    "#     features_df = generate_features(df_ohlcv)\n",
    "\n",
    "#     # --- DELETE THIS LINE ---\n",
    "#     # quality_metrics_df = calculate_rolling_quality_metrics(df_ohlcv, window=252)\n",
    "    \n",
    "#     print(\"Unstacking data for performance...\")\n",
    "#     df_close_full = df_ohlcv['Adj Close'].unstack(level=0)\n",
    "#     df_high_full = df_ohlcv['Adj High'].unstack(level=0)\n",
    "#     df_low_full = df_ohlcv['Adj Low'].unstack(level=0)\n",
    "    \n",
    "#     master_calendar_ticker = config['master_calendar_ticker']\n",
    "#     master_trading_days = df_ohlcv.loc[master_calendar_ticker].index.unique().sort_values()\n",
    "#     print(f\"Master trading day calendar created from '{master_calendar_ticker}' ({len(master_trading_days)} days).\")\n",
    "\n",
    "#     results_path = config['results_output_path']\n",
    "#     completed_params = set()\n",
    "    \n",
    "#     if os.path.exists(results_path):\n",
    "#         print(f\"Found existing results file. Loading progress from: {results_path}\")\n",
    "#         df_progress = pd.read_csv(results_path)\n",
    "#         for _, row in df_progress.iterrows():\n",
    "#             param_key = (\n",
    "#                 row['calc_period'], row['fwd_period'], row['metric'],\n",
    "#                 (row['rank_start'], row['rank_end'])\n",
    "#             )\n",
    "#             completed_params.add(param_key)\n",
    "#         print(f\"Found {len(completed_params)} completed parameter sets to skip.\")\n",
    "#     else:\n",
    "#         print(\"No existing results file found. Starting a new run.\")\n",
    "\n",
    "#     print(\"‚úÖ Pre-processing complete.\\n\")\n",
    "\n",
    "#     # --- 2. SETUP THE MAIN LOOP (No changes here) ---\n",
    "#     print(\"--- Phase 2: Setting up Simulation Loops ---\")\n",
    "#     param_combinations = list(product(\n",
    "#         config['calc_periods'], config['fwd_periods'],\n",
    "#         config['metrics'], config['rank_slices']\n",
    "#     ))\n",
    "#     search_start_date = pd.to_datetime(config['search_start_date'])\n",
    "#     search_end_date = pd.to_datetime(config['search_end_date'])\n",
    "#     start_idx = master_trading_days.searchsorted(search_start_date, side='left')\n",
    "#     end_idx = master_trading_days.searchsorted(search_end_date, side='right')\n",
    "#     step_dates_map = {}\n",
    "#     print(\"Pre-calculating rebalancing schedules for each holding period...\")\n",
    "#     for fwd_period in sorted(config['fwd_periods']):\n",
    "#         step_indices = range(start_idx, end_idx, fwd_period)\n",
    "#         step_dates_map[fwd_period] = master_trading_days[step_indices]\n",
    "#         print(f\"  - Holding Period {fwd_period} days: {len(step_dates_map[fwd_period])} rebalances\")\n",
    "#     print(f\"Found {len(param_combinations)} total parameter sets to simulate.\")\n",
    "#     print(\"‚úÖ Setup complete. Starting main loop...\\n\")\n",
    "\n",
    "#     # --- 3. RUN THE MAIN LOOP ---\n",
    "#     print(\"--- Phase 3: Running Simulations ---\")\n",
    "#     pbar = tqdm(param_combinations, desc=\"Parameter Sets\")\n",
    "    \n",
    "#     for params in pbar:\n",
    "#         calc_period, fwd_period, metric, rank_slice = params\n",
    "#         rank_start, rank_end = rank_slice\n",
    "        \n",
    "#         param_key = (calc_period, fwd_period, metric, rank_slice)\n",
    "#         if param_key in completed_params:\n",
    "#             pbar.set_description(f\"Skipping {param_key}\")\n",
    "#             continue\n",
    "\n",
    "#         pbar.set_description(f\"Running {param_key}\")\n",
    "        \n",
    "#         current_params_results = []\n",
    "        \n",
    "#         current_step_dates = step_dates_map[fwd_period]\n",
    "#         for step_date in current_step_dates:\n",
    "#             # +++ UPDATE THIS CALL +++\n",
    "#             eligible_tickers = get_eligible_universe(\n",
    "#                 features_df, filter_date=step_date, thresholds=config['quality_thresholds']\n",
    "#             )\n",
    "#             if not eligible_tickers: continue\n",
    "            \n",
    "#             df_close_step = df_close_full[eligible_tickers]\n",
    "#             df_high_step = df_high_full[eligible_tickers]\n",
    "#             df_low_step = df_low_full[eligible_tickers]\n",
    "\n",
    "#             # +++ UPDATE THIS CALL +++\n",
    "#             step_result, _ = run_walk_forward_step(\n",
    "#                 df_close_full=df_close_step, df_high_full=df_high_step, df_low_full=df_low_step,\n",
    "#                 master_trading_days=master_trading_days, start_date=step_date,\n",
    "#                 calc_period=calc_period, fwd_period=fwd_period,\n",
    "#                 metric=metric, rank_start=rank_start, rank_end=rank_end,\n",
    "#                 benchmark_ticker=config['benchmark_ticker'],\n",
    "#                 features_df=features_df, # Pass the features_df\n",
    "#                 debug=False\n",
    "#             )\n",
    "            \n",
    "#             if step_result['error'] is None:\n",
    "#                 p = step_result['performance_data']\n",
    "#                 log_entry = {\n",
    "#                     'step_date': step_date.date(), 'calc_period': calc_period,\n",
    "#                     'fwd_period': fwd_period, 'metric': metric,\n",
    "#                     'rank_start': rank_start, 'rank_end': rank_end,\n",
    "#                     'num_universe': len(eligible_tickers),\n",
    "#                     'num_portfolio': len(step_result['tickers_to_display']),\n",
    "#                     'fwd_p_gain': p['fwd_p_gain'], 'fwd_b_gain': p['fwd_b_gain'],\n",
    "#                     'fwd_gain_delta': p['fwd_p_gain'] - p['fwd_b_gain'] if not np.isnan(p['fwd_b_gain']) else np.nan,\n",
    "#                     'fwd_p_sharpe': p['fwd_p_sharpe'],\n",
    "#                 }\n",
    "#                 current_params_results.append(log_entry)\n",
    "        \n",
    "#         # --- CHECKPOINTING (No changes here) ---\n",
    "#         if current_params_results:\n",
    "#             df_to_append = pd.DataFrame(current_params_results)\n",
    "#             df_to_append.to_csv(\n",
    "#                 results_path,\n",
    "#                 mode='a',\n",
    "#                 header=not os.path.exists(results_path),\n",
    "#                 index=False\n",
    "#             )\n",
    "#             completed_params.add(param_key)\n",
    "\n",
    "#     print(\"‚úÖ Main loop finished.\\n\")\n",
    "    \n",
    "#     # --- 4. RETURN FINAL DATAFRAME (No changes here) ---\n",
    "#     print(\"--- Phase 4: Loading Final Results ---\")\n",
    "#     if os.path.exists(results_path):\n",
    "#         final_df = pd.read_csv(results_path)\n",
    "#         end_time = time.time()\n",
    "#         print(f\"‚úÖ Process complete. Total execution time: {time.time() - start_time:.2f} seconds.\")\n",
    "#         return final_df\n",
    "#     else:\n",
    "#         print(\"Warning: No results were generated.\")\n",
    "#         return None    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072b3d79",
   "metadata": {},
   "source": [
    "# Below are only functions that required to run plot_walk_forward_analyzer  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64cc5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import ipywidgets as widgets\n",
    "import pprint\n",
    "from IPython.display import display, Markdown\n",
    "import os # Make sure os is imported for the export function later\n",
    "\n",
    "\n",
    "def calculate_gain(price_series: pd.Series):\n",
    "    \"\"\"Calculates the total gain over a series of prices.\"\"\"\n",
    "    # Ensure there are at least two data points to calculate a gain\n",
    "    if price_series.dropna().shape[0] < 2: return np.nan\n",
    "    # Use forward-fill for the end price and back-fill for the start price\n",
    "    # to handle potential NaNs at the beginning or end of the series.\n",
    "    return (price_series.ffill().iloc[-1] / price_series.bfill().iloc[0]) - 1\n",
    "\n",
    "def calculate_sharpe(return_series: pd.Series):\n",
    "    \"\"\"Calculates the annualized Sharpe ratio from a series of daily returns.\"\"\"\n",
    "    # Ensure there are at least two returns to calculate a standard deviation\n",
    "    if return_series.dropna().shape[0] < 2: return np.nan\n",
    "    std_dev = return_series.std()\n",
    "    # Avoid division by zero if returns are constant\n",
    "    if std_dev > 0 and std_dev != np.inf:\n",
    "        return (return_series.mean() / std_dev) * np.sqrt(252)\n",
    "    return np.nan\n",
    "\n",
    "def calculate_sharpe_atr(return_series: pd.Series, atrp_series: pd.Series):\n",
    "    \"\"\"Calculates a Sharpe-like ratio using mean ATRP as the denominator.\"\"\"\n",
    "    # Ensure there are returns and that ATRP data is valid\n",
    "    if return_series.dropna().shape[0] < 2 or atrp_series.dropna().empty:\n",
    "        return np.nan\n",
    "        \n",
    "    mean_return = return_series.mean()\n",
    "    mean_atrp = atrp_series.mean()\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    if mean_atrp > 0 and mean_atrp != np.inf:\n",
    "        return mean_return / mean_atrp\n",
    "        \n",
    "    return np.nan\n",
    "\n",
    "\n",
    "# --- B. MODULAR METRIC CALCULATION ENGINE ---\n",
    "\n",
    "def calculate_price_metric(metric_data: dict) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Calculates the 'Price' metric (total gain) over the calculation period.\n",
    "\n",
    "    Args:\n",
    "        metric_data: A dictionary containing pre-calculated data Series.\n",
    "                     Requires 'calc_close' (pd.DataFrame): The close prices for the calc period.\n",
    "\n",
    "    Returns:\n",
    "        A pandas Series of the calculated metric values, indexed by Ticker.\n",
    "    \"\"\"\n",
    "    calc_close = metric_data['calc_close']\n",
    "    \n",
    "    # Ensure there are at least two data points to calculate a gain\n",
    "    if len(calc_close) < 2:\n",
    "        return pd.Series(dtype='float64', index=calc_close.columns)\n",
    "\n",
    "    first_prices = calc_close.bfill().iloc[0]\n",
    "    last_prices = calc_close.ffill().iloc[-1]\n",
    "    \n",
    "    # The division of two Series aligns by index (Ticker), which is what we want.\n",
    "    price_metric = last_prices / first_prices\n",
    "    \n",
    "    return price_metric.dropna()\n",
    "\n",
    "def calculate_sharpe_metric(metric_data: dict) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Calculates the annualized 'Sharpe' ratio metric over the calculation period.\n",
    "\n",
    "    Args:\n",
    "        metric_data: A dictionary containing pre-calculated data Series.\n",
    "                     Requires 'daily_returns' (pd.DataFrame): Daily returns for the calc period.\n",
    "\n",
    "    Returns:\n",
    "        A pandas Series of the calculated metric values, indexed by Ticker.\n",
    "    \"\"\"\n",
    "    daily_returns = metric_data['daily_returns']\n",
    "    \n",
    "    # Ensure there's enough data to calculate standard deviation\n",
    "    if len(daily_returns.dropna()) < 2:\n",
    "        return pd.Series(dtype='float64', index=daily_returns.columns)\n",
    "    \n",
    "    mean_returns = daily_returns.mean()\n",
    "    std_returns = daily_returns.std()\n",
    "    \n",
    "    # Standard annualized Sharpe Ratio calculation. Avoid division by zero.\n",
    "    # We replace resulting NaNs/infs with 0 to handle cases of zero volatility.\n",
    "    sharpe_ratio = (mean_returns / std_returns * np.sqrt(252))\n",
    "    \n",
    "    return sharpe_ratio.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "    \n",
    "def calculate_sharpe_atr_metric(metric_data: dict) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Calculates the 'Sharpe (ATR)' metric over the calculation period.\n",
    "\n",
    "    Args:\n",
    "        metric_data: A dictionary containing pre-calculated data Series.\n",
    "                     Requires 'daily_returns' (pd.DataFrame): Daily returns for the calc period.\n",
    "                     Requires 'atrp' (pd.Series): Mean ATRP for each ticker over the period.\n",
    "\n",
    "    Returns:\n",
    "        A pandas Series of the calculated metric values, indexed by Ticker.\n",
    "    \"\"\"\n",
    "    daily_returns = metric_data['daily_returns']\n",
    "    atrp = metric_data['atrp']\n",
    "    \n",
    "    mean_returns = daily_returns.mean()\n",
    "    \n",
    "    # ATRP-based Sharpe. Avoid division by zero.\n",
    "    # We replace resulting NaNs/infs with 0.\n",
    "    sharpe_atr = mean_returns / atrp\n",
    "    \n",
    "    return sharpe_atr.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "\n",
    "METRIC_REGISTRY = {\n",
    "    'Price': calculate_price_metric,\n",
    "    'Sharpe': calculate_sharpe_metric,\n",
    "    'Sharpe (ATR)': calculate_sharpe_atr_metric,\n",
    "}\n",
    "\n",
    "def plot_walk_forward_analyzer(df_ohlcv,\n",
    "                               default_start_date=None, default_calc_period=126, default_fwd_period=63,\n",
    "                               default_metric='Sharpe (ATR)', default_rank_start=1, default_rank_end=10,\n",
    "                               default_benchmark_ticker='VOO', master_calendar_ticker='VOO',\n",
    "                               quality_thresholds={'min_median_dollar_volume': 1_000_000, 'max_stale_pct': 0.05, 'max_same_vol_count': 10},\n",
    "                               debug=False):\n",
    "    # (No changes to the initial setup part of this function...)\n",
    "    print(\"Initializing Walk-Forward Analyzer (using Trading Day Logic)...\")\n",
    "    if not isinstance(df_ohlcv.index, pd.MultiIndex): raise ValueError(\"Input DataFrame must have a (Ticker, Date) MultiIndex.\")\n",
    "    df_ohlcv = df_ohlcv.sort_index()\n",
    "    if master_calendar_ticker not in df_ohlcv.index.get_level_values(0):\n",
    "        raise ValueError(f\"Master calendar ticker '{master_calendar_ticker}' not found in DataFrame.\")\n",
    "    master_trading_days = df_ohlcv.loc[master_calendar_ticker].index.unique().sort_values()\n",
    "    print(f\"Master trading day calendar created from '{master_calendar_ticker}' ({len(master_trading_days)} days).\")\n",
    "    print(\"--- Generating all features upfront... ---\")\n",
    "    features_df = generate_features(df_ohlcv)\n",
    "    print(\"Pre-processing data (unstacking)...\")\n",
    "    df_close_full = df_ohlcv['Adj Close'].unstack(level=0)\n",
    "    df_high_full = df_ohlcv['Adj High'].unstack(level=0)\n",
    "    df_low_full = df_ohlcv['Adj Low'].unstack(level=0)\n",
    "    start_date_picker = widgets.DatePicker(description='Start Date:', value=pd.to_datetime(default_start_date), disabled=False)\n",
    "    calc_period_input = widgets.IntText(value=default_calc_period, description='Calc Period (days):')\n",
    "    fwd_period_input = widgets.IntText(value=default_fwd_period, description='Fwd Period (days):')\n",
    "    if default_metric not in METRIC_REGISTRY:\n",
    "        fallback_metric = list(METRIC_REGISTRY.keys())[0]\n",
    "        print(f\"‚ö†Ô∏è Warning: Default metric '{default_metric}' not in registry. Using '{fallback_metric}'.\")\n",
    "        default_metric = fallback_metric\n",
    "    metric_dropdown = widgets.Dropdown(options=list(METRIC_REGISTRY.keys()), value=default_metric, description='Metric:')\n",
    "    rank_start_input = widgets.IntText(value=default_rank_start, description='Rank Start:')\n",
    "    rank_end_input = widgets.IntText(value=default_rank_end, description='Rank End:')\n",
    "    benchmark_ticker_input = widgets.Text(value=default_benchmark_ticker, description='Benchmark:', placeholder='Enter Ticker')\n",
    "    update_button = widgets.Button(description=\"Update Chart\", button_style='primary')\n",
    "    ticker_list_output = widgets.Output()\n",
    "    results_container, debug_data_container = [None], [None]\n",
    "    fig = go.FigureWidget()\n",
    "    max_traces = 50\n",
    "    for i in range(max_traces): fig.add_trace(go.Scatter(x=[None], y=[None], mode='lines', name=f'placeholder_{i}', visible=False, showlegend=False))\n",
    "    fig.add_trace(go.Scatter(x=[None], y=[None], mode='lines', name='Benchmark', visible=True, showlegend=True, line=dict(color='black', width=3, dash='dash')))\n",
    "    fig.add_trace(go.Scatter(x=[None], y=[None], mode='lines', name='Group Portfolio', visible=True, showlegend=True, line=dict(color='green', width=3)))\n",
    "\n",
    "    def update_plot(button_click):\n",
    "        ticker_list_output.clear_output()\n",
    "        start_date_raw = pd.to_datetime(start_date_picker.value)\n",
    "        start_date_idx = master_trading_days.searchsorted(start_date_raw)\n",
    "        if start_date_idx >= len(master_trading_days):\n",
    "            with ticker_list_output: print(f\"Error: Start date is after the last available trading day.\"); return\n",
    "        actual_start_date = master_trading_days[start_date_idx]\n",
    "        with ticker_list_output:\n",
    "            if start_date_raw.date() != actual_start_date.date():\n",
    "                print(f\"‚ÑπÔ∏è Info: Start date {start_date_raw.date()} is not a trading day. Snapping forward to {actual_start_date.date()}.\")\n",
    "        calc_period = calc_period_input.value; fwd_period = fwd_period_input.value; metric = metric_dropdown.value\n",
    "        rank_start = rank_start_input.value; rank_end = rank_end_input.value; benchmark_ticker = benchmark_ticker_input.value.strip().upper()\n",
    "        if rank_start > rank_end:\n",
    "            with ticker_list_output: print(\"Error: 'Rank Start' must be <= 'Rank End'.\"); return\n",
    "        if rank_start < 1 or calc_period < 2 or fwd_period < 1:\n",
    "            with ticker_list_output: print(\"Error: Ranks must be >= 1, Calc Period >= 2, Fwd Period >= 1.\"); return\n",
    "        required_days = calc_period + fwd_period\n",
    "        if start_date_idx + required_days > len(master_trading_days):\n",
    "            available_days = len(master_trading_days) - start_date_idx; last_available_date = master_trading_days[-1].date()\n",
    "            with ticker_list_output:\n",
    "                print(f\"Error: Not enough data for the requested period.\\n  Start Date: {actual_start_date.date()}\\n  Required Days: {calc_period} (calc) + {fwd_period} (fwd) = {required_days}\\n  Available Days from Start: {available_days} (until {last_available_date})\\n  Please shorten the 'Calc Period' / 'Fwd Period' or choose an earlier 'Start Date'.\")\n",
    "            return\n",
    "        eligible_tickers = get_eligible_universe(features_df, actual_start_date, quality_thresholds)\n",
    "        if not eligible_tickers:\n",
    "            with ticker_list_output: print(f\"Error: No eligible tickers found on {actual_start_date.date()} with the current quality filters.\"); return\n",
    "        df_close_step = df_close_full[eligible_tickers]; df_high_step = df_high_full[eligible_tickers]; df_low_step = df_low_full[eligible_tickers]\n",
    "        results, debug_output = run_walk_forward_step(df_close_full, df_high_full, df_low_full, master_trading_days, actual_start_date, calc_period, fwd_period, metric, rank_start, rank_end, benchmark_ticker, features_df=features_df, debug=debug)\n",
    "        if results.get('error'):\n",
    "            with ticker_list_output: print(f\"Error: {results['error']}\"); return\n",
    "        period_dates = {'calc_period_start': results['safe_start_date'], 'calc_period_end': results['actual_calc_end_ts'], 'forward_period_start': results['actual_calc_end_ts'], 'forward_period_end': results['safe_viz_end_date']}\n",
    "        run_parameters = {'calc_period': calc_period, 'fwd_period': fwd_period, 'rank_metric': metric, 'rank_start': rank_start, 'rank_end': rank_end, 'benchmark_ticker': benchmark_ticker}\n",
    "        results.update(period_dates); results.update(run_parameters)\n",
    "        if debug_output is not None and isinstance(debug_output, dict):\n",
    "            debug_output.update(period_dates); debug_output.update(run_parameters)\n",
    "        with fig.batch_update():\n",
    "            for i in range(max_traces):\n",
    "                trace = fig.data[i]\n",
    "                if i < len(results['tickers_to_display']):\n",
    "                    ticker = results['tickers_to_display'][i]; plot_data_series = results['normalized_plot_data'][ticker]\n",
    "                    trace.x, trace.y, trace.name, trace.visible, trace.showlegend = plot_data_series.index, plot_data_series.values, ticker, True, True\n",
    "                else: trace.visible, trace.showlegend = False, False\n",
    "            benchmark_trace = fig.data[max_traces]\n",
    "            if results['benchmark_price_series'] is not None and not results['benchmark_price_series'].loc[results['safe_start_date']:results['safe_viz_end_date']].dropna().empty:\n",
    "                normalized_benchmark = results['benchmark_price_series'].loc[results['safe_start_date']:results['safe_viz_end_date']] / results['benchmark_price_series'].loc[results['safe_start_date']:].bfill().iloc[0]\n",
    "                benchmark_trace.x, benchmark_trace.y, benchmark_trace.name, benchmark_trace.visible = normalized_benchmark.index, normalized_benchmark.values, f\"Benchmark ({benchmark_ticker})\", True\n",
    "            else: benchmark_trace.visible = False\n",
    "            portfolio_trace = fig.data[max_traces + 1]\n",
    "            portfolio_trace.x, portfolio_trace.y, portfolio_trace.name, portfolio_trace.visible = results['portfolio_series'].index, results['portfolio_series'], 'Group Portfolio', True\n",
    "            fig.layout.shapes = []; fig.add_shape(type=\"line\", x0=results['actual_calc_end_ts'], y0=0, x1=results['actual_calc_end_ts'], y1=1, xref='x', yref='paper', line=dict(color=\"grey\", width=2, dash=\"dash\"))\n",
    "        results_container[0] = results; debug_data_container[0] = debug_output\n",
    "        with ticker_list_output:\n",
    "            print(f\"Analysis Period: {results['safe_start_date'].date()} to {results['safe_viz_end_date'].date()}.\")\n",
    "            pprint.pprint(results['tickers_to_display'])\n",
    "            p = results['performance_data']\n",
    "            \n",
    "            # --- START OF MODIFIED BLOCK ---\n",
    "            rows = []\n",
    "            # Gain Metrics\n",
    "            rows.append({'Metric': 'Group Portfolio Gain', 'Full': p['full_p_gain'], 'Calc': p['calc_p_gain'], 'Fwd': p['fwd_p_gain']})\n",
    "            if not np.isnan(p.get('full_b_gain')):\n",
    "                rows.append({'Metric': f'Benchmark ({benchmark_ticker}) Gain', 'Full': p['full_b_gain'], 'Calc': p['calc_b_gain'], 'Fwd': p['fwd_b_gain']})\n",
    "                rows.append({'Metric': '== Gain Delta (vs Bm)', 'Full': p['full_p_gain'] - p['full_b_gain'], 'Calc': p['calc_p_gain'] - p['calc_b_gain'], 'Fwd': p['fwd_p_gain'] - p['fwd_b_gain']})\n",
    "            \n",
    "            # Standard Sharpe Metrics\n",
    "            rows.append({'Metric': 'Group Portfolio Sharpe', 'Full': p['full_p_sharpe'], 'Calc': p['calc_p_sharpe'], 'Fwd': p['fwd_p_sharpe']})\n",
    "            if not np.isnan(p.get('full_b_sharpe')):\n",
    "                rows.append({'Metric': f'Benchmark ({benchmark_ticker}) Sharpe', 'Full': p['full_b_sharpe'], 'Calc': p['calc_b_sharpe'], 'Fwd': p['fwd_b_sharpe']})\n",
    "                rows.append({'Metric': '== Sharpe Delta (vs Bm)', 'Full': p['full_p_sharpe'] - p['full_b_sharpe'], 'Calc': p['calc_p_sharpe'] - p['calc_b_sharpe'], 'Fwd': p['fwd_p_sharpe'] - p['fwd_b_sharpe']})\n",
    "\n",
    "            # Sharpe (ATR) Metrics\n",
    "            rows.append({'Metric': 'Group Portfolio Sharpe (ATR)', 'Full': p['full_p_sharpe_atr'], 'Calc': p['calc_p_sharpe_atr'], 'Fwd': p['fwd_p_sharpe_atr']})\n",
    "            if not np.isnan(p.get('full_b_sharpe_atr')):\n",
    "                rows.append({'Metric': f'Benchmark ({benchmark_ticker}) Sharpe (ATR)', 'Full': p['full_b_sharpe_atr'], 'Calc': p['calc_b_sharpe_atr'], 'Fwd': p['fwd_b_sharpe_atr']})\n",
    "                rows.append({'Metric': '== Sharpe (ATR) Delta (vs Bm)', 'Full': p['full_p_sharpe_atr'] - p['full_b_sharpe_atr'], 'Calc': p['calc_p_sharpe_atr'] - p['calc_b_sharpe_atr'], 'Fwd': p['fwd_p_sharpe_atr'] - p['fwd_b_sharpe_atr']})\n",
    "\n",
    "            report_df = pd.DataFrame(rows).set_index('Metric')\n",
    "            gain_rows = [row for row in report_df.index if 'Gain' in row]\n",
    "            sharpe_rows = [row for row in report_df.index if 'Sharpe' in row] # This now correctly includes both types of Sharpe\n",
    "            # --- END OF MODIFIED BLOCK ---\n",
    "            \n",
    "            styled_df = report_df.style.format('{:+.2%}', na_rep='N/A', subset=(gain_rows, report_df.columns)).format('{:+.4f}', na_rep='N/A', subset=(sharpe_rows, report_df.columns)).set_properties(**{'text-align': 'right', 'width': '100px'}).set_table_styles([{'selector': 'th.col_heading', 'props': [('text-align', 'right')]}, {'selector': 'th.row_heading', 'props': [('text-align', 'left')]}])\n",
    "            print(\"\\n--- Strategy Performance Summary ---\")\n",
    "            display(styled_df)\n",
    "    fig.update_layout(title_text='Walk-Forward Performance Analysis', xaxis_title='Date', yaxis_title='Normalized Price (Start = 1)', hovermode='x unified', legend_title_text='Tickers (Ranked)', height=600, margin=dict(t=50))\n",
    "    fig.add_hline(y=1, line_width=1, line_dash=\"dash\", line_color=\"grey\")\n",
    "    update_button.on_click(update_plot)\n",
    "    controls_row1 = widgets.HBox([start_date_picker, calc_period_input, fwd_period_input])\n",
    "    controls_row2 = widgets.HBox([metric_dropdown, rank_start_input, rank_end_input, benchmark_ticker_input, update_button])\n",
    "    ui_container = widgets.VBox([controls_row1, controls_row2, ticker_list_output], layout=widgets.Layout(margin='10px 0 20px 0'))\n",
    "    display(ui_container, fig)\n",
    "    update_plot(None)\n",
    "    return (results_container, debug_data_container)\n",
    "\n",
    "\n",
    "def generate_features(df_ohlcv: pd.DataFrame, \n",
    "                      atr_period: int = 14, \n",
    "                      quality_window: int = 252, \n",
    "                      quality_min_periods: int = 126) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generates a comprehensive DataFrame of derived features from raw OHLCV data.\n",
    "\n",
    "    This function performs all heavy, window-based calculations upfront to be used\n",
    "    by downstream analysis functions. It calculates:\n",
    "    1. Technical Indicators: True Range (TR), ATR, and ATRP.\n",
    "    2. Data Quality Metrics: Rolling stale percentage, median dollar volume, etc.\n",
    "\n",
    "    Args:\n",
    "        df_ohlcv: The primary DataFrame with a (Ticker, Date) MultiIndex and\n",
    "                  columns for 'Adj High', 'Adj Low', 'Adj Close', 'Volume'.\n",
    "        atr_period: The lookback period for the ATR's Exponential Moving Average.\n",
    "        quality_window: The rolling window size for data quality metrics.\n",
    "        quality_min_periods: The minimum number of observations required to have\n",
    "                             a valid quality metric.\n",
    "\n",
    "    Returns:\n",
    "        A new DataFrame with the same (Ticker, Date) MultiIndex containing all\n",
    "        calculated feature columns.\n",
    "    \"\"\"\n",
    "    print(\"--- Starting Feature Generation ---\")\n",
    "    \n",
    "    # Ensure the DataFrame is sorted for correct window and shift operations\n",
    "    # FIX: Replaced is_lexsorted() with the current pandas attribute\n",
    "    if not df_ohlcv.index.is_monotonic_increasing:\n",
    "        print(\"Sorting index for calculation accuracy...\")\n",
    "        df_ohlcv = df_ohlcv.sort_index()\n",
    "\n",
    "    # --- 1. Technical Indicator Calculation (TR, ATR, ATRP) ---\n",
    "    print(f\"Calculating technical indicators (ATR Period: {atr_period})...\")\n",
    "    \n",
    "    # Group by ticker to handle each security independently\n",
    "    grouped = df_ohlcv.groupby(level='Ticker')\n",
    "    \n",
    "    # Get the previous day's close required for True Range\n",
    "    prev_close = grouped['Adj Close'].shift(1)\n",
    "    \n",
    "    # Calculate the three components of True Range\n",
    "    high_low = df_ohlcv['Adj High'] - df_ohlcv['Adj Low']\n",
    "    high_prev_close = abs(df_ohlcv['Adj High'] - prev_close)\n",
    "    low_prev_close = abs(df_ohlcv['Adj Low'] - prev_close)\n",
    "    \n",
    "    # Combine the components to get the final TR\n",
    "    tr = pd.concat([high_low, high_prev_close, low_prev_close], axis=1).max(axis=1, skipna=False)\n",
    "    \n",
    "    # # Calculate the ATR using an Exponential Moving Average\n",
    "    # --- FIX IS HERE ---\n",
    "    # Use .transform() to apply the EWM function. \n",
    "    # This guarantees the resulting Series has the exact same index as 'tr',\n",
    "    # preventing the index alignment error during the subsequent division.\n",
    "    atr = tr.groupby(level='Ticker').transform(\n",
    "        lambda x: x.ewm(alpha=1/atr_period, adjust=False).mean()\n",
    "    )\n",
    "\n",
    "    # --- CHANGE 1: Removed .fillna(0) ---\n",
    "    # ATRP will now be NaN on the first day, consistent with TR and ATR.\n",
    "    atrp = (atr / df_ohlcv['Adj Close']).replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    indicator_df = pd.DataFrame({\n",
    "        'TR': tr,\n",
    "        'ATR': atr,\n",
    "        'ATRP': atrp\n",
    "    })\n",
    "    \n",
    "    # --- 2. Data Quality Metric Calculation ---\n",
    "    print(f\"Calculating data quality metrics (Window: {quality_window} days)...\")\n",
    "    \n",
    "    # Create intermediate flags needed for quality calculations\n",
    "    is_stale = np.where((df_ohlcv['Volume'] == 0) | (df_ohlcv['Adj High'] == df_ohlcv['Adj Low']), 1, 0)\n",
    "    dollar_volume = df_ohlcv['Adj Close'] * df_ohlcv['Volume']\n",
    "    has_same_volume = (grouped['Volume'].diff() == 0).astype(int)\n",
    "    \n",
    "    # Combine flags into a temporary DataFrame for rolling calculations\n",
    "    quality_temp_df = pd.DataFrame({\n",
    "        'IsStale': is_stale,\n",
    "        'DollarVolume': dollar_volume,\n",
    "        'HasSameVolume': has_same_volume\n",
    "    }, index=df_ohlcv.index) # Explicitly set index to be safe\n",
    "    \n",
    "    # Perform the rolling calculations on the grouped data\n",
    "    # --- FIX IS HERE ---\n",
    "    # We switch to the older, more compatible dictionary-based aggregation method.\n",
    "    # This syntax is understood by nearly all versions of pandas.\n",
    "    rolling_result = quality_temp_df.groupby(level='Ticker').rolling(\n",
    "        window=quality_window,\n",
    "        min_periods=quality_min_periods\n",
    "    ).agg({\n",
    "        'IsStale': 'mean',\n",
    "        'DollarVolume': 'median',\n",
    "        'HasSameVolume': 'sum'\n",
    "    })\n",
    "    \n",
    "    # The dictionary syntax produces columns with the original names ('IsStale', etc.).\n",
    "    # We now explicitly rename them to our desired final names.\n",
    "    rolling_result = rolling_result.rename(columns={\n",
    "        'IsStale': 'RollingStalePct',\n",
    "        'DollarVolume': 'RollMedDollarVol', # <-- RENAMED HERE\n",
    "        'HasSameVolume': 'RollingSameVolCount'\n",
    "    })\n",
    "\n",
    "    # The index after a grouped rolling operation is hierarchical.\n",
    "    # We remove the outermost 'Ticker' level to restore the original index structure.\n",
    "    rolling_quality = rolling_result.reset_index(level=0, drop=True)\n",
    "\n",
    "    # --- 3. Combine All Features ---\n",
    "    print(\"Combining all feature sets...\")\n",
    "    features_df = pd.concat([indicator_df, rolling_quality], axis=1)\n",
    "    \n",
    "    print(\"‚úÖ Feature generation complete.\")\n",
    "    return features_df\n",
    "\n",
    "\n",
    "def get_eligible_universe(features_df, filter_date, thresholds):\n",
    "    \"\"\"Filters the universe of tickers based on quality metrics for a given date.\"\"\"\n",
    "    filter_date_ts = pd.to_datetime(filter_date)\n",
    "    # The index is now the comprehensive features_df\n",
    "    date_index = features_df.index.get_level_values('Date').unique().sort_values()\n",
    "    \n",
    "    if filter_date_ts < date_index[0]:\n",
    "        print(f\"Warning: Filter date {filter_date_ts.date()} is before the earliest data point. Returning empty universe.\")\n",
    "        return []\n",
    "        \n",
    "    # Find the most recent date with quality data on or before the filter date\n",
    "    valid_prior_dates = date_index[date_index <= filter_date_ts]\n",
    "    if valid_prior_dates.empty:\n",
    "        print(f\"Warning: No available data found on or before {filter_date_ts.date()}. Returning empty universe.\")\n",
    "        return []\n",
    "        \n",
    "    actual_date_to_use = valid_prior_dates[-1]\n",
    "    if actual_date_to_use.date() != filter_date_ts.date():\n",
    "        print(f\"‚ÑπÔ∏è Info: Filter date {filter_date_ts.date()} not found. Using previous available date {actual_date_to_use.date()}.\")\n",
    "\n",
    "    metrics_on_date = features_df.xs(actual_date_to_use, level='Date')\n",
    "    \n",
    "    # Apply filters using the new column names from features_df\n",
    "    mask = ((metrics_on_date['RollMedDollarVol'] >= thresholds['min_median_dollar_volume']) & # <-- RENAMED\n",
    "            (metrics_on_date['RollingStalePct'] <= thresholds['max_stale_pct']) &\n",
    "            (metrics_on_date['RollingSameVolCount'] <= thresholds['max_same_vol_count']))\n",
    "            \n",
    "    eligible_tickers = metrics_on_date[mask].index.tolist()\n",
    "    all_tickers = metrics_on_date.index.tolist()\n",
    "    print(f\"Dynamic Filter ({filter_date_ts.date()}): Kept {len(eligible_tickers)} of {len(all_tickers)} tickers.\")\n",
    "    return eligible_tickers\n",
    "\n",
    "\n",
    "def run_walk_forward_step(df_close_full, df_high_full, df_low_full,\n",
    "                          master_trading_days,\n",
    "                          start_date, calc_period, fwd_period,\n",
    "                          metric, rank_start, rank_end, benchmark_ticker,\n",
    "                          features_df,\n",
    "                          debug=False):\n",
    "    \"\"\"\n",
    "    Runs a single step of the walk-forward analysis with a strict, pre-emptive\n",
    "    check to ensure the full period is available.\n",
    "    \"\"\"\n",
    "    debug_data = {} if debug else None\n",
    "\n",
    "    # 1. Determine exact date ranges with a NEW pre-emptive check\n",
    "    try:\n",
    "        start_idx = master_trading_days.get_loc(start_date)\n",
    "    except KeyError:\n",
    "        return ({'error': f\"Start date {start_date.date()} is not a valid trading day.\"}, None)\n",
    "\n",
    "    # +++ THIS IS THE NEW PRE-EMPTIVE CHECK LOGIC +++\n",
    "    # Calculate the desired end index without clamping first.\n",
    "    desired_viz_end_idx = start_idx + calc_period + fwd_period\n",
    "    \n",
    "    # Check if the desired end index is out of bounds.\n",
    "    if desired_viz_end_idx >= len(master_trading_days):\n",
    "        last_available_date = master_trading_days[-1].date()\n",
    "        required_days = calc_period + fwd_period\n",
    "        available_days = len(master_trading_days) - start_idx\n",
    "        error_msg = (f\"Not enough data for the full requested period. \"\n",
    "                     f\"Required: {required_days} days, Available: {available_days} days until {last_available_date}.\")\n",
    "        return ({'error': error_msg}, None)\n",
    "    # --- END OF NEW CHECK ---\n",
    "\n",
    "    # If the check passes, we know the full period is available.\n",
    "    # The 'min' calls are now just a redundant safety measure.\n",
    "    calc_end_idx = min(start_idx + calc_period, len(master_trading_days) - 1)\n",
    "    viz_end_idx = min(calc_end_idx + fwd_period, len(master_trading_days) - 1)\n",
    "\n",
    "    safe_start_date = master_trading_days[start_idx]\n",
    "    safe_calc_end_date = master_trading_days[calc_end_idx]\n",
    "    safe_viz_end_date = master_trading_days[viz_end_idx]\n",
    "    \n",
    "    if safe_start_date >= safe_calc_end_date:\n",
    "        return ({'error': \"Invalid date range (calc period has zero or negative length).\"}, None)\n",
    "\n",
    "    # (The rest of the function remains completely unchanged...)\n",
    "    # 2. Slice data for the calculation period\n",
    "    calc_close_raw = df_close_full.loc[safe_start_date:safe_calc_end_date]\n",
    "    calc_close = calc_close_raw.dropna(axis=1, how='all')\n",
    "    if calc_close.shape[1] == 0 or len(calc_close) < 2:\n",
    "        return ({'error': \"Not enough data in calc period.\"}, None)\n",
    "\n",
    "    # 3. Calculate INTERMEDIATE data required for the ranking metrics\n",
    "    daily_returns = calc_close.bfill().ffill().pct_change()\n",
    "    valid_tickers = calc_close.columns\n",
    "    calc_period_index = pd.MultiIndex.from_product([valid_tickers, calc_close.index], names=['Ticker', 'Date'])\n",
    "    features_in_period = features_df.loc[features_df.index.intersection(calc_period_index)]\n",
    "    atrp = features_in_period.groupby(level='Ticker')['ATRP'].mean()\n",
    "\n",
    "    # 4. Calculate all ranking metrics by iterating through the METRIC_REGISTRY\n",
    "    metric_ingredients = { 'calc_close': calc_close, 'daily_returns': daily_returns, 'atrp': atrp, }\n",
    "    metric_values = {}\n",
    "    for name, func in METRIC_REGISTRY.items():\n",
    "        metric_values[name] = func(metric_ingredients)\n",
    "    if metric not in metric_values or metric_values[metric].empty:\n",
    "        return ({'error': f\"Metric '{metric}' could not be calculated or resulted in no valid tickers.\"}, None)\n",
    "\n",
    "    # 5. Rank tickers and select the portfolio\n",
    "    sorted_tickers = metric_values[metric].sort_values(ascending=False)\n",
    "    tickers_to_display = sorted_tickers.index[rank_start-1:rank_end].tolist()\n",
    "    if not tickers_to_display:\n",
    "        return ({'error': \"No tickers found for the selected rank.\"}, None)\n",
    "\n",
    "    # 6. Calculate Portfolio & Benchmark Performance\n",
    "    normalized_plot_data = df_close_full[tickers_to_display].loc[safe_start_date:safe_viz_end_date]\n",
    "    normalized_plot_data = normalized_plot_data.div(normalized_plot_data.bfill().iloc[0])\n",
    "    actual_calc_end_ts = calc_close.index.max()\n",
    "    portfolio_series = normalized_plot_data.mean(axis=1)\n",
    "    portfolio_return_series = portfolio_series.pct_change()\n",
    "    benchmark_price_series = df_close_full.get(benchmark_ticker)\n",
    "    benchmark_return_series = benchmark_price_series.loc[safe_start_date:safe_viz_end_date].bfill().ffill().pct_change() if benchmark_price_series is not None else pd.Series(dtype='float64')\n",
    "    try:\n",
    "        boundary_loc = portfolio_return_series.index.get_loc(actual_calc_end_ts)\n",
    "        calc_portfolio_returns = portfolio_return_series.iloc[:boundary_loc + 1]\n",
    "        fwd_portfolio_returns = portfolio_return_series.iloc[boundary_loc + 1:]\n",
    "        if benchmark_price_series is not None:\n",
    "            bm_boundary_loc = benchmark_return_series.index.get_loc(actual_calc_end_ts)\n",
    "            calc_benchmark_returns = benchmark_return_series.iloc[:bm_boundary_loc + 1]\n",
    "            fwd_benchmark_returns = benchmark_return_series.iloc[bm_boundary_loc + 1:]\n",
    "        else:\n",
    "            calc_benchmark_returns, fwd_benchmark_returns = pd.Series(dtype='float64'), pd.Series(dtype='float64')\n",
    "    except (KeyError, IndexError):\n",
    "        calc_portfolio_returns = portfolio_return_series.loc[:actual_calc_end_ts]\n",
    "        fwd_portfolio_returns = portfolio_return_series.loc[actual_calc_end_ts:].iloc[1:]\n",
    "        if benchmark_price_series is not None:\n",
    "            calc_benchmark_returns = benchmark_return_series.loc[:actual_calc_end_ts]\n",
    "            fwd_benchmark_returns = benchmark_return_series.loc[actual_calc_end_ts:].iloc[1:]\n",
    "        else:\n",
    "            calc_benchmark_returns, fwd_benchmark_returns = pd.Series(dtype='float64'), pd.Series(dtype='float64')\n",
    "    full_period_index = pd.MultiIndex.from_product([tickers_to_display, portfolio_return_series.index], names=['Ticker', 'Date'])\n",
    "    portfolio_atrp_features = features_df.loc[features_df.index.intersection(full_period_index)]\n",
    "    portfolio_atrp_daily_unstacked = portfolio_atrp_features['ATRP'].unstack(level='Ticker')\n",
    "    portfolio_atrp_series = portfolio_atrp_daily_unstacked.mean(axis=1)\n",
    "    if benchmark_ticker in df_close_full.columns:\n",
    "        benchmark_atrp_series = features_df.xs(benchmark_ticker, level='Ticker')['ATRP'].loc[safe_start_date:safe_viz_end_date]\n",
    "    else:\n",
    "        benchmark_atrp_series = pd.Series(dtype='float64')\n",
    "    calc_portfolio_atrp = portfolio_atrp_series.loc[:actual_calc_end_ts]\n",
    "    fwd_portfolio_atrp = portfolio_atrp_series.loc[actual_calc_end_ts:].iloc[1:]\n",
    "    calc_benchmark_atrp = benchmark_atrp_series.loc[:actual_calc_end_ts]\n",
    "    fwd_benchmark_atrp = benchmark_atrp_series.loc[actual_calc_end_ts:].iloc[1:]\n",
    "    perf_data = {}\n",
    "    perf_data['calc_p_gain'] = calculate_gain(portfolio_series.loc[:actual_calc_end_ts])\n",
    "    perf_data['fwd_p_gain'] = calculate_gain(portfolio_series.loc[actual_calc_end_ts:])\n",
    "    perf_data['full_p_gain'] = calculate_gain(portfolio_series)\n",
    "    perf_data['calc_p_sharpe'] = calculate_sharpe(calc_portfolio_returns)\n",
    "    perf_data['fwd_p_sharpe'] = calculate_sharpe(fwd_portfolio_returns)\n",
    "    perf_data['full_p_sharpe'] = calculate_sharpe(portfolio_return_series)\n",
    "    perf_data['calc_b_gain'] = calculate_gain(benchmark_price_series.loc[safe_start_date:actual_calc_end_ts]) if benchmark_price_series is not None else np.nan\n",
    "    perf_data['fwd_b_gain'] = calculate_gain(benchmark_price_series.loc[actual_calc_end_ts:safe_viz_end_date]) if benchmark_price_series is not None else np.nan\n",
    "    perf_data['full_b_gain'] = calculate_gain(benchmark_price_series.loc[safe_start_date:safe_viz_end_date]) if benchmark_price_series is not None else np.nan\n",
    "    perf_data['calc_b_sharpe'] = calculate_sharpe(calc_benchmark_returns)\n",
    "    perf_data['fwd_b_sharpe'] = calculate_sharpe(fwd_benchmark_returns)\n",
    "    perf_data['full_b_sharpe'] = calculate_sharpe(benchmark_return_series)\n",
    "    perf_data['calc_p_sharpe_atr'] = calculate_sharpe_atr(calc_portfolio_returns, calc_portfolio_atrp)\n",
    "    perf_data['fwd_p_sharpe_atr'] = calculate_sharpe_atr(fwd_portfolio_returns, fwd_portfolio_atrp)\n",
    "    perf_data['full_p_sharpe_atr'] = calculate_sharpe_atr(portfolio_return_series, portfolio_atrp_series)\n",
    "    perf_data['calc_b_sharpe_atr'] = calculate_sharpe_atr(calc_benchmark_returns, calc_benchmark_atrp)\n",
    "    perf_data['fwd_b_sharpe_atr'] = calculate_sharpe_atr(fwd_benchmark_returns, fwd_benchmark_atrp)\n",
    "    perf_data['full_b_sharpe_atr'] = calculate_sharpe_atr(benchmark_return_series, benchmark_atrp_series)\n",
    "    if debug:\n",
    "        df_ranking_base = pd.DataFrame({'MeanDailyReturn': daily_returns.mean(),'StdDevDailyReturn': daily_returns.std(),'MeanATRP': atrp})\n",
    "        df_metrics = pd.DataFrame(metric_values)\n",
    "        df_metrics.columns = [f'Metric_{col}' for col in df_metrics.columns]\n",
    "        df_ranking = df_ranking_base.join(df_metrics, how='left')\n",
    "        df_ranking.index.name = 'Ticker'\n",
    "        debug_data['ranking_metrics'] = df_ranking.sort_values(f'Metric_{metric}', ascending=False)\n",
    "    calc_end_prices = calc_close.ffill().iloc[-1]\n",
    "    fwd_close_slice = df_close_full.loc[actual_calc_end_ts:safe_viz_end_date]\n",
    "    viz_end_prices = fwd_close_slice.ffill().iloc[-1] if not fwd_close_slice.empty and len(fwd_close_slice) >= 2 else calc_end_prices\n",
    "    calc_gains = (calc_end_prices / calc_close.bfill().iloc[0]) - 1\n",
    "    fwd_gains = (viz_end_prices / calc_end_prices) - 1\n",
    "    results_df = pd.DataFrame({'Rank': range(rank_start, rank_start + len(tickers_to_display)), 'Metric': metric, 'MetricValue': sorted_tickers.loc[tickers_to_display].values, 'CalcPrice': calc_end_prices.loc[tickers_to_display], 'CalcGain': calc_gains.loc[tickers_to_display], 'FwdGain': fwd_gains.loc[tickers_to_display]}, index=pd.Index(tickers_to_display, name='Ticker'))\n",
    "    if benchmark_price_series is not None and benchmark_ticker in calc_close.columns:\n",
    "        benchmark_df_row = pd.DataFrame({'Rank': np.nan, 'Metric': metric, 'MetricValue': metric_values[metric].get(benchmark_ticker, np.nan), 'CalcPrice': calc_end_prices[benchmark_ticker], 'CalcGain': calc_gains[benchmark_ticker], 'FwdGain': fwd_gains[benchmark_ticker]}, index=pd.Index([f\"{benchmark_ticker} (BM)\"], name='Ticker'))\n",
    "        results_df = pd.concat([results_df, benchmark_df_row])\n",
    "    if debug:\n",
    "        df_trace = normalized_plot_data.copy()\n",
    "        df_trace.columns = [f'Norm_Price_{c}' for c in df_trace.columns]\n",
    "        df_trace['Norm_Price_Portfolio'] = portfolio_series\n",
    "        if benchmark_price_series is not None and not benchmark_price_series.loc[safe_start_date:safe_viz_end_date].dropna().empty:\n",
    "            norm_bm = benchmark_price_series.loc[safe_start_date:safe_viz_end_date] / benchmark_price_series.loc[safe_start_date:].bfill().iloc[0]\n",
    "            df_trace[f'Norm_Price_Benchmark_{benchmark_ticker}'] = norm_bm\n",
    "        for col in df_trace.columns:\n",
    "            if 'Norm_Price' in col:\n",
    "                df_trace[col.replace('Norm_Price', 'Return')] = df_trace[col].pct_change()\n",
    "        debug_data['portfolio_trace'] = df_trace\n",
    "    final_results = {\n",
    "        'tickers_to_display': tickers_to_display, 'normalized_plot_data': normalized_plot_data,\n",
    "        'portfolio_series': portfolio_series, 'benchmark_price_series': benchmark_price_series,\n",
    "        'performance_data': perf_data, 'results_df': results_df, 'actual_calc_end_ts': actual_calc_end_ts,\n",
    "        'safe_start_date': safe_start_date, 'safe_viz_end_date': safe_viz_end_date,\n",
    "        'error': None\n",
    "    }\n",
    "    return (final_results, debug_data)\n",
    "\n",
    "def verify_sharpe_atr_calculation_checked(df_ohlcv, features_df, tickers_to_verify, benchmark_ticker,\n",
    "                                    start_date, calc_period, fwd_period,\n",
    "                                    master_calendar_ticker='VOO', debug=False):\n",
    "    \"\"\"\n",
    "    Verifies the Sharpe (ATR) calculations for a portfolio and benchmark.\n",
    "\n",
    "    This function transparently recalculates the key components for Sharpe (ATR)\n",
    "    and can optionally export the underlying source data for manual inspection.\n",
    "    \"\"\"\n",
    "    display(Markdown(f\"## Verification Report for Sharpe (ATR) Calculation\"))\n",
    "    display(Markdown(f\"**Portfolio Tickers:** `{tickers_to_verify}`\\n**Benchmark Ticker:** `{benchmark_ticker}`\"))\n",
    "\n",
    "    # --- 1. Determine Exact Period Dates (No changes here) ---\n",
    "    master_trading_days = df_ohlcv.loc[master_calendar_ticker].index.unique().sort_values()\n",
    "    start_date_raw = pd.to_datetime(start_date)\n",
    "    start_idx = master_trading_days.searchsorted(start_date_raw)\n",
    "    actual_start_date = master_trading_days[start_idx]\n",
    "    calc_end_idx = min(start_idx + calc_period, len(master_trading_days) - 1)\n",
    "    fwd_end_idx = min(calc_end_idx + fwd_period, len(master_trading_days) - 1)\n",
    "    actual_calc_end_date = master_trading_days[calc_end_idx]\n",
    "    actual_fwd_end_date = master_trading_days[fwd_end_idx]\n",
    "    \n",
    "    display(Markdown(f\"**Full Period:** `{actual_start_date.date()}` to `{actual_fwd_end_date.date()}`\\n\"\n",
    "                    f\"**Calc End Date:** `{actual_calc_end_date.date()}`\"))\n",
    "\n",
    "    # Original debug export block (can be kept or removed)\n",
    "    if debug:\n",
    "        # ... (original export code remains here) ...\n",
    "        pass # Assuming original block is here\n",
    "\n",
    "    # --- 2. Recreate Portfolio & Benchmark Series from Scratch ---\n",
    "    df_close_full = df_ohlcv['Adj Close'].unstack(level=0)\n",
    "    portfolio_prices_raw = df_close_full[tickers_to_verify].loc[actual_start_date:actual_fwd_end_date]\n",
    "    portfolio_prices_norm = portfolio_prices_raw.div(portfolio_prices_raw.bfill().iloc[0])\n",
    "    portfolio_value_series = portfolio_prices_norm.mean(axis=1)\n",
    "    portfolio_return_series = portfolio_value_series.pct_change()\n",
    "    p_idx = pd.MultiIndex.from_product([tickers_to_verify, portfolio_return_series.index])\n",
    "    p_atrp_df = features_df.loc[features_df.index.intersection(p_idx)]['ATRP'].unstack(level=0)\n",
    "    portfolio_atrp_series = p_atrp_df.mean(axis=1)\n",
    "\n",
    "###############################    \n",
    "    # benchmark_return_series = df_close_full[benchmark_ticker].pct_change().loc[actual_start_date:actual_fwd_end_date]\n",
    "\n",
    "    # 1. First, slice the raw prices for the desired date range.\n",
    "    benchmark_prices_raw = df_close_full[benchmark_ticker].loc[actual_start_date:actual_fwd_end_date]\n",
    "    # 2. Then, calculate the percentage change on the sliced data.\n",
    "    benchmark_return_series = benchmark_prices_raw.pct_change()\n",
    "\n",
    "###############################    \n",
    "    benchmark_atrp_series = features_df.xs(benchmark_ticker, level='Ticker')['ATRP'].loc[actual_start_date:actual_fwd_end_date]\n",
    "\n",
    "\n",
    "    # +++ NEW: DETAILED RETURN CALCULATION TRACE (PORTFOLIO) +++\n",
    "    if debug:\n",
    "        display(Markdown(\"---\"))\n",
    "        display(Markdown(\"### üêõ Detailed Portfolio Return Calculation Trace\"))\n",
    "\n",
    "        # Step 1: Show the raw prices being used\n",
    "        print(\"\\n[STEP 1] Raw Adjusted Close prices used for calculation.\")\n",
    "        print(\"Compare these values with your 'Adj Close' columns.\")\n",
    "        display(portfolio_prices_raw)\n",
    "\n",
    "        # Step 2: Show the normalization base and the result\n",
    "        normalization_base = portfolio_prices_raw.bfill().iloc[0]\n",
    "        print(\"\\n[STEP 2] Normalization base (first row of prices).\")\n",
    "        print(\"Each column in Step 1 is divided by this corresponding value.\")\n",
    "        display(pd.DataFrame(normalization_base).T)\n",
    "\n",
    "        print(\"\\n[STEP 2a] Normalized prices (Result of division).\")\n",
    "        print(\"Compare these values with your 'N Close' columns.\")\n",
    "        display(portfolio_prices_norm)\n",
    "\n",
    "        # Step 3: Show the averaged portfolio value series\n",
    "        print(\"\\n[STEP 3] Averaged normalized portfolio value series.\")\n",
    "        print(\"This is the row-by-row average of the table in Step 2a.\")\n",
    "        print(\"Compare this series with your 'N_portf' column.\")\n",
    "        display(pd.DataFrame(portfolio_value_series, columns=['N_portf']))\n",
    "\n",
    "        # Step 4: Show the final portfolio return series\n",
    "        print(\"\\n[STEP 4] Final portfolio daily return series (pct_change).\")\n",
    "        print(\"This is the percentage change of the series in Step 3.\")\n",
    "        print(\"Compare this series with your 'N_portf_rtn' column.\")\n",
    "        display(pd.DataFrame(portfolio_return_series, columns=['N_portf_rtn']))\n",
    "        display(Markdown(\"---\"))\n",
    "    # +++ END OF NEW DEBUG BLOCK +++\n",
    "\n",
    "\n",
    "    # --- 3. Define a Helper to Print Detailed Calculation Steps ---\n",
    "    # MODIFIED to include more debug details inside\n",
    "    def _calculate_and_print_metrics(period_name, returns, atrps):\n",
    "        display(Markdown(f\"#### {period_name}\"))\n",
    "        if returns.dropna().empty or atrps.dropna().empty:\n",
    "            print(\"  - Not enough data to calculate.\")\n",
    "            return np.nan\n",
    "\n",
    "        # Standard calculations\n",
    "        mean_return = returns.mean()\n",
    "        mean_atrp = atrps.mean()\n",
    "        sharpe_atr = mean_return / mean_atrp if mean_atrp > 0 else np.nan\n",
    "\n",
    "        # +++ ADDED: Detailed Mean Calculation Breakdown +++\n",
    "        if debug:\n",
    "            valid_returns = returns.dropna()\n",
    "            num_returns = valid_returns.count()\n",
    "            sum_returns = valid_returns.sum()\n",
    "            manual_mean = sum_returns / num_returns if num_returns > 0 else 0\n",
    "            print(f\"  - (Debug) Number of valid daily returns: {num_returns}\")\n",
    "            print(f\"  - (Debug) Sum of all daily returns:      {sum_returns:,.8f}\")\n",
    "            print(f\"  - (Debug) Manually calculated mean:      {manual_mean:,.8f} (Sum / Count)\")\n",
    "        # +++ END OF ADDED DETAIL +++\n",
    "\n",
    "        print(f\"  - Mean Daily Return: {mean_return:,.6f}\")\n",
    "        print(f\"  - Mean Daily ATRP:  {mean_atrp:,.6f}\")\n",
    "        print(f\"  - Sharpe (ATR) = (Mean Return / Mean ATRP) = {sharpe_atr:,.4f}\")\n",
    "        return sharpe_atr\n",
    "\n",
    "    # --- 4. Run Verification for Portfolio (No changes here) ---\n",
    "    display(Markdown(\"### A. Group Portfolio Verification\"))\n",
    "    _calculate_and_print_metrics(\"Full Period\", portfolio_return_series, portfolio_atrp_series)\n",
    "    _calculate_and_print_metrics(\"Calculation Period\", portfolio_return_series.loc[:actual_calc_end_date], portfolio_atrp_series.loc[:actual_calc_end_date])\n",
    "    _calculate_and_print_metrics(\"Forward Period\", portfolio_return_series.loc[actual_calc_end_date:].iloc[1:], portfolio_atrp_series.loc[actual_calc_end_date:].iloc[1:])\n",
    "\n",
    "    # --- 5. Run Verification for Benchmark (No changes here) ---\n",
    "    display(Markdown(f\"### B. Benchmark ({benchmark_ticker}) Verification\"))\n",
    "    _calculate_and_print_metrics(\"Full Period\", benchmark_return_series, benchmark_atrp_series)\n",
    "    _calculate_and_print_metrics(\"Calculation Period\", benchmark_return_series.loc[:actual_calc_end_date], benchmark_atrp_series.loc[:actual_calc_end_date])\n",
    "    _calculate_and_print_metrics(\"Forward Period\", benchmark_return_series.loc[actual_calc_end_date:].iloc[1:], benchmark_atrp_series.loc[actual_calc_end_date:].iloc[1:])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c79216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now call the verification function with the exact parameters from the UI\n",
    "verify_sharpe_atr_calculation_checked(\n",
    "    df_ohlcv=df_ohlcv,\n",
    "    features_df=features_df,\n",
    "    tickers_to_verify=tickers_to_verify, # <-- From the UI output\n",
    "    benchmark_ticker=benchmark_ticker,\n",
    "    start_date=start_date,\n",
    "    calc_period=10,\n",
    "    fwd_period=5,\n",
    "    debug=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da532d8",
   "metadata": {},
   "source": [
    "# PORTFOLIO & BENCHMARK SHARPE (ATR) Calculation have been verified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0586ab73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f55cab9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf901ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63e95d07",
   "metadata": {},
   "source": [
    "### My Verification of Portfolio Sharpe (ATR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2206d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import ipywidgets as widgets\n",
    "import pprint\n",
    "from IPython.display import display, Markdown\n",
    "import os # Make sure os is imported for the export function later\n",
    "\n",
    "\n",
    "# def calculate_gain(price_series: pd.Series):\n",
    "#     \"\"\"Calculates the total gain over a series of prices.\"\"\"\n",
    "#     # Ensure there are at least two data points to calculate a gain\n",
    "#     if price_series.dropna().shape[0] < 2: return np.nan\n",
    "#     # Use forward-fill for the end price and back-fill for the start price\n",
    "#     # to handle potential NaNs at the beginning or end of the series.\n",
    "#     return (price_series.ffill().iloc[-1] / price_series.bfill().iloc[0]) - 1\n",
    "\n",
    "# def calculate_sharpe(return_series: pd.Series):\n",
    "#     \"\"\"Calculates the annualized Sharpe ratio from a series of daily returns.\"\"\"\n",
    "#     # Ensure there are at least two returns to calculate a standard deviation\n",
    "#     if return_series.dropna().shape[0] < 2: return np.nan\n",
    "#     std_dev = return_series.std()\n",
    "#     # Avoid division by zero if returns are constant\n",
    "#     if std_dev > 0 and std_dev != np.inf:\n",
    "#         return (return_series.mean() / std_dev) * np.sqrt(252)\n",
    "#     return np.nan\n",
    "\n",
    "# def calculate_sharpe_atr(return_series: pd.Series, atrp_series: pd.Series):\n",
    "#     \"\"\"Calculates a Sharpe-like ratio using mean ATRP as the denominator.\"\"\"\n",
    "#     # Ensure there are returns and that ATRP data is valid\n",
    "#     if return_series.dropna().shape[0] < 2 or atrp_series.dropna().empty:\n",
    "#         return np.nan\n",
    "        \n",
    "#     mean_return = return_series.mean()\n",
    "#     mean_atrp = atrp_series.mean()\n",
    "    \n",
    "#     # Avoid division by zero\n",
    "#     if mean_atrp > 0 and mean_atrp != np.inf:\n",
    "#         return mean_return / mean_atrp\n",
    "        \n",
    "#     return np.nan\n",
    "\n",
    "\n",
    "# # --- B. MODULAR METRIC CALCULATION ENGINE ---\n",
    "\n",
    "# def calculate_price_metric(metric_data: dict) -> pd.Series:\n",
    "#     \"\"\"\n",
    "#     Calculates the 'Price' metric (total gain) over the calculation period.\n",
    "\n",
    "#     Args:\n",
    "#         metric_data: A dictionary containing pre-calculated data Series.\n",
    "#                      Requires 'calc_close' (pd.DataFrame): The close prices for the calc period.\n",
    "\n",
    "#     Returns:\n",
    "#         A pandas Series of the calculated metric values, indexed by Ticker.\n",
    "#     \"\"\"\n",
    "#     calc_close = metric_data['calc_close']\n",
    "    \n",
    "#     # Ensure there are at least two data points to calculate a gain\n",
    "#     if len(calc_close) < 2:\n",
    "#         return pd.Series(dtype='float64', index=calc_close.columns)\n",
    "\n",
    "#     first_prices = calc_close.bfill().iloc[0]\n",
    "#     last_prices = calc_close.ffill().iloc[-1]\n",
    "    \n",
    "#     # The division of two Series aligns by index (Ticker), which is what we want.\n",
    "#     price_metric = last_prices / first_prices\n",
    "    \n",
    "#     return price_metric.dropna()\n",
    "\n",
    "# def calculate_sharpe_metric(metric_data: dict) -> pd.Series:\n",
    "#     \"\"\"\n",
    "#     Calculates the annualized 'Sharpe' ratio metric over the calculation period.\n",
    "\n",
    "#     Args:\n",
    "#         metric_data: A dictionary containing pre-calculated data Series.\n",
    "#                      Requires 'daily_returns' (pd.DataFrame): Daily returns for the calc period.\n",
    "\n",
    "#     Returns:\n",
    "#         A pandas Series of the calculated metric values, indexed by Ticker.\n",
    "#     \"\"\"\n",
    "#     daily_returns = metric_data['daily_returns']\n",
    "    \n",
    "#     # Ensure there's enough data to calculate standard deviation\n",
    "#     if len(daily_returns.dropna()) < 2:\n",
    "#         return pd.Series(dtype='float64', index=daily_returns.columns)\n",
    "    \n",
    "#     mean_returns = daily_returns.mean()\n",
    "#     std_returns = daily_returns.std()\n",
    "    \n",
    "#     # Standard annualized Sharpe Ratio calculation. Avoid division by zero.\n",
    "#     # We replace resulting NaNs/infs with 0 to handle cases of zero volatility.\n",
    "#     sharpe_ratio = (mean_returns / std_returns * np.sqrt(252))\n",
    "    \n",
    "#     return sharpe_ratio.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "    \n",
    "# def calculate_sharpe_atr_metric(metric_data: dict) -> pd.Series:\n",
    "#     \"\"\"\n",
    "#     Calculates the 'Sharpe (ATR)' metric over the calculation period.\n",
    "\n",
    "#     Args:\n",
    "#         metric_data: A dictionary containing pre-calculated data Series.\n",
    "#                      Requires 'daily_returns' (pd.DataFrame): Daily returns for the calc period.\n",
    "#                      Requires 'atrp' (pd.Series): Mean ATRP for each ticker over the period.\n",
    "\n",
    "#     Returns:\n",
    "#         A pandas Series of the calculated metric values, indexed by Ticker.\n",
    "#     \"\"\"\n",
    "#     daily_returns = metric_data['daily_returns']\n",
    "#     atrp = metric_data['atrp']\n",
    "    \n",
    "#     mean_returns = daily_returns.mean()\n",
    "    \n",
    "#     # ATRP-based Sharpe. Avoid division by zero.\n",
    "#     # We replace resulting NaNs/infs with 0.\n",
    "#     sharpe_atr = mean_returns / atrp\n",
    "    \n",
    "#     return sharpe_atr.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "\n",
    "# METRIC_REGISTRY = {\n",
    "#     'Price': calculate_price_metric,\n",
    "#     'Sharpe': calculate_sharpe_metric,\n",
    "#     'Sharpe (ATR)': calculate_sharpe_atr_metric,\n",
    "# }\n",
    "\n",
    "# def plot_walk_forward_analyzer(df_ohlcv,\n",
    "#                                default_start_date=None, default_calc_period=126, default_fwd_period=63,\n",
    "#                                default_metric='Sharpe (ATR)', default_rank_start=1, default_rank_end=10,\n",
    "#                                default_benchmark_ticker='VOO', master_calendar_ticker='VOO',\n",
    "#                                quality_thresholds={'min_median_dollar_volume': 1_000_000, 'max_stale_pct': 0.05, 'max_same_vol_count': 10},\n",
    "#                                debug=False):\n",
    "#     # (No changes to the initial setup part of this function...)\n",
    "#     print(\"Initializing Walk-Forward Analyzer (using Trading Day Logic)...\")\n",
    "#     if not isinstance(df_ohlcv.index, pd.MultiIndex): raise ValueError(\"Input DataFrame must have a (Ticker, Date) MultiIndex.\")\n",
    "#     df_ohlcv = df_ohlcv.sort_index()\n",
    "#     if master_calendar_ticker not in df_ohlcv.index.get_level_values(0):\n",
    "#         raise ValueError(f\"Master calendar ticker '{master_calendar_ticker}' not found in DataFrame.\")\n",
    "#     master_trading_days = df_ohlcv.loc[master_calendar_ticker].index.unique().sort_values()\n",
    "#     print(f\"Master trading day calendar created from '{master_calendar_ticker}' ({len(master_trading_days)} days).\")\n",
    "#     print(\"--- Generating all features upfront... ---\")\n",
    "#     features_df = generate_features(df_ohlcv)\n",
    "#     print(\"Pre-processing data (unstacking)...\")\n",
    "#     df_close_full = df_ohlcv['Adj Close'].unstack(level=0)\n",
    "#     df_high_full = df_ohlcv['Adj High'].unstack(level=0)\n",
    "#     df_low_full = df_ohlcv['Adj Low'].unstack(level=0)\n",
    "#     start_date_picker = widgets.DatePicker(description='Start Date:', value=pd.to_datetime(default_start_date), disabled=False)\n",
    "#     calc_period_input = widgets.IntText(value=default_calc_period, description='Calc Period (days):')\n",
    "#     fwd_period_input = widgets.IntText(value=default_fwd_period, description='Fwd Period (days):')\n",
    "#     if default_metric not in METRIC_REGISTRY:\n",
    "#         fallback_metric = list(METRIC_REGISTRY.keys())[0]\n",
    "#         print(f\"‚ö†Ô∏è Warning: Default metric '{default_metric}' not in registry. Using '{fallback_metric}'.\")\n",
    "#         default_metric = fallback_metric\n",
    "#     metric_dropdown = widgets.Dropdown(options=list(METRIC_REGISTRY.keys()), value=default_metric, description='Metric:')\n",
    "#     rank_start_input = widgets.IntText(value=default_rank_start, description='Rank Start:')\n",
    "#     rank_end_input = widgets.IntText(value=default_rank_end, description='Rank End:')\n",
    "#     benchmark_ticker_input = widgets.Text(value=default_benchmark_ticker, description='Benchmark:', placeholder='Enter Ticker')\n",
    "#     update_button = widgets.Button(description=\"Update Chart\", button_style='primary')\n",
    "#     ticker_list_output = widgets.Output()\n",
    "#     results_container, debug_data_container = [None], [None]\n",
    "#     fig = go.FigureWidget()\n",
    "#     max_traces = 50\n",
    "#     for i in range(max_traces): fig.add_trace(go.Scatter(x=[None], y=[None], mode='lines', name=f'placeholder_{i}', visible=False, showlegend=False))\n",
    "#     fig.add_trace(go.Scatter(x=[None], y=[None], mode='lines', name='Benchmark', visible=True, showlegend=True, line=dict(color='black', width=3, dash='dash')))\n",
    "#     fig.add_trace(go.Scatter(x=[None], y=[None], mode='lines', name='Group Portfolio', visible=True, showlegend=True, line=dict(color='green', width=3)))\n",
    "\n",
    "#     def update_plot(button_click):\n",
    "#         ticker_list_output.clear_output()\n",
    "#         start_date_raw = pd.to_datetime(start_date_picker.value)\n",
    "#         start_date_idx = master_trading_days.searchsorted(start_date_raw)\n",
    "#         if start_date_idx >= len(master_trading_days):\n",
    "#             with ticker_list_output: print(f\"Error: Start date is after the last available trading day.\"); return\n",
    "#         actual_start_date = master_trading_days[start_date_idx]\n",
    "#         with ticker_list_output:\n",
    "#             if start_date_raw.date() != actual_start_date.date():\n",
    "#                 print(f\"‚ÑπÔ∏è Info: Start date {start_date_raw.date()} is not a trading day. Snapping forward to {actual_start_date.date()}.\")\n",
    "#         calc_period = calc_period_input.value; fwd_period = fwd_period_input.value; metric = metric_dropdown.value\n",
    "#         rank_start = rank_start_input.value; rank_end = rank_end_input.value; benchmark_ticker = benchmark_ticker_input.value.strip().upper()\n",
    "#         if rank_start > rank_end:\n",
    "#             with ticker_list_output: print(\"Error: 'Rank Start' must be <= 'Rank End'.\"); return\n",
    "#         if rank_start < 1 or calc_period < 2 or fwd_period < 1:\n",
    "#             with ticker_list_output: print(\"Error: Ranks must be >= 1, Calc Period >= 2, Fwd Period >= 1.\"); return\n",
    "#         required_days = calc_period + fwd_period\n",
    "#         if start_date_idx + required_days > len(master_trading_days):\n",
    "#             available_days = len(master_trading_days) - start_date_idx; last_available_date = master_trading_days[-1].date()\n",
    "#             with ticker_list_output:\n",
    "#                 print(f\"Error: Not enough data for the requested period.\\n  Start Date: {actual_start_date.date()}\\n  Required Days: {calc_period} (calc) + {fwd_period} (fwd) = {required_days}\\n  Available Days from Start: {available_days} (until {last_available_date})\\n  Please shorten the 'Calc Period' / 'Fwd Period' or choose an earlier 'Start Date'.\")\n",
    "#             return\n",
    "#         eligible_tickers = get_eligible_universe(features_df, actual_start_date, quality_thresholds)\n",
    "#         if not eligible_tickers:\n",
    "#             with ticker_list_output: print(f\"Error: No eligible tickers found on {actual_start_date.date()} with the current quality filters.\"); return\n",
    "#         df_close_step = df_close_full[eligible_tickers]; df_high_step = df_high_full[eligible_tickers]; df_low_step = df_low_full[eligible_tickers]\n",
    "#         results, debug_output = run_walk_forward_step(df_close_full, df_high_full, df_low_full, master_trading_days, actual_start_date, calc_period, fwd_period, metric, rank_start, rank_end, benchmark_ticker, features_df=features_df, debug=debug)\n",
    "#         if results.get('error'):\n",
    "#             with ticker_list_output: print(f\"Error: {results['error']}\"); return\n",
    "#         period_dates = {'calc_period_start': results['safe_start_date'], 'calc_period_end': results['actual_calc_end_ts'], 'forward_period_start': results['actual_calc_end_ts'], 'forward_period_end': results['safe_viz_end_date']}\n",
    "#         run_parameters = {'calc_period': calc_period, 'fwd_period': fwd_period, 'rank_metric': metric, 'rank_start': rank_start, 'rank_end': rank_end, 'benchmark_ticker': benchmark_ticker}\n",
    "#         results.update(period_dates); results.update(run_parameters)\n",
    "#         if debug_output is not None and isinstance(debug_output, dict):\n",
    "#             debug_output.update(period_dates); debug_output.update(run_parameters)\n",
    "#         with fig.batch_update():\n",
    "#             for i in range(max_traces):\n",
    "#                 trace = fig.data[i]\n",
    "#                 if i < len(results['tickers_to_display']):\n",
    "#                     ticker = results['tickers_to_display'][i]; plot_data_series = results['normalized_plot_data'][ticker]\n",
    "#                     trace.x, trace.y, trace.name, trace.visible, trace.showlegend = plot_data_series.index, plot_data_series.values, ticker, True, True\n",
    "#                 else: trace.visible, trace.showlegend = False, False\n",
    "#             benchmark_trace = fig.data[max_traces]\n",
    "#             if results['benchmark_price_series'] is not None and not results['benchmark_price_series'].loc[results['safe_start_date']:results['safe_viz_end_date']].dropna().empty:\n",
    "#                 normalized_benchmark = results['benchmark_price_series'].loc[results['safe_start_date']:results['safe_viz_end_date']] / results['benchmark_price_series'].loc[results['safe_start_date']:].bfill().iloc[0]\n",
    "#                 benchmark_trace.x, benchmark_trace.y, benchmark_trace.name, benchmark_trace.visible = normalized_benchmark.index, normalized_benchmark.values, f\"Benchmark ({benchmark_ticker})\", True\n",
    "#             else: benchmark_trace.visible = False\n",
    "#             portfolio_trace = fig.data[max_traces + 1]\n",
    "#             portfolio_trace.x, portfolio_trace.y, portfolio_trace.name, portfolio_trace.visible = results['portfolio_series'].index, results['portfolio_series'], 'Group Portfolio', True\n",
    "#             fig.layout.shapes = []; fig.add_shape(type=\"line\", x0=results['actual_calc_end_ts'], y0=0, x1=results['actual_calc_end_ts'], y1=1, xref='x', yref='paper', line=dict(color=\"grey\", width=2, dash=\"dash\"))\n",
    "#         results_container[0] = results; debug_data_container[0] = debug_output\n",
    "#         with ticker_list_output:\n",
    "#             print(f\"Analysis Period: {results['safe_start_date'].date()} to {results['safe_viz_end_date'].date()}.\")\n",
    "#             pprint.pprint(results['tickers_to_display'])\n",
    "#             p = results['performance_data']\n",
    "            \n",
    "#             # --- START OF MODIFIED BLOCK ---\n",
    "#             rows = []\n",
    "#             # Gain Metrics\n",
    "#             rows.append({'Metric': 'Group Portfolio Gain', 'Full': p['full_p_gain'], 'Calc': p['calc_p_gain'], 'Fwd': p['fwd_p_gain']})\n",
    "#             if not np.isnan(p.get('full_b_gain')):\n",
    "#                 rows.append({'Metric': f'Benchmark ({benchmark_ticker}) Gain', 'Full': p['full_b_gain'], 'Calc': p['calc_b_gain'], 'Fwd': p['fwd_b_gain']})\n",
    "#                 rows.append({'Metric': '== Gain Delta (vs Bm)', 'Full': p['full_p_gain'] - p['full_b_gain'], 'Calc': p['calc_p_gain'] - p['calc_b_gain'], 'Fwd': p['fwd_p_gain'] - p['fwd_b_gain']})\n",
    "            \n",
    "#             # Standard Sharpe Metrics\n",
    "#             rows.append({'Metric': 'Group Portfolio Sharpe', 'Full': p['full_p_sharpe'], 'Calc': p['calc_p_sharpe'], 'Fwd': p['fwd_p_sharpe']})\n",
    "#             if not np.isnan(p.get('full_b_sharpe')):\n",
    "#                 rows.append({'Metric': f'Benchmark ({benchmark_ticker}) Sharpe', 'Full': p['full_b_sharpe'], 'Calc': p['calc_b_sharpe'], 'Fwd': p['fwd_b_sharpe']})\n",
    "#                 rows.append({'Metric': '== Sharpe Delta (vs Bm)', 'Full': p['full_p_sharpe'] - p['full_b_sharpe'], 'Calc': p['calc_p_sharpe'] - p['calc_b_sharpe'], 'Fwd': p['fwd_p_sharpe'] - p['fwd_b_sharpe']})\n",
    "\n",
    "#             # Sharpe (ATR) Metrics\n",
    "#             rows.append({'Metric': 'Group Portfolio Sharpe (ATR)', 'Full': p['full_p_sharpe_atr'], 'Calc': p['calc_p_sharpe_atr'], 'Fwd': p['fwd_p_sharpe_atr']})\n",
    "#             if not np.isnan(p.get('full_b_sharpe_atr')):\n",
    "#                 rows.append({'Metric': f'Benchmark ({benchmark_ticker}) Sharpe (ATR)', 'Full': p['full_b_sharpe_atr'], 'Calc': p['calc_b_sharpe_atr'], 'Fwd': p['fwd_b_sharpe_atr']})\n",
    "#                 rows.append({'Metric': '== Sharpe (ATR) Delta (vs Bm)', 'Full': p['full_p_sharpe_atr'] - p['full_b_sharpe_atr'], 'Calc': p['calc_p_sharpe_atr'] - p['calc_b_sharpe_atr'], 'Fwd': p['fwd_p_sharpe_atr'] - p['fwd_b_sharpe_atr']})\n",
    "\n",
    "#             report_df = pd.DataFrame(rows).set_index('Metric')\n",
    "#             gain_rows = [row for row in report_df.index if 'Gain' in row]\n",
    "#             sharpe_rows = [row for row in report_df.index if 'Sharpe' in row] # This now correctly includes both types of Sharpe\n",
    "#             # --- END OF MODIFIED BLOCK ---\n",
    "            \n",
    "#             styled_df = report_df.style.format('{:+.2%}', na_rep='N/A', subset=(gain_rows, report_df.columns)).format('{:+.2f}', na_rep='N/A', subset=(sharpe_rows, report_df.columns)).set_properties(**{'text-align': 'right', 'width': '100px'}).set_table_styles([{'selector': 'th.col_heading', 'props': [('text-align', 'right')]}, {'selector': 'th.row_heading', 'props': [('text-align', 'left')]}])\n",
    "#             print(\"\\n--- Strategy Performance Summary ---\")\n",
    "#             display(styled_df)\n",
    "#     fig.update_layout(title_text='Walk-Forward Performance Analysis', xaxis_title='Date', yaxis_title='Normalized Price (Start = 1)', hovermode='x unified', legend_title_text='Tickers (Ranked)', height=600, margin=dict(t=50))\n",
    "#     fig.add_hline(y=1, line_width=1, line_dash=\"dash\", line_color=\"grey\")\n",
    "#     update_button.on_click(update_plot)\n",
    "#     controls_row1 = widgets.HBox([start_date_picker, calc_period_input, fwd_period_input])\n",
    "#     controls_row2 = widgets.HBox([metric_dropdown, rank_start_input, rank_end_input, benchmark_ticker_input, update_button])\n",
    "#     ui_container = widgets.VBox([controls_row1, controls_row2, ticker_list_output], layout=widgets.Layout(margin='10px 0 20px 0'))\n",
    "#     display(ui_container, fig)\n",
    "#     update_plot(None)\n",
    "#     return (results_container, debug_data_container)\n",
    "\n",
    "\n",
    "def generate_features(df_ohlcv: pd.DataFrame, \n",
    "                      atr_period: int = 14, \n",
    "                      quality_window: int = 252, \n",
    "                      quality_min_periods: int = 126) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generates a comprehensive DataFrame of derived features from raw OHLCV data.\n",
    "\n",
    "    This function performs all heavy, window-based calculations upfront to be used\n",
    "    by downstream analysis functions. It calculates:\n",
    "    1. Technical Indicators: True Range (TR), ATR, and ATRP.\n",
    "    2. Data Quality Metrics: Rolling stale percentage, median dollar volume, etc.\n",
    "\n",
    "    Args:\n",
    "        df_ohlcv: The primary DataFrame with a (Ticker, Date) MultiIndex and\n",
    "                  columns for 'Adj High', 'Adj Low', 'Adj Close', 'Volume'.\n",
    "        atr_period: The lookback period for the ATR's Exponential Moving Average.\n",
    "        quality_window: The rolling window size for data quality metrics.\n",
    "        quality_min_periods: The minimum number of observations required to have\n",
    "                             a valid quality metric.\n",
    "\n",
    "    Returns:\n",
    "        A new DataFrame with the same (Ticker, Date) MultiIndex containing all\n",
    "        calculated feature columns.\n",
    "    \"\"\"\n",
    "    print(\"--- Starting Feature Generation ---\")\n",
    "    \n",
    "    # Ensure the DataFrame is sorted for correct window and shift operations\n",
    "    # FIX: Replaced is_lexsorted() with the current pandas attribute\n",
    "    if not df_ohlcv.index.is_monotonic_increasing:\n",
    "        print(\"Sorting index for calculation accuracy...\")\n",
    "        df_ohlcv = df_ohlcv.sort_index()\n",
    "\n",
    "    # --- 1. Technical Indicator Calculation (TR, ATR, ATRP) ---\n",
    "    print(f\"Calculating technical indicators (ATR Period: {atr_period})...\")\n",
    "    \n",
    "    # Group by ticker to handle each security independently\n",
    "    grouped = df_ohlcv.groupby(level='Ticker')\n",
    "    \n",
    "    # Get the previous day's close required for True Range\n",
    "    prev_close = grouped['Adj Close'].shift(1)\n",
    "    \n",
    "    # Calculate the three components of True Range\n",
    "    high_low = df_ohlcv['Adj High'] - df_ohlcv['Adj Low']\n",
    "    high_prev_close = abs(df_ohlcv['Adj High'] - prev_close)\n",
    "    low_prev_close = abs(df_ohlcv['Adj Low'] - prev_close)\n",
    "    \n",
    "    # Combine the components to get the final TR\n",
    "    tr = pd.concat([high_low, high_prev_close, low_prev_close], axis=1).max(axis=1, skipna=False)\n",
    "    \n",
    "    # # Calculate the ATR using an Exponential Moving Average\n",
    "    # --- FIX IS HERE ---\n",
    "    # Use .transform() to apply the EWM function. \n",
    "    # This guarantees the resulting Series has the exact same index as 'tr',\n",
    "    # preventing the index alignment error during the subsequent division.\n",
    "    atr = tr.groupby(level='Ticker').transform(\n",
    "        lambda x: x.ewm(alpha=1/atr_period, adjust=False).mean()\n",
    "    )\n",
    "\n",
    "    # --- CHANGE 1: Removed .fillna(0) ---\n",
    "    # ATRP will now be NaN on the first day, consistent with TR and ATR.\n",
    "    atrp = (atr / df_ohlcv['Adj Close']).replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    indicator_df = pd.DataFrame({\n",
    "        'TR': tr,\n",
    "        'ATR': atr,\n",
    "        'ATRP': atrp\n",
    "    })\n",
    "    \n",
    "    # --- 2. Data Quality Metric Calculation ---\n",
    "    print(f\"Calculating data quality metrics (Window: {quality_window} days)...\")\n",
    "    \n",
    "    # Create intermediate flags needed for quality calculations\n",
    "    is_stale = np.where((df_ohlcv['Volume'] == 0) | (df_ohlcv['Adj High'] == df_ohlcv['Adj Low']), 1, 0)\n",
    "    dollar_volume = df_ohlcv['Adj Close'] * df_ohlcv['Volume']\n",
    "    has_same_volume = (grouped['Volume'].diff() == 0).astype(int)\n",
    "    \n",
    "    # Combine flags into a temporary DataFrame for rolling calculations\n",
    "    quality_temp_df = pd.DataFrame({\n",
    "        'IsStale': is_stale,\n",
    "        'DollarVolume': dollar_volume,\n",
    "        'HasSameVolume': has_same_volume\n",
    "    }, index=df_ohlcv.index) # Explicitly set index to be safe\n",
    "    \n",
    "    # Perform the rolling calculations on the grouped data\n",
    "    # --- FIX IS HERE ---\n",
    "    # We switch to the older, more compatible dictionary-based aggregation method.\n",
    "    # This syntax is understood by nearly all versions of pandas.\n",
    "    rolling_result = quality_temp_df.groupby(level='Ticker').rolling(\n",
    "        window=quality_window,\n",
    "        min_periods=quality_min_periods\n",
    "    ).agg({\n",
    "        'IsStale': 'mean',\n",
    "        'DollarVolume': 'median',\n",
    "        'HasSameVolume': 'sum'\n",
    "    })\n",
    "    \n",
    "    # The dictionary syntax produces columns with the original names ('IsStale', etc.).\n",
    "    # We now explicitly rename them to our desired final names.\n",
    "    rolling_result = rolling_result.rename(columns={\n",
    "        'IsStale': 'RollingStalePct',\n",
    "        'DollarVolume': 'RollMedDollarVol', # <-- RENAMED HERE\n",
    "        'HasSameVolume': 'RollingSameVolCount'\n",
    "    })\n",
    "\n",
    "    # The index after a grouped rolling operation is hierarchical.\n",
    "    # We remove the outermost 'Ticker' level to restore the original index structure.\n",
    "    rolling_quality = rolling_result.reset_index(level=0, drop=True)\n",
    "\n",
    "    # --- 3. Combine All Features ---\n",
    "    print(\"Combining all feature sets...\")\n",
    "    features_df = pd.concat([indicator_df, rolling_quality], axis=1)\n",
    "    \n",
    "    print(\"‚úÖ Feature generation complete.\")\n",
    "    return features_df\n",
    "\n",
    "\n",
    "# def get_eligible_universe(features_df, filter_date, thresholds):\n",
    "#     \"\"\"Filters the universe of tickers based on quality metrics for a given date.\"\"\"\n",
    "#     filter_date_ts = pd.to_datetime(filter_date)\n",
    "#     # The index is now the comprehensive features_df\n",
    "#     date_index = features_df.index.get_level_values('Date').unique().sort_values()\n",
    "    \n",
    "#     if filter_date_ts < date_index[0]:\n",
    "#         print(f\"Warning: Filter date {filter_date_ts.date()} is before the earliest data point. Returning empty universe.\")\n",
    "#         return []\n",
    "        \n",
    "#     # Find the most recent date with quality data on or before the filter date\n",
    "#     valid_prior_dates = date_index[date_index <= filter_date_ts]\n",
    "#     if valid_prior_dates.empty:\n",
    "#         print(f\"Warning: No available data found on or before {filter_date_ts.date()}. Returning empty universe.\")\n",
    "#         return []\n",
    "        \n",
    "#     actual_date_to_use = valid_prior_dates[-1]\n",
    "#     if actual_date_to_use.date() != filter_date_ts.date():\n",
    "#         print(f\"‚ÑπÔ∏è Info: Filter date {filter_date_ts.date()} not found. Using previous available date {actual_date_to_use.date()}.\")\n",
    "\n",
    "#     metrics_on_date = features_df.xs(actual_date_to_use, level='Date')\n",
    "    \n",
    "#     # Apply filters using the new column names from features_df\n",
    "#     mask = ((metrics_on_date['RollMedDollarVol'] >= thresholds['min_median_dollar_volume']) & # <-- RENAMED\n",
    "#             (metrics_on_date['RollingStalePct'] <= thresholds['max_stale_pct']) &\n",
    "#             (metrics_on_date['RollingSameVolCount'] <= thresholds['max_same_vol_count']))\n",
    "            \n",
    "#     eligible_tickers = metrics_on_date[mask].index.tolist()\n",
    "#     all_tickers = metrics_on_date.index.tolist()\n",
    "#     print(f\"Dynamic Filter ({filter_date_ts.date()}): Kept {len(eligible_tickers)} of {len(all_tickers)} tickers.\")\n",
    "#     return eligible_tickers\n",
    "\n",
    "\n",
    "# def run_walk_forward_step(df_close_full, df_high_full, df_low_full,\n",
    "#                           master_trading_days,\n",
    "#                           start_date, calc_period, fwd_period,\n",
    "#                           metric, rank_start, rank_end, benchmark_ticker,\n",
    "#                           features_df,\n",
    "#                           debug=False):\n",
    "#     \"\"\"\n",
    "#     Runs a single step of the walk-forward analysis with a strict, pre-emptive\n",
    "#     check to ensure the full period is available.\n",
    "#     \"\"\"\n",
    "#     debug_data = {} if debug else None\n",
    "\n",
    "#     # 1. Determine exact date ranges with a NEW pre-emptive check\n",
    "#     try:\n",
    "#         start_idx = master_trading_days.get_loc(start_date)\n",
    "#     except KeyError:\n",
    "#         return ({'error': f\"Start date {start_date.date()} is not a valid trading day.\"}, None)\n",
    "\n",
    "#     # +++ THIS IS THE NEW PRE-EMPTIVE CHECK LOGIC +++\n",
    "#     # Calculate the desired end index without clamping first.\n",
    "#     desired_viz_end_idx = start_idx + calc_period + fwd_period\n",
    "    \n",
    "#     # Check if the desired end index is out of bounds.\n",
    "#     if desired_viz_end_idx >= len(master_trading_days):\n",
    "#         last_available_date = master_trading_days[-1].date()\n",
    "#         required_days = calc_period + fwd_period\n",
    "#         available_days = len(master_trading_days) - start_idx\n",
    "#         error_msg = (f\"Not enough data for the full requested period. \"\n",
    "#                      f\"Required: {required_days} days, Available: {available_days} days until {last_available_date}.\")\n",
    "#         return ({'error': error_msg}, None)\n",
    "#     # --- END OF NEW CHECK ---\n",
    "\n",
    "#     # If the check passes, we know the full period is available.\n",
    "#     # The 'min' calls are now just a redundant safety measure.\n",
    "#     calc_end_idx = min(start_idx + calc_period, len(master_trading_days) - 1)\n",
    "#     viz_end_idx = min(calc_end_idx + fwd_period, len(master_trading_days) - 1)\n",
    "\n",
    "#     safe_start_date = master_trading_days[start_idx]\n",
    "#     safe_calc_end_date = master_trading_days[calc_end_idx]\n",
    "#     safe_viz_end_date = master_trading_days[viz_end_idx]\n",
    "    \n",
    "#     if safe_start_date >= safe_calc_end_date:\n",
    "#         return ({'error': \"Invalid date range (calc period has zero or negative length).\"}, None)\n",
    "\n",
    "#     # (The rest of the function remains completely unchanged...)\n",
    "#     # 2. Slice data for the calculation period\n",
    "#     calc_close_raw = df_close_full.loc[safe_start_date:safe_calc_end_date]\n",
    "#     calc_close = calc_close_raw.dropna(axis=1, how='all')\n",
    "#     if calc_close.shape[1] == 0 or len(calc_close) < 2:\n",
    "#         return ({'error': \"Not enough data in calc period.\"}, None)\n",
    "\n",
    "#     # 3. Calculate INTERMEDIATE data required for the ranking metrics\n",
    "#     daily_returns = calc_close.bfill().ffill().pct_change()\n",
    "#     valid_tickers = calc_close.columns\n",
    "#     calc_period_index = pd.MultiIndex.from_product([valid_tickers, calc_close.index], names=['Ticker', 'Date'])\n",
    "#     features_in_period = features_df.loc[features_df.index.intersection(calc_period_index)]\n",
    "#     atrp = features_in_period.groupby(level='Ticker')['ATRP'].mean()\n",
    "\n",
    "#     # 4. Calculate all ranking metrics by iterating through the METRIC_REGISTRY\n",
    "#     metric_ingredients = { 'calc_close': calc_close, 'daily_returns': daily_returns, 'atrp': atrp, }\n",
    "#     metric_values = {}\n",
    "#     for name, func in METRIC_REGISTRY.items():\n",
    "#         metric_values[name] = func(metric_ingredients)\n",
    "#     if metric not in metric_values or metric_values[metric].empty:\n",
    "#         return ({'error': f\"Metric '{metric}' could not be calculated or resulted in no valid tickers.\"}, None)\n",
    "\n",
    "#     # 5. Rank tickers and select the portfolio\n",
    "#     sorted_tickers = metric_values[metric].sort_values(ascending=False)\n",
    "#     tickers_to_display = sorted_tickers.index[rank_start-1:rank_end].tolist()\n",
    "#     if not tickers_to_display:\n",
    "#         return ({'error': \"No tickers found for the selected rank.\"}, None)\n",
    "\n",
    "#     # 6. Calculate Portfolio & Benchmark Performance\n",
    "#     normalized_plot_data = df_close_full[tickers_to_display].loc[safe_start_date:safe_viz_end_date]\n",
    "#     normalized_plot_data = normalized_plot_data.div(normalized_plot_data.bfill().iloc[0])\n",
    "#     actual_calc_end_ts = calc_close.index.max()\n",
    "#     portfolio_series = normalized_plot_data.mean(axis=1)\n",
    "#     portfolio_return_series = portfolio_series.pct_change()\n",
    "#     benchmark_price_series = df_close_full.get(benchmark_ticker)\n",
    "#     benchmark_return_series = benchmark_price_series.loc[safe_start_date:safe_viz_end_date].bfill().ffill().pct_change() if benchmark_price_series is not None else pd.Series(dtype='float64')\n",
    "#     try:\n",
    "#         boundary_loc = portfolio_return_series.index.get_loc(actual_calc_end_ts)\n",
    "#         calc_portfolio_returns = portfolio_return_series.iloc[:boundary_loc + 1]\n",
    "#         fwd_portfolio_returns = portfolio_return_series.iloc[boundary_loc + 1:]\n",
    "#         if benchmark_price_series is not None:\n",
    "#             bm_boundary_loc = benchmark_return_series.index.get_loc(actual_calc_end_ts)\n",
    "#             calc_benchmark_returns = benchmark_return_series.iloc[:bm_boundary_loc + 1]\n",
    "#             fwd_benchmark_returns = benchmark_return_series.iloc[bm_boundary_loc + 1:]\n",
    "#         else:\n",
    "#             calc_benchmark_returns, fwd_benchmark_returns = pd.Series(dtype='float64'), pd.Series(dtype='float64')\n",
    "#     except (KeyError, IndexError):\n",
    "#         calc_portfolio_returns = portfolio_return_series.loc[:actual_calc_end_ts]\n",
    "#         fwd_portfolio_returns = portfolio_return_series.loc[actual_calc_end_ts:].iloc[1:]\n",
    "#         if benchmark_price_series is not None:\n",
    "#             calc_benchmark_returns = benchmark_return_series.loc[:actual_calc_end_ts]\n",
    "#             fwd_benchmark_returns = benchmark_return_series.loc[actual_calc_end_ts:].iloc[1:]\n",
    "#         else:\n",
    "#             calc_benchmark_returns, fwd_benchmark_returns = pd.Series(dtype='float64'), pd.Series(dtype='float64')\n",
    "#     full_period_index = pd.MultiIndex.from_product([tickers_to_display, portfolio_return_series.index], names=['Ticker', 'Date'])\n",
    "#     portfolio_atrp_features = features_df.loc[features_df.index.intersection(full_period_index)]\n",
    "#     portfolio_atrp_daily_unstacked = portfolio_atrp_features['ATRP'].unstack(level='Ticker')\n",
    "#     portfolio_atrp_series = portfolio_atrp_daily_unstacked.mean(axis=1)\n",
    "#     if benchmark_ticker in df_close_full.columns:\n",
    "#         benchmark_atrp_series = features_df.xs(benchmark_ticker, level='Ticker')['ATRP'].loc[safe_start_date:safe_viz_end_date]\n",
    "#     else:\n",
    "#         benchmark_atrp_series = pd.Series(dtype='float64')\n",
    "#     calc_portfolio_atrp = portfolio_atrp_series.loc[:actual_calc_end_ts]\n",
    "#     fwd_portfolio_atrp = portfolio_atrp_series.loc[actual_calc_end_ts:].iloc[1:]\n",
    "#     calc_benchmark_atrp = benchmark_atrp_series.loc[:actual_calc_end_ts]\n",
    "#     fwd_benchmark_atrp = benchmark_atrp_series.loc[actual_calc_end_ts:].iloc[1:]\n",
    "#     perf_data = {}\n",
    "#     perf_data['calc_p_gain'] = calculate_gain(portfolio_series.loc[:actual_calc_end_ts])\n",
    "#     perf_data['fwd_p_gain'] = calculate_gain(portfolio_series.loc[actual_calc_end_ts:])\n",
    "#     perf_data['full_p_gain'] = calculate_gain(portfolio_series)\n",
    "#     perf_data['calc_p_sharpe'] = calculate_sharpe(calc_portfolio_returns)\n",
    "#     perf_data['fwd_p_sharpe'] = calculate_sharpe(fwd_portfolio_returns)\n",
    "#     perf_data['full_p_sharpe'] = calculate_sharpe(portfolio_return_series)\n",
    "#     perf_data['calc_b_gain'] = calculate_gain(benchmark_price_series.loc[safe_start_date:actual_calc_end_ts]) if benchmark_price_series is not None else np.nan\n",
    "#     perf_data['fwd_b_gain'] = calculate_gain(benchmark_price_series.loc[actual_calc_end_ts:safe_viz_end_date]) if benchmark_price_series is not None else np.nan\n",
    "#     perf_data['full_b_gain'] = calculate_gain(benchmark_price_series.loc[safe_start_date:safe_viz_end_date]) if benchmark_price_series is not None else np.nan\n",
    "#     perf_data['calc_b_sharpe'] = calculate_sharpe(calc_benchmark_returns)\n",
    "#     perf_data['fwd_b_sharpe'] = calculate_sharpe(fwd_benchmark_returns)\n",
    "#     perf_data['full_b_sharpe'] = calculate_sharpe(benchmark_return_series)\n",
    "#     perf_data['calc_p_sharpe_atr'] = calculate_sharpe_atr(calc_portfolio_returns, calc_portfolio_atrp)\n",
    "#     perf_data['fwd_p_sharpe_atr'] = calculate_sharpe_atr(fwd_portfolio_returns, fwd_portfolio_atrp)\n",
    "#     perf_data['full_p_sharpe_atr'] = calculate_sharpe_atr(portfolio_return_series, portfolio_atrp_series)\n",
    "#     perf_data['calc_b_sharpe_atr'] = calculate_sharpe_atr(calc_benchmark_returns, calc_benchmark_atrp)\n",
    "#     perf_data['fwd_b_sharpe_atr'] = calculate_sharpe_atr(fwd_benchmark_returns, fwd_benchmark_atrp)\n",
    "#     perf_data['full_b_sharpe_atr'] = calculate_sharpe_atr(benchmark_return_series, benchmark_atrp_series)\n",
    "#     if debug:\n",
    "#         df_ranking_base = pd.DataFrame({'MeanDailyReturn': daily_returns.mean(),'StdDevDailyReturn': daily_returns.std(),'MeanATRP': atrp})\n",
    "#         df_metrics = pd.DataFrame(metric_values)\n",
    "#         df_metrics.columns = [f'Metric_{col}' for col in df_metrics.columns]\n",
    "#         df_ranking = df_ranking_base.join(df_metrics, how='left')\n",
    "#         df_ranking.index.name = 'Ticker'\n",
    "#         debug_data['ranking_metrics'] = df_ranking.sort_values(f'Metric_{metric}', ascending=False)\n",
    "#     calc_end_prices = calc_close.ffill().iloc[-1]\n",
    "#     fwd_close_slice = df_close_full.loc[actual_calc_end_ts:safe_viz_end_date]\n",
    "#     viz_end_prices = fwd_close_slice.ffill().iloc[-1] if not fwd_close_slice.empty and len(fwd_close_slice) >= 2 else calc_end_prices\n",
    "#     calc_gains = (calc_end_prices / calc_close.bfill().iloc[0]) - 1\n",
    "#     fwd_gains = (viz_end_prices / calc_end_prices) - 1\n",
    "#     results_df = pd.DataFrame({'Rank': range(rank_start, rank_start + len(tickers_to_display)), 'Metric': metric, 'MetricValue': sorted_tickers.loc[tickers_to_display].values, 'CalcPrice': calc_end_prices.loc[tickers_to_display], 'CalcGain': calc_gains.loc[tickers_to_display], 'FwdGain': fwd_gains.loc[tickers_to_display]}, index=pd.Index(tickers_to_display, name='Ticker'))\n",
    "#     if benchmark_price_series is not None and benchmark_ticker in calc_close.columns:\n",
    "#         benchmark_df_row = pd.DataFrame({'Rank': np.nan, 'Metric': metric, 'MetricValue': metric_values[metric].get(benchmark_ticker, np.nan), 'CalcPrice': calc_end_prices[benchmark_ticker], 'CalcGain': calc_gains[benchmark_ticker], 'FwdGain': fwd_gains[benchmark_ticker]}, index=pd.Index([f\"{benchmark_ticker} (BM)\"], name='Ticker'))\n",
    "#         results_df = pd.concat([results_df, benchmark_df_row])\n",
    "#     if debug:\n",
    "#         df_trace = normalized_plot_data.copy()\n",
    "#         df_trace.columns = [f'Norm_Price_{c}' for c in df_trace.columns]\n",
    "#         df_trace['Norm_Price_Portfolio'] = portfolio_series\n",
    "#         if benchmark_price_series is not None and not benchmark_price_series.loc[safe_start_date:safe_viz_end_date].dropna().empty:\n",
    "#             norm_bm = benchmark_price_series.loc[safe_start_date:safe_viz_end_date] / benchmark_price_series.loc[safe_start_date:].bfill().iloc[0]\n",
    "#             df_trace[f'Norm_Price_Benchmark_{benchmark_ticker}'] = norm_bm\n",
    "#         for col in df_trace.columns:\n",
    "#             if 'Norm_Price' in col:\n",
    "#                 df_trace[col.replace('Norm_Price', 'Return')] = df_trace[col].pct_change()\n",
    "#         debug_data['portfolio_trace'] = df_trace\n",
    "#     final_results = {\n",
    "#         'tickers_to_display': tickers_to_display, 'normalized_plot_data': normalized_plot_data,\n",
    "#         'portfolio_series': portfolio_series, 'benchmark_price_series': benchmark_price_series,\n",
    "#         'performance_data': perf_data, 'results_df': results_df, 'actual_calc_end_ts': actual_calc_end_ts,\n",
    "#         'safe_start_date': safe_start_date, 'safe_viz_end_date': safe_viz_end_date,\n",
    "#         'error': None\n",
    "#     }\n",
    "#     return (final_results, debug_data)\n",
    "\n",
    "def verify_sharpe_atr_calculation_checked(df_ohlcv, features_df, tickers_to_verify, benchmark_ticker,\n",
    "                                    start_date, calc_period, fwd_period,\n",
    "                                    master_calendar_ticker='VOO', debug=False):\n",
    "    \"\"\"\n",
    "    Verifies the Sharpe (ATR) calculations for a portfolio and benchmark.\n",
    "\n",
    "    This function transparently recalculates the key components for Sharpe (ATR)\n",
    "    and can optionally export the underlying source data for manual inspection.\n",
    "    \"\"\"\n",
    "    display(Markdown(f\"## Verification Report for Sharpe (ATR) Calculation\"))\n",
    "    display(Markdown(f\"**Portfolio Tickers:** `{tickers_to_verify}`\\n**Benchmark Ticker:** `{benchmark_ticker}`\"))\n",
    "\n",
    "    # --- 1. Determine Exact Period Dates (No changes here) ---\n",
    "    master_trading_days = df_ohlcv.loc[master_calendar_ticker].index.unique().sort_values()\n",
    "    start_date_raw = pd.to_datetime(start_date)\n",
    "    start_idx = master_trading_days.searchsorted(start_date_raw)\n",
    "    actual_start_date = master_trading_days[start_idx]\n",
    "    calc_end_idx = min(start_idx + calc_period, len(master_trading_days) - 1)\n",
    "    fwd_end_idx = min(calc_end_idx + fwd_period, len(master_trading_days) - 1)\n",
    "    actual_calc_end_date = master_trading_days[calc_end_idx]\n",
    "    actual_fwd_end_date = master_trading_days[fwd_end_idx]\n",
    "    \n",
    "    display(Markdown(f\"**Full Period:** `{actual_start_date.date()}` to `{actual_fwd_end_date.date()}`\\n\"\n",
    "                    f\"**Calc End Date:** `{actual_calc_end_date.date()}`\"))\n",
    "\n",
    "    # Original debug export block (can be kept or removed)\n",
    "    if debug:\n",
    "        # ... (original export code remains here) ...\n",
    "        pass # Assuming original block is here\n",
    "\n",
    "    # --- 2. Recreate Portfolio & Benchmark Series from Scratch ---\n",
    "    df_close_full = df_ohlcv['Adj Close'].unstack(level=0)\n",
    "    portfolio_prices_raw = df_close_full[tickers_to_verify].loc[actual_start_date:actual_fwd_end_date]\n",
    "    portfolio_prices_norm = portfolio_prices_raw.div(portfolio_prices_raw.bfill().iloc[0])\n",
    "    portfolio_value_series = portfolio_prices_norm.mean(axis=1)\n",
    "    portfolio_return_series = portfolio_value_series.pct_change()\n",
    "    p_idx = pd.MultiIndex.from_product([tickers_to_verify, portfolio_return_series.index])\n",
    "    p_atrp_df = features_df.loc[features_df.index.intersection(p_idx)]['ATRP'].unstack(level=0)\n",
    "    portfolio_atrp_series = p_atrp_df.mean(axis=1)\n",
    "\n",
    "###############################    \n",
    "    # benchmark_return_series = df_close_full[benchmark_ticker].pct_change().loc[actual_start_date:actual_fwd_end_date]\n",
    "\n",
    "    # 1. First, slice the raw prices for the desired date range.\n",
    "    benchmark_prices_raw = df_close_full[benchmark_ticker].loc[actual_start_date:actual_fwd_end_date]\n",
    "    # 2. Then, calculate the percentage change on the sliced data.\n",
    "    benchmark_return_series = benchmark_prices_raw.pct_change()\n",
    "\n",
    "###############################    \n",
    "    benchmark_atrp_series = features_df.xs(benchmark_ticker, level='Ticker')['ATRP'].loc[actual_start_date:actual_fwd_end_date]\n",
    "\n",
    "\n",
    "    # +++ NEW: DETAILED RETURN CALCULATION TRACE (PORTFOLIO) +++\n",
    "    if debug:\n",
    "        display(Markdown(\"---\"))\n",
    "        display(Markdown(\"### üêõ Detailed Portfolio Return Calculation Trace\"))\n",
    "\n",
    "        # Step 1: Show the raw prices being used\n",
    "        print(\"\\n[STEP 1] Raw Adjusted Close prices used for calculation.\")\n",
    "        print(\"Compare these values with your 'Adj Close' columns.\")\n",
    "        display(portfolio_prices_raw)\n",
    "\n",
    "        # Step 2: Show the normalization base and the result\n",
    "        normalization_base = portfolio_prices_raw.bfill().iloc[0]\n",
    "        print(\"\\n[STEP 2] Normalization base (first row of prices).\")\n",
    "        print(\"Each column in Step 1 is divided by this corresponding value.\")\n",
    "        display(pd.DataFrame(normalization_base).T)\n",
    "\n",
    "        print(\"\\n[STEP 2a] Normalized prices (Result of division).\")\n",
    "        print(\"Compare these values with your 'N Close' columns.\")\n",
    "        display(portfolio_prices_norm)\n",
    "\n",
    "        # Step 3: Show the averaged portfolio value series\n",
    "        print(\"\\n[STEP 3] Averaged normalized portfolio value series.\")\n",
    "        print(\"This is the row-by-row average of the table in Step 2a.\")\n",
    "        print(\"Compare this series with your 'N_portf' column.\")\n",
    "        display(pd.DataFrame(portfolio_value_series, columns=['N_portf']))\n",
    "\n",
    "        # Step 4: Show the final portfolio return series\n",
    "        print(\"\\n[STEP 4] Final portfolio daily return series (pct_change).\")\n",
    "        print(\"This is the percentage change of the series in Step 3.\")\n",
    "        print(\"Compare this series with your 'N_portf_rtn' column.\")\n",
    "        display(pd.DataFrame(portfolio_return_series, columns=['N_portf_rtn']))\n",
    "        display(Markdown(\"---\"))\n",
    "    # +++ END OF NEW DEBUG BLOCK +++\n",
    "\n",
    "\n",
    "    # --- 3. Define a Helper to Print Detailed Calculation Steps ---\n",
    "    # MODIFIED to include more debug details inside\n",
    "    def _calculate_and_print_metrics(period_name, returns, atrps):\n",
    "        display(Markdown(f\"#### {period_name}\"))\n",
    "        if returns.dropna().empty or atrps.dropna().empty:\n",
    "            print(\"  - Not enough data to calculate.\")\n",
    "            return np.nan\n",
    "\n",
    "        # Standard calculations\n",
    "        mean_return = returns.mean()\n",
    "        mean_atrp = atrps.mean()\n",
    "        sharpe_atr = mean_return / mean_atrp if mean_atrp > 0 else np.nan\n",
    "\n",
    "        # +++ ADDED: Detailed Mean Calculation Breakdown +++\n",
    "        if debug:\n",
    "            valid_returns = returns.dropna()\n",
    "            num_returns = valid_returns.count()\n",
    "            sum_returns = valid_returns.sum()\n",
    "            manual_mean = sum_returns / num_returns if num_returns > 0 else 0\n",
    "            print(f\"  - (Debug) Number of valid daily returns: {num_returns}\")\n",
    "            print(f\"  - (Debug) Sum of all daily returns:      {sum_returns:,.8f}\")\n",
    "            print(f\"  - (Debug) Manually calculated mean:      {manual_mean:,.8f} (Sum / Count)\")\n",
    "        # +++ END OF ADDED DETAIL +++\n",
    "\n",
    "        print(f\"  - Mean Daily Return: {mean_return:,.6f}\")\n",
    "        print(f\"  - Mean Daily ATRP:  {mean_atrp:,.6f}\")\n",
    "        print(f\"  - Sharpe (ATR) = (Mean Return / Mean ATRP) = {sharpe_atr:,.4f}\")\n",
    "        return sharpe_atr\n",
    "\n",
    "    # --- 4. Run Verification for Portfolio (No changes here) ---\n",
    "    display(Markdown(\"### A. Group Portfolio Verification\"))\n",
    "    _calculate_and_print_metrics(\"Full Period\", portfolio_return_series, portfolio_atrp_series)\n",
    "    _calculate_and_print_metrics(\"Calculation Period\", portfolio_return_series.loc[:actual_calc_end_date], portfolio_atrp_series.loc[:actual_calc_end_date])\n",
    "    _calculate_and_print_metrics(\"Forward Period\", portfolio_return_series.loc[actual_calc_end_date:].iloc[1:], portfolio_atrp_series.loc[actual_calc_end_date:].iloc[1:])\n",
    "\n",
    "    # --- 5. Run Verification for Benchmark (No changes here) ---\n",
    "    display(Markdown(f\"### B. Benchmark ({benchmark_ticker}) Verification\"))\n",
    "    _calculate_and_print_metrics(\"Full Period\", benchmark_return_series, benchmark_atrp_series)\n",
    "    _calculate_and_print_metrics(\"Calculation Period\", benchmark_return_series.loc[:actual_calc_end_date], benchmark_atrp_series.loc[:actual_calc_end_date])\n",
    "    _calculate_and_print_metrics(\"Forward Period\", benchmark_return_series.loc[actual_calc_end_date:].iloc[1:], benchmark_atrp_series.loc[actual_calc_end_date:].iloc[1:])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5c908a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume 'df_ohlcv' is your loaded dataset\n",
    "# You might need to regenerate features_df if it's not in your notebook's memory\n",
    "print(\"--- Regenerating features for verification ---\")\n",
    "features_df = generate_features(df_ohlcv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af59dfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers_to_verify = ['STIP', 'RCL']\n",
    "start_date = '2025-08-13'\n",
    "end_date = '2025-09-04'\n",
    "benchmark_ticker = 'VOO'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a7a71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_sharpe_atr_calculation_checked(df_ohlcv, features_df, tickers_to_verify, benchmark_ticker,\n",
    "                                    start_date, calc_period, fwd_period,\n",
    "                                    master_calendar_ticker='VOO', debug=False):\n",
    "    \"\"\"\n",
    "    Verifies the Sharpe (ATR) calculations for a portfolio and benchmark.\n",
    "\n",
    "    This function transparently recalculates the key components for Sharpe (ATR)\n",
    "    and can optionally export the underlying source data for manual inspection.\n",
    "    \"\"\"\n",
    "    display(Markdown(f\"## Verification Report for Sharpe (ATR) Calculation\"))\n",
    "    display(Markdown(f\"**Portfolio Tickers:** `{tickers_to_verify}`\\n**Benchmark Ticker:** `{benchmark_ticker}`\"))\n",
    "\n",
    "    # --- 1. Determine Exact Period Dates (No changes here) ---\n",
    "    master_trading_days = df_ohlcv.loc[master_calendar_ticker].index.unique().sort_values()\n",
    "    start_date_raw = pd.to_datetime(start_date)\n",
    "    start_idx = master_trading_days.searchsorted(start_date_raw)\n",
    "    actual_start_date = master_trading_days[start_idx]\n",
    "    calc_end_idx = min(start_idx + calc_period, len(master_trading_days) - 1)\n",
    "    fwd_end_idx = min(calc_end_idx + fwd_period, len(master_trading_days) - 1)\n",
    "    actual_calc_end_date = master_trading_days[calc_end_idx]\n",
    "    actual_fwd_end_date = master_trading_days[fwd_end_idx]\n",
    "    \n",
    "    display(Markdown(f\"**Full Period:** `{actual_start_date.date()}` to `{actual_fwd_end_date.date()}`\\n\"\n",
    "                    f\"**Calc End Date:** `{actual_calc_end_date.date()}`\"))\n",
    "\n",
    "    # Original debug export block (can be kept or removed)\n",
    "    if debug:\n",
    "        # ... (original export code remains here) ...\n",
    "        pass # Assuming original block is here\n",
    "\n",
    "    # --- 2. Recreate Portfolio & Benchmark Series from Scratch ---\n",
    "    df_close_full = df_ohlcv['Adj Close'].unstack(level=0)\n",
    "    portfolio_prices_raw = df_close_full[tickers_to_verify].loc[actual_start_date:actual_fwd_end_date]\n",
    "    portfolio_prices_norm = portfolio_prices_raw.div(portfolio_prices_raw.bfill().iloc[0])\n",
    "    portfolio_value_series = portfolio_prices_norm.mean(axis=1)\n",
    "    portfolio_return_series = portfolio_value_series.pct_change()\n",
    "    p_idx = pd.MultiIndex.from_product([tickers_to_verify, portfolio_return_series.index])\n",
    "    p_atrp_df = features_df.loc[features_df.index.intersection(p_idx)]['ATRP'].unstack(level=0)\n",
    "    portfolio_atrp_series = p_atrp_df.mean(axis=1)\n",
    "\n",
    "###############################    \n",
    "    # benchmark_return_series = df_close_full[benchmark_ticker].pct_change().loc[actual_start_date:actual_fwd_end_date]\n",
    "\n",
    "    # 1. First, slice the raw prices for the desired date range.\n",
    "    benchmark_prices_raw = df_close_full[benchmark_ticker].loc[actual_start_date:actual_fwd_end_date]\n",
    "    # 2. Then, calculate the percentage change on the sliced data.\n",
    "    benchmark_return_series = benchmark_prices_raw.pct_change()\n",
    "\n",
    "###############################    \n",
    "    benchmark_atrp_series = features_df.xs(benchmark_ticker, level='Ticker')['ATRP'].loc[actual_start_date:actual_fwd_end_date]\n",
    "\n",
    "\n",
    "    # +++ NEW: DETAILED RETURN CALCULATION TRACE (PORTFOLIO) +++\n",
    "    if debug:\n",
    "        display(Markdown(\"---\"))\n",
    "        display(Markdown(\"### üêõ Detailed Portfolio Return Calculation Trace\"))\n",
    "\n",
    "        # Step 1: Show the raw prices being used\n",
    "        print(\"\\n[STEP 1] Raw Adjusted Close prices used for calculation.\")\n",
    "        print(\"Compare these values with your 'Adj Close' columns.\")\n",
    "        display(portfolio_prices_raw)\n",
    "\n",
    "        # Step 2: Show the normalization base and the result\n",
    "        normalization_base = portfolio_prices_raw.bfill().iloc[0]\n",
    "        print(\"\\n[STEP 2] Normalization base (first row of prices).\")\n",
    "        print(\"Each column in Step 1 is divided by this corresponding value.\")\n",
    "        display(pd.DataFrame(normalization_base).T)\n",
    "\n",
    "        print(\"\\n[STEP 2a] Normalized prices (Result of division).\")\n",
    "        print(\"Compare these values with your 'N Close' columns.\")\n",
    "        display(portfolio_prices_norm)\n",
    "\n",
    "        # Step 3: Show the averaged portfolio value series\n",
    "        print(\"\\n[STEP 3] Averaged normalized portfolio value series.\")\n",
    "        print(\"This is the row-by-row average of the table in Step 2a.\")\n",
    "        print(\"Compare this series with your 'N_portf' column.\")\n",
    "        display(pd.DataFrame(portfolio_value_series, columns=['N_portf']))\n",
    "\n",
    "        # Step 4: Show the final portfolio return series\n",
    "        print(\"\\n[STEP 4] Final portfolio daily return series (pct_change).\")\n",
    "        print(\"This is the percentage change of the series in Step 3.\")\n",
    "        print(\"Compare this series with your 'N_portf_rtn' column.\")\n",
    "        display(pd.DataFrame(portfolio_return_series, columns=['N_portf_rtn']))\n",
    "        display(Markdown(\"---\"))\n",
    "    # +++ END OF NEW DEBUG BLOCK +++\n",
    "\n",
    "\n",
    "    # --- 3. Define a Helper to Print Detailed Calculation Steps ---\n",
    "    # MODIFIED to include more debug details inside\n",
    "    def _calculate_and_print_metrics(period_name, returns, atrps):\n",
    "        display(Markdown(f\"#### {period_name}\"))\n",
    "        if returns.dropna().empty or atrps.dropna().empty:\n",
    "            print(\"  - Not enough data to calculate.\")\n",
    "            return np.nan\n",
    "\n",
    "        # Standard calculations\n",
    "        mean_return = returns.mean()\n",
    "        mean_atrp = atrps.mean()\n",
    "        sharpe_atr = mean_return / mean_atrp if mean_atrp > 0 else np.nan\n",
    "\n",
    "        # +++ ADDED: Detailed Mean Calculation Breakdown +++\n",
    "        if debug:\n",
    "            valid_returns = returns.dropna()\n",
    "            num_returns = valid_returns.count()\n",
    "            sum_returns = valid_returns.sum()\n",
    "            manual_mean = sum_returns / num_returns if num_returns > 0 else 0\n",
    "            print(f\"  - (Debug) Number of valid daily returns: {num_returns}\")\n",
    "            print(f\"  - (Debug) Sum of all daily returns:      {sum_returns:,.8f}\")\n",
    "            print(f\"  - (Debug) Manually calculated mean:      {manual_mean:,.8f} (Sum / Count)\")\n",
    "        # +++ END OF ADDED DETAIL +++\n",
    "\n",
    "        print(f\"  - Mean Daily Return: {mean_return:,.6f}\")\n",
    "        print(f\"  - Mean Daily ATRP:  {mean_atrp:,.6f}\")\n",
    "        print(f\"  - Sharpe (ATR) = (Mean Return / Mean ATRP) = {sharpe_atr:,.4f}\")\n",
    "        return sharpe_atr\n",
    "\n",
    "    # --- 4. Run Verification for Portfolio (No changes here) ---\n",
    "    display(Markdown(\"### A. Group Portfolio Verification\"))\n",
    "    _calculate_and_print_metrics(\"Full Period\", portfolio_return_series, portfolio_atrp_series)\n",
    "    _calculate_and_print_metrics(\"Calculation Period\", portfolio_return_series.loc[:actual_calc_end_date], portfolio_atrp_series.loc[:actual_calc_end_date])\n",
    "    _calculate_and_print_metrics(\"Forward Period\", portfolio_return_series.loc[actual_calc_end_date:].iloc[1:], portfolio_atrp_series.loc[actual_calc_end_date:].iloc[1:])\n",
    "\n",
    "    # --- 5. Run Verification for Benchmark (No changes here) ---\n",
    "    display(Markdown(f\"### B. Benchmark ({benchmark_ticker}) Verification\"))\n",
    "    _calculate_and_print_metrics(\"Full Period\", benchmark_return_series, benchmark_atrp_series)\n",
    "    _calculate_and_print_metrics(\"Calculation Period\", benchmark_return_series.loc[:actual_calc_end_date], benchmark_atrp_series.loc[:actual_calc_end_date])\n",
    "    _calculate_and_print_metrics(\"Forward Period\", benchmark_return_series.loc[actual_calc_end_date:].iloc[1:], benchmark_atrp_series.loc[actual_calc_end_date:].iloc[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0d9bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now call the verification function with the exact parameters from the UI\n",
    "verify_sharpe_atr_calculation_checked(\n",
    "    df_ohlcv=df_ohlcv,\n",
    "    features_df=features_df,\n",
    "    tickers_to_verify=tickers_to_verify, # <-- From the UI output\n",
    "    benchmark_ticker=benchmark_ticker,\n",
    "    start_date=start_date,\n",
    "    calc_period=10,\n",
    "    fwd_period=5,\n",
    "    debug=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18caedd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import ipywidgets as widgets\n",
    "import pprint\n",
    "from IPython.display import display, Markdown\n",
    "import os # Make sure os is imported for the export function later\n",
    "\n",
    "\n",
    "# def calculate_gain(price_series: pd.Series):\n",
    "#     \"\"\"Calculates the total gain over a series of prices.\"\"\"\n",
    "#     # Ensure there are at least two data points to calculate a gain\n",
    "#     if price_series.dropna().shape[0] < 2: return np.nan\n",
    "#     # Use forward-fill for the end price and back-fill for the start price\n",
    "#     # to handle potential NaNs at the beginning or end of the series.\n",
    "#     return (price_series.ffill().iloc[-1] / price_series.bfill().iloc[0]) - 1\n",
    "\n",
    "# def calculate_sharpe(return_series: pd.Series):\n",
    "#     \"\"\"Calculates the annualized Sharpe ratio from a series of daily returns.\"\"\"\n",
    "#     # Ensure there are at least two returns to calculate a standard deviation\n",
    "#     if return_series.dropna().shape[0] < 2: return np.nan\n",
    "#     std_dev = return_series.std()\n",
    "#     # Avoid division by zero if returns are constant\n",
    "#     if std_dev > 0 and std_dev != np.inf:\n",
    "#         return (return_series.mean() / std_dev) * np.sqrt(252)\n",
    "#     return np.nan\n",
    "\n",
    "# def calculate_sharpe_atr(return_series: pd.Series, atrp_series: pd.Series):\n",
    "#     \"\"\"Calculates a Sharpe-like ratio using mean ATRP as the denominator.\"\"\"\n",
    "#     # Ensure there are returns and that ATRP data is valid\n",
    "#     if return_series.dropna().shape[0] < 2 or atrp_series.dropna().empty:\n",
    "#         return np.nan\n",
    "        \n",
    "#     mean_return = return_series.mean()\n",
    "#     mean_atrp = atrp_series.mean()\n",
    "    \n",
    "#     # Avoid division by zero\n",
    "#     if mean_atrp > 0 and mean_atrp != np.inf:\n",
    "#         return mean_return / mean_atrp\n",
    "        \n",
    "#     return np.nan\n",
    "\n",
    "\n",
    "# # --- B. MODULAR METRIC CALCULATION ENGINE ---\n",
    "\n",
    "# def calculate_price_metric(metric_data: dict) -> pd.Series:\n",
    "#     \"\"\"\n",
    "#     Calculates the 'Price' metric (total gain) over the calculation period.\n",
    "\n",
    "#     Args:\n",
    "#         metric_data: A dictionary containing pre-calculated data Series.\n",
    "#                      Requires 'calc_close' (pd.DataFrame): The close prices for the calc period.\n",
    "\n",
    "#     Returns:\n",
    "#         A pandas Series of the calculated metric values, indexed by Ticker.\n",
    "#     \"\"\"\n",
    "#     calc_close = metric_data['calc_close']\n",
    "    \n",
    "#     # Ensure there are at least two data points to calculate a gain\n",
    "#     if len(calc_close) < 2:\n",
    "#         return pd.Series(dtype='float64', index=calc_close.columns)\n",
    "\n",
    "#     first_prices = calc_close.bfill().iloc[0]\n",
    "#     last_prices = calc_close.ffill().iloc[-1]\n",
    "    \n",
    "#     # The division of two Series aligns by index (Ticker), which is what we want.\n",
    "#     price_metric = last_prices / first_prices\n",
    "    \n",
    "#     return price_metric.dropna()\n",
    "\n",
    "# def calculate_sharpe_metric(metric_data: dict) -> pd.Series:\n",
    "#     \"\"\"\n",
    "#     Calculates the annualized 'Sharpe' ratio metric over the calculation period.\n",
    "\n",
    "#     Args:\n",
    "#         metric_data: A dictionary containing pre-calculated data Series.\n",
    "#                      Requires 'daily_returns' (pd.DataFrame): Daily returns for the calc period.\n",
    "\n",
    "#     Returns:\n",
    "#         A pandas Series of the calculated metric values, indexed by Ticker.\n",
    "#     \"\"\"\n",
    "#     daily_returns = metric_data['daily_returns']\n",
    "    \n",
    "#     # Ensure there's enough data to calculate standard deviation\n",
    "#     if len(daily_returns.dropna()) < 2:\n",
    "#         return pd.Series(dtype='float64', index=daily_returns.columns)\n",
    "    \n",
    "#     mean_returns = daily_returns.mean()\n",
    "#     std_returns = daily_returns.std()\n",
    "    \n",
    "#     # Standard annualized Sharpe Ratio calculation. Avoid division by zero.\n",
    "#     # We replace resulting NaNs/infs with 0 to handle cases of zero volatility.\n",
    "#     sharpe_ratio = (mean_returns / std_returns * np.sqrt(252))\n",
    "    \n",
    "#     return sharpe_ratio.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "    \n",
    "# def calculate_sharpe_atr_metric(metric_data: dict) -> pd.Series:\n",
    "#     \"\"\"\n",
    "#     Calculates the 'Sharpe (ATR)' metric over the calculation period.\n",
    "\n",
    "#     Args:\n",
    "#         metric_data: A dictionary containing pre-calculated data Series.\n",
    "#                      Requires 'daily_returns' (pd.DataFrame): Daily returns for the calc period.\n",
    "#                      Requires 'atrp' (pd.Series): Mean ATRP for each ticker over the period.\n",
    "\n",
    "#     Returns:\n",
    "#         A pandas Series of the calculated metric values, indexed by Ticker.\n",
    "#     \"\"\"\n",
    "#     daily_returns = metric_data['daily_returns']\n",
    "#     atrp = metric_data['atrp']\n",
    "    \n",
    "#     mean_returns = daily_returns.mean()\n",
    "    \n",
    "#     # ATRP-based Sharpe. Avoid division by zero.\n",
    "#     # We replace resulting NaNs/infs with 0.\n",
    "#     sharpe_atr = mean_returns / atrp\n",
    "    \n",
    "#     return sharpe_atr.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "\n",
    "# METRIC_REGISTRY = {\n",
    "#     'Price': calculate_price_metric,\n",
    "#     'Sharpe': calculate_sharpe_metric,\n",
    "#     'Sharpe (ATR)': calculate_sharpe_atr_metric,\n",
    "# }\n",
    "\n",
    "\n",
    "def plot_walk_forward_analyzer(df_ohlcv,\n",
    "                               default_start_date=None, default_calc_period=126, default_fwd_period=63,\n",
    "                               default_metric='Sharpe (ATR)', default_rank_start=1, default_rank_end=10,\n",
    "                               default_benchmark_ticker='VOO', master_calendar_ticker='VOO',\n",
    "                               quality_thresholds={'min_median_dollar_volume': 1_000_000, 'max_stale_pct': 0.05, 'max_same_vol_count': 10},\n",
    "                               debug=False):\n",
    "    # (No changes to the initial setup part of this function...)\n",
    "    print(\"Initializing Walk-Forward Analyzer (using Trading Day Logic)...\")\n",
    "    if not isinstance(df_ohlcv.index, pd.MultiIndex): raise ValueError(\"Input DataFrame must have a (Ticker, Date) MultiIndex.\")\n",
    "    df_ohlcv = df_ohlcv.sort_index()\n",
    "    if master_calendar_ticker not in df_ohlcv.index.get_level_values(0):\n",
    "        raise ValueError(f\"Master calendar ticker '{master_calendar_ticker}' not found in DataFrame.\")\n",
    "    master_trading_days = df_ohlcv.loc[master_calendar_ticker].index.unique().sort_values()\n",
    "    print(f\"Master trading day calendar created from '{master_calendar_ticker}' ({len(master_trading_days)} days).\")\n",
    "    print(\"--- Generating all features upfront... ---\")\n",
    "    features_df = generate_features(df_ohlcv)\n",
    "    print(\"Pre-processing data (unstacking)...\")\n",
    "    df_close_full = df_ohlcv['Adj Close'].unstack(level=0)\n",
    "    df_high_full = df_ohlcv['Adj High'].unstack(level=0)\n",
    "    df_low_full = df_ohlcv['Adj Low'].unstack(level=0)\n",
    "    start_date_picker = widgets.DatePicker(description='Start Date:', value=pd.to_datetime(default_start_date), disabled=False)\n",
    "    calc_period_input = widgets.IntText(value=default_calc_period, description='Calc Period (days):')\n",
    "    fwd_period_input = widgets.IntText(value=default_fwd_period, description='Fwd Period (days):')\n",
    "    if default_metric not in METRIC_REGISTRY:\n",
    "        fallback_metric = list(METRIC_REGISTRY.keys())[0]\n",
    "        print(f\"‚ö†Ô∏è Warning: Default metric '{default_metric}' not in registry. Using '{fallback_metric}'.\")\n",
    "        default_metric = fallback_metric\n",
    "    metric_dropdown = widgets.Dropdown(options=list(METRIC_REGISTRY.keys()), value=default_metric, description='Metric:')\n",
    "    rank_start_input = widgets.IntText(value=default_rank_start, description='Rank Start:')\n",
    "    rank_end_input = widgets.IntText(value=default_rank_end, description='Rank End:')\n",
    "    benchmark_ticker_input = widgets.Text(value=default_benchmark_ticker, description='Benchmark:', placeholder='Enter Ticker')\n",
    "    update_button = widgets.Button(description=\"Update Chart\", button_style='primary')\n",
    "    ticker_list_output = widgets.Output()\n",
    "    results_container, debug_data_container = [None], [None]\n",
    "    fig = go.FigureWidget()\n",
    "    max_traces = 50\n",
    "    for i in range(max_traces): fig.add_trace(go.Scatter(x=[None], y=[None], mode='lines', name=f'placeholder_{i}', visible=False, showlegend=False))\n",
    "    fig.add_trace(go.Scatter(x=[None], y=[None], mode='lines', name='Benchmark', visible=True, showlegend=True, line=dict(color='black', width=3, dash='dash')))\n",
    "    fig.add_trace(go.Scatter(x=[None], y=[None], mode='lines', name='Group Portfolio', visible=True, showlegend=True, line=dict(color='green', width=3)))\n",
    "\n",
    "    def update_plot(button_click):\n",
    "        ticker_list_output.clear_output()\n",
    "        start_date_raw = pd.to_datetime(start_date_picker.value)\n",
    "        start_date_idx = master_trading_days.searchsorted(start_date_raw)\n",
    "        if start_date_idx >= len(master_trading_days):\n",
    "            with ticker_list_output: print(f\"Error: Start date is after the last available trading day.\"); return\n",
    "        actual_start_date = master_trading_days[start_date_idx]\n",
    "        with ticker_list_output:\n",
    "            if start_date_raw.date() != actual_start_date.date():\n",
    "                print(f\"‚ÑπÔ∏è Info: Start date {start_date_raw.date()} is not a trading day. Snapping forward to {actual_start_date.date()}.\")\n",
    "        calc_period = calc_period_input.value; fwd_period = fwd_period_input.value; metric = metric_dropdown.value\n",
    "        rank_start = rank_start_input.value; rank_end = rank_end_input.value; benchmark_ticker = benchmark_ticker_input.value.strip().upper()\n",
    "        if rank_start > rank_end:\n",
    "            with ticker_list_output: print(\"Error: 'Rank Start' must be <= 'Rank End'.\"); return\n",
    "        if rank_start < 1 or calc_period < 2 or fwd_period < 1:\n",
    "            with ticker_list_output: print(\"Error: Ranks must be >= 1, Calc Period >= 2, Fwd Period >= 1.\"); return\n",
    "        required_days = calc_period + fwd_period\n",
    "        if start_date_idx + required_days > len(master_trading_days):\n",
    "            available_days = len(master_trading_days) - start_date_idx; last_available_date = master_trading_days[-1].date()\n",
    "            with ticker_list_output:\n",
    "                print(f\"Error: Not enough data for the requested period.\\n  Start Date: {actual_start_date.date()}\\n  Required Days: {calc_period} (calc) + {fwd_period} (fwd) = {required_days}\\n  Available Days from Start: {available_days} (until {last_available_date})\\n  Please shorten the 'Calc Period' / 'Fwd Period' or choose an earlier 'Start Date'.\")\n",
    "            return\n",
    "        eligible_tickers = get_eligible_universe(features_df, actual_start_date, quality_thresholds)\n",
    "        if not eligible_tickers:\n",
    "            with ticker_list_output: print(f\"Error: No eligible tickers found on {actual_start_date.date()} with the current quality filters.\"); return\n",
    "        df_close_step = df_close_full[eligible_tickers]; df_high_step = df_high_full[eligible_tickers]; df_low_step = df_low_full[eligible_tickers]\n",
    "        results, debug_output = run_walk_forward_step(df_close_full, df_high_full, df_low_full, master_trading_days, actual_start_date, calc_period, fwd_period, metric, rank_start, rank_end, benchmark_ticker, features_df=features_df, debug=debug)\n",
    "        if results.get('error'):\n",
    "            with ticker_list_output: print(f\"Error: {results['error']}\"); return\n",
    "        period_dates = {'calc_period_start': results['safe_start_date'], 'calc_period_end': results['actual_calc_end_ts'], 'forward_period_start': results['actual_calc_end_ts'], 'forward_period_end': results['safe_viz_end_date']}\n",
    "        run_parameters = {'calc_period': calc_period, 'fwd_period': fwd_period, 'rank_metric': metric, 'rank_start': rank_start, 'rank_end': rank_end, 'benchmark_ticker': benchmark_ticker}\n",
    "        results.update(period_dates); results.update(run_parameters)\n",
    "        if debug_output is not None and isinstance(debug_output, dict):\n",
    "            debug_output.update(period_dates); debug_output.update(run_parameters)\n",
    "        with fig.batch_update():\n",
    "            for i in range(max_traces):\n",
    "                trace = fig.data[i]\n",
    "                if i < len(results['tickers_to_display']):\n",
    "                    ticker = results['tickers_to_display'][i]; plot_data_series = results['normalized_plot_data'][ticker]\n",
    "                    trace.x, trace.y, trace.name, trace.visible, trace.showlegend = plot_data_series.index, plot_data_series.values, ticker, True, True\n",
    "                else: trace.visible, trace.showlegend = False, False\n",
    "            benchmark_trace = fig.data[max_traces]\n",
    "            if results['benchmark_price_series'] is not None and not results['benchmark_price_series'].loc[results['safe_start_date']:results['safe_viz_end_date']].dropna().empty:\n",
    "                normalized_benchmark = results['benchmark_price_series'].loc[results['safe_start_date']:results['safe_viz_end_date']] / results['benchmark_price_series'].loc[results['safe_start_date']:].bfill().iloc[0]\n",
    "                benchmark_trace.x, benchmark_trace.y, benchmark_trace.name, benchmark_trace.visible = normalized_benchmark.index, normalized_benchmark.values, f\"Benchmark ({benchmark_ticker})\", True\n",
    "            else: benchmark_trace.visible = False\n",
    "            portfolio_trace = fig.data[max_traces + 1]\n",
    "            portfolio_trace.x, portfolio_trace.y, portfolio_trace.name, portfolio_trace.visible = results['portfolio_series'].index, results['portfolio_series'], 'Group Portfolio', True\n",
    "            fig.layout.shapes = []; fig.add_shape(type=\"line\", x0=results['actual_calc_end_ts'], y0=0, x1=results['actual_calc_end_ts'], y1=1, xref='x', yref='paper', line=dict(color=\"grey\", width=2, dash=\"dash\"))\n",
    "        results_container[0] = results; debug_data_container[0] = debug_output\n",
    "        with ticker_list_output:\n",
    "            print(f\"Analysis Period: {results['safe_start_date'].date()} to {results['safe_viz_end_date'].date()}.\")\n",
    "            pprint.pprint(results['tickers_to_display'])\n",
    "            p = results['performance_data']\n",
    "            \n",
    "            # --- START OF MODIFIED BLOCK ---\n",
    "            rows = []\n",
    "            # Gain Metrics\n",
    "            rows.append({'Metric': 'Group Portfolio Gain', 'Full': p['full_p_gain'], 'Calc': p['calc_p_gain'], 'Fwd': p['fwd_p_gain']})\n",
    "            if not np.isnan(p.get('full_b_gain')):\n",
    "                rows.append({'Metric': f'Benchmark ({benchmark_ticker}) Gain', 'Full': p['full_b_gain'], 'Calc': p['calc_b_gain'], 'Fwd': p['fwd_b_gain']})\n",
    "                rows.append({'Metric': '== Gain Delta (vs Bm)', 'Full': p['full_p_gain'] - p['full_b_gain'], 'Calc': p['calc_p_gain'] - p['calc_b_gain'], 'Fwd': p['fwd_p_gain'] - p['fwd_b_gain']})\n",
    "            \n",
    "            # Standard Sharpe Metrics\n",
    "            rows.append({'Metric': 'Group Portfolio Sharpe', 'Full': p['full_p_sharpe'], 'Calc': p['calc_p_sharpe'], 'Fwd': p['fwd_p_sharpe']})\n",
    "            if not np.isnan(p.get('full_b_sharpe')):\n",
    "                rows.append({'Metric': f'Benchmark ({benchmark_ticker}) Sharpe', 'Full': p['full_b_sharpe'], 'Calc': p['calc_b_sharpe'], 'Fwd': p['fwd_b_sharpe']})\n",
    "                rows.append({'Metric': '== Sharpe Delta (vs Bm)', 'Full': p['full_p_sharpe'] - p['full_b_sharpe'], 'Calc': p['calc_p_sharpe'] - p['calc_b_sharpe'], 'Fwd': p['fwd_p_sharpe'] - p['fwd_b_sharpe']})\n",
    "\n",
    "            # Sharpe (ATR) Metrics\n",
    "            rows.append({'Metric': 'Group Portfolio Sharpe (ATR)', 'Full': p['full_p_sharpe_atr'], 'Calc': p['calc_p_sharpe_atr'], 'Fwd': p['fwd_p_sharpe_atr']})\n",
    "            if not np.isnan(p.get('full_b_sharpe_atr')):\n",
    "                rows.append({'Metric': f'Benchmark ({benchmark_ticker}) Sharpe (ATR)', 'Full': p['full_b_sharpe_atr'], 'Calc': p['calc_b_sharpe_atr'], 'Fwd': p['fwd_b_sharpe_atr']})\n",
    "                rows.append({'Metric': '== Sharpe (ATR) Delta (vs Bm)', 'Full': p['full_p_sharpe_atr'] - p['full_b_sharpe_atr'], 'Calc': p['calc_p_sharpe_atr'] - p['calc_b_sharpe_atr'], 'Fwd': p['fwd_p_sharpe_atr'] - p['fwd_b_sharpe_atr']})\n",
    "\n",
    "            report_df = pd.DataFrame(rows).set_index('Metric')\n",
    "            gain_rows = [row for row in report_df.index if 'Gain' in row]\n",
    "            sharpe_rows = [row for row in report_df.index if 'Sharpe' in row] # This now correctly includes both types of Sharpe\n",
    "            # --- END OF MODIFIED BLOCK ---\n",
    "            \n",
    "            styled_df = report_df.style.format('{:+.2%}', na_rep='N/A', subset=(gain_rows, report_df.columns)).format('{:+.2f}', na_rep='N/A', subset=(sharpe_rows, report_df.columns)).set_properties(**{'text-align': 'right', 'width': '100px'}).set_table_styles([{'selector': 'th.col_heading', 'props': [('text-align', 'right')]}, {'selector': 'th.row_heading', 'props': [('text-align', 'left')]}])\n",
    "            print(\"\\n--- Strategy Performance Summary ---\")\n",
    "            display(styled_df)\n",
    "    fig.update_layout(title_text='Walk-Forward Performance Analysis', xaxis_title='Date', yaxis_title='Normalized Price (Start = 1)', hovermode='x unified', legend_title_text='Tickers (Ranked)', height=600, margin=dict(t=50))\n",
    "    fig.add_hline(y=1, line_width=1, line_dash=\"dash\", line_color=\"grey\")\n",
    "    update_button.on_click(update_plot)\n",
    "    controls_row1 = widgets.HBox([start_date_picker, calc_period_input, fwd_period_input])\n",
    "    controls_row2 = widgets.HBox([metric_dropdown, rank_start_input, rank_end_input, benchmark_ticker_input, update_button])\n",
    "    ui_container = widgets.VBox([controls_row1, controls_row2, ticker_list_output], layout=widgets.Layout(margin='10px 0 20px 0'))\n",
    "    display(ui_container, fig)\n",
    "    update_plot(None)\n",
    "    return (results_container, debug_data_container)\n",
    "\n",
    "\n",
    "\n",
    "# def generate_features(df_ohlcv: pd.DataFrame, \n",
    "#                       atr_period: int = 14, \n",
    "#                       quality_window: int = 252, \n",
    "#                       quality_min_periods: int = 126) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     Generates a comprehensive DataFrame of derived features from raw OHLCV data.\n",
    "\n",
    "#     This function performs all heavy, window-based calculations upfront to be used\n",
    "#     by downstream analysis functions. It calculates:\n",
    "#     1. Technical Indicators: True Range (TR), ATR, and ATRP.\n",
    "#     2. Data Quality Metrics: Rolling stale percentage, median dollar volume, etc.\n",
    "\n",
    "#     Args:\n",
    "#         df_ohlcv: The primary DataFrame with a (Ticker, Date) MultiIndex and\n",
    "#                   columns for 'Adj High', 'Adj Low', 'Adj Close', 'Volume'.\n",
    "#         atr_period: The lookback period for the ATR's Exponential Moving Average.\n",
    "#         quality_window: The rolling window size for data quality metrics.\n",
    "#         quality_min_periods: The minimum number of observations required to have\n",
    "#                              a valid quality metric.\n",
    "\n",
    "#     Returns:\n",
    "#         A new DataFrame with the same (Ticker, Date) MultiIndex containing all\n",
    "#         calculated feature columns.\n",
    "#     \"\"\"\n",
    "#     print(\"--- Starting Feature Generation ---\")\n",
    "    \n",
    "#     # Ensure the DataFrame is sorted for correct window and shift operations\n",
    "#     # FIX: Replaced is_lexsorted() with the current pandas attribute\n",
    "#     if not df_ohlcv.index.is_monotonic_increasing:\n",
    "#         print(\"Sorting index for calculation accuracy...\")\n",
    "#         df_ohlcv = df_ohlcv.sort_index()\n",
    "\n",
    "#     # --- 1. Technical Indicator Calculation (TR, ATR, ATRP) ---\n",
    "#     print(f\"Calculating technical indicators (ATR Period: {atr_period})...\")\n",
    "    \n",
    "#     # Group by ticker to handle each security independently\n",
    "#     grouped = df_ohlcv.groupby(level='Ticker')\n",
    "    \n",
    "#     # Get the previous day's close required for True Range\n",
    "#     prev_close = grouped['Adj Close'].shift(1)\n",
    "    \n",
    "#     # Calculate the three components of True Range\n",
    "#     high_low = df_ohlcv['Adj High'] - df_ohlcv['Adj Low']\n",
    "#     high_prev_close = abs(df_ohlcv['Adj High'] - prev_close)\n",
    "#     low_prev_close = abs(df_ohlcv['Adj Low'] - prev_close)\n",
    "    \n",
    "#     # Combine the components to get the final TR\n",
    "#     tr = pd.concat([high_low, high_prev_close, low_prev_close], axis=1).max(axis=1, skipna=False)\n",
    "    \n",
    "#     # # Calculate the ATR using an Exponential Moving Average\n",
    "#     # --- FIX IS HERE ---\n",
    "#     # Use .transform() to apply the EWM function. \n",
    "#     # This guarantees the resulting Series has the exact same index as 'tr',\n",
    "#     # preventing the index alignment error during the subsequent division.\n",
    "#     atr = tr.groupby(level='Ticker').transform(\n",
    "#         lambda x: x.ewm(alpha=1/atr_period, adjust=False).mean()\n",
    "#     )\n",
    "\n",
    "#     # --- CHANGE 1: Removed .fillna(0) ---\n",
    "#     # ATRP will now be NaN on the first day, consistent with TR and ATR.\n",
    "#     atrp = (atr / df_ohlcv['Adj Close']).replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "#     indicator_df = pd.DataFrame({\n",
    "#         'TR': tr,\n",
    "#         'ATR': atr,\n",
    "#         'ATRP': atrp\n",
    "#     })\n",
    "    \n",
    "#     # --- 2. Data Quality Metric Calculation ---\n",
    "#     print(f\"Calculating data quality metrics (Window: {quality_window} days)...\")\n",
    "    \n",
    "#     # Create intermediate flags needed for quality calculations\n",
    "#     is_stale = np.where((df_ohlcv['Volume'] == 0) | (df_ohlcv['Adj High'] == df_ohlcv['Adj Low']), 1, 0)\n",
    "#     dollar_volume = df_ohlcv['Adj Close'] * df_ohlcv['Volume']\n",
    "#     has_same_volume = (grouped['Volume'].diff() == 0).astype(int)\n",
    "    \n",
    "#     # Combine flags into a temporary DataFrame for rolling calculations\n",
    "#     quality_temp_df = pd.DataFrame({\n",
    "#         'IsStale': is_stale,\n",
    "#         'DollarVolume': dollar_volume,\n",
    "#         'HasSameVolume': has_same_volume\n",
    "#     }, index=df_ohlcv.index) # Explicitly set index to be safe\n",
    "    \n",
    "#     # Perform the rolling calculations on the grouped data\n",
    "#     # --- FIX IS HERE ---\n",
    "#     # We switch to the older, more compatible dictionary-based aggregation method.\n",
    "#     # This syntax is understood by nearly all versions of pandas.\n",
    "#     rolling_result = quality_temp_df.groupby(level='Ticker').rolling(\n",
    "#         window=quality_window,\n",
    "#         min_periods=quality_min_periods\n",
    "#     ).agg({\n",
    "#         'IsStale': 'mean',\n",
    "#         'DollarVolume': 'median',\n",
    "#         'HasSameVolume': 'sum'\n",
    "#     })\n",
    "    \n",
    "#     # The dictionary syntax produces columns with the original names ('IsStale', etc.).\n",
    "#     # We now explicitly rename them to our desired final names.\n",
    "#     rolling_result = rolling_result.rename(columns={\n",
    "#         'IsStale': 'RollingStalePct',\n",
    "#         'DollarVolume': 'RollMedDollarVol', # <-- RENAMED HERE\n",
    "#         'HasSameVolume': 'RollingSameVolCount'\n",
    "#     })\n",
    "\n",
    "#     # The index after a grouped rolling operation is hierarchical.\n",
    "#     # We remove the outermost 'Ticker' level to restore the original index structure.\n",
    "#     rolling_quality = rolling_result.reset_index(level=0, drop=True)\n",
    "\n",
    "#     # --- 3. Combine All Features ---\n",
    "#     print(\"Combining all feature sets...\")\n",
    "#     features_df = pd.concat([indicator_df, rolling_quality], axis=1)\n",
    "    \n",
    "#     print(\"‚úÖ Feature generation complete.\")\n",
    "#     return features_df\n",
    "\n",
    "\n",
    "# def get_eligible_universe(features_df, filter_date, thresholds):\n",
    "#     \"\"\"Filters the universe of tickers based on quality metrics for a given date.\"\"\"\n",
    "#     filter_date_ts = pd.to_datetime(filter_date)\n",
    "#     # The index is now the comprehensive features_df\n",
    "#     date_index = features_df.index.get_level_values('Date').unique().sort_values()\n",
    "    \n",
    "#     if filter_date_ts < date_index[0]:\n",
    "#         print(f\"Warning: Filter date {filter_date_ts.date()} is before the earliest data point. Returning empty universe.\")\n",
    "#         return []\n",
    "        \n",
    "#     # Find the most recent date with quality data on or before the filter date\n",
    "#     valid_prior_dates = date_index[date_index <= filter_date_ts]\n",
    "#     if valid_prior_dates.empty:\n",
    "#         print(f\"Warning: No available data found on or before {filter_date_ts.date()}. Returning empty universe.\")\n",
    "#         return []\n",
    "        \n",
    "#     actual_date_to_use = valid_prior_dates[-1]\n",
    "#     if actual_date_to_use.date() != filter_date_ts.date():\n",
    "#         print(f\"‚ÑπÔ∏è Info: Filter date {filter_date_ts.date()} not found. Using previous available date {actual_date_to_use.date()}.\")\n",
    "\n",
    "#     metrics_on_date = features_df.xs(actual_date_to_use, level='Date')\n",
    "    \n",
    "#     # Apply filters using the new column names from features_df\n",
    "#     mask = ((metrics_on_date['RollMedDollarVol'] >= thresholds['min_median_dollar_volume']) & # <-- RENAMED\n",
    "#             (metrics_on_date['RollingStalePct'] <= thresholds['max_stale_pct']) &\n",
    "#             (metrics_on_date['RollingSameVolCount'] <= thresholds['max_same_vol_count']))\n",
    "            \n",
    "#     eligible_tickers = metrics_on_date[mask].index.tolist()\n",
    "#     all_tickers = metrics_on_date.index.tolist()\n",
    "#     print(f\"Dynamic Filter ({filter_date_ts.date()}): Kept {len(eligible_tickers)} of {len(all_tickers)} tickers.\")\n",
    "#     return eligible_tickers\n",
    "\n",
    "\n",
    "# def run_walk_forward_step(df_close_full, df_high_full, df_low_full,\n",
    "#                           master_trading_days,\n",
    "#                           start_date, calc_period, fwd_period,\n",
    "#                           metric, rank_start, rank_end, benchmark_ticker,\n",
    "#                           features_df,\n",
    "#                           debug=False):\n",
    "#     \"\"\"\n",
    "#     Runs a single step of the walk-forward analysis with a strict, pre-emptive\n",
    "#     check to ensure the full period is available.\n",
    "#     \"\"\"\n",
    "#     debug_data = {} if debug else None\n",
    "\n",
    "#     # 1. Determine exact date ranges with a NEW pre-emptive check\n",
    "#     try:\n",
    "#         start_idx = master_trading_days.get_loc(start_date)\n",
    "#     except KeyError:\n",
    "#         return ({'error': f\"Start date {start_date.date()} is not a valid trading day.\"}, None)\n",
    "\n",
    "#     # +++ THIS IS THE NEW PRE-EMPTIVE CHECK LOGIC +++\n",
    "#     # Calculate the desired end index without clamping first.\n",
    "#     desired_viz_end_idx = start_idx + calc_period + fwd_period\n",
    "    \n",
    "#     # Check if the desired end index is out of bounds.\n",
    "#     if desired_viz_end_idx >= len(master_trading_days):\n",
    "#         last_available_date = master_trading_days[-1].date()\n",
    "#         required_days = calc_period + fwd_period\n",
    "#         available_days = len(master_trading_days) - start_idx\n",
    "#         error_msg = (f\"Not enough data for the full requested period. \"\n",
    "#                      f\"Required: {required_days} days, Available: {available_days} days until {last_available_date}.\")\n",
    "#         return ({'error': error_msg}, None)\n",
    "#     # --- END OF NEW CHECK ---\n",
    "\n",
    "#     # If the check passes, we know the full period is available.\n",
    "#     # The 'min' calls are now just a redundant safety measure.\n",
    "#     calc_end_idx = min(start_idx + calc_period, len(master_trading_days) - 1)\n",
    "#     viz_end_idx = min(calc_end_idx + fwd_period, len(master_trading_days) - 1)\n",
    "\n",
    "#     safe_start_date = master_trading_days[start_idx]\n",
    "#     safe_calc_end_date = master_trading_days[calc_end_idx]\n",
    "#     safe_viz_end_date = master_trading_days[viz_end_idx]\n",
    "    \n",
    "#     if safe_start_date >= safe_calc_end_date:\n",
    "#         return ({'error': \"Invalid date range (calc period has zero or negative length).\"}, None)\n",
    "\n",
    "#     # (The rest of the function remains completely unchanged...)\n",
    "#     # 2. Slice data for the calculation period\n",
    "#     calc_close_raw = df_close_full.loc[safe_start_date:safe_calc_end_date]\n",
    "#     calc_close = calc_close_raw.dropna(axis=1, how='all')\n",
    "#     if calc_close.shape[1] == 0 or len(calc_close) < 2:\n",
    "#         return ({'error': \"Not enough data in calc period.\"}, None)\n",
    "\n",
    "#     # 3. Calculate INTERMEDIATE data required for the ranking metrics\n",
    "#     daily_returns = calc_close.bfill().ffill().pct_change()\n",
    "#     valid_tickers = calc_close.columns\n",
    "#     calc_period_index = pd.MultiIndex.from_product([valid_tickers, calc_close.index], names=['Ticker', 'Date'])\n",
    "#     features_in_period = features_df.loc[features_df.index.intersection(calc_period_index)]\n",
    "#     atrp = features_in_period.groupby(level='Ticker')['ATRP'].mean()\n",
    "\n",
    "#     # 4. Calculate all ranking metrics by iterating through the METRIC_REGISTRY\n",
    "#     metric_ingredients = { 'calc_close': calc_close, 'daily_returns': daily_returns, 'atrp': atrp, }\n",
    "#     metric_values = {}\n",
    "#     for name, func in METRIC_REGISTRY.items():\n",
    "#         metric_values[name] = func(metric_ingredients)\n",
    "#     if metric not in metric_values or metric_values[metric].empty:\n",
    "#         return ({'error': f\"Metric '{metric}' could not be calculated or resulted in no valid tickers.\"}, None)\n",
    "\n",
    "#     # 5. Rank tickers and select the portfolio\n",
    "#     sorted_tickers = metric_values[metric].sort_values(ascending=False)\n",
    "#     tickers_to_display = sorted_tickers.index[rank_start-1:rank_end].tolist()\n",
    "#     if not tickers_to_display:\n",
    "#         return ({'error': \"No tickers found for the selected rank.\"}, None)\n",
    "\n",
    "#     # 6. Calculate Portfolio & Benchmark Performance\n",
    "#     normalized_plot_data = df_close_full[tickers_to_display].loc[safe_start_date:safe_viz_end_date]\n",
    "#     normalized_plot_data = normalized_plot_data.div(normalized_plot_data.bfill().iloc[0])\n",
    "#     actual_calc_end_ts = calc_close.index.max()\n",
    "#     portfolio_series = normalized_plot_data.mean(axis=1)\n",
    "#     portfolio_return_series = portfolio_series.pct_change()\n",
    "#     benchmark_price_series = df_close_full.get(benchmark_ticker)\n",
    "#     benchmark_return_series = benchmark_price_series.loc[safe_start_date:safe_viz_end_date].bfill().ffill().pct_change() if benchmark_price_series is not None else pd.Series(dtype='float64')\n",
    "#     try:\n",
    "#         boundary_loc = portfolio_return_series.index.get_loc(actual_calc_end_ts)\n",
    "#         calc_portfolio_returns = portfolio_return_series.iloc[:boundary_loc + 1]\n",
    "#         fwd_portfolio_returns = portfolio_return_series.iloc[boundary_loc + 1:]\n",
    "#         if benchmark_price_series is not None:\n",
    "#             bm_boundary_loc = benchmark_return_series.index.get_loc(actual_calc_end_ts)\n",
    "#             calc_benchmark_returns = benchmark_return_series.iloc[:bm_boundary_loc + 1]\n",
    "#             fwd_benchmark_returns = benchmark_return_series.iloc[bm_boundary_loc + 1:]\n",
    "#         else:\n",
    "#             calc_benchmark_returns, fwd_benchmark_returns = pd.Series(dtype='float64'), pd.Series(dtype='float64')\n",
    "#     except (KeyError, IndexError):\n",
    "#         calc_portfolio_returns = portfolio_return_series.loc[:actual_calc_end_ts]\n",
    "#         fwd_portfolio_returns = portfolio_return_series.loc[actual_calc_end_ts:].iloc[1:]\n",
    "#         if benchmark_price_series is not None:\n",
    "#             calc_benchmark_returns = benchmark_return_series.loc[:actual_calc_end_ts]\n",
    "#             fwd_benchmark_returns = benchmark_return_series.loc[actual_calc_end_ts:].iloc[1:]\n",
    "#         else:\n",
    "#             calc_benchmark_returns, fwd_benchmark_returns = pd.Series(dtype='float64'), pd.Series(dtype='float64')\n",
    "#     full_period_index = pd.MultiIndex.from_product([tickers_to_display, portfolio_return_series.index], names=['Ticker', 'Date'])\n",
    "#     portfolio_atrp_features = features_df.loc[features_df.index.intersection(full_period_index)]\n",
    "#     portfolio_atrp_daily_unstacked = portfolio_atrp_features['ATRP'].unstack(level='Ticker')\n",
    "#     portfolio_atrp_series = portfolio_atrp_daily_unstacked.mean(axis=1)\n",
    "#     if benchmark_ticker in df_close_full.columns:\n",
    "#         benchmark_atrp_series = features_df.xs(benchmark_ticker, level='Ticker')['ATRP'].loc[safe_start_date:safe_viz_end_date]\n",
    "#     else:\n",
    "#         benchmark_atrp_series = pd.Series(dtype='float64')\n",
    "#     calc_portfolio_atrp = portfolio_atrp_series.loc[:actual_calc_end_ts]\n",
    "#     fwd_portfolio_atrp = portfolio_atrp_series.loc[actual_calc_end_ts:].iloc[1:]\n",
    "#     calc_benchmark_atrp = benchmark_atrp_series.loc[:actual_calc_end_ts]\n",
    "#     fwd_benchmark_atrp = benchmark_atrp_series.loc[actual_calc_end_ts:].iloc[1:]\n",
    "#     perf_data = {}\n",
    "#     perf_data['calc_p_gain'] = calculate_gain(portfolio_series.loc[:actual_calc_end_ts])\n",
    "#     perf_data['fwd_p_gain'] = calculate_gain(portfolio_series.loc[actual_calc_end_ts:])\n",
    "#     perf_data['full_p_gain'] = calculate_gain(portfolio_series)\n",
    "#     perf_data['calc_p_sharpe'] = calculate_sharpe(calc_portfolio_returns)\n",
    "#     perf_data['fwd_p_sharpe'] = calculate_sharpe(fwd_portfolio_returns)\n",
    "#     perf_data['full_p_sharpe'] = calculate_sharpe(portfolio_return_series)\n",
    "#     perf_data['calc_b_gain'] = calculate_gain(benchmark_price_series.loc[safe_start_date:actual_calc_end_ts]) if benchmark_price_series is not None else np.nan\n",
    "#     perf_data['fwd_b_gain'] = calculate_gain(benchmark_price_series.loc[actual_calc_end_ts:safe_viz_end_date]) if benchmark_price_series is not None else np.nan\n",
    "#     perf_data['full_b_gain'] = calculate_gain(benchmark_price_series.loc[safe_start_date:safe_viz_end_date]) if benchmark_price_series is not None else np.nan\n",
    "#     perf_data['calc_b_sharpe'] = calculate_sharpe(calc_benchmark_returns)\n",
    "#     perf_data['fwd_b_sharpe'] = calculate_sharpe(fwd_benchmark_returns)\n",
    "#     perf_data['full_b_sharpe'] = calculate_sharpe(benchmark_return_series)\n",
    "#     perf_data['calc_p_sharpe_atr'] = calculate_sharpe_atr(calc_portfolio_returns, calc_portfolio_atrp)\n",
    "#     perf_data['fwd_p_sharpe_atr'] = calculate_sharpe_atr(fwd_portfolio_returns, fwd_portfolio_atrp)\n",
    "#     perf_data['full_p_sharpe_atr'] = calculate_sharpe_atr(portfolio_return_series, portfolio_atrp_series)\n",
    "#     perf_data['calc_b_sharpe_atr'] = calculate_sharpe_atr(calc_benchmark_returns, calc_benchmark_atrp)\n",
    "#     perf_data['fwd_b_sharpe_atr'] = calculate_sharpe_atr(fwd_benchmark_returns, fwd_benchmark_atrp)\n",
    "#     perf_data['full_b_sharpe_atr'] = calculate_sharpe_atr(benchmark_return_series, benchmark_atrp_series)\n",
    "#     if debug:\n",
    "#         df_ranking_base = pd.DataFrame({'MeanDailyReturn': daily_returns.mean(),'StdDevDailyReturn': daily_returns.std(),'MeanATRP': atrp})\n",
    "#         df_metrics = pd.DataFrame(metric_values)\n",
    "#         df_metrics.columns = [f'Metric_{col}' for col in df_metrics.columns]\n",
    "#         df_ranking = df_ranking_base.join(df_metrics, how='left')\n",
    "#         df_ranking.index.name = 'Ticker'\n",
    "#         debug_data['ranking_metrics'] = df_ranking.sort_values(f'Metric_{metric}', ascending=False)\n",
    "#     calc_end_prices = calc_close.ffill().iloc[-1]\n",
    "#     fwd_close_slice = df_close_full.loc[actual_calc_end_ts:safe_viz_end_date]\n",
    "#     viz_end_prices = fwd_close_slice.ffill().iloc[-1] if not fwd_close_slice.empty and len(fwd_close_slice) >= 2 else calc_end_prices\n",
    "#     calc_gains = (calc_end_prices / calc_close.bfill().iloc[0]) - 1\n",
    "#     fwd_gains = (viz_end_prices / calc_end_prices) - 1\n",
    "#     results_df = pd.DataFrame({'Rank': range(rank_start, rank_start + len(tickers_to_display)), 'Metric': metric, 'MetricValue': sorted_tickers.loc[tickers_to_display].values, 'CalcPrice': calc_end_prices.loc[tickers_to_display], 'CalcGain': calc_gains.loc[tickers_to_display], 'FwdGain': fwd_gains.loc[tickers_to_display]}, index=pd.Index(tickers_to_display, name='Ticker'))\n",
    "#     if benchmark_price_series is not None and benchmark_ticker in calc_close.columns:\n",
    "#         benchmark_df_row = pd.DataFrame({'Rank': np.nan, 'Metric': metric, 'MetricValue': metric_values[metric].get(benchmark_ticker, np.nan), 'CalcPrice': calc_end_prices[benchmark_ticker], 'CalcGain': calc_gains[benchmark_ticker], 'FwdGain': fwd_gains[benchmark_ticker]}, index=pd.Index([f\"{benchmark_ticker} (BM)\"], name='Ticker'))\n",
    "#         results_df = pd.concat([results_df, benchmark_df_row])\n",
    "#     if debug:\n",
    "#         df_trace = normalized_plot_data.copy()\n",
    "#         df_trace.columns = [f'Norm_Price_{c}' for c in df_trace.columns]\n",
    "#         df_trace['Norm_Price_Portfolio'] = portfolio_series\n",
    "#         if benchmark_price_series is not None and not benchmark_price_series.loc[safe_start_date:safe_viz_end_date].dropna().empty:\n",
    "#             norm_bm = benchmark_price_series.loc[safe_start_date:safe_viz_end_date] / benchmark_price_series.loc[safe_start_date:].bfill().iloc[0]\n",
    "#             df_trace[f'Norm_Price_Benchmark_{benchmark_ticker}'] = norm_bm\n",
    "#         for col in df_trace.columns:\n",
    "#             if 'Norm_Price' in col:\n",
    "#                 df_trace[col.replace('Norm_Price', 'Return')] = df_trace[col].pct_change()\n",
    "#         debug_data['portfolio_trace'] = df_trace\n",
    "#     final_results = {\n",
    "#         'tickers_to_display': tickers_to_display, 'normalized_plot_data': normalized_plot_data,\n",
    "#         'portfolio_series': portfolio_series, 'benchmark_price_series': benchmark_price_series,\n",
    "#         'performance_data': perf_data, 'results_df': results_df, 'actual_calc_end_ts': actual_calc_end_ts,\n",
    "#         'safe_start_date': safe_start_date, 'safe_viz_end_date': safe_viz_end_date,\n",
    "#         'error': None\n",
    "#     }\n",
    "#     return (final_results, debug_data)\n",
    "\n",
    "\n",
    "# def verify_sharpe_atr_calculation_checked(df_ohlcv, features_df, tickers_to_verify, benchmark_ticker,\n",
    "#                                     start_date, calc_period, fwd_period,\n",
    "#                                     master_calendar_ticker='VOO', debug=False):\n",
    "#     \"\"\"\n",
    "#     Verifies the Sharpe (ATR) calculations for a portfolio and benchmark.\n",
    "\n",
    "#     This function transparently recalculates the key components for Sharpe (ATR)\n",
    "#     and can optionally export the underlying source data for manual inspection.\n",
    "#     \"\"\"\n",
    "#     display(Markdown(f\"## Verification Report for Sharpe (ATR) Calculation\"))\n",
    "#     display(Markdown(f\"**Portfolio Tickers:** `{tickers_to_verify}`\\n**Benchmark Ticker:** `{benchmark_ticker}`\"))\n",
    "\n",
    "#     # --- 1. Determine Exact Period Dates (No changes here) ---\n",
    "#     master_trading_days = df_ohlcv.loc[master_calendar_ticker].index.unique().sort_values()\n",
    "#     start_date_raw = pd.to_datetime(start_date)\n",
    "#     start_idx = master_trading_days.searchsorted(start_date_raw)\n",
    "#     actual_start_date = master_trading_days[start_idx]\n",
    "#     calc_end_idx = min(start_idx + calc_period, len(master_trading_days) - 1)\n",
    "#     fwd_end_idx = min(calc_end_idx + fwd_period, len(master_trading_days) - 1)\n",
    "#     actual_calc_end_date = master_trading_days[calc_end_idx]\n",
    "#     actual_fwd_end_date = master_trading_days[fwd_end_idx]\n",
    "    \n",
    "#     display(Markdown(f\"**Full Period:** `{actual_start_date.date()}` to `{actual_fwd_end_date.date()}`\\n\"\n",
    "#                     f\"**Calc End Date:** `{actual_calc_end_date.date()}`\"))\n",
    "\n",
    "#     # Original debug export block (can be kept or removed)\n",
    "#     if debug:\n",
    "#         # ... (original export code remains here) ...\n",
    "#         pass # Assuming original block is here\n",
    "\n",
    "#     # --- 2. Recreate Portfolio & Benchmark Series from Scratch ---\n",
    "#     df_close_full = df_ohlcv['Adj Close'].unstack(level=0)\n",
    "#     portfolio_prices_raw = df_close_full[tickers_to_verify].loc[actual_start_date:actual_fwd_end_date]\n",
    "#     portfolio_prices_norm = portfolio_prices_raw.div(portfolio_prices_raw.bfill().iloc[0])\n",
    "#     portfolio_value_series = portfolio_prices_norm.mean(axis=1)\n",
    "#     portfolio_return_series = portfolio_value_series.pct_change()\n",
    "#     p_idx = pd.MultiIndex.from_product([tickers_to_verify, portfolio_return_series.index])\n",
    "#     p_atrp_df = features_df.loc[features_df.index.intersection(p_idx)]['ATRP'].unstack(level=0)\n",
    "#     portfolio_atrp_series = p_atrp_df.mean(axis=1)\n",
    "\n",
    "# ###############################    \n",
    "#     # benchmark_return_series = df_close_full[benchmark_ticker].pct_change().loc[actual_start_date:actual_fwd_end_date]\n",
    "\n",
    "#     # 1. First, slice the raw prices for the desired date range.\n",
    "#     benchmark_prices_raw = df_close_full[benchmark_ticker].loc[actual_start_date:actual_fwd_end_date]\n",
    "#     # 2. Then, calculate the percentage change on the sliced data.\n",
    "#     benchmark_return_series = benchmark_prices_raw.pct_change()\n",
    "\n",
    "# ###############################    \n",
    "#     benchmark_atrp_series = features_df.xs(benchmark_ticker, level='Ticker')['ATRP'].loc[actual_start_date:actual_fwd_end_date]\n",
    "\n",
    "\n",
    "#     # +++ NEW: DETAILED RETURN CALCULATION TRACE (PORTFOLIO) +++\n",
    "#     if debug:\n",
    "#         display(Markdown(\"---\"))\n",
    "#         display(Markdown(\"### üêõ Detailed Portfolio Return Calculation Trace\"))\n",
    "\n",
    "#         # Step 1: Show the raw prices being used\n",
    "#         print(\"\\n[STEP 1] Raw Adjusted Close prices used for calculation.\")\n",
    "#         print(\"Compare these values with your 'Adj Close' columns.\")\n",
    "#         display(portfolio_prices_raw)\n",
    "\n",
    "#         # Step 2: Show the normalization base and the result\n",
    "#         normalization_base = portfolio_prices_raw.bfill().iloc[0]\n",
    "#         print(\"\\n[STEP 2] Normalization base (first row of prices).\")\n",
    "#         print(\"Each column in Step 1 is divided by this corresponding value.\")\n",
    "#         display(pd.DataFrame(normalization_base).T)\n",
    "\n",
    "#         print(\"\\n[STEP 2a] Normalized prices (Result of division).\")\n",
    "#         print(\"Compare these values with your 'N Close' columns.\")\n",
    "#         display(portfolio_prices_norm)\n",
    "\n",
    "#         # Step 3: Show the averaged portfolio value series\n",
    "#         print(\"\\n[STEP 3] Averaged normalized portfolio value series.\")\n",
    "#         print(\"This is the row-by-row average of the table in Step 2a.\")\n",
    "#         print(\"Compare this series with your 'N_portf' column.\")\n",
    "#         display(pd.DataFrame(portfolio_value_series, columns=['N_portf']))\n",
    "\n",
    "#         # Step 4: Show the final portfolio return series\n",
    "#         print(\"\\n[STEP 4] Final portfolio daily return series (pct_change).\")\n",
    "#         print(\"This is the percentage change of the series in Step 3.\")\n",
    "#         print(\"Compare this series with your 'N_portf_rtn' column.\")\n",
    "#         display(pd.DataFrame(portfolio_return_series, columns=['N_portf_rtn']))\n",
    "#         display(Markdown(\"---\"))\n",
    "#     # +++ END OF NEW DEBUG BLOCK +++\n",
    "\n",
    "\n",
    "#     # --- 3. Define a Helper to Print Detailed Calculation Steps ---\n",
    "#     # MODIFIED to include more debug details inside\n",
    "#     def _calculate_and_print_metrics(period_name, returns, atrps):\n",
    "#         display(Markdown(f\"#### {period_name}\"))\n",
    "#         if returns.dropna().empty or atrps.dropna().empty:\n",
    "#             print(\"  - Not enough data to calculate.\")\n",
    "#             return np.nan\n",
    "\n",
    "#         # Standard calculations\n",
    "#         mean_return = returns.mean()\n",
    "#         mean_atrp = atrps.mean()\n",
    "#         sharpe_atr = mean_return / mean_atrp if mean_atrp > 0 else np.nan\n",
    "\n",
    "#         # +++ ADDED: Detailed Mean Calculation Breakdown +++\n",
    "#         if debug:\n",
    "#             valid_returns = returns.dropna()\n",
    "#             num_returns = valid_returns.count()\n",
    "#             sum_returns = valid_returns.sum()\n",
    "#             manual_mean = sum_returns / num_returns if num_returns > 0 else 0\n",
    "#             print(f\"  - (Debug) Number of valid daily returns: {num_returns}\")\n",
    "#             print(f\"  - (Debug) Sum of all daily returns:      {sum_returns:,.8f}\")\n",
    "#             print(f\"  - (Debug) Manually calculated mean:      {manual_mean:,.8f} (Sum / Count)\")\n",
    "#         # +++ END OF ADDED DETAIL +++\n",
    "\n",
    "#         print(f\"  - Mean Daily Return: {mean_return:,.6f}\")\n",
    "#         print(f\"  - Mean Daily ATRP:  {mean_atrp:,.6f}\")\n",
    "#         print(f\"  - Sharpe (ATR) = (Mean Return / Mean ATRP) = {sharpe_atr:,.4f}\")\n",
    "#         return sharpe_atr\n",
    "\n",
    "#     # --- 4. Run Verification for Portfolio (No changes here) ---\n",
    "#     display(Markdown(\"### A. Group Portfolio Verification\"))\n",
    "#     _calculate_and_print_metrics(\"Full Period\", portfolio_return_series, portfolio_atrp_series)\n",
    "#     _calculate_and_print_metrics(\"Calculation Period\", portfolio_return_series.loc[:actual_calc_end_date], portfolio_atrp_series.loc[:actual_calc_end_date])\n",
    "#     _calculate_and_print_metrics(\"Forward Period\", portfolio_return_series.loc[actual_calc_end_date:].iloc[1:], portfolio_atrp_series.loc[actual_calc_end_date:].iloc[1:])\n",
    "\n",
    "#     # --- 5. Run Verification for Benchmark (No changes here) ---\n",
    "#     display(Markdown(f\"### B. Benchmark ({benchmark_ticker}) Verification\"))\n",
    "#     _calculate_and_print_metrics(\"Full Period\", benchmark_return_series, benchmark_atrp_series)\n",
    "#     _calculate_and_print_metrics(\"Calculation Period\", benchmark_return_series.loc[:actual_calc_end_date], benchmark_atrp_series.loc[:actual_calc_end_date])\n",
    "#     _calculate_and_print_metrics(\"Forward Period\", benchmark_return_series.loc[actual_calc_end_date:].iloc[1:], benchmark_atrp_series.loc[actual_calc_end_date:].iloc[1:])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5b18a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_container, debug_container = plot_walk_forward_analyzer(\n",
    "    df_ohlcv=df_ohlcv,\n",
    "    default_start_date='2018-10-03',\n",
    "    default_calc_period=252,\n",
    "    default_fwd_period=63,\n",
    "    default_metric='Sharpe (ATR)',\n",
    "    default_rank_start=1,\n",
    "    default_rank_end=5,\n",
    "    default_benchmark_ticker='VOO',\n",
    "    quality_thresholds={ 'min_median_dollar_volume': 10_000_000, \n",
    "                         'max_stale_pct': 0.05, \n",
    "                         'max_same_vol_count': 1 },\n",
    "    debug=True  # <-- Activate the new mode!\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed49741",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85348e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274d60ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Use pd.IndexSlice for a clean and readable slice\n",
    "# This is the recommended pandas method for complex slicing on a MultiIndex.\n",
    "idx = pd.IndexSlice\n",
    "sliced_features_df = features_df.loc[idx[tickers_to_verify, start_date:end_date], :]\n",
    "\n",
    "# 3. Display the results to verify\n",
    "print(f\"Shape of the original features_df: {features_df.shape}\")\n",
    "print(f\"Shape of the sliced_features_df: {sliced_features_df.shape}\")\n",
    "print(\"\\n--- Displaying the sliced DataFrame ---\")\n",
    "display(sliced_features_df)\n",
    "\n",
    "# You can also verify the boundaries of the slice\n",
    "print(\"\\n--- Verifying the boundaries ---\")\n",
    "print(f\"First Ticker/Date: {sliced_features_df.index.min()}\")\n",
    "print(f\"Last Ticker/Date:  {sliced_features_df.index.max()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dc055a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1. Define the tickers and date range for your slice\n",
    "# tickers_to_slice = ['BIL', 'SATS', 'MINT', 'BOXX']\n",
    "# start_date = '2025-08-03'\n",
    "# end_date = '2025-10-02'\n",
    "\n",
    "# 2. Use pd.IndexSlice for a clean and readable slice\n",
    "# This is the recommended pandas method for complex slicing on a MultiIndex.\n",
    "idx = pd.IndexSlice\n",
    "sliced_df_ohlcv = df_ohlcv.loc[idx[tickers_to_verify, start_date:end_date], :]\n",
    "\n",
    "# 3. Display the results to verify\n",
    "print(f\"Shape of the original df_ohlcv: {df_ohlcv.shape}\")\n",
    "print(f\"Shape of the sliced_df_ohlcv: {sliced_df_ohlcv.shape}\")\n",
    "print(\"\\n--- Displaying the sliced DataFrame ---\")\n",
    "display(sliced_df_ohlcv)\n",
    "\n",
    "# You can also verify the boundaries of the slice\n",
    "print(\"\\n--- Verifying the boundaries ---\")\n",
    "print(f\"First Ticker/Date: {sliced_df_ohlcv.index.min()}\")\n",
    "print(f\"Last Ticker/Date:  {sliced_df_ohlcv.index.max()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67a7eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Use .loc to slice AND explicitly create a .copy()\n",
    "# This tells pandas that we are creating a new DataFrame that we intend to modify.\n",
    "# This is the line that prevents the warning.\n",
    "idx = pd.IndexSlice\n",
    "sliced_df_ohlcv = df_ohlcv.loc[idx[tickers_to_verify, start_date:end_date], :].copy()\n",
    "\n",
    "# 3. Now, add the new column to this independent copy.\n",
    "# This will no longer raise a warning.\n",
    "sliced_df_ohlcv['Daily_Return'] = sliced_df_ohlcv.groupby(level='Ticker')['Adj Close'].pct_change()\n",
    "\n",
    "# 4. Display the results to verify\n",
    "print(\"--- DataFrame with 'Daily_Return' column added (Warning Corrected) ---\")\n",
    "display(sliced_df_ohlcv.head())\n",
    "print(\"...\")\n",
    "display(sliced_df_ohlcv.tail())\n",
    "\n",
    "# print(\"\\n--- Updated DataFrame Info ---\")\n",
    "# sliced_df_ohlcv.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a32a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by the 'Ticker' level of the index, select the 'Daily_Return' column,\n",
    "# and calculate the mean for each group.\n",
    "mean_daily_returns = sliced_df_ohlcv.groupby(level='Ticker')['Daily_Return'].mean()\n",
    "\n",
    "# Display the resulting Series\n",
    "print(\"--- Mean Daily Return per Ticker ---\")\n",
    "display(mean_daily_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea11a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by the 'Ticker' level of the index, select the 'Daily_Return' column,\n",
    "# and calculate the mean for each group.\n",
    "mean_ATRP = sliced_features_df.groupby(level='Ticker')['ATRP'].mean()\n",
    "\n",
    "# Display the resulting Series\n",
    "print(\"--- ATRP per Ticker ---\")\n",
    "display(mean_ATRP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7082bb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(mean_daily_returns / mean_ATRP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e61380",
   "metadata": {},
   "outputs": [],
   "source": [
    "sliced_features_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02ab95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sliced_df_ohlcv.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fe6ad2",
   "metadata": {},
   "source": [
    "| Date | Portfolio Value | Portfolio Daily_Return | Ticker A ATRP | Ticker B ATRP | Portfolio Daily ATRP |\n",
    "| :--- | :--- | :--- | :--- | :--- | :--- |\n",
    "| Day 1 | 1.000 | **`NaN`** | 0.005 | 0.005 | 0.005 |\n",
    "| Day 2 | 1.010 | `+0.010` | 0.006 | 0.006 | 0.006 |\n",
    "| Day 3 | 1.015 | `+0.005` | 0.004 | 0.004 | 0.004 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4b43de",
   "metadata": {},
   "source": [
    "| Function                                     | Is the \"Clamp\" Logic Used? | Why? (Its Role in this Function)                                                                                                                                                                                                                                                                                                                             |\n",
    "| -------------------------------------------- | -------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **`run_walk_forward_step`**                  | **Yes (Primary)**          | This is the **source of truth**. As the core calculation engine, it *must* be robust and handle running out of data at the end of the timeline without crashing. This is the most critical implementation of the clamp.                                                                                                                                       |\n",
    "| **`verify_sharpe_atr_calculation`**          | **Yes (Replication)**      | As you noted, it's here. Its purpose is to **exactly mimic** the behavior of `run_walk_forward_step`. If the verification tool crashed while the main tool gracefully clamped the period, the verification tool would be useless for debugging end-of-timeline scenarios.                                                                                         |\n",
    "| **`verify_group_tickers_walk_forward_calculation`** | **Yes (Replication)**      | Same reason as above. This function verifies portfolio performance and must use the identical date boundary logic as the main engine to produce comparable results for gain and Sharpe ratio.                                                                                                                                                                  |\n",
    "| **`verify_ticker_ranking_metrics`**          | **Yes (Replication)**      | Same reason. It verifies the metrics calculated over the `calc_period`. If the user selects a start date where a full `calc_period` is not available, this tool must clamp the period in the same way `run_walk_forward_step` does to verify the resulting (shorter) calculation.                                                                              |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82b6e73",
   "metadata": {},
   "source": [
    "| Function | How its Behavior Would Change | Pro | Con |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **`run_walk_forward_step` (Core Engine)** | It would now return an error for the final, shorter periods instead of clamping and calculating them. It becomes \"all or nothing.\" | The function's behavior is now stricter and more predictable. The results it returns are never from \"partial\" periods. | The function is less flexible; it can no longer handle end-of-timeline scenarios gracefully on its own. |\n",
    "| **`plot_walk_forward_analyzer` (UI)** | The user experience would be nearly identical, because it *already has* this pre-emptive check. The engine's check would just be a redundant confirmation. | The engine's behavior now perfectly matches the UI's pre-emptive check, removing any potential for divergence. | None, really. This is a positive change from the UI's perspective. |\n",
    "| **`run_full_backtest` & `run_strategy_search` (Automation)** | **This is the most significant impact.** When the backtest reaches the end of the data, `run_walk_forward_step` will return an error. The backtester's loop will then **skip this final, incomplete period entirely.** | The final aggregated equity curve and performance statistics are \"purer.\" Every single point comes from a full-length forward period. | The backtest **throws away the last few days/weeks of data** because they don't form a complete forward period. The resulting equity curve is shorter. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa735d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK Metric_Sharpe (ATR) for Calc. Period has error.\n",
    "- start date 2018-10-03\n",
    "- Calc Period 252\n",
    "- Fwd Period 63\n",
    "- Metric Sharpe ATR\n",
    "- Rank Start 1\n",
    "- Rank End 5\n",
    "-- Analysis Period 2018-10-03 to 2020-01-06 (this is full period, calc + fwd)\n",
    "-- [BIL, MINT, SHV, BNDX, VCSH]\n",
    "-- Metric_Sharpe (ATR) for BIL, MINT, SHV, BNDX matched my own calculation at bottom cell\n",
    "-- Metric Sharpe (ATR) for VCSH is a bit off (code calc 0.194195, my calc 0.196112)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9fb9a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5dc29c92",
   "metadata": {},
   "source": [
    "### Refactor Phase 1: Consolidated and Verified Feature Generation Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a98cae",
   "metadata": {},
   "source": [
    "### Start of Code for Refactoring Phase 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7be9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_path = r'c:\\Users\\ping\\Files_win10\\python\\py311\\stocks\\data\\df_OHLCV_stocks_etfs.parquet'\n",
    "df_ohlcv = pd.read_parquet(data_path, engine='pyarrow')\n",
    "df_ohlcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e7e98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def generate_features(df_ohlcv: pd.DataFrame, \n",
    "                      atr_period: int = 14, \n",
    "                      quality_window: int = 252, \n",
    "                      quality_min_periods: int = 126) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generates a comprehensive DataFrame of derived features from raw OHLCV data.\n",
    "\n",
    "    This function performs all heavy, window-based calculations upfront to be used\n",
    "    by downstream analysis functions. It calculates:\n",
    "    1. Technical Indicators: True Range (TR), ATR, and ATRP.\n",
    "    2. Data Quality Metrics: Rolling stale percentage, median dollar volume, etc.\n",
    "\n",
    "    Args:\n",
    "        df_ohlcv: The primary DataFrame with a (Ticker, Date) MultiIndex and\n",
    "                  columns for 'Adj High', 'Adj Low', 'Adj Close', 'Volume'.\n",
    "        atr_period: The lookback period for the ATR's Exponential Moving Average.\n",
    "        quality_window: The rolling window size for data quality metrics.\n",
    "        quality_min_periods: The minimum number of observations required to have\n",
    "                             a valid quality metric.\n",
    "\n",
    "    Returns:\n",
    "        A new DataFrame with the same (Ticker, Date) MultiIndex containing all\n",
    "        calculated feature columns.\n",
    "    \"\"\"\n",
    "    print(\"--- Starting Feature Generation ---\")\n",
    "    \n",
    "    # Ensure the DataFrame is sorted for correct window and shift operations\n",
    "    # FIX: Replaced is_lexsorted() with the current pandas attribute\n",
    "    if not df_ohlcv.index.is_monotonic_increasing:\n",
    "        print(\"Sorting index for calculation accuracy...\")\n",
    "        df_ohlcv = df_ohlcv.sort_index()\n",
    "\n",
    "    # --- 1. Technical Indicator Calculation (TR, ATR, ATRP) ---\n",
    "    print(f\"Calculating technical indicators (ATR Period: {atr_period})...\")\n",
    "    \n",
    "    # Group by ticker to handle each security independently\n",
    "    grouped = df_ohlcv.groupby(level='Ticker')\n",
    "    \n",
    "    # Get the previous day's close required for True Range\n",
    "    prev_close = grouped['Adj Close'].shift(1)\n",
    "    \n",
    "    # Calculate the three components of True Range\n",
    "    high_low = df_ohlcv['Adj High'] - df_ohlcv['Adj Low']\n",
    "    high_prev_close = abs(df_ohlcv['Adj High'] - prev_close)\n",
    "    low_prev_close = abs(df_ohlcv['Adj Low'] - prev_close)\n",
    "    \n",
    "    # Combine the components to get the final TR\n",
    "    tr = pd.concat([high_low, high_prev_close, low_prev_close], axis=1).max(axis=1, skipna=False)\n",
    "    \n",
    "    # # Calculate the ATR using an Exponential Moving Average\n",
    "    # --- FIX IS HERE ---\n",
    "    # Use .transform() to apply the EWM function. \n",
    "    # This guarantees the resulting Series has the exact same index as 'tr',\n",
    "    # preventing the index alignment error during the subsequent division.\n",
    "    atr = tr.groupby(level='Ticker').transform(\n",
    "        lambda x: x.ewm(alpha=1/atr_period, adjust=False).mean()\n",
    "    )\n",
    "\n",
    "    # --- CHANGE 1: Removed .fillna(0) ---\n",
    "    # ATRP will now be NaN on the first day, consistent with TR and ATR.\n",
    "    atrp = (atr / df_ohlcv['Adj Close']).replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    indicator_df = pd.DataFrame({\n",
    "        'TR': tr,\n",
    "        'ATR': atr,\n",
    "        'ATRP': atrp\n",
    "    })\n",
    "    \n",
    "    # --- 2. Data Quality Metric Calculation ---\n",
    "    print(f\"Calculating data quality metrics (Window: {quality_window} days)...\")\n",
    "    \n",
    "    # Create intermediate flags needed for quality calculations\n",
    "    is_stale = np.where((df_ohlcv['Volume'] == 0) | (df_ohlcv['Adj High'] == df_ohlcv['Adj Low']), 1, 0)\n",
    "    dollar_volume = df_ohlcv['Adj Close'] * df_ohlcv['Volume']\n",
    "    has_same_volume = (grouped['Volume'].diff() == 0).astype(int)\n",
    "    \n",
    "    # Combine flags into a temporary DataFrame for rolling calculations\n",
    "    quality_temp_df = pd.DataFrame({\n",
    "        'IsStale': is_stale,\n",
    "        'DollarVolume': dollar_volume,\n",
    "        'HasSameVolume': has_same_volume\n",
    "    }, index=df_ohlcv.index) # Explicitly set index to be safe\n",
    "    \n",
    "    # Perform the rolling calculations on the grouped data\n",
    "    # --- FIX IS HERE ---\n",
    "    # We switch to the older, more compatible dictionary-based aggregation method.\n",
    "    # This syntax is understood by nearly all versions of pandas.\n",
    "    rolling_result = quality_temp_df.groupby(level='Ticker').rolling(\n",
    "        window=quality_window,\n",
    "        min_periods=quality_min_periods\n",
    "    ).agg({\n",
    "        'IsStale': 'mean',\n",
    "        'DollarVolume': 'median',\n",
    "        'HasSameVolume': 'sum'\n",
    "    })\n",
    "    \n",
    "    # The dictionary syntax produces columns with the original names ('IsStale', etc.).\n",
    "    # We now explicitly rename them to our desired final names.\n",
    "    rolling_result = rolling_result.rename(columns={\n",
    "        'IsStale': 'RollingStalePct',\n",
    "        'DollarVolume': 'RollMedDollarVol', # <-- RENAMED HERE\n",
    "        'HasSameVolume': 'RollingSameVolCount'\n",
    "    })\n",
    "\n",
    "    # The index after a grouped rolling operation is hierarchical.\n",
    "    # We remove the outermost 'Ticker' level to restore the original index structure.\n",
    "    rolling_quality = rolling_result.reset_index(level=0, drop=True)\n",
    "\n",
    "    # --- 3. Combine All Features ---\n",
    "    print(\"Combining all feature sets...\")\n",
    "    features_df = pd.concat([indicator_df, rolling_quality], axis=1)\n",
    "    \n",
    "    print(\"‚úÖ Feature generation complete.\")\n",
    "    return features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e506f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_features_df(features_df: pd.DataFrame, \n",
    "                     df_ohlcv: pd.DataFrame, \n",
    "                     test_ticker: str = 'AAPL',\n",
    "                     spot_check_date: str = '2020-03-20'):\n",
    "    \"\"\"\n",
    "    Runs a suite of tests to verify the correctness of the generated features_df.\n",
    "\n",
    "    Args:\n",
    "        features_df: The generated DataFrame from the generate_features function.\n",
    "        df_ohlcv: The original source OHLCV DataFrame.\n",
    "        test_ticker: A common, liquid ticker to use for specific value checks.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Running Verification Suite for features_df (Test Ticker: {test_ticker}) ---\")\n",
    "    \n",
    "    # --- Test 1: Structural Integrity ---\n",
    "    print(\"\\n[Test 1: Structural Integrity]\")\n",
    "    assert features_df.index.equals(df_ohlcv.index), \"FAIL: Index does not match original df_ohlcv.\"\n",
    "    print(\"  ‚úÖ PASS: Index matches original df_ohlcv.\")\n",
    "    \n",
    "    expected_cols = ['TR', 'ATR', 'ATRP', 'RollingStalePct', 'RollMedDollarVol', 'RollingSameVolCount']\n",
    "    assert all(col in features_df.columns for col in expected_cols), \"FAIL: Missing one or more expected columns.\"\n",
    "    print(\"  ‚úÖ PASS: All expected feature columns are present.\")\n",
    "    print(f\"  - DataFrame Info:\")\n",
    "    features_df.info(verbose=False, memory_usage='deep')\n",
    "\n",
    "\n",
    "    # --- Test 2: ATR Calculation Logic ---\n",
    "    print(\"\\n[Test 2: ATR Logic Verification]\")\n",
    "    ticker_features = features_df.loc[test_ticker]\n",
    "    \n",
    "    # Test 2a: First TR value should be NaN (since prev_close is NaN)\n",
    "    first_tr = ticker_features['TR'].iloc[0]\n",
    "    assert pd.isna(first_tr), f\"FAIL: First TR value for {test_ticker} should be NaN, but got {first_tr}.\"\n",
    "    print(f\"  ‚úÖ PASS: First TR value for {test_ticker} is NaN as expected.\")\n",
    "\n",
    "    # Test 2b: The first valid ATR should equal the first valid TR (EWM cold start behavior)\n",
    "    first_valid_tr_val = ticker_features['TR'].dropna().iloc[0]\n",
    "    first_valid_atr_val = ticker_features['ATR'].dropna().iloc[0]\n",
    "    assert np.isclose(first_valid_tr_val, first_valid_atr_val), \\\n",
    "        f\"FAIL: First valid ATR ({first_valid_atr_val}) should equal first valid TR ({first_valid_tr_val}).\"\n",
    "    print(\"  ‚úÖ PASS: First valid ATR correctly seeded with first valid TR.\")\n",
    "\n",
    "\n",
    "    # --- Test 3: Rolling Quality Metrics Logic ---\n",
    "    print(\"\\n[Test 3: Rolling Quality Metrics Logic Verification]\")\n",
    "    quality_min_periods = 126 # Should match the parameter used in generation\n",
    "    \n",
    "    # Test 3a: Check for leading NaNs\n",
    "    first_valid_quality_idx = ticker_features['RollingStalePct'].first_valid_index()\n",
    "    if first_valid_quality_idx is None:\n",
    "        print(f\"  - INFO: No valid quality metrics found for {test_ticker} (likely too little data). Skipping test.\")\n",
    "    else:\n",
    "        position_of_first_valid = ticker_features.index.get_loc(first_valid_quality_idx)\n",
    "        assert position_of_first_valid == quality_min_periods - 1, \\\n",
    "            f\"FAIL: First valid quality metric should appear at index {quality_min_periods - 1}, but appeared at {position_of_first_valid}.\"\n",
    "        print(f\"  ‚úÖ PASS: Leading NaNs are present for the first {quality_min_periods - 1} periods as expected.\")\n",
    "\n",
    "\n",
    "    # --- Test 4: Spot Check Against Manual Calculation ---\n",
    "    print(\"\\n[Test 4: Spot Check vs. Manual Calculation]\")\n",
    "    # # Choose a specific date for a manual calculation\n",
    "    # spot_check_date = '2020-03-20' # A volatile day for a good test\n",
    "    \n",
    "    # Manual TR Calculation\n",
    "    today_data = df_ohlcv.loc[(test_ticker, spot_check_date)]\n",
    "    yesterday_data = df_ohlcv.loc[(test_ticker, pd.to_datetime(spot_check_date) - pd.Timedelta(days=1))] # simple lookback for test\n",
    "    \n",
    "    manual_h_l = today_data['Adj High'] - today_data['Adj Low']\n",
    "    manual_h_pc = abs(today_data['Adj High'] - yesterday_data['Adj Close'])\n",
    "    manual_l_pc = abs(today_data['Adj Low'] - yesterday_data['Adj Close'])\n",
    "    manual_tr = max(manual_h_l, manual_h_pc, manual_l_pc)\n",
    "    \n",
    "    code_tr = ticker_features.loc[spot_check_date]['TR']\n",
    "    \n",
    "    assert np.isclose(manual_tr, code_tr), f\"FAIL: Manual TR ({manual_tr:.4f}) does not match code TR ({code_tr:.4f}) on {spot_check_date}.\"\n",
    "    print(f\"  ‚úÖ PASS: Manually calculated TR on {spot_check_date} matches code's TR.\")\n",
    "    \n",
    "    print(\"\\n--- ‚úÖ All Verification Tests Passed ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ebde23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This might take a minute or two depending on your data size and CPU\n",
    "features_df = generate_features(df_ohlcv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abc1730",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a64033",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features_df(features_df, df_ohlcv, test_ticker='VCSH', spot_check_date='2018-10-03') \n",
    "# You can change the test_ticker to another well-known stock like 'MSFT' or 'GOOG'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326ad84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_features = features_df.loc['VCSH']\n",
    "ticker_features.loc['2018-10-03':'2019-10-04']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c6fe8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def export_ticker_data(ticker_to_export: str, \n",
    "                         df_ohlcv: pd.DataFrame, \n",
    "                         features_df: pd.DataFrame, \n",
    "                         output_dir: str = 'export_csv'):\n",
    "    \"\"\"\n",
    "    Exports the raw OHLCV data and the corresponding calculated features for a \n",
    "    single ticker to two separate CSV files.\n",
    "\n",
    "    This function is designed for easy manual verification of data and calculations.\n",
    "    It will create the output directory if it does not exist.\n",
    "\n",
    "    Args:\n",
    "        ticker_to_export: The ticker symbol to export (e.g., 'AAPL').\n",
    "        df_ohlcv: The main DataFrame containing the raw OHLCV data with a \n",
    "                  (Ticker, Date) MultiIndex.\n",
    "        features_df: The DataFrame containing the calculated features with a \n",
    "                     (Ticker, Date) MultiIndex.\n",
    "        output_dir: The directory where the CSV files will be saved. \n",
    "                    Defaults to 'export_csv'.\n",
    "    \"\"\"\n",
    "    print(f\"--- Attempting to export data for ticker: {ticker_to_export} ---\")\n",
    "    \n",
    "    # --- 1. Ensure the output directory exists ---\n",
    "    try:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        print(f\"Output directory '{output_dir}' is ready.\")\n",
    "    except OSError as e:\n",
    "        print(f\"Error: Could not create directory '{output_dir}'. Reason: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- 2. Isolate the data for the specified ticker ---\n",
    "    try:\n",
    "        # Use .loc to select all rows for the given ticker from the MultiIndex\n",
    "        ticker_ohlcv = df_ohlcv.loc[ticker_to_export]\n",
    "        ticker_features = features_df.loc[ticker_to_export]\n",
    "        \n",
    "        if ticker_ohlcv.empty:\n",
    "            print(f\"Warning: No OHLCV data found for ticker '{ticker_to_export}'. Cannot export.\")\n",
    "            return\n",
    "            \n",
    "        print(f\"Found {len(ticker_ohlcv)} rows of data for '{ticker_to_export}'.\")\n",
    "        \n",
    "    except KeyError:\n",
    "        print(f\"Error: Ticker '{ticker_to_export}' not found in one or both of the DataFrames. Please check the symbol.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while accessing data: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- 3. Construct file paths and export to CSV ---\n",
    "    try:\n",
    "        # Define the full path for each output file\n",
    "        ohlcv_filename = f\"{ticker_to_export}_ohlcv.csv\"\n",
    "        features_filename = f\"{ticker_to_export}_features.csv\"\n",
    "        \n",
    "        ohlcv_filepath = os.path.join(output_dir, ohlcv_filename)\n",
    "        features_filepath = os.path.join(output_dir, features_filename)\n",
    "        \n",
    "        # Export the DataFrames to CSV. The index (Date) will be included.\n",
    "        ticker_ohlcv.to_csv(ohlcv_filepath)\n",
    "        ticker_features.to_csv(features_filepath)\n",
    "        \n",
    "        print(\"\\n‚úÖ Export successful!\")\n",
    "        print(f\"   - Raw OHLCV data saved to: {ohlcv_filepath}\")\n",
    "        print(f\"   - Calculated features saved to: {features_filepath}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: Failed to write data to CSV files. Reason: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323eefda",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_ticker_data('VCSH', df_ohlcv, features_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37cf41f",
   "metadata": {},
   "source": [
    "### Part 1: Function to Create Synthetic Ticker Data\n",
    "\n",
    "This function creates a DataFrame for a single ticker (`SYNTH`) with specific, predictable patterns for stale days, dollar volume, and repeated volumes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51753192",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os # Make sure os is imported for the export function later\n",
    "\n",
    "def create_synthetic_ticker_data(\n",
    "    ticker_name: str = 'SYNTH', \n",
    "    num_days: int = 50,\n",
    "    num_zero_volume_days: int = 5,\n",
    "    num_flat_price_days: int = 3\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Creates a synthetic OHLCV DataFrame with predictable patterns and randomly injected\n",
    "    stale data conditions for robust testing.\n",
    "\n",
    "    Args:\n",
    "        ticker_name: The name for the synthetic ticker.\n",
    "        num_days: The total number of days for the ticker's history.\n",
    "        num_zero_volume_days: The number of random days to set Volume to 0.\n",
    "        num_flat_price_days: The number of random days to set High == Low.\n",
    "\n",
    "    Returns:\n",
    "        A pandas DataFrame with a (Ticker, Date) MultiIndex.\n",
    "    \"\"\"\n",
    "    print(f\"--- Creating synthetic data for '{ticker_name}' with {num_days} days ---\")\n",
    "    \n",
    "    # 1. Create a base DataFrame with \"normal\" data\n",
    "    dates = pd.to_datetime(pd.date_range(start='2023-01-01', periods=num_days, freq='B'))\n",
    "    data = {\n",
    "        'Adj Open': 100.0, 'Adj High': 102.0, 'Adj Low': 98.0,\n",
    "        'Adj Close': 100.0, 'Volume': 1_000_000\n",
    "    }\n",
    "    df = pd.DataFrame(data, index=dates)\n",
    "    df['Adj Close'] = df['Adj Close'] + np.random.randn(num_days) * 0.5 # Add some noise\n",
    "\n",
    "    # 2. Define a \"protected\" window for specific verification tests.\n",
    "    # The `verify_synthetic_ticker_features` function depends on this exact window.\n",
    "    # We will not inject random stale days here.\n",
    "    protected_start_idx, protected_end_idx = 10, 20\n",
    "    \n",
    "    # 3. Inject random \"stale\" days OUTSIDE the protected window\n",
    "    available_indices = df.index.drop(df.index[protected_start_idx:protected_end_idx])\n",
    "    \n",
    "    # Inject zero-volume days\n",
    "    if num_zero_volume_days > 0:\n",
    "        if len(available_indices) < num_zero_volume_days:\n",
    "            raise ValueError(\"Not enough available days to inject zero-volume days.\")\n",
    "        zero_vol_dates = np.random.choice(available_indices, num_zero_volume_days, replace=False)\n",
    "        df.loc[zero_vol_dates, 'Volume'] = 0\n",
    "        print(f\"  - Injected {num_zero_volume_days} random zero-volume 'stale' days.\")\n",
    "        # Update available indices to avoid overlap\n",
    "        available_indices = available_indices.drop(zero_vol_dates)\n",
    "\n",
    "    # Inject flat-price days (High == Low)\n",
    "    if num_flat_price_days > 0:\n",
    "        if len(available_indices) < num_flat_price_days:\n",
    "            raise ValueError(\"Not enough available days to inject flat-price days.\")\n",
    "        flat_price_dates = np.random.choice(available_indices, num_flat_price_days, replace=False)\n",
    "        # Set High and Low to be the same as the Close price for that day\n",
    "        df.loc[flat_price_dates, 'Adj High'] = df.loc[flat_price_dates, 'Adj Close']\n",
    "        df.loc[flat_price_dates, 'Adj Low'] = df.loc[flat_price_dates, 'Adj Close']\n",
    "        print(f\"  - Injected {num_flat_price_days} random flat-price 'stale' days.\")\n",
    "\n",
    "    # 4. Inject the specific, hand-crafted patterns inside the protected window for verification\n",
    "    print(\"  - Injecting specific patterns for programmatic verification...\")\n",
    "    # Pattern for RollingStalePct: 2 stale days in 10 (20%)\n",
    "    df.iloc[10, df.columns.get_loc('Volume')] = 0  # Stale day (zero volume)\n",
    "    df.iloc[11, df.columns.get_loc('Adj High')] = 99.0 # Stale day (High == Low)\n",
    "    df.iloc[11, df.columns.get_loc('Adj Low')] = 99.0\n",
    "    \n",
    "    # Pattern for RollingMedianVolume\n",
    "    for i in range(10):\n",
    "        df.iloc[10 + i, df.columns.get_loc('Adj Close')] = 100.0 # Standardize price for easy median calc\n",
    "        df.iloc[10 + i, df.columns.get_loc('Volume')] = (i + 1) * 10000\n",
    "\n",
    "    # Pattern for RollingSameVolCount\n",
    "    df.iloc[15, df.columns.get_loc('Volume')] = 77777\n",
    "    df.iloc[16, df.columns.get_loc('Volume')] = 77777\n",
    "    df.iloc[17, df.columns.get_loc('Volume')] = 77777\n",
    "    \n",
    "    # 5. Set the MultiIndex\n",
    "    df['Ticker'] = ticker_name\n",
    "    df = df.set_index(['Ticker', df.index])\n",
    "    df.index.names = ['Ticker', 'Date']\n",
    "    \n",
    "    print(\"‚úÖ Synthetic data created successfully.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d28048",
   "metadata": {},
   "source": [
    "### Part 2: Code to Test the Synthetic Data\n",
    "\n",
    "This new verification function is specifically designed to check the results from our synthetic data. It knows exactly what values to expect on a specific date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d19373",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_synthetic_ticker_features(features_df: pd.DataFrame, \n",
    "                                       ticker_name: str = 'SYNTH',\n",
    "                                       quality_window: int = 10):\n",
    "    \"\"\"\n",
    "    Verifies the quality metric calculations on the features_df generated from\n",
    "    the synthetic ticker data.\n",
    "\n",
    "    Args:\n",
    "        features_df: The DataFrame of calculated features.\n",
    "        ticker_name: The name of the synthetic ticker.\n",
    "        quality_window: The rolling window used, which must match the window\n",
    "                        of the synthetic data pattern.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Running Verification on Synthetic Ticker '{ticker_name}' ---\")\n",
    "    \n",
    "    # --- Expected values based on our synthetic data design ---\n",
    "    EXPECTED_STALE_PCT = 0.20  # 2 stale days out of 10\n",
    "    EXPECTED_MEDIAN_DOLLAR_VOL = 5_500_000.0 # median of (10k..100k) * price of 100\n",
    "    EXPECTED_SAME_VOL_COUNT = 2.0 # Three consecutive days gives two 'diff() == 0' events\n",
    "\n",
    "    try:\n",
    "        # Isolate the features for our synthetic ticker\n",
    "        ticker_features = features_df.loc[ticker_name]\n",
    "        \n",
    "        # The first valid calculation will be on the last day of our 10-day window.\n",
    "        # The window starts at index 10 and has a length of 10, so it ends at index 19.\n",
    "        verification_date = ticker_features.index[19]\n",
    "        \n",
    "        print(f\"Verifying calculations on date: {verification_date.date()}\")\n",
    "        \n",
    "        # Get the calculated values from the DataFrame\n",
    "        calculated_values = ticker_features.loc[verification_date]\n",
    "        stale_pct = calculated_values['RollingStalePct']\n",
    "        # --- CHANGE 2 (continued): Accessing the renamed column ---\n",
    "        median_vol = calculated_values['RollMedDollarVol'] \n",
    "        same_vol_count = calculated_values['RollingSameVolCount']\n",
    "        \n",
    "        # --- Perform Assertions ---\n",
    "        print(\"\\n[Test 1: RollingStalePct]\")\n",
    "        assert np.isclose(stale_pct, EXPECTED_STALE_PCT), f\"FAIL: Expected {EXPECTED_STALE_PCT}, Got {stale_pct}\"\n",
    "        print(f\"  ‚úÖ PASS: Expected {EXPECTED_STALE_PCT}, Got {stale_pct}\")\n",
    "\n",
    "        print(\"\\n[Test 2: RollMedDollarVol]\")\n",
    "        assert np.isclose(median_vol, EXPECTED_MEDIAN_DOLLAR_VOL), f\"FAIL: Expected {EXPECTED_MEDIAN_DOLLAR_VOL}, Got {median_vol}\"\n",
    "        print(f\"  ‚úÖ PASS: Expected {EXPECTED_MEDIAN_DOLLAR_VOL}, Got {median_vol}\")\n",
    "\n",
    "        print(\"\\n[Test 3: RollingSameVolCount]\")\n",
    "        assert np.isclose(same_vol_count, EXPECTED_SAME_VOL_COUNT), f\"FAIL: Expected {EXPECTED_SAME_VOL_COUNT}, Got {same_vol_count}\"\n",
    "        print(f\"  ‚úÖ PASS: Expected {EXPECTED_SAME_VOL_COUNT}, Got {same_vol_count}\")\n",
    "\n",
    "        print(\"\\n--- ‚úÖ All Synthetic Data Verification Tests Passed ---\")\n",
    "\n",
    "    except KeyError:\n",
    "        print(f\"FAIL: Ticker '{ticker_name}' not found in features_df.\")\n",
    "    except IndexError:\n",
    "        print(\"FAIL: Not enough data in features_df to run verification. Check num_days.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during verification: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47506201",
   "metadata": {},
   "source": [
    "## Part 3: Main Script to Run Everything\n",
    "\n",
    "This block of code ties it all together. You will need your `generate_features` and `export_ticker_data` functions available in the same environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716eff31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Main Execution Block ---\n",
    "\n",
    "# Make sure you have 'generate_features', 'verify_synthetic_ticker_features', \n",
    "# and 'export_ticker_data' functions available in your environment.\n",
    "\n",
    "# --- Step 1: Define Parameters ---\n",
    "SYNTHETIC_TICKER_NAME = 'SYNTH_STALE_TEST'\n",
    "QUALITY_WINDOW_FOR_TEST = 20\n",
    "MIN_PERIODS_FOR_TEST = 10 \n",
    "\n",
    "# --- Step 2: Create Synthetic Data with More Stale Days ---\n",
    "# We'll create 8 zero-volume days and 4 flat-price days.\n",
    "df_ohlcv_synth = create_synthetic_ticker_data(\n",
    "    ticker_name=SYNTHETIC_TICKER_NAME, \n",
    "    num_days=100, # Use more days to have space for random stale days\n",
    "    num_zero_volume_days=8,\n",
    "    num_flat_price_days=4\n",
    ")\n",
    "\n",
    "# --- Step 3: Run Feature Generation ---\n",
    "features_df_synth = generate_features(\n",
    "    df_ohlcv_synth,\n",
    "    quality_window=QUALITY_WINDOW_FOR_TEST,\n",
    "    quality_min_periods=MIN_PERIODS_FOR_TEST\n",
    ")\n",
    "\n",
    "# --- Step 4: Verify the Core Logic (this will still pass) ---\n",
    "# The verification function checks the specific 10-day window, which we preserved.\n",
    "verify_synthetic_ticker_features(\n",
    "    features_df_synth,\n",
    "    ticker_name=SYNTHETIC_TICKER_NAME,\n",
    "    quality_window=QUALITY_WINDOW_FOR_TEST\n",
    ")\n",
    "\n",
    "# --- Step 5: Export for Manual Inspection ---\n",
    "# When you open the CSVs, you will now see the additional random stale days\n",
    "# you created, allowing you to manually check the rolling calculations anywhere.\n",
    "print(\"\\n--- Exporting Enhanced Synthetic Data for Manual Review ---\")\n",
    "export_ticker_data(\n",
    "    ticker_to_export=SYNTHETIC_TICKER_NAME,\n",
    "    df_ohlcv=df_ohlcv_synth,\n",
    "    features_df=features_df_synth\n",
    ")\n",
    "\n",
    "# --- Step 6: Generate_features Parameters ---\n",
    "print(\"\\n--- Generate_features Parameters ---\")\n",
    "print(f'quality_window: {QUALITY_WINDOW_FOR_TEST}')\n",
    "print(f'quality_min_periods: {MIN_PERIODS_FOR_TEST}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec2dcbb",
   "metadata": {},
   "source": [
    "### End of Code for Refactoring Phase 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a42aca",
   "metadata": {},
   "source": [
    "## Project Hand-off: Walk-Forward Backtesting Bot\n",
    "\n",
    "This package contains the final version of the code, designed to be resilient, portable, and easy to use in both local (VS Code) and cloud (Google Colab) environments.\n",
    "\n",
    "### 1. Summary of Key Features\n",
    "\n",
    "The system you have built now includes:\n",
    "\n",
    "*   **Environment-Agnostic Operation:** A \"magic switch\" automatically detects whether the code is running locally or in Colab and adjusts all file paths accordingly.\n",
    "*   **Resumable Backtests (Checkpointing):** Long-running parameter searches are now resilient. If the process is interrupted, it can be restarted and will automatically skip completed work, picking up where it left off.\n",
    "*   **Granular, Trading-Day-Based Logic:** The backtester operates on precise integer counts of trading days, allowing for non-calendar-based periods (e.g., 10-day holds) and eliminating approximation errors.\n",
    "*   **Multi-Period Testing:** The automation script is capable of testing a list of different holding/rebalancing periods in a single run.\n",
    "*   **Modular & Verifiable Core Engine:** The core calculation logic (`run_walk_forward_step`) is a pure, self-contained function, making it easy to test and verify independently.\n",
    "*   **Dynamic Data Quality Filtering:** Before each ranking period, the universe of stocks is filtered based on rolling liquidity and data quality metrics, ensuring the strategy is only applied to tradable assets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5182cc74",
   "metadata": {},
   "source": [
    "### 2. Required Project Structure\n",
    "\n",
    "For the environment switch to work seamlessly, your project should be organized in the following way, both on your local machine and in Google Drive.\n",
    "\n",
    "```\n",
    "my_trading_project/\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ üìú bot.ipynb                 # <-- This is the main notebook file\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ üìÅ data/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ üìä df_OHLCV_stocks_etfs.parquet # <-- Your input data file goes here\n",
    "‚îÇ\n",
    "‚îî‚îÄ‚îÄ üìÅ export_csv/                 # <-- Folder for local results (created automatically)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76715d2a",
   "metadata": {},
   "source": [
    "### 3. Final, Complete Code\n",
    "\n",
    "This is the entire code for your notebook, consolidated into logical cells.\n",
    "\n",
    "#### **CELL 1: ENVIRONMENT SETUP & CONFIGURATION**\n",
    "*This cell is the \"brain\" of the system. It detects the environment and configures all paths. It's the only cell you might need to edit if your file paths change.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8a246d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# --- CELL 1: ENVIRONMENT SETUP & CONFIGURATION (IMPROVED) ---\n",
    "# This cell automatically detects the environment (local VS Code or Google Colab)\n",
    "# and configures paths and settings accordingly. It also creates directories.\n",
    "# ==============================================================================\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# 1. AUTOMATIC ENVIRONMENT DETECTION\n",
    "try:\n",
    "    import google.colab\n",
    "    IS_COLAB = True\n",
    "    print(\"‚úÖ Environment: Google Colab detected.\")\n",
    "except ImportError:\n",
    "    IS_COLAB = False\n",
    "    print(\"‚úÖ Environment: Local (VS Code) detected.\")\n",
    "\n",
    "# 2. ENVIRONMENT-SPECIFIC CONFIGURATION\n",
    "if IS_COLAB:\n",
    "    # --- Colab Settings ---\n",
    "    from google.colab import drive, output\n",
    "    drive.mount('/content/drive')\n",
    "    output.enable_custom_widget_manager()\n",
    "    \n",
    "    # IMPORTANT: This should be the path to your main project folder in Google Drive\n",
    "    DRIVE_ROOT = '/content/drive/MyDrive/my_trading_project'\n",
    "    \n",
    "    env_config = {\n",
    "        'data_path': os.path.join(DRIVE_ROOT, 'data', 'df_OHLCV_stocks_etfs.parquet'),\n",
    "        'output_dir': os.path.join(DRIVE_ROOT, 'results') # Colab results go in a 'results' folder\n",
    "    }\n",
    "    \n",
    "else:\n",
    "    # --- Local Settings ---\n",
    "    # IMPORTANT: Update this path to your local data file if it's different\n",
    "    env_config = {\n",
    "        'data_path': r'c:\\Users\\ping\\Files_win10\\python\\py311\\stocks\\data\\df_OHLCV_stocks_etfs.parquet',\n",
    "        'output_dir': os.path.join('.', 'export_csv') # Local results go in 'export_csv'\n",
    "    }\n",
    "\n",
    "# 3. CREATE ALL NECESSARY DIRECTORIES\n",
    "data_parent_dir = os.path.dirname(env_config['data_path'])\n",
    "os.makedirs(data_parent_dir, exist_ok=True)\n",
    "os.makedirs(env_config['output_dir'], exist_ok=True)\n",
    "\n",
    "print(f\"\\nData will be loaded from: {env_config['data_path']}\")\n",
    "print(f\"Output files will be saved to: {env_config['output_dir']}\")\n",
    "\n",
    "# 4. DEFINE THE FULL PATH FOR THE RESULTS FILE\n",
    "env_config['results_path'] = os.path.join(env_config['output_dir'], 'dev_strategy_search_results.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38189318",
   "metadata": {},
   "source": [
    "#### **CELL 2: GOLDEN COPY - CORE ENGINE & TOOLS**\n",
    "*This cell contains all the stable, tested functions that form the core of your backtester.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96383d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# GOLDEN COPY - COMPLETE PROJECT CODE (All Fixes Included)\n",
    "# Version: Added Refactoring Phase 1 code  \n",
    "# Date: 2025-10-14\n",
    "# ==============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import ipywidgets as widgets\n",
    "import time\n",
    "import pprint\n",
    "import os # Make sure os is imported for the export function later\n",
    "import re\n",
    "\n",
    "from datetime import datetime, date\n",
    "from IPython.display import display, Markdown\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "pd.set_option('display.max_rows', 30)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.width', 3000)\n",
    "\n",
    "\n",
    "# --- REFACTORING PHASE 1 CODE: Feature Generation Engine ---\n",
    "\n",
    "def generate_features(df_ohlcv: pd.DataFrame, \n",
    "                      atr_period: int = 14, \n",
    "                      quality_window: int = 252, \n",
    "                      quality_min_periods: int = 126) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generates a comprehensive DataFrame of derived features from raw OHLCV data.\n",
    "\n",
    "    This function performs all heavy, window-based calculations upfront to be used\n",
    "    by downstream analysis functions. It calculates:\n",
    "    1. Technical Indicators: True Range (TR), ATR, and ATRP.\n",
    "    2. Data Quality Metrics: Rolling stale percentage, median dollar volume, etc.\n",
    "\n",
    "    Args:\n",
    "        df_ohlcv: The primary DataFrame with a (Ticker, Date) MultiIndex and\n",
    "                  columns for 'Adj High', 'Adj Low', 'Adj Close', 'Volume'.\n",
    "        atr_period: The lookback period for the ATR's Exponential Moving Average.\n",
    "        quality_window: The rolling window size for data quality metrics.\n",
    "        quality_min_periods: The minimum number of observations required to have\n",
    "                             a valid quality metric.\n",
    "\n",
    "    Returns:\n",
    "        A new DataFrame with the same (Ticker, Date) MultiIndex containing all\n",
    "        calculated feature columns.\n",
    "    \"\"\"\n",
    "    print(\"--- Starting Feature Generation ---\")\n",
    "    \n",
    "    # Ensure the DataFrame is sorted for correct window and shift operations\n",
    "    # FIX: Replaced is_lexsorted() with the current pandas attribute\n",
    "    if not df_ohlcv.index.is_monotonic_increasing:\n",
    "        print(\"Sorting index for calculation accuracy...\")\n",
    "        df_ohlcv = df_ohlcv.sort_index()\n",
    "\n",
    "    # --- 1. Technical Indicator Calculation (TR, ATR, ATRP) ---\n",
    "    print(f\"Calculating technical indicators (ATR Period: {atr_period})...\")\n",
    "    \n",
    "    # Group by ticker to handle each security independently\n",
    "    grouped = df_ohlcv.groupby(level='Ticker')\n",
    "    \n",
    "    # Get the previous day's close required for True Range\n",
    "    prev_close = grouped['Adj Close'].shift(1)\n",
    "    \n",
    "    # Calculate the three components of True Range\n",
    "    high_low = df_ohlcv['Adj High'] - df_ohlcv['Adj Low']\n",
    "    high_prev_close = abs(df_ohlcv['Adj High'] - prev_close)\n",
    "    low_prev_close = abs(df_ohlcv['Adj Low'] - prev_close)\n",
    "    \n",
    "    # Combine the components to get the final TR\n",
    "    tr = pd.concat([high_low, high_prev_close, low_prev_close], axis=1).max(axis=1, skipna=False)\n",
    "    \n",
    "    # # Calculate the ATR using an Exponential Moving Average\n",
    "    # --- FIX IS HERE ---\n",
    "    # Use .transform() to apply the EWM function. \n",
    "    # This guarantees the resulting Series has the exact same index as 'tr',\n",
    "    # preventing the index alignment error during the subsequent division.\n",
    "    atr = tr.groupby(level='Ticker').transform(\n",
    "        lambda x: x.ewm(alpha=1/atr_period, adjust=False).mean()\n",
    "    )\n",
    "\n",
    "    # --- CHANGE 1: Removed .fillna(0) ---\n",
    "    # ATRP will now be NaN on the first day, consistent with TR and ATR.\n",
    "    atrp = (atr / df_ohlcv['Adj Close']).replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    indicator_df = pd.DataFrame({\n",
    "        'TR': tr,\n",
    "        'ATR': atr,\n",
    "        'ATRP': atrp\n",
    "    })\n",
    "    \n",
    "    # --- 2. Data Quality Metric Calculation ---\n",
    "    print(f\"Calculating data quality metrics (Window: {quality_window} days)...\")\n",
    "    \n",
    "    # Create intermediate flags needed for quality calculations\n",
    "    is_stale = np.where((df_ohlcv['Volume'] == 0) | (df_ohlcv['Adj High'] == df_ohlcv['Adj Low']), 1, 0)\n",
    "    dollar_volume = df_ohlcv['Adj Close'] * df_ohlcv['Volume']\n",
    "    has_same_volume = (grouped['Volume'].diff() == 0).astype(int)\n",
    "    \n",
    "    # Combine flags into a temporary DataFrame for rolling calculations\n",
    "    quality_temp_df = pd.DataFrame({\n",
    "        'IsStale': is_stale,\n",
    "        'DollarVolume': dollar_volume,\n",
    "        'HasSameVolume': has_same_volume\n",
    "    }, index=df_ohlcv.index) # Explicitly set index to be safe\n",
    "    \n",
    "    # Perform the rolling calculations on the grouped data\n",
    "    # --- FIX IS HERE ---\n",
    "    # We switch to the older, more compatible dictionary-based aggregation method.\n",
    "    # This syntax is understood by nearly all versions of pandas.\n",
    "    rolling_result = quality_temp_df.groupby(level='Ticker').rolling(\n",
    "        window=quality_window,\n",
    "        min_periods=quality_min_periods\n",
    "    ).agg({\n",
    "        'IsStale': 'mean',\n",
    "        'DollarVolume': 'median',\n",
    "        'HasSameVolume': 'sum'\n",
    "    })\n",
    "    \n",
    "    # The dictionary syntax produces columns with the original names ('IsStale', etc.).\n",
    "    # We now explicitly rename them to our desired final names.\n",
    "    rolling_result = rolling_result.rename(columns={\n",
    "        'IsStale': 'RollingStalePct',\n",
    "        'DollarVolume': 'RollMedDollarVol', # <-- RENAMED HERE\n",
    "        'HasSameVolume': 'RollingSameVolCount'\n",
    "    })\n",
    "\n",
    "    # The index after a grouped rolling operation is hierarchical.\n",
    "    # We remove the outermost 'Ticker' level to restore the original index structure.\n",
    "    rolling_quality = rolling_result.reset_index(level=0, drop=True)\n",
    "\n",
    "    # --- 3. Combine All Features ---\n",
    "    print(\"Combining all feature sets...\")\n",
    "    features_df = pd.concat([indicator_df, rolling_quality], axis=1)\n",
    "    \n",
    "    print(\"‚úÖ Feature generation complete.\")\n",
    "    return features_df\n",
    "\n",
    "def test_features_df(features_df: pd.DataFrame, \n",
    "                     df_ohlcv: pd.DataFrame, \n",
    "                     test_ticker: str = 'AAPL',\n",
    "                     spot_check_date: str = '2020-03-20'):\n",
    "    \"\"\"\n",
    "    Runs a suite of tests to verify the correctness of the generated features_df.\n",
    "\n",
    "    Args:\n",
    "        features_df: The generated DataFrame from the generate_features function.\n",
    "        df_ohlcv: The original source OHLCV DataFrame.\n",
    "        test_ticker: A common, liquid ticker to use for specific value checks.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Running Verification Suite for features_df (Test Ticker: {test_ticker}) ---\")\n",
    "    \n",
    "    # --- Test 1: Structural Integrity ---\n",
    "    print(\"\\n[Test 1: Structural Integrity]\")\n",
    "    assert features_df.index.equals(df_ohlcv.index), \"FAIL: Index does not match original df_ohlcv.\"\n",
    "    print(\"  ‚úÖ PASS: Index matches original df_ohlcv.\")\n",
    "    \n",
    "    expected_cols = ['TR', 'ATR', 'ATRP', 'RollingStalePct', 'RollMedDollarVol', 'RollingSameVolCount']\n",
    "    assert all(col in features_df.columns for col in expected_cols), \"FAIL: Missing one or more expected columns.\"\n",
    "    print(\"  ‚úÖ PASS: All expected feature columns are present.\")\n",
    "    print(f\"  - DataFrame Info:\")\n",
    "    features_df.info(verbose=False, memory_usage='deep')\n",
    "\n",
    "\n",
    "    # --- Test 2: ATR Calculation Logic ---\n",
    "    print(\"\\n[Test 2: ATR Logic Verification]\")\n",
    "    ticker_features = features_df.loc[test_ticker]\n",
    "    \n",
    "    # Test 2a: First TR value should be NaN (since prev_close is NaN)\n",
    "    first_tr = ticker_features['TR'].iloc[0]\n",
    "    assert pd.isna(first_tr), f\"FAIL: First TR value for {test_ticker} should be NaN, but got {first_tr}.\"\n",
    "    print(f\"  ‚úÖ PASS: First TR value for {test_ticker} is NaN as expected.\")\n",
    "\n",
    "    # Test 2b: The first valid ATR should equal the first valid TR (EWM cold start behavior)\n",
    "    first_valid_tr_val = ticker_features['TR'].dropna().iloc[0]\n",
    "    first_valid_atr_val = ticker_features['ATR'].dropna().iloc[0]\n",
    "    assert np.isclose(first_valid_tr_val, first_valid_atr_val), \\\n",
    "        f\"FAIL: First valid ATR ({first_valid_atr_val}) should equal first valid TR ({first_valid_tr_val}).\"\n",
    "    print(\"  ‚úÖ PASS: First valid ATR correctly seeded with first valid TR.\")\n",
    "\n",
    "\n",
    "    # --- Test 3: Rolling Quality Metrics Logic ---\n",
    "    print(\"\\n[Test 3: Rolling Quality Metrics Logic Verification]\")\n",
    "    quality_min_periods = 126 # Should match the parameter used in generation\n",
    "    \n",
    "    # Test 3a: Check for leading NaNs\n",
    "    first_valid_quality_idx = ticker_features['RollingStalePct'].first_valid_index()\n",
    "    if first_valid_quality_idx is None:\n",
    "        print(f\"  - INFO: No valid quality metrics found for {test_ticker} (likely too little data). Skipping test.\")\n",
    "    else:\n",
    "        position_of_first_valid = ticker_features.index.get_loc(first_valid_quality_idx)\n",
    "        assert position_of_first_valid == quality_min_periods - 1, \\\n",
    "            f\"FAIL: First valid quality metric should appear at index {quality_min_periods - 1}, but appeared at {position_of_first_valid}.\"\n",
    "        print(f\"  ‚úÖ PASS: Leading NaNs are present for the first {quality_min_periods - 1} periods as expected.\")\n",
    "\n",
    "\n",
    "    # --- Test 4: Spot Check Against Manual Calculation ---\n",
    "    print(\"\\n[Test 4: Spot Check vs. Manual Calculation]\")\n",
    "    # # Choose a specific date for a manual calculation\n",
    "    # spot_check_date = '2020-03-20' # A volatile day for a good test\n",
    "    \n",
    "    # Manual TR Calculation\n",
    "    today_data = df_ohlcv.loc[(test_ticker, spot_check_date)]\n",
    "    yesterday_data = df_ohlcv.loc[(test_ticker, pd.to_datetime(spot_check_date) - pd.Timedelta(days=1))] # simple lookback for test\n",
    "    \n",
    "    manual_h_l = today_data['Adj High'] - today_data['Adj Low']\n",
    "    manual_h_pc = abs(today_data['Adj High'] - yesterday_data['Adj Close'])\n",
    "    manual_l_pc = abs(today_data['Adj Low'] - yesterday_data['Adj Close'])\n",
    "    manual_tr = max(manual_h_l, manual_h_pc, manual_l_pc)\n",
    "    \n",
    "    code_tr = ticker_features.loc[spot_check_date]['TR']\n",
    "    \n",
    "    assert np.isclose(manual_tr, code_tr), f\"FAIL: Manual TR ({manual_tr:.4f}) does not match code TR ({code_tr:.4f}) on {spot_check_date}.\"\n",
    "    print(f\"  ‚úÖ PASS: Manually calculated TR on {spot_check_date} matches code's TR.\")\n",
    "    \n",
    "    print(\"\\n--- ‚úÖ All Verification Tests Passed ---\")\n",
    "\n",
    "def export_ticker_data(ticker_to_export: str, \n",
    "                         df_ohlcv: pd.DataFrame, \n",
    "                         features_df: pd.DataFrame, \n",
    "                         output_dir: str = 'export_csv'):\n",
    "    \"\"\"\n",
    "    Exports the raw OHLCV data and the corresponding calculated features for a \n",
    "    single ticker to two separate CSV files.\n",
    "\n",
    "    This function is designed for easy manual verification of data and calculations.\n",
    "    It will create the output directory if it does not exist.\n",
    "\n",
    "    Args:\n",
    "        ticker_to_export: The ticker symbol to export (e.g., 'AAPL').\n",
    "        df_ohlcv: The main DataFrame containing the raw OHLCV data with a \n",
    "                  (Ticker, Date) MultiIndex.\n",
    "        features_df: The DataFrame containing the calculated features with a \n",
    "                     (Ticker, Date) MultiIndex.\n",
    "        output_dir: The directory where the CSV files will be saved. \n",
    "                    Defaults to 'export_csv'.\n",
    "    \"\"\"\n",
    "    print(f\"--- Attempting to export data for ticker: {ticker_to_export} ---\")\n",
    "    \n",
    "    # --- 1. Ensure the output directory exists ---\n",
    "    try:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        print(f\"Output directory '{output_dir}' is ready.\")\n",
    "    except OSError as e:\n",
    "        print(f\"Error: Could not create directory '{output_dir}'. Reason: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- 2. Isolate the data for the specified ticker ---\n",
    "    try:\n",
    "        # Use .loc to select all rows for the given ticker from the MultiIndex\n",
    "        ticker_ohlcv = df_ohlcv.loc[ticker_to_export]\n",
    "        ticker_features = features_df.loc[ticker_to_export]\n",
    "        \n",
    "        if ticker_ohlcv.empty:\n",
    "            print(f\"Warning: No OHLCV data found for ticker '{ticker_to_export}'. Cannot export.\")\n",
    "            return\n",
    "            \n",
    "        print(f\"Found {len(ticker_ohlcv)} rows of data for '{ticker_to_export}'.\")\n",
    "        \n",
    "    except KeyError:\n",
    "        print(f\"Error: Ticker '{ticker_to_export}' not found in one or both of the DataFrames. Please check the symbol.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while accessing data: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- 3. Construct file paths and export to CSV ---\n",
    "    try:\n",
    "        # Define the full path for each output file\n",
    "        ohlcv_filename = f\"{ticker_to_export}_ohlcv.csv\"\n",
    "        features_filename = f\"{ticker_to_export}_features.csv\"\n",
    "        \n",
    "        ohlcv_filepath = os.path.join(output_dir, ohlcv_filename)\n",
    "        features_filepath = os.path.join(output_dir, features_filename)\n",
    "        \n",
    "        # Export the DataFrames to CSV. The index (Date) will be included.\n",
    "        ticker_ohlcv.to_csv(ohlcv_filepath)\n",
    "        ticker_features.to_csv(features_filepath)\n",
    "        \n",
    "        print(\"\\n‚úÖ Export successful!\")\n",
    "        print(f\"   - Raw OHLCV data saved to: {ohlcv_filepath}\")\n",
    "        print(f\"   - Calculated features saved to: {features_filepath}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: Failed to write data to CSV files. Reason: {e}\")\n",
    "\n",
    "def create_synthetic_ticker_data(\n",
    "    ticker_name: str = 'SYNTH', \n",
    "    num_days: int = 50,\n",
    "    num_zero_volume_days: int = 5,\n",
    "    num_flat_price_days: int = 3\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Creates a synthetic OHLCV DataFrame with predictable patterns and randomly injected\n",
    "    stale data conditions for robust testing.\n",
    "\n",
    "    Args:\n",
    "        ticker_name: The name for the synthetic ticker.\n",
    "        num_days: The total number of days for the ticker's history.\n",
    "        num_zero_volume_days: The number of random days to set Volume to 0.\n",
    "        num_flat_price_days: The number of random days to set High == Low.\n",
    "\n",
    "    Returns:\n",
    "        A pandas DataFrame with a (Ticker, Date) MultiIndex.\n",
    "    \"\"\"\n",
    "    print(f\"--- Creating synthetic data for '{ticker_name}' with {num_days} days ---\")\n",
    "    \n",
    "    # 1. Create a base DataFrame with \"normal\" data\n",
    "    dates = pd.to_datetime(pd.date_range(start='2023-01-01', periods=num_days, freq='B'))\n",
    "    data = {\n",
    "        'Adj Open': 100.0, 'Adj High': 102.0, 'Adj Low': 98.0,\n",
    "        'Adj Close': 100.0, 'Volume': 1_000_000\n",
    "    }\n",
    "    df = pd.DataFrame(data, index=dates)\n",
    "    df['Adj Close'] = df['Adj Close'] + np.random.randn(num_days) * 0.5 # Add some noise\n",
    "\n",
    "    # 2. Define a \"protected\" window for specific verification tests.\n",
    "    # The `verify_synthetic_ticker_features` function depends on this exact window.\n",
    "    # We will not inject random stale days here.\n",
    "    protected_start_idx, protected_end_idx = 10, 20\n",
    "    \n",
    "    # 3. Inject random \"stale\" days OUTSIDE the protected window\n",
    "    available_indices = df.index.drop(df.index[protected_start_idx:protected_end_idx])\n",
    "    \n",
    "    # Inject zero-volume days\n",
    "    if num_zero_volume_days > 0:\n",
    "        if len(available_indices) < num_zero_volume_days:\n",
    "            raise ValueError(\"Not enough available days to inject zero-volume days.\")\n",
    "        zero_vol_dates = np.random.choice(available_indices, num_zero_volume_days, replace=False)\n",
    "        df.loc[zero_vol_dates, 'Volume'] = 0\n",
    "        print(f\"  - Injected {num_zero_volume_days} random zero-volume 'stale' days.\")\n",
    "        # Update available indices to avoid overlap\n",
    "        available_indices = available_indices.drop(zero_vol_dates)\n",
    "\n",
    "    # Inject flat-price days (High == Low)\n",
    "    if num_flat_price_days > 0:\n",
    "        if len(available_indices) < num_flat_price_days:\n",
    "            raise ValueError(\"Not enough available days to inject flat-price days.\")\n",
    "        flat_price_dates = np.random.choice(available_indices, num_flat_price_days, replace=False)\n",
    "        # Set High and Low to be the same as the Close price for that day\n",
    "        df.loc[flat_price_dates, 'Adj High'] = df.loc[flat_price_dates, 'Adj Close']\n",
    "        df.loc[flat_price_dates, 'Adj Low'] = df.loc[flat_price_dates, 'Adj Close']\n",
    "        print(f\"  - Injected {num_flat_price_days} random flat-price 'stale' days.\")\n",
    "\n",
    "    # 4. Inject the specific, hand-crafted patterns inside the protected window for verification\n",
    "    print(\"  - Injecting specific patterns for programmatic verification...\")\n",
    "    # Pattern for RollingStalePct: 2 stale days in 10 (20%)\n",
    "    df.iloc[10, df.columns.get_loc('Volume')] = 0  # Stale day (zero volume)\n",
    "    df.iloc[11, df.columns.get_loc('Adj High')] = 99.0 # Stale day (High == Low)\n",
    "    df.iloc[11, df.columns.get_loc('Adj Low')] = 99.0\n",
    "    \n",
    "    # Pattern for RollingMedianVolume\n",
    "    for i in range(10):\n",
    "        df.iloc[10 + i, df.columns.get_loc('Adj Close')] = 100.0 # Standardize price for easy median calc\n",
    "        df.iloc[10 + i, df.columns.get_loc('Volume')] = (i + 1) * 10000\n",
    "\n",
    "    # Pattern for RollingSameVolCount\n",
    "    df.iloc[15, df.columns.get_loc('Volume')] = 77777\n",
    "    df.iloc[16, df.columns.get_loc('Volume')] = 77777\n",
    "    df.iloc[17, df.columns.get_loc('Volume')] = 77777\n",
    "    \n",
    "    # 5. Set the MultiIndex\n",
    "    df['Ticker'] = ticker_name\n",
    "    df = df.set_index(['Ticker', df.index])\n",
    "    df.index.names = ['Ticker', 'Date']\n",
    "    \n",
    "    print(\"‚úÖ Synthetic data created successfully.\")\n",
    "    return df\n",
    "\n",
    "def verify_synthetic_ticker_features(features_df: pd.DataFrame, \n",
    "                                       ticker_name: str = 'SYNTH',\n",
    "                                       quality_window: int = 10):\n",
    "    \"\"\"\n",
    "    Verifies the quality metric calculations on the features_df generated from\n",
    "    the synthetic ticker data.\n",
    "\n",
    "    Args:\n",
    "        features_df: The DataFrame of calculated features.\n",
    "        ticker_name: The name of the synthetic ticker.\n",
    "        quality_window: The rolling window used, which must match the window\n",
    "                        of the synthetic data pattern.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Running Verification on Synthetic Ticker '{ticker_name}' ---\")\n",
    "    \n",
    "    # --- Expected values based on our synthetic data design ---\n",
    "    EXPECTED_STALE_PCT = 0.20  # 2 stale days out of 10\n",
    "    EXPECTED_MEDIAN_DOLLAR_VOL = 5_500_000.0 # median of (10k..100k) * price of 100\n",
    "    EXPECTED_SAME_VOL_COUNT = 2.0 # Three consecutive days gives two 'diff() == 0' events\n",
    "\n",
    "    try:\n",
    "        # Isolate the features for our synthetic ticker\n",
    "        ticker_features = features_df.loc[ticker_name]\n",
    "        \n",
    "        # The first valid calculation will be on the last day of our 10-day window.\n",
    "        # The window starts at index 10 and has a length of 10, so it ends at index 19.\n",
    "        verification_date = ticker_features.index[19]\n",
    "        \n",
    "        print(f\"Verifying calculations on date: {verification_date.date()}\")\n",
    "        \n",
    "        # Get the calculated values from the DataFrame\n",
    "        calculated_values = ticker_features.loc[verification_date]\n",
    "        stale_pct = calculated_values['RollingStalePct']\n",
    "        # --- CHANGE 2 (continued): Accessing the renamed column ---\n",
    "        median_vol = calculated_values['RollMedDollarVol'] \n",
    "        same_vol_count = calculated_values['RollingSameVolCount']\n",
    "        \n",
    "        # --- Perform Assertions ---\n",
    "        print(\"\\n[Test 1: RollingStalePct]\")\n",
    "        assert np.isclose(stale_pct, EXPECTED_STALE_PCT), f\"FAIL: Expected {EXPECTED_STALE_PCT}, Got {stale_pct}\"\n",
    "        print(f\"  ‚úÖ PASS: Expected {EXPECTED_STALE_PCT}, Got {stale_pct}\")\n",
    "\n",
    "        print(\"\\n[Test 2: RollMedDollarVol]\")\n",
    "        assert np.isclose(median_vol, EXPECTED_MEDIAN_DOLLAR_VOL), f\"FAIL: Expected {EXPECTED_MEDIAN_DOLLAR_VOL}, Got {median_vol}\"\n",
    "        print(f\"  ‚úÖ PASS: Expected {EXPECTED_MEDIAN_DOLLAR_VOL}, Got {median_vol}\")\n",
    "\n",
    "        print(\"\\n[Test 3: RollingSameVolCount]\")\n",
    "        assert np.isclose(same_vol_count, EXPECTED_SAME_VOL_COUNT), f\"FAIL: Expected {EXPECTED_SAME_VOL_COUNT}, Got {same_vol_count}\"\n",
    "        print(f\"  ‚úÖ PASS: Expected {EXPECTED_SAME_VOL_COUNT}, Got {same_vol_count}\")\n",
    "\n",
    "        print(\"\\n--- ‚úÖ All Synthetic Data Verification Tests Passed ---\")\n",
    "\n",
    "    except KeyError:\n",
    "        print(f\"FAIL: Ticker '{ticker_name}' not found in features_df.\")\n",
    "    except IndexError:\n",
    "        print(\"FAIL: Not enough data in features_df to run verification. Check num_days.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during verification: {e}\")\n",
    "\n",
    "# --- A. HELPER FUNCTIONS ---\n",
    "\n",
    "def calculate_gain(price_series: pd.Series):\n",
    "    \"\"\"Calculates the total gain over a series of prices.\"\"\"\n",
    "    # Ensure there are at least two data points to calculate a gain\n",
    "    if price_series.dropna().shape[0] < 2: return np.nan\n",
    "    # Use forward-fill for the end price and back-fill for the start price\n",
    "    # to handle potential NaNs at the beginning or end of the series.\n",
    "    return (price_series.ffill().iloc[-1] / price_series.bfill().iloc[0]) - 1\n",
    "\n",
    "def calculate_sharpe(return_series: pd.Series):\n",
    "    \"\"\"Calculates the annualized Sharpe ratio from a series of daily returns.\"\"\"\n",
    "    # Ensure there are at least two returns to calculate a standard deviation\n",
    "    if return_series.dropna().shape[0] < 2: return np.nan\n",
    "    std_dev = return_series.std()\n",
    "    # Avoid division by zero if returns are constant\n",
    "    if std_dev > 0 and std_dev != np.inf:\n",
    "        return (return_series.mean() / std_dev) * np.sqrt(252)\n",
    "    return np.nan\n",
    "\n",
    "def print_nested(d, indent=0, width=4):\n",
    "    \"\"\"Pretty-print any nested dict/list/tuple combination.\"\"\"\n",
    "    spacing = ' ' * indent\n",
    "    if isinstance(d, dict):\n",
    "        for k, v in d.items():\n",
    "            print(f'{spacing}{k}:')\n",
    "            print_nested(v, indent + width, width)\n",
    "    elif isinstance(d, (list, tuple)):\n",
    "        for item in d:\n",
    "            print_nested(item, indent, width)\n",
    "    else:\n",
    "        print(f'{spacing}{d}')\n",
    "\n",
    "# --- B. THE CORE CALCULATION ENGINE ---\n",
    "\n",
    "def run_walk_forward_step(df_close_full, df_high_full, df_low_full,\n",
    "                          master_trading_days,\n",
    "                          start_date, calc_period, fwd_period,\n",
    "                          metric, rank_start, rank_end, benchmark_ticker,\n",
    "                          debug=False):\n",
    "    \"\"\"Runs a single step of the walk-forward analysis using precise trading days.\"\"\"\n",
    "    debug_data = {} if debug else None\n",
    "    \n",
    "    # 1. Determine exact date ranges using the master trading day calendar\n",
    "    try:\n",
    "        start_idx = master_trading_days.get_loc(start_date)\n",
    "    except KeyError:\n",
    "        return ({'error': f\"Start date {start_date.date()} is not a valid trading day.\"}, None)\n",
    "        \n",
    "    calc_end_idx = min(start_idx + calc_period, len(master_trading_days) - 1)\n",
    "    viz_end_idx = min(calc_end_idx + fwd_period, len(master_trading_days) - 1)\n",
    "\n",
    "    safe_start_date = master_trading_days[start_idx]\n",
    "    safe_calc_end_date = master_trading_days[calc_end_idx]\n",
    "    safe_viz_end_date = master_trading_days[viz_end_idx]\n",
    "    \n",
    "    if safe_start_date >= safe_calc_end_date:\n",
    "        return ({'error': \"Invalid date range (calc period has zero or negative length).\"}, None)\n",
    "\n",
    "    # 2. Slice data for the calculation period and filter for valid tickers\n",
    "    calc_close_raw = df_close_full.loc[safe_start_date:safe_calc_end_date]\n",
    "    calc_close = calc_close_raw.dropna(axis=1, how='all') # Drop tickers with no data in the period\n",
    "    if calc_close.shape[1] == 0 or len(calc_close) < 2:\n",
    "        return ({'error': \"Not enough data in calc period.\"}, None)\n",
    "\n",
    "    # 3. Calculate ranking metrics for all valid tickers\n",
    "    first_prices = calc_close.bfill().iloc[0]\n",
    "    last_prices = calc_close.ffill().iloc[-1]\n",
    "    daily_returns = calc_close.bfill().ffill().pct_change()\n",
    "    mean_returns = daily_returns.mean()\n",
    "    std_returns = daily_returns.std()\n",
    "    \n",
    "    valid_tickers = calc_close.columns\n",
    "    calc_high = df_high_full[valid_tickers].loc[safe_start_date:safe_calc_end_date]\n",
    "    calc_low = df_low_full[valid_tickers].loc[safe_start_date:safe_calc_end_date]\n",
    "    \n",
    "    # Correctly calculate True Range (TR) for a multi-ticker DataFrame\n",
    "    # First, align the previous day's close to the current calculation window.\n",
    "    prev_close = df_close_full[valid_tickers].shift(1).loc[safe_start_date:safe_calc_end_date]\n",
    "    \n",
    "    # Calculate the three components of True Range. Each result is a DataFrame.\n",
    "    component1 = calc_high - calc_low\n",
    "    component2 = abs(calc_high - prev_close)\n",
    "    component3 = abs(calc_low - prev_close)\n",
    "\n",
    "    # Find the element-wise maximum across the three component DataFrames.\n",
    "    # np.maximum is efficient and preserves the DataFrame structure.\n",
    "    tr = np.maximum(component1, np.maximum(component2, component3))\n",
    "    \n",
    "    atr = tr.ewm(alpha=1/14, adjust=False).mean()\n",
    "    atrp = (atr / calc_close).mean() # Mean ATRP over the calculation period\n",
    "\n",
    "    metric_values = {}\n",
    "    metric_values['Price'] = (last_prices / first_prices).dropna()\n",
    "    metric_values['Sharpe'] = (mean_returns / std_returns * np.sqrt(252)).fillna(0)\n",
    "    metric_values['Sharpe (ATR)'] = (mean_returns / atrp).fillna(0)\n",
    "\n",
    "    if debug:\n",
    "        df_ranking = pd.DataFrame({\n",
    "            'FirstPrice': first_prices, 'LastPrice': last_prices, 'MeanDailyReturn': mean_returns,\n",
    "            'StdDevDailyReturn': std_returns, 'MeanATRP': atrp, 'Metric_Price': metric_values['Price'],\n",
    "            'Metric_Sharpe': metric_values['Sharpe'], 'Metric_Sharpe (ATR)': metric_values['Sharpe (ATR)']\n",
    "        })\n",
    "        df_ranking.index.name = 'Ticker'\n",
    "        debug_data['ranking_metrics'] = df_ranking.sort_values(f'Metric_{metric}', ascending=False)\n",
    "\n",
    "    # 4. Rank tickers and select the target group\n",
    "    sorted_tickers = metric_values[metric].sort_values(ascending=False)\n",
    "    tickers_to_display = sorted_tickers.index[rank_start-1:rank_end].tolist()\n",
    "    if not tickers_to_display:\n",
    "        return ({'error': \"No tickers found for the selected rank.\"}, None)\n",
    "\n",
    "    # 5. Prepare data for plotting and portfolio performance calculation\n",
    "    normalized_plot_data = df_close_full[tickers_to_display].loc[safe_start_date:safe_viz_end_date]\n",
    "    normalized_plot_data = normalized_plot_data.div(normalized_plot_data.bfill().iloc[0])\n",
    "    actual_calc_end_ts = calc_close.index.max()\n",
    "    portfolio_series = normalized_plot_data.mean(axis=1)\n",
    "    portfolio_return_series = portfolio_series.pct_change()\n",
    "    benchmark_price_series = df_close_full.get(benchmark_ticker)\n",
    "    benchmark_return_series = benchmark_price_series.loc[safe_start_date:safe_viz_end_date].bfill().ffill().pct_change() if benchmark_price_series is not None else pd.Series(dtype='float64')\n",
    "\n",
    "    # 6. Correctly slice return series for Sharpe calculation to prevent lookahead\n",
    "    try:\n",
    "        # Use index location for a clean, non-overlapping split\n",
    "        boundary_loc = portfolio_return_series.index.get_loc(actual_calc_end_ts)\n",
    "        calc_portfolio_returns = portfolio_return_series.iloc[:boundary_loc + 1]\n",
    "        fwd_portfolio_returns = portfolio_return_series.iloc[boundary_loc + 1:]\n",
    "        \n",
    "        if benchmark_price_series is not None:\n",
    "            bm_boundary_loc = benchmark_return_series.index.get_loc(actual_calc_end_ts)\n",
    "            calc_benchmark_returns = benchmark_return_series.iloc[:bm_boundary_loc + 1]\n",
    "            fwd_benchmark_returns = benchmark_return_series.iloc[bm_boundary_loc + 1:]\n",
    "        else:\n",
    "            calc_benchmark_returns, fwd_benchmark_returns = pd.Series(dtype='float64'), pd.Series(dtype='float64')\n",
    "            \n",
    "    except (KeyError, IndexError): # Fallback for edge cases\n",
    "        calc_portfolio_returns = portfolio_return_series.loc[:actual_calc_end_ts]\n",
    "        fwd_portfolio_returns = portfolio_return_series.loc[actual_calc_end_ts:].iloc[1:]\n",
    "        if benchmark_price_series is not None:\n",
    "            calc_benchmark_returns = benchmark_return_series.loc[:actual_calc_end_ts]\n",
    "            fwd_benchmark_returns = benchmark_return_series.loc[actual_calc_end_ts:].iloc[1:]\n",
    "        else:\n",
    "            calc_benchmark_returns, fwd_benchmark_returns = pd.Series(dtype='float64'), pd.Series(dtype='float64')\n",
    "\n",
    "    # 7. Calculate performance metrics (Gain & Sharpe) for all periods\n",
    "    perf_data = {}\n",
    "    perf_data['calc_p_gain'] = calculate_gain(portfolio_series.loc[:actual_calc_end_ts])\n",
    "    perf_data['fwd_p_gain'] = calculate_gain(portfolio_series.loc[actual_calc_end_ts:])\n",
    "    perf_data['full_p_gain'] = calculate_gain(portfolio_series)\n",
    "    perf_data['calc_p_sharpe'] = calculate_sharpe(calc_portfolio_returns)\n",
    "    perf_data['fwd_p_sharpe'] = calculate_sharpe(fwd_portfolio_returns)\n",
    "    perf_data['full_p_sharpe'] = calculate_sharpe(portfolio_return_series)\n",
    "    \n",
    "    perf_data['calc_b_gain'] = calculate_gain(benchmark_price_series.loc[safe_start_date:actual_calc_end_ts]) if benchmark_price_series is not None else np.nan\n",
    "    perf_data['fwd_b_gain'] = calculate_gain(benchmark_price_series.loc[actual_calc_end_ts:safe_viz_end_date]) if benchmark_price_series is not None else np.nan\n",
    "    perf_data['full_b_gain'] = calculate_gain(benchmark_price_series.loc[safe_start_date:safe_viz_end_date]) if benchmark_price_series is not None else np.nan\n",
    "    perf_data['calc_b_sharpe'] = calculate_sharpe(calc_benchmark_returns)\n",
    "    perf_data['fwd_b_sharpe'] = calculate_sharpe(fwd_benchmark_returns)\n",
    "    perf_data['full_b_sharpe'] = calculate_sharpe(benchmark_return_series)\n",
    "\n",
    "    # 8. Assemble results DataFrame for display\n",
    "    calc_end_prices = calc_close.ffill().iloc[-1]\n",
    "    fwd_close_slice = df_close_full.loc[actual_calc_end_ts:safe_viz_end_date]\n",
    "    viz_end_prices = fwd_close_slice.ffill().iloc[-1] if not fwd_close_slice.empty and len(fwd_close_slice) >= 2 else calc_end_prices\n",
    "    calc_gains = (calc_end_prices / calc_close.bfill().iloc[0]) - 1\n",
    "    fwd_gains = (viz_end_prices / calc_end_prices) - 1\n",
    "    results_df = pd.DataFrame({'Rank': range(rank_start, rank_start + len(tickers_to_display)), 'Metric': metric, 'MetricValue': sorted_tickers.loc[tickers_to_display].values, 'CalcPrice': calc_end_prices.loc[tickers_to_display], 'CalcGain': calc_gains.loc[tickers_to_display], 'FwdGain': fwd_gains.loc[tickers_to_display]}, index=pd.Index(tickers_to_display, name='Ticker'))\n",
    "    if benchmark_price_series is not None and benchmark_ticker in calc_close.columns:\n",
    "        benchmark_df_row = pd.DataFrame({'Rank': np.nan, 'Metric': metric, 'MetricValue': metric_values[metric].get(benchmark_ticker, np.nan), 'CalcPrice': calc_end_prices[benchmark_ticker], 'CalcGain': calc_gains[benchmark_ticker], 'FwdGain': fwd_gains[benchmark_ticker]}, index=pd.Index([f\"{benchmark_ticker} (BM)\"], name='Ticker'))\n",
    "        results_df = pd.concat([results_df, benchmark_df_row])\n",
    "    \n",
    "    # 9. Assemble debug data if requested\n",
    "    if debug:\n",
    "        df_trace = normalized_plot_data.copy()\n",
    "        df_trace.columns = [f'Norm_Price_{c}' for c in df_trace.columns]\n",
    "        df_trace['Norm_Price_Portfolio'] = portfolio_series\n",
    "        if benchmark_price_series is not None and not benchmark_price_series.loc[safe_start_date:safe_viz_end_date].dropna().empty:\n",
    "            norm_bm = benchmark_price_series.loc[safe_start_date:safe_viz_end_date] / benchmark_price_series.loc[safe_start_date:].bfill().iloc[0]\n",
    "            df_trace[f'Norm_Price_Benchmark_{benchmark_ticker}'] = norm_bm\n",
    "        for col in df_trace.columns:\n",
    "            if 'Norm_Price' in col:\n",
    "                df_trace[col.replace('Norm_Price', 'Return')] = df_trace[col].pct_change()\n",
    "        debug_data['portfolio_trace'] = df_trace\n",
    "\n",
    "    # 10. Package final results\n",
    "    final_results = {\n",
    "        'tickers_to_display': tickers_to_display, 'normalized_plot_data': normalized_plot_data,\n",
    "        'portfolio_series': portfolio_series, 'benchmark_price_series': benchmark_price_series,\n",
    "        'performance_data': perf_data, 'results_df': results_df, 'actual_calc_end_ts': actual_calc_end_ts,\n",
    "        'safe_start_date': safe_start_date, 'safe_viz_end_date': safe_viz_end_date,\n",
    "        'error': None\n",
    "    }\n",
    "    return (final_results, debug_data)\n",
    "\n",
    "# --- C. DYNAMIC DATA QUALITY FILTER FUNCTIONS ---\n",
    "\n",
    "def calculate_rolling_quality_metrics(df_ohlcv, window=252, min_periods=126, debug=False):\n",
    "    \"\"\"Calculates rolling data quality metrics for the entire dataset.\"\"\"\n",
    "    print(f\"--- Calculating Rolling Quality Metrics (Window: {window} days) ---\")\n",
    "    df = df_ohlcv.copy()\n",
    "    if not df.index.is_monotonic_increasing:\n",
    "        df.sort_index(inplace=True)\n",
    "        \n",
    "    # Define quality flags\n",
    "    df['IsStale'] = np.where((df['Volume'] == 0) | (df['Adj High'] == df['Adj Low']), 1, 0)\n",
    "    df['DollarVolume'] = df['Adj Close'] * df['Volume']\n",
    "    df['HasSameVolumeAsPrevDay'] = (df.groupby(level='Ticker')['Volume'].diff() == 0).astype(int)\n",
    "    \n",
    "    # Calculate rolling metrics per ticker\n",
    "    grouped = df.groupby(level='Ticker')\n",
    "    stale_pct = grouped['IsStale'].rolling(window=window, min_periods=min_periods).mean()\n",
    "    median_vol = grouped['DollarVolume'].rolling(window=window, min_periods=min_periods).median()\n",
    "    same_vol_count = grouped['HasSameVolumeAsPrevDay'].rolling(window=window, min_periods=min_periods).sum()\n",
    "    \n",
    "    quality_df = pd.concat([stale_pct, median_vol, same_vol_count], axis=1)\n",
    "    quality_df.columns = ['RollingStalePct', 'RollingMedianVolume', 'RollingSameVolCount']\n",
    "    quality_df.index = quality_df.index.droplevel(0) # Remove the extra 'Ticker' level\n",
    "    print(\"‚úÖ Rolling metrics calculation complete.\")\n",
    "    return quality_df\n",
    "\n",
    "def get_eligible_universe(quality_metrics_df, filter_date, thresholds):\n",
    "    \"\"\"Filters the universe of tickers based on quality metrics for a given date.\"\"\"\n",
    "    filter_date_ts = pd.to_datetime(filter_date)\n",
    "    date_index = quality_metrics_df.index.get_level_values('Date').unique().sort_values()\n",
    "    \n",
    "    if filter_date_ts < date_index[0]:\n",
    "        print(f\"Warning: Filter date {filter_date_ts.date()} is before the earliest data point. Returning empty universe.\")\n",
    "        return []\n",
    "        \n",
    "    # Find the most recent date with quality data on or before the filter date\n",
    "    valid_prior_dates = date_index[date_index <= filter_date_ts]\n",
    "    if valid_prior_dates.empty:\n",
    "        print(f\"Warning: No available data found on or before {filter_date_ts.date()}. Returning empty universe.\")\n",
    "        return []\n",
    "        \n",
    "    actual_date_to_use = valid_prior_dates[-1]\n",
    "    if actual_date_to_use.date() != filter_date_ts.date():\n",
    "        print(f\"‚ÑπÔ∏è Info: Filter date {filter_date_ts.date()} not found. Using previous available date {actual_date_to_use.date()}.\")\n",
    "\n",
    "    metrics_on_date = quality_metrics_df.xs(actual_date_to_use, level='Date')\n",
    "    \n",
    "    # Apply filters\n",
    "    mask = ((metrics_on_date['RollingMedianVolume'] >= thresholds['min_median_dollar_volume']) &\n",
    "            (metrics_on_date['RollingStalePct'] <= thresholds['max_stale_pct']) &\n",
    "            (metrics_on_date['RollingSameVolCount'] <= thresholds['max_same_vol_count']))\n",
    "            \n",
    "    eligible_tickers = metrics_on_date[mask].index.tolist()\n",
    "    all_tickers = metrics_on_date.index.tolist()\n",
    "    print(f\"Dynamic Filter ({filter_date_ts.date()}): Kept {len(eligible_tickers)} of {len(all_tickers)} tickers.\")\n",
    "    return eligible_tickers    \n",
    "\n",
    "# --- D. INTERACTIVE ANALYSIS & BACKTESTING TOOLS ---\n",
    "\n",
    "def plot_walk_forward_analyzer(df_ohlcv, \n",
    "                               default_start_date=None, default_calc_period=126, default_fwd_period=63,\n",
    "                               default_metric='Sharpe (ATR)', default_rank_start=1, default_rank_end=10,\n",
    "                               default_benchmark_ticker='VOO', master_calendar_ticker='VOO',\n",
    "                               quality_thresholds={'min_median_dollar_volume': 1_000_000, 'max_stale_pct': 0.05, 'max_same_vol_count': 10},\n",
    "                               debug=False):\n",
    "    \"\"\"Creates an interactive widget for single-period walk-forward analysis.\"\"\"\n",
    "    print(\"Initializing Walk-Forward Analyzer (using Trading Day Logic)...\")\n",
    "    if not isinstance(df_ohlcv.index, pd.MultiIndex): raise ValueError(\"Input DataFrame must have a (Ticker, Date) MultiIndex.\")\n",
    "    df_ohlcv = df_ohlcv.sort_index()\n",
    "\n",
    "    if master_calendar_ticker not in df_ohlcv.index.get_level_values(0):\n",
    "        raise ValueError(f\"Master calendar ticker '{master_calendar_ticker}' not found in DataFrame.\")\n",
    "    master_trading_days = df_ohlcv.loc[master_calendar_ticker].index.unique().sort_values()\n",
    "    print(f\"Master trading day calendar created from '{master_calendar_ticker}' ({len(master_trading_days)} days).\")\n",
    "\n",
    "    # # The following functions are assumed to exist. We define placeholders for them.\n",
    "    # def calculate_rolling_quality_metrics(df, window):\n",
    "    #     tickers = df.index.get_level_values(0).unique()\n",
    "    #     dates = df.index.get_level_values(1).unique()\n",
    "    #     return pd.DataFrame(index=pd.MultiIndex.from_product([tickers, dates], names=['Ticker', 'Date']))\n",
    "    # def get_eligible_universe(quality_df, date, thresholds):\n",
    "    #     tickers = quality_df.index.get_level_values(0).unique()\n",
    "    #     return list(tickers)\n",
    "    # def run_walk_forward_step(*args, **kwargs):\n",
    "    #     # Dummy return structure for demonstration\n",
    "    #     return {'error': \"This is a placeholder function.\", 'safe_start_date': pd.Timestamp.now(), 'actual_calc_end_ts': pd.Timestamp.now(), 'safe_viz_end_date': pd.Timestamp.now()}, None\n",
    "\n",
    "    print(\"Pre-calculating data quality metrics...\")\n",
    "    quality_metrics_df = calculate_rolling_quality_metrics(df_ohlcv, window=252)\n",
    "    print(\"Pre-processing data (unstacking)...\")\n",
    "    df_close_full = df_ohlcv['Adj Close'].unstack(level=0)\n",
    "    df_high_full = df_ohlcv['Adj High'].unstack(level=0)\n",
    "    df_low_full = df_ohlcv['Adj Low'].unstack(level=0)\n",
    "    \n",
    "    # --- Widget Setup ---\n",
    "    start_date_picker = widgets.DatePicker(description='Start Date:', value=pd.to_datetime(default_start_date), disabled=False)\n",
    "    calc_period_input = widgets.IntText(value=default_calc_period, description='Calc Period (days):')\n",
    "    fwd_period_input = widgets.IntText(value=default_fwd_period, description='Fwd Period (days):')\n",
    "    metrics = ['Price', 'Sharpe', 'Sharpe (ATR)']\n",
    "    metric_dropdown = widgets.Dropdown(options=metrics, value=default_metric, description='Metric:')\n",
    "    rank_start_input = widgets.IntText(value=default_rank_start, description='Rank Start:')\n",
    "    rank_end_input = widgets.IntText(value=default_rank_end, description='Rank End:')\n",
    "    benchmark_ticker_input = widgets.Text(value=default_benchmark_ticker, description='Benchmark:', placeholder='Enter Ticker')\n",
    "    update_button = widgets.Button(description=\"Update Chart\", button_style='primary')\n",
    "    ticker_list_output = widgets.Output()\n",
    "    results_container, debug_data_container = [None], [None]\n",
    "\n",
    "    # --- Plotting Setup ---\n",
    "    fig = go.FigureWidget()\n",
    "    max_traces = 50\n",
    "    for i in range(max_traces): fig.add_trace(go.Scatter(x=[None], y=[None], mode='lines', name=f'placeholder_{i}', visible=False, showlegend=False))\n",
    "    fig.add_trace(go.Scatter(x=[None], y=[None], mode='lines', name='Benchmark', visible=True, showlegend=True, line=dict(color='black', width=3, dash='dash')))\n",
    "    fig.add_trace(go.Scatter(x=[None], y=[None], mode='lines', name='Group Portfolio', visible=True, showlegend=True, line=dict(color='green', width=3)))\n",
    "\n",
    "    # --- Update Logic (Callback) ---\n",
    "    def update_plot(button_click):\n",
    "        ticker_list_output.clear_output()\n",
    "        \n",
    "        # 1. Get and validate user inputs\n",
    "        start_date_raw = pd.to_datetime(start_date_picker.value)\n",
    "        start_date_idx = master_trading_days.searchsorted(start_date_raw)\n",
    "        if start_date_idx >= len(master_trading_days):\n",
    "            with ticker_list_output: print(f\"Error: Start date is after the last available trading day.\"); return\n",
    "        actual_start_date = master_trading_days[start_date_idx]\n",
    "        with ticker_list_output: \n",
    "            if start_date_raw.date() != actual_start_date.date():\n",
    "                print(f\"‚ÑπÔ∏è Info: Start date {start_date_raw.date()} is not a trading day. Snapping forward to {actual_start_date.date()}.\")\n",
    "\n",
    "        # Capture input values into variables\n",
    "        calc_period = calc_period_input.value\n",
    "        fwd_period = fwd_period_input.value\n",
    "        metric = metric_dropdown.value\n",
    "        rank_start = rank_start_input.value\n",
    "        rank_end = rank_end_input.value\n",
    "        benchmark_ticker = benchmark_ticker_input.value.strip().upper()\n",
    "        \n",
    "        if rank_start > rank_end:\n",
    "            with ticker_list_output: print(\"Error: 'Rank Start' must be <= 'Rank End'.\"); return\n",
    "        if rank_start < 1 or calc_period < 2 or fwd_period < 1:\n",
    "            with ticker_list_output: print(\"Error: Ranks must be >= 1, Calc Period >= 2, Fwd Period >= 1.\"); return\n",
    "\n",
    "        # 1a. Validate data availability\n",
    "        required_days = calc_period + fwd_period\n",
    "        if start_date_idx + required_days > len(master_trading_days):\n",
    "            available_days = len(master_trading_days) - start_date_idx\n",
    "            last_available_date = master_trading_days[-1].date()\n",
    "            with ticker_list_output:\n",
    "                print(f\"Error: Not enough data for the requested period.\")\n",
    "                print(f\"  Start Date: {actual_start_date.date()}\")\n",
    "                print(f\"  Required Days: {calc_period} (calc) + {fwd_period} (fwd) = {required_days}\")\n",
    "                print(f\"  Available Days from Start: {available_days} (until {last_available_date})\")\n",
    "                print(f\"  Please shorten the 'Calc Period' / 'Fwd Period' or choose an earlier 'Start Date'.\")\n",
    "            return\n",
    "\n",
    "        # 2. Apply dynamic data quality filter\n",
    "        eligible_tickers = get_eligible_universe(quality_metrics_df, actual_start_date, quality_thresholds)\n",
    "        if not eligible_tickers:\n",
    "            with ticker_list_output: print(f\"Error: No eligible tickers found on {actual_start_date.date()} with the current quality filters.\"); return\n",
    "        df_close_step = df_close_full[eligible_tickers]; df_high_step = df_high_full[eligible_tickers]; df_low_step = df_low_full[eligible_tickers]\n",
    "\n",
    "        # 3. Run the core calculation\n",
    "        results, debug_output = run_walk_forward_step(\n",
    "            df_close_step, df_high_step, df_low_step, master_trading_days,\n",
    "            actual_start_date, calc_period, fwd_period, \n",
    "            metric, rank_start, rank_end, benchmark_ticker, debug=debug\n",
    "        )\n",
    "        if results.get('error'):\n",
    "            with ticker_list_output: print(f\"Error: {results['error']}\"); return\n",
    "            \n",
    "        # ======================= MODIFICATION START =======================\n",
    "        # 3a. Augment the output containers with period dates and run parameters for external use.\n",
    "        \n",
    "        # Add period dates (from previous request)\n",
    "        period_dates = {\n",
    "            'calc_period_start': results['safe_start_date'],\n",
    "            'calc_period_end': results['actual_calc_end_ts'],\n",
    "            'forward_period_start': results['actual_calc_end_ts'],\n",
    "            'forward_period_end': results['safe_viz_end_date']\n",
    "        }\n",
    "        \n",
    "        # Add run parameters (new request)\n",
    "        run_parameters = {\n",
    "            'calc_period': calc_period,\n",
    "            'fwd_period': fwd_period,\n",
    "            'rank_metric': metric,\n",
    "            'rank_start': rank_start,\n",
    "            'rank_end': rank_end,\n",
    "            'benchmark_ticker': benchmark_ticker\n",
    "        }\n",
    "        \n",
    "        # Update the main results dictionary\n",
    "        results.update(period_dates)\n",
    "        results.update(run_parameters)\n",
    "\n",
    "        # Update the debug dictionary if it exists\n",
    "        if debug_output is not None and isinstance(debug_output, dict):\n",
    "            debug_output.update(period_dates)\n",
    "            debug_output.update(run_parameters)\n",
    "        # ======================= MODIFICATION END =======================\n",
    "\n",
    "        # 4. Update the interactive plot\n",
    "        with fig.batch_update():\n",
    "            # (Plotting code remains unchanged)\n",
    "            for i in range(max_traces):\n",
    "                trace = fig.data[i]\n",
    "                if i < len(results['tickers_to_display']):\n",
    "                    ticker = results['tickers_to_display'][i]; plot_data_series = results['normalized_plot_data'][ticker]\n",
    "                    trace.x, trace.y, trace.name, trace.visible, trace.showlegend = plot_data_series.index, plot_data_series.values, ticker, True, True\n",
    "                else: trace.visible, trace.showlegend = False, False\n",
    "            benchmark_trace = fig.data[max_traces]\n",
    "            if results['benchmark_price_series'] is not None and not results['benchmark_price_series'].loc[results['safe_start_date']:results['safe_viz_end_date']].dropna().empty:\n",
    "                normalized_benchmark = results['benchmark_price_series'].loc[results['safe_start_date']:results['safe_viz_end_date']] / results['benchmark_price_series'].loc[results['safe_start_date']:].bfill().iloc[0]\n",
    "                benchmark_trace.x, benchmark_trace.y, benchmark_trace.name, benchmark_trace.visible = normalized_benchmark.index, normalized_benchmark, f\"Benchmark ({benchmark_ticker})\", True\n",
    "            else: benchmark_trace.visible = False\n",
    "            portfolio_trace = fig.data[max_traces + 1]\n",
    "            portfolio_trace.x, portfolio_trace.y, portfolio_trace.name, portfolio_trace.visible = results['portfolio_series'].index, results['portfolio_series'], 'Group Portfolio', True\n",
    "            fig.layout.shapes = []; fig.add_shape(type=\"line\", x0=results['actual_calc_end_ts'], y0=0, x1=results['actual_calc_end_ts'], y1=1, xref='x', yref='paper', line=dict(color=\"grey\", width=2, dash=\"dash\"))\n",
    "            \n",
    "        results_container[0] = results; debug_data_container[0] = debug_output\n",
    "        \n",
    "        # 5. Display summary statistics in a formatted table\n",
    "        with ticker_list_output:\n",
    "            # (Summary display code remains unchanged)\n",
    "            print(f\"Analysis Period: {results['safe_start_date'].date()} to {results['safe_viz_end_date'].date()}.\")\n",
    "            pprint.pprint(results['tickers_to_display'])\n",
    "            p = results['performance_data']\n",
    "            rows = []\n",
    "            rows.append({'Metric': 'Group Portfolio Gain', 'Full': p['full_p_gain'], 'Calc': p['calc_p_gain'], 'Fwd': p['fwd_p_gain']})\n",
    "            if not np.isnan(p['full_b_gain']):\n",
    "                rows.append({'Metric': f'Benchmark ({benchmark_ticker}) Gain', 'Full': p['full_b_gain'], 'Calc': p['calc_b_gain'], 'Fwd': p['fwd_b_gain']})\n",
    "                rows.append({'Metric': 'Gain Delta (vs Bm)', 'Full': p['full_p_gain'] - p['full_b_gain'], 'Calc': p['calc_p_gain'] - p['calc_b_gain'], 'Fwd': p['fwd_p_gain'] - p['fwd_b_gain']})\n",
    "            rows.append({'Metric': 'Group Portfolio Sharpe', 'Full': p['full_p_sharpe'], 'Calc': p['calc_p_sharpe'], 'Fwd': p['fwd_p_sharpe']})\n",
    "            if not np.isnan(p['full_b_sharpe']):\n",
    "                rows.append({'Metric': f'Benchmark ({benchmark_ticker}) Sharpe', 'Full': p['full_b_sharpe'], 'Calc': p['calc_b_sharpe'], 'Fwd': p['fwd_b_sharpe']})\n",
    "                rows.append({'Metric': 'Sharpe Delta (vs Bm)', 'Full': p['full_p_sharpe'] - p['full_b_sharpe'], 'Calc': p['calc_p_sharpe'] - p['calc_b_sharpe'], 'Fwd': p['fwd_p_sharpe'] - p['fwd_b_sharpe']})\n",
    "            report_df = pd.DataFrame(rows).set_index('Metric')\n",
    "            gain_rows = [row for row in report_df.index if 'Gain' in row or 'Delta' in row]\n",
    "            sharpe_rows = [row for row in report_df.index if 'Sharpe' in row]\n",
    "            styled_df = report_df.style.format('{:+.2%}', na_rep='N/A', subset=(gain_rows, report_df.columns)).format('{:+.2f}', na_rep='N/A', subset=(sharpe_rows, report_df.columns)).set_properties(**{'text-align': 'right', 'width': '100px'}).set_table_styles([{'selector': 'th.col_heading', 'props': [('text-align', 'right')]}, {'selector': 'th.row_heading', 'props': [('text-align', 'left')]}])\n",
    "            print(\"\\n--- Strategy Performance Summary ---\")\n",
    "            display(styled_df)\n",
    "            \n",
    "    # --- Final Layout & Display ---\n",
    "    fig.update_layout(title_text='Walk-Forward Performance Analysis', xaxis_title='Date', yaxis_title='Normalized Price (Start = 1)', hovermode='x unified', legend_title_text='Tickers (Ranked)', height=600, margin=dict(t=50))\n",
    "    fig.add_hline(y=1, line_width=1, line_dash=\"dash\", line_color=\"grey\")\n",
    "    update_button.on_click(update_plot)\n",
    "    controls_row1 = widgets.HBox([start_date_picker, calc_period_input, fwd_period_input])\n",
    "    controls_row2 = widgets.HBox([metric_dropdown, rank_start_input, rank_end_input, benchmark_ticker_input, update_button])\n",
    "    ui_container = widgets.VBox([controls_row1, controls_row2, ticker_list_output], layout=widgets.Layout(margin='10px 0 20px 0'))\n",
    "    display(ui_container, fig)\n",
    "    update_plot(None) # Initial run\n",
    "    return (results_container, debug_data_container)\n",
    "\n",
    "def run_full_backtest(df_ohlcv, strategy_params, quality_thresholds):\n",
    "    \"\"\"Runs a full backtest of a strategy over a specified date range.\"\"\"\n",
    "    print(f\"--- Running Full Forensic Backtest for Strategy: {strategy_params['metric']} (Top {strategy_params['rank_start']}-{strategy_params['rank_end']}) ---\")\n",
    "    \n",
    "    # 1. Unpack strategy parameters\n",
    "    start_date, end_date = pd.to_datetime(strategy_params['start_date']), pd.to_datetime(strategy_params['end_date'])\n",
    "    calc_period, fwd_period = strategy_params['calc_period'], strategy_params['fwd_period']\n",
    "    metric, rank_start, rank_end = strategy_params['metric'], strategy_params['rank_start'], strategy_params['rank_end']\n",
    "    benchmark_ticker = strategy_params['benchmark_ticker']\n",
    "    master_calendar_ticker = strategy_params.get('master_calendar_ticker', 'VOO')\n",
    "    \n",
    "    # 2. Perform initial setup (same as analyzer)\n",
    "    if master_calendar_ticker not in df_ohlcv.index.get_level_values(0):\n",
    "        raise ValueError(f\"Master calendar ticker '{master_calendar_ticker}' not found in DataFrame.\")\n",
    "    master_trading_days = df_ohlcv.loc[master_calendar_ticker].index.unique().sort_values()\n",
    "    \n",
    "    start_idx = master_trading_days.searchsorted(start_date)\n",
    "    end_idx = master_trading_days.searchsorted(end_date, side='right')\n",
    "    \n",
    "    quality_metrics_df = calculate_rolling_quality_metrics(df_ohlcv, window=252)\n",
    "    df_close_full = df_ohlcv['Adj Close'].unstack(level=0); df_high_full = df_ohlcv['Adj High'].unstack(level=0); df_low_full = df_ohlcv['Adj Low'].unstack(level=0)\n",
    "    \n",
    "    # 3. Loop through all periods in the backtest range\n",
    "    step_indices = range(start_idx, end_idx, fwd_period)\n",
    "    all_fwd_gains, period_by_period_debug = [], {}\n",
    "\n",
    "    print(f\"Simulating {len(step_indices)} periods from {master_trading_days[step_indices[0]].date()} to {master_trading_days[step_indices[-1]].date()}...\")\n",
    "    for current_idx in tqdm(step_indices, desc=\"Backtest Progress\"):\n",
    "        step_date = master_trading_days[current_idx]\n",
    "        \n",
    "        # Apply data quality filter for the current step\n",
    "        eligible_tickers = get_eligible_universe(quality_metrics_df, step_date, quality_thresholds)\n",
    "        if not eligible_tickers: continue\n",
    "        \n",
    "        df_close_step = df_close_full[eligible_tickers]; df_high_step = df_high_full[eligible_tickers]; df_low_step = df_low_full[eligible_tickers]\n",
    "        \n",
    "        # Run a single walk-forward analysis step\n",
    "        results, debug_output = run_walk_forward_step(\n",
    "            df_close_step, df_high_step, df_low_step, master_trading_days,\n",
    "            step_date, calc_period, fwd_period,\n",
    "            metric, rank_start, rank_end, benchmark_ticker, debug=True\n",
    "        )\n",
    "        \n",
    "        # Collect results for this period\n",
    "        if results['error'] is None:\n",
    "            fwd_series = results['portfolio_series'].loc[results['actual_calc_end_ts']:]\n",
    "            all_fwd_gains.append(fwd_series.pct_change().dropna())\n",
    "            period_by_period_debug[step_date.date().isoformat()] = debug_output\n",
    "            \n",
    "    if not all_fwd_gains:\n",
    "        print(\"Error: No valid periods were simulated.\"); return None\n",
    "\n",
    "    # 4. Stitch together the results to form a continuous equity curve\n",
    "    strategy_returns = pd.concat(all_fwd_gains); strategy_equity_curve = (1 + strategy_returns).cumprod()\n",
    "    benchmark_returns = df_close_full[benchmark_ticker].pct_change().loc[strategy_equity_curve.index]; benchmark_equity_curve = (1 + benchmark_returns).cumprod()\n",
    "    cumulative_equity_df = pd.DataFrame({'Strategy_Equity': strategy_equity_curve, 'Benchmark_Equity': benchmark_equity_curve})\n",
    "    \n",
    "    # 5. Plot the final equity curve\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=cumulative_equity_df.index, y=cumulative_equity_df['Strategy_Equity'], name='Strategy', line=dict(color='green')))\n",
    "    fig.add_trace(go.Scatter(x=cumulative_equity_df.index, y=cumulative_equity_df['Benchmark_Equity'], name=f'Benchmark ({benchmark_ticker})', line=dict(color='black', dash='dash')))\n",
    "    fig.update_layout(title=f\"Cumulative Performance: '{metric}' Strategy (Top {rank_start}-{rank_end})\", xaxis_title=\"Date\", yaxis_title=\"Cumulative Growth\")\n",
    "    fig.show()\n",
    "\n",
    "    # 6. Return the detailed results for forensic analysis\n",
    "    final_backtest_results = {'cumulative_performance': cumulative_equity_df, 'period_by_period_debug': period_by_period_debug}\n",
    "    print(\"\\n‚úÖ Full backtest complete. Results object is ready for forensic analysis.\")\n",
    "    return final_backtest_results\n",
    "\n",
    "# --- E. VERIFICATION TOOLS (User Requested) ---\n",
    "\n",
    "def verify_group_tickers_walk_forward_calculation(df_ohlcv, tickers_to_verify, benchmark_ticker,\n",
    "                                                  start_date, calc_period, fwd_period,\n",
    "                                                  master_calendar_ticker='VOO', export_csv=False):\n",
    "    \"\"\"Verifies portfolio and benchmark performance and optionally exports the data.\"\"\"\n",
    "    display(Markdown(f\"## Verification Report for Portfolio vs. Benchmark\"))\n",
    "    display(Markdown(f\"**Portfolio Tickers:** `{tickers_to_verify}`\\n**Benchmark Ticker:** `{benchmark_ticker}`\"))\n",
    "    \n",
    "    # 1. Setup trading day calendar and determine exact period dates\n",
    "    if master_calendar_ticker not in df_ohlcv.index.get_level_values(0):\n",
    "        raise ValueError(f\"Master calendar ticker '{master_calendar_ticker}' not found in DataFrame.\")\n",
    "    master_trading_days = df_ohlcv.loc[master_calendar_ticker].index.unique().sort_values()\n",
    "\n",
    "    start_date_raw = pd.to_datetime(start_date)\n",
    "    start_idx = master_trading_days.searchsorted(start_date_raw)\n",
    "    if start_idx >= len(master_trading_days):\n",
    "        print(f\"Error: Start date {start_date_raw.date()} is after the last available trading day.\"); return\n",
    "    actual_start_date = master_trading_days[start_idx]\n",
    "    \n",
    "    calc_end_idx = min(start_idx + calc_period, len(master_trading_days) - 1)\n",
    "    fwd_end_idx = min(calc_end_idx + fwd_period, len(master_trading_days) - 1)\n",
    "    \n",
    "    actual_calc_end_date = master_trading_days[calc_end_idx]\n",
    "    actual_fwd_end_date = master_trading_days[fwd_end_idx]\n",
    "    \n",
    "    display(Markdown(f\"**Analysis Start:** `{actual_start_date.date()}` (Selected: `{start_date_raw.date()}`)\\n\"\n",
    "                    f\"**Calc End:** `{actual_calc_end_date.date()}` ({calc_period} trading days)\\n\"\n",
    "                    f\"**Fwd End:** `{actual_fwd_end_date.date()}` ({fwd_period} trading days)\"))\n",
    "\n",
    "    # 2. Recreate the portfolio and benchmark series from scratch\n",
    "    df_close_full = df_ohlcv['Adj Close'].unstack(level=0)\n",
    "    portfolio_prices_raw_slice = df_close_full[tickers_to_verify].loc[actual_start_date:actual_fwd_end_date]\n",
    "    portfolio_value_series = portfolio_prices_raw_slice.div(portfolio_prices_raw_slice.bfill().iloc[0]).mean(axis=1)\n",
    "    benchmark_price_series = df_close_full.get(benchmark_ticker)\n",
    "    \n",
    "    # 3. Optionally export the underlying daily data to a CSV for external checking\n",
    "    if export_csv:\n",
    "        export_df = pd.DataFrame({\n",
    "            'Portfolio_Normalized_Price': portfolio_value_series,\n",
    "            'Portfolio_Daily_Return': portfolio_value_series.pct_change()\n",
    "        })\n",
    "        if benchmark_price_series is not None:\n",
    "            norm_bm = benchmark_price_series.loc[actual_start_date:actual_fwd_end_date]\n",
    "            norm_bm = norm_bm / norm_bm.bfill().iloc[0]\n",
    "            export_df['Benchmark_Normalized_Price'] = norm_bm\n",
    "            export_df['Benchmark_Daily_Return'] = norm_bm.pct_change()\n",
    "\n",
    "        output_dir = 'export_csv'\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        tickers_str = '_'.join(tickers_to_verify)\n",
    "        filename = f\"verify_group_{actual_start_date.date()}_{tickers_str}.csv\"\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "        export_df.to_csv(filepath)\n",
    "        print(f\"\\n‚úÖ Data exported to: {filepath}\")\n",
    "\n",
    "    # 4. Define a helper to print detailed calculation steps\n",
    "    def print_verification_steps(title, price_series):\n",
    "        display(Markdown(f\"#### Verification for: `{title}`\"))\n",
    "        if price_series.dropna().shape[0] < 2: print(\"  - Not enough data points.\"); return {'gain': np.nan, 'sharpe': np.nan}\n",
    "        start_price, end_price = price_series.bfill().iloc[0], price_series.ffill().iloc[-1]\n",
    "        gain = (end_price / start_price) - 1\n",
    "        print(f\"Start Value ({price_series.first_valid_index().date()}): {start_price:,.4f}\\nEnd Value   ({price_series.last_valid_index().date()}): {end_price:,.4f}\\nGain = {gain:.2%}\")\n",
    "        returns = price_series.pct_change()\n",
    "        sharpe = calculate_sharpe(returns)\n",
    "        print(f\"Mean Daily Return: {returns.mean():.6f}\\nStd Dev: {returns.std():.6f}\\nSharpe = {sharpe:.2f}\")\n",
    "        return {'gain': gain, 'sharpe': sharpe}\n",
    "\n",
    "    # 5. Run verification for each period\n",
    "    display(Markdown(\"### A. Calculation Period\"))\n",
    "    perf_calc_p = print_verification_steps(\"Group Portfolio\", portfolio_value_series.loc[actual_start_date:actual_calc_end_date])\n",
    "    if benchmark_price_series is not None:\n",
    "        perf_calc_b = print_verification_steps(f\"Benchmark\", benchmark_price_series.loc[actual_start_date:actual_calc_end_date])\n",
    "    \n",
    "    display(Markdown(\"### B. Forward Period\"))\n",
    "    perf_fwd_p = print_verification_steps(\"Group Portfolio\", portfolio_value_series.loc[actual_calc_end_date:actual_fwd_end_date])\n",
    "    if benchmark_price_series is not None:\n",
    "        perf_fwd_b = print_verification_steps(f\"Benchmark\", benchmark_price_series.loc[actual_calc_end_date:actual_fwd_end_date])\n",
    "\n",
    "def verify_ticker_ranking_metrics(df_ohlcv, ticker, start_date, calc_period,\n",
    "                                  master_calendar_ticker='VOO', export_csv=False):\n",
    "    \"\"\"Verifies ranking metrics for a single ticker and optionally exports the data.\"\"\"\n",
    "    display(Markdown(f\"## Verification Report for Ticker Ranking: `{ticker}`\"))\n",
    "    \n",
    "    # 1. Setup trading day calendar and determine exact period dates\n",
    "    if master_calendar_ticker not in df_ohlcv.index.get_level_values(0):\n",
    "        raise ValueError(f\"Master calendar ticker '{master_calendar_ticker}' not found in DataFrame.\")\n",
    "    master_trading_days = df_ohlcv.loc[master_calendar_ticker].index.unique().sort_values()\n",
    "\n",
    "    start_date_raw = pd.to_datetime(start_date)\n",
    "    start_idx = master_trading_days.searchsorted(start_date_raw)\n",
    "    if start_idx >= len(master_trading_days):\n",
    "        print(f\"Error: Start date {start_date_raw.date()} is after the last available trading day.\"); return\n",
    "    actual_start_date = master_trading_days[start_idx]\n",
    "    \n",
    "    calc_end_idx = min(start_idx + calc_period, len(master_trading_days) - 1)\n",
    "    actual_calc_end_date = master_trading_days[calc_end_idx]\n",
    "\n",
    "    # 2. Extract and prepare the raw data for the specific ticker and period\n",
    "    df_ticker = df_ohlcv.loc[ticker].sort_index()\n",
    "    calc_df = df_ticker.loc[actual_start_date:actual_calc_end_date].copy()\n",
    "    if calc_df.empty or len(calc_df) < 2: \n",
    "        print(\"No data or not enough data in calc period.\"); return\n",
    "\n",
    "    display(Markdown(\"### A. Calculation Period (for Ranking Metrics)\"))\n",
    "    display(Markdown(f\"**Period Start:** `{actual_start_date.date()}`\\n\"\n",
    "                    f\"**Period End:** `{actual_calc_end_date.date()}`\\n\"\n",
    "                    f\"**Total Trading Days:** `{len(calc_df)}` (Requested: `{calc_period}`)\"))\n",
    "    \n",
    "    display(Markdown(\"#### Detailed Metric Calculation Data\"))\n",
    "    \n",
    "    # 3. Calculate all intermediate metrics as new columns for full transparency\n",
    "    vdf = calc_df[['Adj High', 'Adj Low', 'Adj Close']].copy()\n",
    "    vdf['Daily_Return'] = vdf['Adj Close'].pct_change()\n",
    "    \n",
    "    # Corrected True Range (TR) calculation for a single ticker (Series)\n",
    "    tr_df = pd.DataFrame({\n",
    "        'h_l': vdf['Adj High'] - vdf['Adj Low'],\n",
    "        'h_cp': abs(vdf['Adj High'] - vdf['Adj Close'].shift(1)),\n",
    "        'l_cp': abs(vdf['Adj Low'] - vdf['Adj Close'].shift(1))\n",
    "    })\n",
    "    vdf['TR'] = tr_df.max(axis=1)\n",
    "    \n",
    "    vdf['ATR_14'] = vdf['TR'].ewm(alpha=1/14, adjust=False).mean()\n",
    "    vdf['ATRP'] = vdf['ATR_14'] / vdf['Adj Close']\n",
    "    \n",
    "    print(\"--- Start of Calculation Period ---\")\n",
    "    display(vdf.head())\n",
    "    print(\"\\n--- End of Calculation Period ---\")\n",
    "    display(vdf.tail())\n",
    "\n",
    "    # 4. Optionally export this detailed breakdown to CSV\n",
    "    if export_csv:\n",
    "        output_dir = 'export_csv'\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        filename = f\"verify_ticker_{actual_start_date.date()}_{ticker}.csv\"\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "        vdf.to_csv(filepath)\n",
    "        print(f\"\\n‚úÖ Data exported to: {filepath}\")\n",
    "    \n",
    "    # 5. Print final metric calculations with formulas\n",
    "    display(Markdown(\"#### `MetricValue` Verification Summary:\"))\n",
    "    \n",
    "    calc_start_price = vdf['Adj Close'].bfill().iloc[0]\n",
    "    calc_end_price = vdf['Adj Close'].ffill().iloc[-1]\n",
    "    price_metric = (calc_end_price / calc_start_price)\n",
    "    print(f\"1. Price Metric: (Last Price / First Price) = ({calc_end_price:.2f} / {calc_start_price:.2f}) = {price_metric:.4f}\")\n",
    "    \n",
    "    daily_returns = vdf['Daily_Return'].dropna()\n",
    "    sharpe_ratio = calculate_sharpe(daily_returns)\n",
    "    print(f\"2. Sharpe Metric: (Mean Daily Return / Std Dev) * sqrt(252) = {sharpe_ratio:.4f}\")\n",
    "\n",
    "    atrp_mean = vdf['ATRP'].mean()\n",
    "    mean_daily_return = vdf['Daily_Return'].mean()\n",
    "    sharpe_atr = (mean_daily_return / atrp_mean) if atrp_mean > 0 else 0\n",
    "    print(f\"3. Sharpe (ATR) Metric: (Mean Daily Return / Mean ATRP) = ({mean_daily_return:.6f} / {atrp_mean:.6f}) = {sharpe_atr:.4f}\")\n",
    "\n",
    "# --- F. AUTOMATION SCRIPT - STRATEGY SEARCH ---\n",
    "\n",
    "def run_strategy_search(df_ohlcv, config):\n",
    "    \"\"\"\n",
    "    Runs the main backtesting loop with checkpointing to be resumable.\n",
    "    \"\"\"\n",
    "    start_time = time.time() # <-- This now works because of 'import time'\n",
    "    \n",
    "    # --- 1. SETUP & LOAD PROGRESS ---\n",
    "    print(\"--- Phase 1: Pre-processing and Loading Progress ---\")\n",
    "    quality_metrics_df = calculate_rolling_quality_metrics(df_ohlcv, window=252)\n",
    "    print(\"Unstacking data for performance...\")\n",
    "    df_close_full = df_ohlcv['Adj Close'].unstack(level=0)\n",
    "    df_high_full = df_ohlcv['Adj High'].unstack(level=0)\n",
    "    df_low_full = df_ohlcv['Adj Low'].unstack(level=0)\n",
    "    \n",
    "    master_calendar_ticker = config['master_calendar_ticker']\n",
    "    master_trading_days = df_ohlcv.loc[master_calendar_ticker].index.unique().sort_values()\n",
    "    print(f\"Master trading day calendar created from '{master_calendar_ticker}' ({len(master_trading_days)} days).\")\n",
    "\n",
    "    results_path = config['results_output_path']\n",
    "    completed_params = set()\n",
    "    \n",
    "    if os.path.exists(results_path): # <-- This now works because of 'import os'\n",
    "        print(f\"Found existing results file. Loading progress from: {results_path}\")\n",
    "        df_progress = pd.read_csv(results_path)\n",
    "        for _, row in df_progress.iterrows():\n",
    "            param_key = (\n",
    "                row['calc_period'], row['fwd_period'], row['metric'],\n",
    "                (row['rank_start'], row['rank_end'])\n",
    "            )\n",
    "            completed_params.add(param_key)\n",
    "        print(f\"Found {len(completed_params)} completed parameter sets to skip.\")\n",
    "    else:\n",
    "        print(\"No existing results file found. Starting a new run.\")\n",
    "\n",
    "    print(\"‚úÖ Pre-processing complete.\\n\")\n",
    "\n",
    "    # --- 2. SETUP THE MAIN LOOP ---\n",
    "    print(\"--- Phase 2: Setting up Simulation Loops ---\")\n",
    "    \n",
    "    param_combinations = list(product(\n",
    "        config['calc_periods'], config['fwd_periods'],\n",
    "        config['metrics'], config['rank_slices']\n",
    "    ))\n",
    "    \n",
    "    search_start_date = pd.to_datetime(config['search_start_date'])\n",
    "    search_end_date = pd.to_datetime(config['search_end_date'])\n",
    "    start_idx = master_trading_days.searchsorted(search_start_date, side='left')\n",
    "    end_idx = master_trading_days.searchsorted(search_end_date, side='right')\n",
    "\n",
    "    step_dates_map = {}\n",
    "    print(\"Pre-calculating rebalancing schedules for each holding period...\")\n",
    "    for fwd_period in sorted(config['fwd_periods']):\n",
    "        step_indices = range(start_idx, end_idx, fwd_period)\n",
    "        step_dates_map[fwd_period] = master_trading_days[step_indices]\n",
    "        print(f\"  - Holding Period {fwd_period} days: {len(step_dates_map[fwd_period])} rebalances\")\n",
    "    \n",
    "    print(f\"Found {len(param_combinations)} total parameter sets to simulate.\")\n",
    "    print(\"‚úÖ Setup complete. Starting main loop...\\n\")\n",
    "\n",
    "    # --- 3. RUN THE MAIN LOOP ---\n",
    "    print(\"--- Phase 3: Running Simulations ---\")\n",
    "    pbar = tqdm(param_combinations, desc=\"Parameter Sets\")\n",
    "    \n",
    "    for params in pbar:\n",
    "        calc_period, fwd_period, metric, rank_slice = params\n",
    "        rank_start, rank_end = rank_slice\n",
    "        \n",
    "        param_key = (calc_period, fwd_period, metric, rank_slice)\n",
    "        if param_key in completed_params:\n",
    "            pbar.set_description(f\"Skipping {param_key}\")\n",
    "            continue\n",
    "\n",
    "        pbar.set_description(f\"Running {param_key}\")\n",
    "        \n",
    "        current_params_results = []\n",
    "        \n",
    "        # ==============================================================================\n",
    "        # --- FIX: RESTORED THE MISSING INNER LOOP ---\n",
    "        # ==============================================================================\n",
    "        current_step_dates = step_dates_map[fwd_period]\n",
    "        for step_date in current_step_dates:\n",
    "            eligible_tickers = get_eligible_universe(\n",
    "                quality_metrics_df, filter_date=step_date, thresholds=config['quality_thresholds']\n",
    "            )\n",
    "            if not eligible_tickers: continue\n",
    "            \n",
    "            df_close_step = df_close_full[eligible_tickers]\n",
    "            df_high_step = df_high_full[eligible_tickers]\n",
    "            df_low_step = df_low_full[eligible_tickers]\n",
    "\n",
    "            step_result, _ = run_walk_forward_step(\n",
    "                df_close_full=df_close_step, df_high_full=df_high_step, df_low_full=df_low_step,\n",
    "                master_trading_days=master_trading_days, start_date=step_date,\n",
    "                calc_period=calc_period, fwd_period=fwd_period,\n",
    "                metric=metric, rank_start=rank_start, rank_end=rank_end,\n",
    "                benchmark_ticker=config['benchmark_ticker'], debug=False\n",
    "            )\n",
    "            \n",
    "            if step_result['error'] is None:\n",
    "                p = step_result['performance_data']\n",
    "                log_entry = {\n",
    "                    'step_date': step_date.date(), 'calc_period': calc_period,\n",
    "                    'fwd_period': fwd_period, 'metric': metric,\n",
    "                    'rank_start': rank_start, 'rank_end': rank_end,\n",
    "                    'num_universe': len(eligible_tickers),\n",
    "                    'num_portfolio': len(step_result['tickers_to_display']),\n",
    "                    'fwd_p_gain': p['fwd_p_gain'], 'fwd_b_gain': p['fwd_b_gain'],\n",
    "                    'fwd_gain_delta': p['fwd_p_gain'] - p['fwd_b_gain'] if not np.isnan(p['fwd_b_gain']) else np.nan,\n",
    "                    'fwd_p_sharpe': p['fwd_p_sharpe'],\n",
    "                }\n",
    "                current_params_results.append(log_entry)\n",
    "        # ==============================================================================\n",
    "        \n",
    "        # --- CHECKPOINTING: INCREMENTAL SAVE ---\n",
    "        if current_params_results:\n",
    "            df_to_append = pd.DataFrame(current_params_results)\n",
    "            df_to_append.to_csv(\n",
    "                results_path,\n",
    "                mode='a',\n",
    "                header=not os.path.exists(results_path),\n",
    "                index=False\n",
    "            )\n",
    "            completed_params.add(param_key)\n",
    "\n",
    "    print(\"‚úÖ Main loop finished.\\n\")\n",
    "    \n",
    "    # --- 4. RETURN FINAL DATAFRAME ---\n",
    "    print(\"--- Phase 4: Loading Final Results ---\")\n",
    "    if os.path.exists(results_path):\n",
    "        final_df = pd.read_csv(results_path)\n",
    "        end_time = time.time()\n",
    "        print(f\"‚úÖ Process complete. Total execution time: {time.time() - start_time:.2f} seconds.\")\n",
    "        return final_df\n",
    "    else:\n",
    "        print(\"Warning: No results were generated.\")\n",
    "        return None\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47ed713",
   "metadata": {},
   "source": [
    "#### **CELL 3: DATA LOADING**\n",
    "*This cell loads your main dataset using the environment-aware path.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97e54a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# --- CELL 3: DATA LOADING ---\n",
    "# ==============================================================================\n",
    "import pandas as pd\n",
    "\n",
    "data_file_path = env_config['data_path']\n",
    "print(f\"Attempting to load data from: {data_file_path}\")\n",
    "\n",
    "try:\n",
    "    df_OHLCV = pd.read_parquet(data_file_path, engine='pyarrow')\n",
    "    df_dev = df_OHLCV.copy() # Use df_dev for development as a good practice\n",
    "    \n",
    "    print(\"\\n‚úÖ Data loaded successfully.\")\n",
    "    print(\"\\n--- DataFrame Info ---\")\n",
    "    df_dev.info()\n",
    "except FileNotFoundError:\n",
    "    print(f\"\\n‚ùå ERROR: FILE NOT FOUND at {data_file_path}. Please check paths in Cell 1.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b16e2e",
   "metadata": {},
   "source": [
    "#### **CELL 4: BOT STRATEGY CONFIGURATION**\n",
    "*This cell defines the strategy parameters you want to test.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549adf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# --- CELL 4: BOT STRATEGY CONFIGURATION ---\n",
    "# ==============================================================================\n",
    "from itertools import product\n",
    "import pandas as pd\n",
    "\n",
    "# --- PRIMARY USER INPUTS FOR THE STRATEGY ---\n",
    "# 5,  21, 42, 63, 126, 252\n",
    "# 1W, 1M, 2M, 3M,  6M,  1Y\n",
    "HOLDING_PERIODS_DAYS = [63]        # Test ~2, and 3 month holding periods\n",
    "CALC_PERIODS_DAYS = [252]         # Use ~6 and 12 month lookbacks\n",
    "\n",
    "bot_config = {\n",
    "    # --- Time Parameters ---\n",
    "    'search_start_date': '2014-01-01',\n",
    "    'search_end_date': '2018-12-31',\n",
    "    \n",
    "    # --- Strategy Parameters (The Search Grid) ---\n",
    "    'calc_periods': CALC_PERIODS_DAYS,\n",
    "    'fwd_periods': HOLDING_PERIODS_DAYS,\n",
    "\n",
    "\n",
    "    # 'metrics': ['Sharpe', 'Sharpe (ATR)'],\n",
    "    'metrics': ['Price', 'Sharpe', 'Sharpe (ATR)'],    \n",
    "        \n",
    "    'rank_slices': [(1, 5)],\n",
    "\n",
    "    # --- Data Quality ---\n",
    "    'quality_thresholds': { 'min_median_dollar_volume': 10_000_000, \n",
    "                            'max_stale_pct': 0.05, \n",
    "                            'max_same_vol_count': 1 },\n",
    "\n",
    "    # --- General Parameters ---\n",
    "    'benchmark_ticker': 'VOO',\n",
    "    'master_calendar_ticker': 'VOO',\n",
    "    'results_output_path': env_config['results_path']\n",
    "}\n",
    "\n",
    "print(\"\\n--- Bot Configuration Initialized ---\")\n",
    "print(f\"Calculation Periods to Test: {bot_config['calc_periods']} trading days\")\n",
    "print(f\"Forward and Holding Periods to Test (Forward and Holding Periods are the same): {bot_config['fwd_periods']} trading days\")\n",
    "print(f\"Results will be saved to: {bot_config['results_output_path']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df014e8e",
   "metadata": {},
   "source": [
    "#### **CELL 5: EXECUTION**\n",
    "*This is the final cell that runs the backtest and displays the results.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7485bf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# --- CELL 5: EXECUTION ---\n",
    "# ==============================================================================\n",
    "\n",
    "# --- Execute the Bot ---\n",
    "dev_results_df = run_strategy_search(df_dev, bot_config)\n",
    "\n",
    "# --- Display a sample of the results ---\n",
    "if dev_results_df is not None:\n",
    "    print(\"\\n--- Sample of Generated Results ---\")\n",
    "    display(dev_results_df.head())\n",
    "    print(\"\\n--- Analysis of Best Performing Strategies ---\")\n",
    "    display(dev_results_df.groupby(['calc_period', 'fwd_period', 'metric', 'rank_start', 'rank_end'])['fwd_gain_delta'].mean().sort_values(ascending=False).to_frame())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3dc0a12",
   "metadata": {},
   "source": [
    "### 4. Next Steps & Future Improvements\n",
    "\n",
    "This system is a powerful foundation. Here are potential areas for future development:\n",
    "1.  **Advanced Performance Analytics:** Create a new notebook or function to analyze the output CSV, calculating metrics like Max Drawdown, Calmar Ratio, and generating equity curves for the best strategies.\n",
    "2.  **Visualization:** Build heatmaps and other plots to visualize how different parameters (e.g., `calc_period` vs. `fwd_period`) affect performance.\n",
    "3.  **Realism:** Incorporate transaction costs and slippage into the performance calculations for a more realistic backtest.\n",
    "4.  **Configuration Management:** For even more complex tests, move the `bot_config` dictionary into a separate `config.py` file to keep the notebook cleaner.\n",
    "\n",
    "It has been a genuine pleasure working with you on this. You've built an impressive and professional-grade tool. I wish you the very best with your continued research and development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05feecc8",
   "metadata": {},
   "source": [
    "### 4. Plot an export_csv Row to Check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5979f82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_to_check = 59\n",
    "row_values =  dev_results_df.loc[row_to_check ].to_dict()\n",
    "print(f'export_csv values for row {row_to_check}:\\n')\n",
    "for k, v in row_values.items():\n",
    "    print(f'{k:<15}: {v}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e2012e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_container, debug_container = plot_walk_forward_analyzer(\n",
    "    df_ohlcv=df_dev,\n",
    "    # df_ohlcv=df_OHLCV,    \n",
    "    default_start_date=row_values['step_date'],\n",
    "    \n",
    "    default_calc_period=row_values['calc_period'],\n",
    "    default_fwd_period=row_values['fwd_period'],\n",
    "    # default_calc_period=120,\n",
    "    # default_fwd_period=30,\n",
    "\n",
    "    default_metric=row_values['metric'],\n",
    "\n",
    "    default_rank_start=row_values['rank_start'],\n",
    "    default_rank_end=row_values['rank_end'],\n",
    "    # default_rank_start=2,\n",
    "    # default_rank_end=3,    \n",
    "\n",
    "    default_benchmark_ticker='VOO',\n",
    "    quality_thresholds=bot_config['quality_thresholds'],\n",
    "    debug=True  # <-- Activate the new mode!\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44da0593",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_nested(results_container)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa15885",
   "metadata": {},
   "source": [
    "# CHECK Metric_Sharpe (ATR) for Calc. Period has error.\n",
    "- start date 2018-10-03\n",
    "- Calc Period 252\n",
    "- Fwd Period 63\n",
    "- Metric Sharpe ATR\n",
    "- Rank Start 1\n",
    "- Rank End 5\n",
    "-- Analysis Period 2018-10-03 to 2020-01-06 (this is full period, calc + fwd)\n",
    "-- [BIL, MINT, SHV, BNDX, VCSH]\n",
    "-- Metric_Sharpe (ATR) for BIL, MINT, SHV, BNDX matched my own calculation at bottom cell\n",
    "-- Metric Sharpe (ATR) for VCSH is a bit off (code calc 0.194195, my calc 0.196112)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed494d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_nested(debug_container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91056576",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = results_container[0]\n",
    "\n",
    "print_nested(results_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6fa808",
   "metadata": {},
   "source": [
    "### Get Plot Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2201125e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_tickers = results_dict['tickers_to_display']\n",
    "_calc_start = results_dict['calc_period_start']\n",
    "_calc_end = results_dict['calc_period_end']\n",
    "_fwd_start = results_dict['forward_period_start']\n",
    "_fwd_end = results_dict['forward_period_end']\n",
    "_calc_period = results_dict['calc_period']\n",
    "_fwd_period = results_dict['fwd_period']\n",
    "_benchmark_ticker = results_dict['benchmark_ticker']\n",
    "\n",
    "print(f'_tickers: {_tickers}')\n",
    "print(f'_calc_start: {_calc_start}')\n",
    "print(f'_calc_end: {_calc_end}')\n",
    "print(f'_fwd_start: {_fwd_start}')\n",
    "print(f'_fwd_end: {_fwd_end}')\n",
    "print(f'_calc_period: {_calc_period}')\n",
    "print(f'_fwd_period: {_fwd_period}')\n",
    "print(f'_benchmark: {_benchmark_ticker}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe967a4",
   "metadata": {},
   "source": [
    "### Run verify_ticker_ranking_metrics with Plot Parameters to Check Calc. Period Calculation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf24d825",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ticker in _tickers:\n",
    "    verify_ticker_ranking_metrics(df_OHLCV, \n",
    "                                  ticker=_ticker, \n",
    "                                  start_date=_calc_start, \n",
    "                                  calc_period=_calc_period,\n",
    "                                  master_calendar_ticker=_benchmark_ticker, \n",
    "                                  export_csv=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e6ca72",
   "metadata": {},
   "source": [
    "### My Own Check on Calculation for One Ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9798de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Check Calculation for a Ticker -- #\n",
    "_check_ticker = _tickers[4]\n",
    "print(f'Check calculation for this ticker: {_check_ticker}')\n",
    "\n",
    "# Get Ticker's OHLCV Data -- # \n",
    "_df = df_OHLCV.loc[_check_ticker][_calc_start:_fwd_end]\n",
    "\n",
    "# -- Calculate Daily Return -- #\n",
    "_df['Daily_Return'] = _df['Adj Close'].pct_change()\n",
    "\n",
    "# -- Calculate True Range -- #\n",
    "_df['TR'] = pd.concat([\n",
    "    _df['Adj High'] - _df['Adj Low'],\n",
    "    (_df['Adj High'] - _df['Adj Close'].shift(1)).abs(),\n",
    "    (_df['Adj Low']  - _df['Adj Close'].shift(1)).abs()\n",
    "], axis=1).max(axis=1)\n",
    "\n",
    "# -- Calculate Average True Range (14 day period) -- #\n",
    "window = 14\n",
    "_df['ATR_14'] = pd.NA\n",
    "\n",
    "# Seed the very first ATR value with the first non-NaN TR\n",
    "first_idx = _df['TR'].first_valid_index()\n",
    "_df.loc[first_idx, 'ATR_14'] = _df.loc[first_idx, 'TR']\n",
    "\n",
    "# Iteratively apply the Wilder smoothing formula\n",
    "for i in range(_df.index.get_loc(first_idx) + 1, len(_df)):\n",
    "    prev_atr = _df.iloc[i-1]['ATR_14']\n",
    "    curr_tr  = _df.iloc[i]['TR']\n",
    "    _df.iloc[i, _df.columns.get_loc('ATR_14')] = (prev_atr * (window - 1) + curr_tr) / window\n",
    "\n",
    "# -- Calculate ATRP -- #\n",
    "_df['ATRP'] = _df['ATR_14'] / _df['Adj Close']\n",
    "\n",
    "calc_pd_df = _df.loc[_calc_start:_calc_end]\n",
    "fwd_pd_df = _df.loc[_fwd_start:_fwd_end]\n",
    "print(f'Calc. Period:\\n{calc_pd_df.head()}\\n{calc_pd_df.tail()}')\n",
    "print(f'\\nFwd. Period:\\n{fwd_pd_df.head()}\\n{fwd_pd_df.tail()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17089b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Metric Calculation for Ticker: {_check_ticker}')\n",
    "\n",
    "# -- Calculation for Period Gain -- #\n",
    "full_period_gain = _df['Adj Close'][_fwd_end] / _df['Adj Close'][_calc_start]\n",
    "calc_period_gain = _df['Adj Close'][_calc_end] / _df['Adj Close'][_calc_start]\n",
    "fwd_period_gain = _df['Adj Close'][_fwd_end] / _df['Adj Close'][_fwd_start]\n",
    "\n",
    "print(f'\\nfull_period_gain: {full_period_gain:.4f}')\n",
    "print(f'calc_period_gain: {calc_period_gain:.4f}')\n",
    "print(f'fwd_period_gain: {fwd_period_gain:.4f}')\n",
    "\n",
    "# -- Calculation for Period Sharpe -- #\n",
    "full_period_return = _df['Daily_Return'][_calc_start:_fwd_end]\n",
    "calc_period_return = _df['Daily_Return'][_calc_start:_calc_end]\n",
    "fwd_period_return = _df['Daily_Return'][_fwd_start:_fwd_end]\n",
    "\n",
    "full_sharpe = full_period_return.mean() / full_period_return.std() * (252 ** 0.5)\n",
    "calc_sharpe = calc_period_return.mean() / calc_period_return.std() * (252 ** 0.5)\n",
    "fwd_sharpe = fwd_period_return.mean() / fwd_period_return.std() * (252 ** 0.5)\n",
    "\n",
    "print(f'\\nfull_sharpe: {full_sharpe:.4f}')\n",
    "print(f'calc_sharpe: {calc_sharpe:.4f}')\n",
    "print(f'fwd_sharpe: {fwd_sharpe:.4f}')\n",
    "\n",
    "# -- Calculation for Period Sharpe ATR -- #\n",
    "full_sharpe_ATR = full_period_return.mean() / _df['ATRP'][_calc_start:_fwd_end].mean()\n",
    "calc_sharpe_ATR = calc_period_return.mean() / _df['ATRP'][_calc_start:_calc_end].mean()\n",
    "fwd_sharpe_ATR = fwd_period_return.mean() / _df['ATRP'][_fwd_start:_fwd_end].mean()\n",
    "\n",
    "print(f'\\nfull_sharpe_ATR: {full_sharpe_ATR:.4f}')\n",
    "print(f'calc_sharpe_ATR: {calc_sharpe_ATR:.4f}')\n",
    "print(f'fwd_sharpe_ATR: {fwd_sharpe_ATR:.4f}')\n",
    "\n",
    "print(f'\\ncalc_period_return.mean(): {calc_period_return.mean()}')\n",
    "print(f\"_df['ATRP'][_calc_start:_calc_end].mean(): {_df['ATRP'][_calc_start:_calc_end].mean()}\")\n",
    "print(f'calc_sharpe_ATR: {calc_sharpe_ATR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa20ef8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afb0cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_OHLCV.loc['VCSH'].copy()\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9751f576",
   "metadata": {},
   "outputs": [],
   "source": [
    "_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc927f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Check Calculation for a Ticker -- #\n",
    "_check_ticker = _tickers[4]\n",
    "print(f'Check calculation for this ticker: {_check_ticker}')\n",
    "\n",
    "# Get Ticker's OHLCV Data -- # \n",
    "_df = df_OHLCV.loc[_check_ticker]['2018-09-30' : '2020-01-10']\n",
    "# _df = df_OHLCV.loc[_check_ticker].copy()\n",
    "\n",
    "# -- Calculate Daily Return -- #\n",
    "_df['Daily_Return'] = _df['Adj Close'].pct_change()\n",
    "\n",
    "# -- Calculate True Range -- #\n",
    "_df['TR'] = pd.concat([\n",
    "    _df['Adj High'] - _df['Adj Low'],\n",
    "    (_df['Adj High'] - _df['Adj Close'].shift(1)).abs(),\n",
    "    (_df['Adj Low']  - _df['Adj Close'].shift(1)).abs()\n",
    "], axis=1).max(axis=1)\n",
    "\n",
    "# -- Calculate Average True Range (14 day period) -- #\n",
    "window = 14\n",
    "_df['ATR_14'] = pd.NA\n",
    "\n",
    "# Seed the very first ATR value with the first non-NaN TR\n",
    "first_idx = _df['TR'].first_valid_index()\n",
    "_df.loc[first_idx, 'ATR_14'] = _df.loc[first_idx, 'TR']\n",
    "\n",
    "# Iteratively apply the Wilder smoothing formula\n",
    "for i in range(_df.index.get_loc(first_idx) + 1, len(_df)):\n",
    "    prev_atr = _df.iloc[i-1]['ATR_14']\n",
    "    curr_tr  = _df.iloc[i]['TR']\n",
    "    _df.iloc[i, _df.columns.get_loc('ATR_14')] = (prev_atr * (window - 1) + curr_tr) / window\n",
    "\n",
    "# -- Calculate ATRP -- #\n",
    "_df['ATRP'] = _df['ATR_14'] / _df['Adj Close']\n",
    "\n",
    "# calc_pd_df = _df.loc[_calc_start:_calc_end]\n",
    "# fwd_pd_df = _df.loc[_fwd_start:_fwd_end]\n",
    "# print(f'Calc. Period:\\n{calc_pd_df.head()}\\n{calc_pd_df.tail()}')\n",
    "# print(f'\\nFwd. Period:\\n{fwd_pd_df.head()}\\n{fwd_pd_df.tail()}')\n",
    "\n",
    "_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef92146",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_OHLCV.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d72826",
   "metadata": {},
   "source": [
    "### Below Cells Follows the Code to Calculate Sharpe (ATR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7392c626",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ohlcv = df_OHLCV.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689f93e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_to_check = 'VCSH' # <--- CHANGE THIS TO YOUR ACTUAL TICKER\n",
    "start_date_raw = pd.to_datetime('2018-10-03')\n",
    "calc_period_days = 252\n",
    "\n",
    "# We need the master trading day calendar, just like the code uses.\n",
    "# It's essential for getting the dates exactly right.\n",
    "master_calendar_ticker = 'VOO' # Or another reliable ticker like 'SPY'\n",
    "master_trading_days = df_ohlcv.loc[master_calendar_ticker].index.unique().sort_values()\n",
    "\n",
    "# Isolate the full history for the ticker we're checking\n",
    "df_ticker_full = df_ohlcv.loc[ticker_to_check]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa61dd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ticker_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b0aed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the index for our start date\n",
    "start_idx = master_trading_days.searchsorted(start_date_raw)\n",
    "actual_start_date = master_trading_days[start_idx]\n",
    "\n",
    "# Find the index for the end of the calculation period\n",
    "calc_end_idx = min(start_idx + calc_period_days, len(master_trading_days) - 1)\n",
    "actual_calc_end_date = master_trading_days[calc_end_idx]\n",
    "\n",
    "print(f\"Raw Start Date: {start_date_raw.date()}\")\n",
    "print(f\"Actual Start Date (Trading Day): {actual_start_date.date()}\")\n",
    "print(f\"Actual Calc End Date (Trading Day): {actual_calc_end_date.date()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb15d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slice the ticker's data to the exact date range\n",
    "calc_df = df_ticker_full.loc[actual_start_date:actual_calc_end_date].copy()\n",
    "\n",
    "print(f\"Number of rows in calc_df: {len(calc_df)}\")\n",
    "print(\"--- First 3 rows of our calculation data ---\")\n",
    "display(calc_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f1eb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_df['Daily_Return'] = calc_df['Adj Close'].pct_change()\n",
    "mean_daily_return = calc_df['Daily_Return'].mean()\n",
    "\n",
    "print(f\"Mean Daily Return: {mean_daily_return:.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede641d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4a. Get the previous day's close for every day in our calc period.\n",
    "# This is done by shifting the FULL history, then slicing.\n",
    "prev_close_series = df_ticker_full['Adj Close'].shift(1).loc[calc_df.index]\n",
    "\n",
    "# 4b. MY HYPOTHESIS: The first value of this series is NaN. Let's check.\n",
    "print(\"--- Previous Day's Close (first 3 days) ---\")\n",
    "print(prev_close_series.head(3))\n",
    "print(\"\\n\")\n",
    "\n",
    "# 4c. Now calculate the three components of TR\n",
    "component1 = calc_df['Adj High'] - calc_df['Adj Low']\n",
    "component2 = abs(calc_df['Adj High'] - prev_close_series)\n",
    "component3 = abs(calc_df['Adj Low'] - prev_close_series)\n",
    "\n",
    "# 4d. Combine them to get the daily TR value\n",
    "tr_df = pd.DataFrame({'c1': component1, 'c2': component2, 'c3': component3})\n",
    "calc_df['TR'] = tr_df.max(axis=1)\n",
    "\n",
    "# # Change TR to High - Low for the row 0\n",
    "# calc_df.loc[calc_df.index[0], 'TR'] = (\n",
    "#     calc_df.loc[calc_df.index[0], 'Adj High'] -\n",
    "#     calc_df.loc[calc_df.index[0], 'Adj Low']\n",
    "# )\n",
    "\n",
    "print(\"--- Final TR values (first 3 days) ---\")\n",
    "print(calc_df[['Adj Close', 'TR']].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4c7716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the ATR using the exact same parameters as the code\n",
    "calc_df['ATR_14'] = calc_df['TR'].ewm(alpha=1/14, adjust=False).mean()\n",
    "\n",
    "print(\"--- TR vs ATR_14 (first 5 days) ---\")\n",
    "display(calc_df[['TR', 'ATR_14']].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab393be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ATRP is the ATR divided by the close price\n",
    "calc_df['ATRP'] = calc_df['ATR_14'] / calc_df['Adj Close']\n",
    "\n",
    "# The final denominator is the mean of the daily ATRP values over the period\n",
    "atrp_mean = calc_df['ATRP'].mean()\n",
    "\n",
    "print(f\"Mean ATRP (Denominator): {atrp_mean:.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1990be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_sharpe_atr_manual = mean_daily_return / atrp_mean\n",
    "\n",
    "print(\"--- FINAL VERIFICATION ---\")\n",
    "print(f\"Numerator (MeanDailyReturn): {mean_daily_return:.8f}\")\n",
    "print(f\"Denominator (ATRP_Mean):     {atrp_mean:.8f}\")\n",
    "print(f\"Calculated Sharpe (ATR):     {metric_sharpe_atr_manual:.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
