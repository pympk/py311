{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and Configuration\n",
    "\n",
    "This cell contains all imports and user-configurable parameters for the analysis pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pprint\n",
    "import inspect  # <--- ADD THIS LINE\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# --- 1. PANDAS & IPYTHON OPTIONS ---\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 3000)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# --- 2. PROJECT PATH CONFIGURATION ---\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "ROOT_DIR = NOTEBOOK_DIR.parent.parent  # Adjust if your notebook is in a 'notebooks' subdirectory\n",
    "DATA_DIR = ROOT_DIR / 'data'\n",
    "SRC_DIR = ROOT_DIR / 'src'\n",
    "\n",
    "# Add 'src' to the Python path to import custom modules\n",
    "if str(SRC_DIR) not in sys.path:\n",
    "    sys.path.append(str(SRC_DIR))\n",
    "\n",
    "# --- 3. IMPORT CUSTOM MODULES ---\n",
    "import utils\n",
    "import plotting_utils\n",
    "\n",
    "# --- 4. ANALYSIS & FILTERING CONFIGURATION ---\n",
    "\n",
    "# File searching parameters\n",
    "# FILE_PREFIX = ''  # e.g., '2024'\n",
    "FILE_CONTAINS_PATTERN = 'df_OHLCV_clean_stocks_etfs'\n",
    "\n",
    "# # Parameters defining the time windows for metric calculation\n",
    "PERIOD_PARAMS = {\n",
    "    'lookback_days': 40,\n",
    "    'recent_days': 8,\n",
    "}\n",
    "\n",
    "# --- 5. VERIFICATION ---\n",
    "print(\"--- Path Configuration ---\")\n",
    "print(f\"✅ Project Root: {ROOT_DIR}\")\n",
    "print(f\"✅ Data Dir:     {DATA_DIR}\")\n",
    "print(f\"✅ Source Dir:   {SRC_DIR}\")\n",
    "assert all([ROOT_DIR.exists(), DATA_DIR.exists(), SRC_DIR.exists()]), \"A key directory was not found!\"\n",
    "\n",
    "print(\"\\n--- Module Verification ---\")\n",
    "print(f\"✅ Successfully imported 'utils' and 'plotting_utils'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load Latest Merged Finviz Data\n",
    "\n",
    "Find and load the single most recent `df_finviz_merged` file. This DataFrame contains supplementary data like `Price` and `ATR/Price %` that will be used to enhance our final analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Step 1: Loading latest consolidated OHLCV data ---\")\n",
    "\n",
    "# Find the most recent file matching the pattern\n",
    "# This function is now understood to return List[str] (filenames), not List[Path].\n",
    "latest_OHLCV_filepaths = utils.get_recent_files(\n",
    "    directory_path=DATA_DIR,\n",
    "    extension='parquet',\n",
    "    # prefix=FILE_PREFIX,\n",
    "    contains_pattern=FILE_CONTAINS_PATTERN,\n",
    "    count=1\n",
    ")\n",
    "\n",
    "if not latest_OHLCV_filepaths:\n",
    "    raise FileNotFoundError(f\"No files found in '{DATA_DIR}' with prefix '{FILE_PREFIX}' and pattern '{FILE_CONTAINS_PATTERN}'\")\n",
    "\n",
    "# Get the filename string from the list\n",
    "latest_filename = latest_OHLCV_filepaths[0]\n",
    "\n",
    "# Manually construct the full path before loading\n",
    "full_file_path = DATA_DIR / latest_filename\n",
    "df_OHLCV = pd.read_parquet(full_file_path, engine='pyarrow')\n",
    "\n",
    "\n",
    "# --- Robust Index Setting (this logic remains correct) ---\n",
    "if df_OHLCV.index.name == 'Ticker':\n",
    "    print(\"Info: 'Ticker' is already the index. No action needed.\")\n",
    "elif 'Ticker' in df_OHLCV.columns:\n",
    "    print(\"Info: 'Ticker' column found. Setting it as the DataFrame index.\")\n",
    "    df_OHLCV.set_index('Ticker', inplace=True)\n",
    "elif 'ticker' in df_OHLCV.columns:\n",
    "    print(\"Info: 'ticker' column found. Renaming and setting as index.\")\n",
    "    df_OHLCV.rename(columns={'ticker': 'Ticker'}, inplace=True)\n",
    "    df_OHLCV.set_index('Ticker', inplace=True)\n",
    "elif df_OHLCV.index.name is None:\n",
    "    print(\"Info: Index is unnamed. Assuming it contains tickers and assigning the name 'Ticker'.\")\n",
    "    df_OHLCV.index.name = 'Ticker'\n",
    "else:\n",
    "    print(\"ERROR: Loaded DataFrame has an unexpected format.\")\n",
    "    print(f\"Columns: {df_OHLCV.columns.tolist()}\")\n",
    "    print(f\"Index Name: '{df_OHLCV.index.name}'\")\n",
    "    raise ValueError(\"Could not find a 'Ticker' column or a usable index to proceed.\")\n",
    "\n",
    "\n",
    "# Correct the print statement to work with the filename string\n",
    "print(f\"✅ Successfully loaded: {latest_filename}\")\n",
    "print(f\"Shape: {df_OHLCV.shape}\")\n",
    "print(df_OHLCV.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a single boolean: True if any NaN exists, False otherwise.\n",
    "has_nan = df_OHLCV.isna().any().any()\n",
    "\n",
    "print(f\"Are there any NaN values in the DataFrame? {has_nan}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the 'Adj High' series and unstack the first level of the index ('Ticker')\n",
    "df_adj_high = df_OHLCV['Adj High'].unstack(level=1)\n",
    "\n",
    "# Display the first few rows of the newly shaped DataFrame\n",
    "print(df_adj_high.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Build Rank History Matrix\n",
    "\n",
    "Load the last `N` daily data files to construct a comprehensive rank history DataFrame. This matrix is the primary input for all subsequent trend and performance calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"--- Step 2: Building rank history from the latest {HISTORY_FILE_COUNT} files ---\")\n",
    "\n",
    "# Get a list of all recent daily files\n",
    "daily_files_list = utils.get_recent_files(\n",
    "    directory_path=DATA_DIR,\n",
    "    extension='parquet',\n",
    "    prefix=FILE_PREFIX,\n",
    "    contains_pattern=FILE_CONTAINS_PATTERN,\n",
    "    count=HISTORY_FILE_COUNT\n",
    ")\n",
    "\n",
    "# Use the utility function to create the rank history dataframe\n",
    "# Assumes 'create_rank_history_df' is now in utils.py\n",
    "df_rank_history = utils.create_rank_history_df(daily_files_list, DATA_DIR)\n",
    "\n",
    "print(f\"✅ Rank history matrix created successfully.\")\n",
    "print(f\"Shape: {df_rank_history.shape} (Tickers, Days)\")\n",
    "print(f\"Date Range: {df_rank_history.columns.min().strftime('%Y-%m-%d')} to {df_rank_history.columns.max().strftime('%Y-%m-%d')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Print the original shape\n",
    "# df_rank_history.shape returns a tuple (number_of_rows, number_of_columns)\n",
    "print(\"\\nOriginal shape:\", df_rank_history.shape)\n",
    "\n",
    "# 3. Remove all rows with any NaN values\n",
    "# The dropna() method returns a new DataFrame by default\n",
    "df_rank_history_cleaned = df_rank_history.dropna()\n",
    "\n",
    "# 4. Print the new shape and the cleaned DataFrame\n",
    "print(\"\\nDataFrame after removing rows with any NaN values:\")\n",
    "print(df_rank_history_cleaned)\n",
    "\n",
    "print(\"\\nNew shape:\", df_rank_history_cleaned.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Calculate Metrics for All Tickers\n",
    "\n",
    "Process the rank history matrix to compute performance metrics for **every ticker**. This creates a master metrics DataFrame that serves as a single source of truth for all subsequent filtering and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Step 3: Calculating performance metrics for all tickers ---\")\n",
    "\n",
    "# This call remains the same. It correctly passes the arguments to the new function.\n",
    "all_metrics_data = utils.calculate_rank_metrics( # Assuming it's in the same file, or use utils.\n",
    "    df_rank_history,\n",
    "    tickers_list=df_rank_history.index.tolist(),\n",
    "    **PERIOD_PARAMS\n",
    ")\n",
    "\n",
    "# # Convert the list of dicts into a DataFrame.\n",
    "# # The new 'r_squared' and 'penalty_score' keys will automatically become columns.\n",
    "# df_all_tickers_metrics = pd.DataFrame(all_metrics_data)\n",
    "# if not df_all_tickers_metrics.empty:\n",
    "#     df_all_tickers_metrics.set_index('ticker', inplace=True)\n",
    "#     df_all_tickers_metrics.index.name = 'Ticker'\n",
    "\n",
    "# print(f\"✅ Calculated metrics for {len(df_all_tickers_metrics)} tickers.\")\n",
    "# print(\"\\nDataFrame head, now including 'r_squared' and 'penalty_score':\")\n",
    "# # display(df_all_tickers_metrics.head()) # Use display() in a notebook, or print()\n",
    "# print(df_all_tickers_metrics.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Step 3: Calculating performance metrics for all tickers ---\")\n",
    "\n",
    "# This call remains the same. It correctly passes the arguments to the new function.\n",
    "all_metrics_data = utils.calculate_rank_metrics( # Assuming it's in the same file, or use utils.\n",
    "    df_rank_history,\n",
    "    tickers_list=df_rank_history.index.tolist(),\n",
    "    **PERIOD_PARAMS\n",
    ")\n",
    "\n",
    "# Convert the list of dicts into a DataFrame.\n",
    "# The new 'r_squared' and 'penalty_score' keys will automatically become columns.\n",
    "df_all_tickers_metrics = pd.DataFrame(all_metrics_data)\n",
    "if not df_all_tickers_metrics.empty:\n",
    "    df_all_tickers_metrics.set_index('ticker', inplace=True)\n",
    "    df_all_tickers_metrics.index.name = 'Ticker'\n",
    "\n",
    "print(f\"✅ Calculated metrics for {len(df_all_tickers_metrics)} tickers.\")\n",
    "print(\"\\nDataFrame head, now including 'r_squared' and 'penalty_score':\")\n",
    "# display(df_all_tickers_metrics.head()) # Use display() in a notebook, or print()\n",
    "print(df_all_tickers_metrics.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Filter for 'Reversal' Candidates\n",
    "\n",
    "Apply the predefined filtering rules from the configuration cell to the master metrics DataFrame to identify a list of promising candidates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Step 4: Filtering metrics to find candidates ---\")\n",
    "print(\"METRIC_FILTERS\")\n",
    "pprint.pprint(METRIC_FILTERS)\n",
    "\n",
    "# Use the utility function, passing only the relevant filter arguments.\n",
    "# This is now much cleaner than the previous version.\n",
    "df_filtered_candidates = utils.filter_rank_metrics(\n",
    "    df_all_tickers_metrics,\n",
    "    **METRIC_FILTERS\n",
    ")\n",
    "\n",
    "print(f\"✅ Found {len(df_filtered_candidates)} candidates matching the criteria.\")\n",
    "display(df_filtered_candidates.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Enhance, Sort, and Select Top Candidates\n",
    "\n",
    "Enrich the filtered candidates with the latest price data, sort them according to the specified rules, and select the top N for visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the following columns have been calculated earlier for each stock\n",
    "# and are present in df_filtered_candidates:\n",
    "# - 'slope': The slope of the regression line (higher is better).\n",
    "# - 'r_squared': The R-squared value (higher is better).\n",
    "# - 'penalty_score': The combined penalty score (lower is better).\n",
    "\n",
    "print(\"--- Step 5: Applying the Tiered Filtering & Ranking Funnel ---\")\n",
    "\n",
    "# --- Funnel Strategy Parameters (you can tune these) ---\n",
    "# SLOPE_THRESHOLD = -1.0      # Viability: Trend must be meaningfully positive.\n",
    "# R_SQUARED_THRESHOLD = 0.75  # Reliability: Trend must be consistent and not random.\n",
    "SLOPE_THRESHOLD = 100      # Viability: Trend must be meaningfully positive.\n",
    "R_SQUARED_THRESHOLD = -10  # Reliability: Trend must be consistent and not random.\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# ### MODIFICATION START: Replaced the original sorting logic with the funnel ###\n",
    "# ==============================================================================\n",
    "\n",
    "# --- Step 1: The Viability Filter (using Slope) ---\n",
    "print(f\"Starting with {len(df_filtered_candidates)} initial candidates.\")\n",
    "df_funnel_step1 = df_filtered_candidates[df_filtered_candidates['lookback_slope'] < SLOPE_THRESHOLD]\n",
    "print(f\"-> {len(df_funnel_step1)} candidates remain after Slope Filter (lookback_slope < {SLOPE_THRESHOLD}).\")\n",
    "\n",
    "\n",
    "# --- Step 2: The Reliability Filter (using R-Squared) ---\n",
    "df_funnel_step2 = df_funnel_step1[df_funnel_step1['r_squared'] > R_SQUARED_THRESHOLD]\n",
    "print(f\"-> {len(df_funnel_step2)} candidates remain after R-Squared Filter (r_squared > {R_SQUARED_THRESHOLD}).\")\n",
    "df_funnel_candidates = df_funnel_step2\n",
    "\n",
    "# --- Step 3: Enhance, Calculate, and Rank the High-Quality Candidates ---\n",
    "# Join with latest Finviz data to add Price, MktCap, etc.\n",
    "cols_to_add = ['Price', 'Change %', 'MktCap AUM, M', 'ATR/Price %', 'Rel Volume']\n",
    "# Use .loc to avoid potential SettingWithCopyWarning\n",
    "df_candidates_enhanced = df_funnel_candidates.join(df_finviz_latest[cols_to_add])\n",
    "\n",
    "# Calculate the new 'Change/(ATR/Price)' metric\n",
    "df_candidates_enhanced['Change/(ATR/Price)'] = np.where(\n",
    "    df_candidates_enhanced['ATR/Price %'] != 0,\n",
    "    df_candidates_enhanced['Change %'] / df_candidates_enhanced['ATR/Price %'],\n",
    "    0\n",
    ")\n",
    "\n",
    "# --- Define the NEW Sorting Order based on the Funnel Philosophy ---\n",
    "# The primary sort is now by 'penalty_score'. Other metrics act as tie-breakers.\n",
    "# This replaces your original SORT_ORDER.\n",
    "SORT_ORDER_FUNNEL = {\n",
    "    'lookback_slope': True,             # Lower is better (steeper improving trend) - PRIMARY SORT    \n",
    "    'r_squared': False,                 # Higher is better (stronger growth) - TIE BREAKER 1    \n",
    "    'penalty_score': True,              # Lower is better (smoother, more linear trend) - TIE BREAKER 2\n",
    "    'Change/(ATR/Price)': False,        # Higher is better (strong recent momentum) - TIE BREAKER 3  \n",
    "}\n",
    "\n",
    "sort_keys = list(SORT_ORDER_FUNNEL.keys())\n",
    "sort_ascending = list(SORT_ORDER_FUNNEL.values())\n",
    "\n",
    "# Apply the new sorting logic\n",
    "df_sorted_candidates = df_candidates_enhanced.sort_values(by=sort_keys, ascending=sort_ascending)\n",
    "\n",
    "# ==============================================================================\n",
    "# ### MODIFICATION END ###\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "# --- Define and Apply Final Column Order (your existing logic is great here) ---\n",
    "leading_cols = [\n",
    "    'MktCap AUM, M', 'Price', 'Change %', 'ATR/Price %', 'Change/(ATR/Price)', 'Rel Volume',\n",
    "    'lookback_slope', 'r_squared', 'penalty_score' # Add our key metrics to the front\n",
    "]\n",
    "priority_cols = list(dict.fromkeys(leading_cols + sort_keys))\n",
    "remaining_cols = [c for c in df_sorted_candidates.columns if c not in priority_cols]\n",
    "final_col_order = priority_cols + remaining_cols\n",
    "df_sorted_candidates = df_sorted_candidates[final_col_order]\n",
    "\n",
    "\n",
    "# --- Select Top Candidates for Plotting ---\n",
    "\n",
    "####################################\n",
    "# tickers_to_plot = df_sorted_candidates.head(CANDIDATES_TO_PLOT).index.tolist()\n",
    "tickers_to_plot = df_sorted_candidates.index.tolist()\n",
    "####################################\n",
    "\n",
    "\n",
    "# --- Display Final Results with Context ---\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"      FINAL CANDIDATE REPORT (TIERED FUNNEL)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\nFunnel Filter Parameters:\")\n",
    "print(f\"  - Minimum Slope: {SLOPE_THRESHOLD}\")\n",
    "print(f\"  - Minimum R-Squared: {R_SQUARED_THRESHOLD}\")\n",
    "\n",
    "print(\"\\nApplied Metric Filters (from earlier steps):\")\n",
    "pprint.pprint(METRIC_FILTERS, sort_dicts=False)\n",
    "\n",
    "print(\"\\nSorting Order (Primary: Penalty Score):\")\n",
    "pprint.pprint(SORT_ORDER_FUNNEL, sort_dicts=False)\n",
    "\n",
    "# print(f\"\\nDisplaying Top {CANDIDATES_TO_PLOT} Candidates from Funnel:\")\n",
    "# display(df_sorted_candidates.head(CANDIDATES_TO_PLOT))\n",
    "\n",
    "print(f\"\\nNumber of Tickers selected for plotting: : {len(tickers_to_plot)}\")\n",
    "print(f\"\\nTickers selected for plotting: {tickers_to_plot}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tickers_to_view = ['NVDA', 'META', 'MSFT', 'B', 'GOOG', 'AVGO']\n",
    "# tickers_to_view = ['LYG', 'IEI']\n",
    "tickers_to_view = ['META']\n",
    "df_sorted_candidates.loc[tickers_to_view]\n",
    "# df_sorted_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Step 5: Enhancing and sorting final candidates ---\")\n",
    "\n",
    "# Join with latest Finviz data to add Price, MktCap, etc.\n",
    "cols_to_add = ['Price', 'Change %', 'MktCap AUM, M', 'ATR/Price %', 'Rel Volume']\n",
    "df_candidates_enhanced = df_filtered_candidates.join(df_finviz_latest[cols_to_add])\n",
    "\n",
    "# --- Calculate New Metrics ---\n",
    "# Create a normalized change metric by dividing the daily change by its recent volatility (ATR).\n",
    "# This gives a sense of how significant the day's move is relative to its own behavior.\n",
    "df_candidates_enhanced['Change/(ATR/Price)'] = np.where(\n",
    "    df_candidates_enhanced['ATR/Price %'] != 0,\n",
    "    df_candidates_enhanced['Change %'] / df_candidates_enhanced['ATR/Price %'],\n",
    "    0  # Assign 0 if ATR/Price % is 0 to avoid division errors\n",
    ")\n",
    "\n",
    "# Sort the candidates based on the rules in the SORT_ORDER dictionary\n",
    "sort_keys = list(SORT_ORDER.keys())\n",
    "sort_ascending = list(SORT_ORDER.values())\n",
    "df_sorted_candidates = df_candidates_enhanced.sort_values(by=sort_keys, ascending=sort_ascending)\n",
    "\n",
    "# --- Define and Apply Final Column Order ---\n",
    "# Define the columns that should always appear first, including our new metric.\n",
    "leading_cols = [\n",
    "    'MktCap AUM, M', 'Price', 'Change %', 'ATR/Price %', 'Change/(ATR/Price)', 'Rel Volume', 'current',\n",
    "]\n",
    "\n",
    "# Combine the leading columns with the sort keys for the master order.\n",
    "priority_cols = list(dict.fromkeys(leading_cols + sort_keys))\n",
    "remaining_cols = [c for c in df_sorted_candidates.columns if c not in priority_cols]\n",
    "final_col_order = priority_cols + remaining_cols\n",
    "df_sorted_candidates = df_sorted_candidates[final_col_order]\n",
    "\n",
    "# --- Select Top Candidates for Plotting ---\n",
    "tickers_to_plot = df_sorted_candidates.head(CANDIDATES_TO_PLOT).index.tolist()\n",
    "\n",
    "# --- Display Final Results with Context ---\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"      FINAL CANDIDATE REPORT\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\nPeriod Parameters (for calculation):\")\n",
    "pprint.pprint(PERIOD_PARAMS)\n",
    "\n",
    "print(\"\\nApplied Metric Filters:\")\n",
    "pprint.pprint(METRIC_FILTERS, sort_dicts=False)\n",
    "\n",
    "print(\"\\nSorting Order:\")\n",
    "pprint.pprint(SORT_ORDER, sort_dicts=False)\n",
    "\n",
    "print(f\"\\nDisplaying Top {CANDIDATES_TO_PLOT} Candidates:\")\n",
    "display(df_sorted_candidates.head(CANDIDATES_TO_PLOT))\n",
    "\n",
    "print(f\"\\nTickers selected for plotting: {tickers_to_plot}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'df_finviz_latest.info():\\n{df_finviz_latest.info()}')\n",
    "# print(f'\\ndf_finviz_latest.columns:\\n{df_finviz_latest.columns}')\n",
    "print(f'\\ndf_finviz_latest.columns:\\n{list(df_finviz_latest.columns)}')\n",
    "# with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "#     # 'display.max_rows' controls the truncation of the Index/Series representation\n",
    "#     # 'display.max_columns' is good to include for printing the full DataFrame\n",
    "#     print(f'\\ndf_finviz_latest.columns:\\n{df_finviz_latest.columns}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Sort the DataFrame by the index using the desired order ---\n",
    "# Use .loc with the list of valid, ordered tickers\n",
    "# df_finviz_latest_sorted = df_finviz_latest.loc[valid_tickers_in_order_unique]\n",
    "df_finviz_tickers_to_plot = df_finviz_latest.loc[tickers_to_plot]\n",
    "print(f'df_finviz_tickers_to_plot:\\n{df_finviz_tickers_to_plot}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Visualize Top Candidates\n",
    "\n",
    "Plot the rank history for the top candidates to visually verify their performance and trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Step 6: Plotting rank history for top candidates ---\")\n",
    "\n",
    "if tickers_to_plot:\n",
    "    plot_days = PERIOD_PARAMS['lookback_days'] + PERIOD_PARAMS['recent_days'] + 10\n",
    "    # Combine both dictionaries for complete context in the plot's annotation\n",
    "    full_criteria_for_plot = {**PERIOD_PARAMS, **METRIC_FILTERS}\n",
    "    \n",
    "    plotting_utils.plot_rank_with_criteria(\n",
    "        df_rank_history=df_rank_history.iloc[:, -plot_days:],\n",
    "        ticker_list=tickers_to_plot,\n",
    "        title_suffix=\"Top Filtered Candidates\",\n",
    "        filter_criteria=full_criteria_for_plot, # Pass the combined dict\n",
    "        width=1150,\n",
    "        height=700,\n",
    "    )\n",
    "else:\n",
    "    print(\"No candidates found to plot.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_sorted_candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Analyze Pre-defined Portfolio\n",
    "\n",
    "Perform a detailed analysis on a specific list of tickers. This step correctly uses the master `df_all_tickers_metrics` DataFrame to ensure all portfolio tickers are included, regardless of filter outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted_candidates.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Step 7: Analyzing the pre-defined portfolio ---\")\n",
    "\n",
    "# Correctly filter the *master* metrics dataframe for the portfolio tickers\n",
    "df_portfolio_analysis  = df_sorted_candidates[df_sorted_candidates.index.isin(PORTFOLIO_TICKERS)].copy()\n",
    "\n",
    "# Calculate portfolio weights, only if the dataframe is not empty\n",
    "if not df_portfolio_analysis.empty:\n",
    "    total_aum = df_portfolio_analysis['MktCap AUM, M'].sum()\n",
    "    inv_atr = 1 / df_portfolio_analysis['ATR/Price %']\n",
    "    total_inv_atr = inv_atr.sum()\n",
    "\n",
    "    df_portfolio_analysis['MktCap AUM Weight'] = df_portfolio_analysis['MktCap AUM, M'] / total_aum\n",
    "    df_portfolio_analysis['ATR/Price INV Weight'] = (inv_atr / total_inv_atr)\n",
    "\n",
    "    total_portf = df_portfolio_analysis['MktCap AUM Weight'].sum() + df_portfolio_analysis['ATR/Price INV Weight'].sum()\n",
    "    df_portfolio_analysis['Portf Weight'] = (df_portfolio_analysis['MktCap AUM Weight'] + df_portfolio_analysis['ATR/Price INV Weight']) / total_portf\n",
    "\n",
    "    # --- Define and Apply Final Column Order ---\n",
    "    # The new columns to be inserted\n",
    "    new_cols = ['MktCap AUM Weight', 'ATR/Price INV Weight', 'Portf Weight']\n",
    "\n",
    "    # Convert the original column Index to a list for easy manipulation\n",
    "    original_cols = list(df_sorted_candidates.columns)\n",
    "\n",
    "    # Find the index of the column to insert after\n",
    "    try:\n",
    "        # Find the integer position of the 'current' column\n",
    "        insert_index = original_cols.index('current') + 1 \n",
    "    except ValueError:\n",
    "        # Handle the case where 'current' isn't in the columns, perhaps append to the end\n",
    "        print(\"Warning: 'current' column not found. Appending new columns to the end.\")\n",
    "        insert_index = len(original_cols)\n",
    "\n",
    "    # Reconstruct the list with the new columns inserted\n",
    "    PORTFOLIO_COLUMN_ORDER = original_cols[:insert_index] + new_cols + original_cols[insert_index:]\n",
    "\n",
    "    # Filter the desired order to only include columns that actually exist in the DataFrame\n",
    "    # This makes the code robust against missing data columns.\n",
    "    final_portfolio_cols = [c for c in PORTFOLIO_COLUMN_ORDER if c in df_portfolio_analysis.columns]\n",
    "    df_portfolio_analysis = df_portfolio_analysis[final_portfolio_cols]\n",
    "\n",
    "print(f\"✅ Portfolio analysis complete for {len(df_portfolio_analysis)} tickers.\")\n",
    "print(\"Portfolio metrics, sorted by final portfolio weight:\")\n",
    "print(df_portfolio_analysis.sort_values(by='Portf Weight', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Visualize Portfolio\n",
    "\n",
    "Plot the rank history for the tickers in the pre-defined portfolio to compare their recent performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Step 8: Plotting rank history for the portfolio ---\")\n",
    "\n",
    "if PORTFOLIO_TICKERS:\n",
    "    plot_days = PERIOD_PARAMS['lookback_days'] + PERIOD_PARAMS['recent_days'] + 10\n",
    "    # Combine both dictionaries for complete context in the plot's annotation\n",
    "    full_criteria_for_plot = {**PERIOD_PARAMS, **METRIC_FILTERS}\n",
    "    \n",
    "    plotting_utils.plot_rank_with_criteria(\n",
    "        df_rank_history=df_rank_history.iloc[:, -plot_days:],\n",
    "        ticker_list=PORTFOLIO_TICKERS,\n",
    "        title_suffix=\"Pre-defined Portfolio\",\n",
    "        filter_criteria=full_criteria_for_plot, # Pass the combined dict\n",
    "        width=1150,\n",
    "        height=700,\n",
    "    )\n",
    "else:\n",
    "    print(\"Portfolio ticker list is empty. Nothing to plot.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
