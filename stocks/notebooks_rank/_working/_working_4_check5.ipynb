{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 3000)\n",
    "\n",
    "# DATA_DIR = r'c:\\Users\\ping\\Files_win10\\python\\py311\\stocks\\data'\n",
    "# Manually construct the full path before loading\n",
    "full_file_path = r'c:\\Users\\ping\\Files_win10\\python\\py311\\stocks\\data\\df_OHLCV_clean_stocks_etfs.parquet'\n",
    "df_OHLCV = pd.read_parquet(full_file_path, engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique trading dates in dataset: 250\n",
      "The data will be split on the date: 2025-05-30\n",
      "\n",
      "--- Verification ---\n",
      "Original DataFrame shape: (371250, 5)\n",
      "Training set shape:   (261360, 5)\n",
      "Testing set shape:    (109890, 5)\n",
      "\n",
      "Date Ranges:\n",
      "  Training: 2024-09-17 to 2025-05-30\n",
      "  Testing:  2025-06-02 to 2025-09-16\n",
      "\n",
      "Verification successful: There is no date overlap between train and test sets.\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Find the Chronological Split Point ---\n",
    "\n",
    "# Get all unique dates from the index and sort them\n",
    "unique_dates = df_OHLCV.index.get_level_values('Date').unique().sort_values()\n",
    "\n",
    "# Determine the index for the 70% split\n",
    "split_index = int(len(unique_dates) * 0.7)\n",
    "\n",
    "# Find the actual date at that split index\n",
    "split_date = unique_dates[split_index]\n",
    "\n",
    "print(f\"Total unique trading dates in dataset: {len(unique_dates)}\")\n",
    "print(f\"The data will be split on the date: {split_date.date()}\")\n",
    "\n",
    "# --- 2. Create the Training and Testing Sets ---\n",
    "\n",
    "# The training set includes all data UP TO and INCLUDING the split_date\n",
    "df_train = df_OHLCV[df_OHLCV.index.get_level_values('Date') <= split_date]\n",
    "\n",
    "# The testing set includes all data AFTER the split_date\n",
    "df_test = df_OHLCV[df_OHLCV.index.get_level_values('Date') > split_date]\n",
    "\n",
    "\n",
    "# --- 3. Verify the Split ---\n",
    "\n",
    "print(\"\\n--- Verification ---\")\n",
    "print(f\"Original DataFrame shape: {df_OHLCV.shape}\")\n",
    "print(f\"Training set shape:   {df_train.shape}\")\n",
    "print(f\"Testing set shape:    {df_test.shape}\")\n",
    "\n",
    "print(\"\\nDate Ranges:\")\n",
    "print(f\"  Training: {df_train.index.get_level_values('Date').min().date()} to {df_train.index.get_level_values('Date').max().date()}\")\n",
    "print(f\"  Testing:  {df_test.index.get_level_values('Date').min().date()} to {df_test.index.get_level_values('Date').max().date()}\")\n",
    "\n",
    "# Final check to ensure no overlap\n",
    "assert df_train.index.get_level_values('Date').max() < df_test.index.get_level_values('Date').min()\n",
    "print(\"\\nVerification successful: There is no date overlap between train and test sets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jupyter Notebook: Verifying `analyze_ticker_trends_vectorized`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cell 1: Setup and Imports\n",
    "\n",
    "First, let's import the necessary libraries and define both versions of the function we want to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verification functions are defined.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import linregress\n",
    "\n",
    "# --- Function 1: The NEW, Correct, Log-Vectorized Version (The one we are verifying) ---\n",
    "\n",
    "def analyze_ticker_trends_log_vectorized_obsolete(df_group, lookback_days=60):\n",
    "    \"\"\"\n",
    "    This function output has been verified. \n",
    "\n",
    "    Vectorized analysis of trends using LOG-TRANSFORMED data.\n",
    "    \n",
    "    This is the CORRECTED version. The slope of a log-transformed series represents\n",
    "    the average percentage change, making it comparable across different stocks.\n",
    "\n",
    "    The first n lookback_days rows will be all NaN.\n",
    "    'unified_std_dev_returns' and 'volume_std_dev_returns' will have NaN on \n",
    "    lookback_days + 1 row.\n",
    "    \"\"\"\n",
    "    if len(df_group) < lookback_days:\n",
    "        return None \n",
    "\n",
    "    time_index = pd.Series(np.arange(len(df_group)), index=df_group.index)\n",
    "    var_time = np.var(np.arange(lookback_days), ddof=0)\n",
    "    \n",
    "    series_to_analyze = {\n",
    "        # Uses natural log \n",
    "        'high': np.log(df_group['Adj High']),\n",
    "        'low': np.log(df_group['Adj Low']),\n",
    "        'volume': np.log(df_group['Volume'].astype(float) + 1)\n",
    "    }\n",
    "    \n",
    "    df_results = pd.DataFrame(index=df_group.index)\n",
    "\n",
    "    for name, log_series in series_to_analyze.items():\n",
    "        # Uses covariance and variance of population\n",
    "        rolling_cov = time_index.rolling(window=lookback_days).cov(log_series, ddof=0)\n",
    "        rolling_var_series = log_series.rolling(window=lookback_days).var(ddof=0)\n",
    "        \n",
    "        df_results[f'{name}_slope'] = rolling_cov / var_time\n",
    "        denominator = (var_time * rolling_var_series) + 1e-9\n",
    "        df_results[f'{name}_r_squared'] = (rolling_cov**2) / denominator\n",
    "\n",
    "    yesterday_low = df_group['Adj Low'].shift(1)\n",
    "    worst_case_returns = (df_group['Adj High'] - yesterday_low) / yesterday_low\n",
    "    df_results['unified_std_dev_returns'] = worst_case_returns.rolling(window=lookback_days).std(ddof=0)\n",
    "    \n",
    "    volume_std_dev = df_group['Volume'].pct_change().rolling(window=lookback_days).std(ddof=0)\n",
    "    df_results['volume_std_dev_returns'] = volume_std_dev\n",
    "    \n",
    "    return df_results # Penalty scores omitted for verification clarity\n",
    "\n",
    "\n",
    "def analyze_ticker_trends_log_vectorized(df_group, lookback_days=60):\n",
    "    \"\"\"\n",
    "    Vectorized analysis of trends, including penalty scores and the underlying\n",
    "    volatility metrics used to calculate them.\n",
    "    \"\"\"\n",
    "    if len(df_group) < lookback_days:\n",
    "        return None \n",
    "\n",
    "    time_index = pd.Series(np.arange(len(df_group)), index=df_group.index)\n",
    "    var_time = np.var(np.arange(lookback_days), ddof=0)\n",
    "    \n",
    "    # --- 1. TREND ANALYSIS (EXPANDED TO OHLCV) ---\n",
    "    trend_columns = ['Adj Open', 'Adj High', 'Adj Low', 'Adj Close', 'Volume']\n",
    "    trend_columns_exist = [col for col in trend_columns if col in df_group.columns]\n",
    "\n",
    "    df_results = pd.DataFrame(index=df_group.index)\n",
    "\n",
    "    for name in trend_columns_exist:\n",
    "        series = df_group[name].astype(float)\n",
    "        log_series = np.log(series + 1) if name == 'Volume' else np.log(series)\n",
    "        \n",
    "        rolling_cov = time_index.rolling(window=lookback_days).cov(log_series, ddof=0)\n",
    "        rolling_var_series = log_series.rolling(window=lookback_days).var(ddof=0)\n",
    "        \n",
    "        simple_name = name.replace('Adj ', '').lower() \n",
    "        \n",
    "        df_results[f'{simple_name}_slope'] = rolling_cov / var_time\n",
    "        denominator = (var_time * rolling_var_series) + 1e-9\n",
    "        df_results[f'{simple_name}_r_squared'] = (rolling_cov**2) / denominator\n",
    "\n",
    "    # --- 2. VOLATILITY CALCULATION ---\n",
    "    yesterday_low = df_group['Adj Low'].shift(1)\n",
    "    worst_case_returns = (df_group['Adj High'] - yesterday_low) / yesterday_low\n",
    "    unified_std_dev = worst_case_returns.rolling(window=lookback_days).std(ddof=0)\n",
    "    \n",
    "    volume_std_dev = df_group['Volume'].pct_change().rolling(window=lookback_days).std(ddof=0)\n",
    "    \n",
    "    # --- KEY CHANGE: Add the volatility columns to the output DataFrame ---\n",
    "    df_results['unified_std_dev_returns'] = unified_std_dev\n",
    "    df_results['volume_std_dev_returns'] = volume_std_dev\n",
    "\n",
    "    # --- 3. PENALTY SCORE CALCULATION ---\n",
    "    price_trend_names = ['open', 'high', 'low', 'close']\n",
    "    for name in price_trend_names:\n",
    "        r_squared_col = f'{name}_r_squared'\n",
    "        if r_squared_col in df_results.columns:\n",
    "            df_results[f'{name}_penalty_score'] = (1 - df_results[r_squared_col]) * (unified_std_dev + 1e-9)\n",
    "\n",
    "    if 'volume_r_squared' in df_results.columns:\n",
    "        df_results['volume_penalty_score'] = (1 - df_results['volume_r_squared']) * (volume_std_dev + 1e-9)\n",
    "    \n",
    "    return df_results\n",
    "\n",
    "\n",
    "print(\"Verification functions are defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cell 2: Create Realistic Sample Data\n",
    "\n",
    "We need some sample data that mimics your real dataset to perform the check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSFT = df_train.loc['MSFT'].copy()\n",
    "print(f'MSFT.head(3):\\n{MSFT.head(3)}')\n",
    "print(f'\\nMSFT.tail(3):\\n{MSFT.tail(3)}')\n",
    "print(f'\\nlen(MSFT): {len(MSFT)}')\n",
    "\n",
    "MSFT.to_csv(r'C:\\Users\\ping\\Desktop\\MSFT.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Parameters for our Test ---\n",
    "TICKER_TO_CHECK = 'MSFT'\n",
    "DATE_TO_CHECK = pd.to_datetime('2024-09-26')\n",
    "LOOKBACK_DAYS = 10\n",
    "\n",
    "print(f\"--- Verifying LOG-TRANSFORMED calculations for '{TICKER_TO_CHECK}' on {DATE_TO_CHECK.date()} ---\\n\")\n",
    "\n",
    "# --- 1. Run the FAST Log-Vectorized function ---\n",
    "ticker_history = df_train.loc[TICKER_TO_CHECK]\n",
    "vectorized_results_full = analyze_ticker_trends_log_vectorized(ticker_history, LOOKBACK_DAYS)\n",
    "vectorized_result_today = vectorized_results_full.loc[DATE_TO_CHECK]\n",
    "\n",
    "# --- 2. Run the SLOW Original (Normalized) function ---\n",
    "historical_slice = ticker_history.loc[:DATE_TO_CHECK]\n",
    "original_result_today = analyze_ticker_trends_original(historical_slice, LOOKBACK_DAYS)\n",
    "\n",
    "# --- 3. Compare the results ---\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Log-Vectorized': vectorized_result_today,\n",
    "    'Original (Normalized)': original_result_today\n",
    "}).dropna()\n",
    "\n",
    "print(\"### Important Note ###\")\n",
    "print(\"The slope of a log(price) series is mathematically very similar to the slope of a normalized (price/price_0) series.\")\n",
    "print(\"Therefore, both the SLOPES and R-SQUARED values should now be very close.\\n\")\n",
    "\n",
    "print(\"### Side-by-Side Comparison ###\")\n",
    "print(comparison_df)\n",
    "\n",
    "# --- 4. Programmatic Check for ALL values ---\n",
    "try:\n",
    "    # We now test BOTH slope and r_squared\n",
    "    pd.testing.assert_series_equal(\n",
    "        comparison_df['Log-Vectorized'],\n",
    "        comparison_df['Original (Normalized)'],\n",
    "        atol=0.05 # Use a slightly larger tolerance for slope approximation\n",
    "    )\n",
    "    print(\"\\n[SUCCESS]: Log-vectorized results closely match the original normalized results!\")\n",
    "except AssertionError as e:\n",
    "    print(\"\\n[FAILURE]: Results do not match.\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_results_full.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_results_full.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refactored Code\n",
    "\n",
    "Here is the complete, refactored solution. I've included the previously refactored functions with slight modifications to accept the new configuration structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def calculate_rolling_z_scores_general(df_group, columns_to_process, rolling_window=20):\n",
    "    \"\"\"\n",
    "    This function output has been verified. \n",
    "\n",
    "    Calculates rolling Z-scores for a list of specified columns.\n",
    "    \n",
    "    This is a flexible, reusable, and efficient version.\n",
    "    \n",
    "    Args:\n",
    "        df_group (pd.DataFrame): The DataFrame for a single ticker.\n",
    "        columns_to_process (list): A list of column names to calculate Z-scores for.\n",
    "        rolling_window (int): The lookback window.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with Z-score columns, prefixed with 'z_score_'.\n",
    "                      Returns NaNs for non-computable values.\n",
    "    \"\"\"\n",
    "    if df_group.empty or len(df_group) < rolling_window:\n",
    "        # Return an empty DataFrame with the expected column names for consistency\n",
    "        return pd.DataFrame(columns=[f\"z_score_{col}\" for col in columns_to_process])\n",
    "\n",
    "    # Select the subset of data to work on\n",
    "    data_subset = df_group[columns_to_process]\n",
    "    \n",
    "    # Calculate rolling stats for all columns at once.\n",
    "    # This correctly produces NaNs for the initial, incomplete windows.\n",
    "    rolling_mean = data_subset.rolling(window=rolling_window).mean()\n",
    "    rolling_std = data_subset.rolling(window=rolling_window).std()\n",
    "    \n",
    "    # Calculate Z-scores for all columns in one vectorized operation.\n",
    "    z_scores_df = (data_subset - rolling_mean) / rolling_std\n",
    "    \n",
    "    # Handle true division-by-zero errors (where std is 0)\n",
    "    z_scores_df = z_scores_df.replace([np.inf, -np.inf], 0)\n",
    "    \n",
    "    # Add a descriptive prefix to the column names (e.g., 'Adj Low' -> 'z_score_Adj Low')\n",
    "    return z_scores_df.add_prefix('z_score_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_scores = df_train.groupby(level='Ticker', group_keys=False).apply(\n",
    "    calculate_rolling_z_scores_general, df_train.columns, rolling_window=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_MSFT_w10 = z_scores.loc['MSFT']\n",
    "z_MSFT_w10.to_csv(r'C:\\Users\\ping\\Desktop\\z_MSFT_w10.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Refined Core Backtesting Functions\n",
    "\n",
    "We'll modify the function signatures to accept a single `config` dictionary. This makes them more modular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "def run_backtest(df_ohlcv, config):\n",
    "    \"\"\"\n",
    "    Orchestrates the backtesting process with enhanced logging.\n",
    "    \"\"\"\n",
    "    # Now returns a DataFrame of features for triggered signals\n",
    "    entry_signals_features = precompute_signals(df_ohlcv, config)\n",
    "    \n",
    "    trades = []\n",
    "    open_positions = {}\n",
    "    \n",
    "    all_dates = df_ohlcv.index.get_level_values('Date').unique().sort_values()\n",
    "    start_index = max(config['lookback_days'], config['rolling_window'])\n",
    "\n",
    "    for i in tqdm(range(start_index, len(all_dates) - 1), desc=\"Backtesting\"):\n",
    "        current_date = all_dates[i]\n",
    "        next_day_date = all_dates[i+1]\n",
    "\n",
    "        closed_trades, open_positions = handle_exits_for_day(\n",
    "            current_date, next_day_date, open_positions, df_ohlcv, config\n",
    "        )\n",
    "        trades.extend(closed_trades)\n",
    "\n",
    "        # --- KEY CHANGE: Filter the features DataFrame for today's signals ---\n",
    "        signals_today = entry_signals_features[\n",
    "            entry_signals_features.index.get_level_values('Date') == current_date\n",
    "        ]\n",
    "        \n",
    "        # Pass the full signals_today DataFrame to the handler\n",
    "        open_positions = handle_entries_for_day(\n",
    "            current_date, next_day_date, signals_today, open_positions, df_ohlcv\n",
    "        )\n",
    "                \n",
    "    # --- Create the final DataFrame and reorder columns for clarity ---\n",
    "    if not trades:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    final_trades_df = pd.DataFrame(trades)\n",
    "    log_columns = [\n",
    "        'ticker', 'signal_date', 'entry_date', 'exit_signal_date', 'exit_date', 'reason',\n",
    "        'return', 'entry_price_actual', 'exit_price_actual', 'exit_trigger_price', \n",
    "        'exit_target_value', 'entry_signal_features'\n",
    "    ]\n",
    "    # Ensure all columns exist, fill missing with None\n",
    "    for col in log_columns:\n",
    "        if col not in final_trades_df.columns:\n",
    "            final_trades_df[col] = None\n",
    "            \n",
    "    return final_trades_df[log_columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-computing features for this parameter set...\n",
      "Calculating Z-scores for columns: ['Adj Open', 'Adj High', 'Adj Low', 'Adj Close', 'Volume']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      2\u001b[39m config = {\n\u001b[32m      3\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mlookback_days\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m30\u001b[39m,\n\u001b[32m      4\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mrolling_window\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m10\u001b[39m,  \u001b[38;5;66;03m# Set the value you want to test\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m      8\u001b[39m     \u001b[38;5;66;03m# ... other params if needed\u001b[39;00m\n\u001b[32m      9\u001b[39m }\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# 2. Call the function correctly\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m signals_df, features_df = precompute_signals(df_train, config) \n",
      "\u001b[31mValueError\u001b[39m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# 1. Define the full configuration\n",
    "config = {\n",
    "    'lookback_days': 30,\n",
    "    'rolling_window': 10,  # Set the value you want to test\n",
    "    'slope_thresh': 0.05,\n",
    "    'r2_thresh': 0.50,\n",
    "    'z_entry_thresh': -1.5,\n",
    "    # ... other params if needed\n",
    "}\n",
    "\n",
    "# 2. Call the function correctly\n",
    "signals_df, features_df = precompute_signals(df_train, config) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jupyter Notebook: Testing `precompute_signals`\n",
    "\n",
    "#### Cell 1: Setup and Test Functions\n",
    "\n",
    "First, we need the function definitions and a small, predictable sample dataset. Testing on the full `df_train` is too difficult to verify manually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample DataFrame created.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Adj Open</th>\n",
       "      <th>Adj High</th>\n",
       "      <th>Adj Low</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ticker</th>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">TEST</th>\n",
       "      <th>2023-01-02</th>\n",
       "      <td>100.500000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-03</th>\n",
       "      <td>101.867014</td>\n",
       "      <td>102.367014</td>\n",
       "      <td>100.367014</td>\n",
       "      <td>101.367014</td>\n",
       "      <td>101020.408163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-04</th>\n",
       "      <td>102.999269</td>\n",
       "      <td>103.499269</td>\n",
       "      <td>101.499269</td>\n",
       "      <td>102.499269</td>\n",
       "      <td>102040.816327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-05</th>\n",
       "      <td>103.719480</td>\n",
       "      <td>104.219480</td>\n",
       "      <td>102.219480</td>\n",
       "      <td>103.219480</td>\n",
       "      <td>103061.224490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-06</th>\n",
       "      <td>103.951248</td>\n",
       "      <td>104.451248</td>\n",
       "      <td>102.451248</td>\n",
       "      <td>103.451248</td>\n",
       "      <td>104081.632653</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Adj Open    Adj High     Adj Low   Adj Close         Volume\n",
       "Ticker Date                                                                     \n",
       "TEST   2023-01-02  100.500000  101.000000   99.000000  100.000000  100000.000000\n",
       "       2023-01-03  101.867014  102.367014  100.367014  101.367014  101020.408163\n",
       "       2023-01-04  102.999269  103.499269  101.499269  102.499269  102040.816327\n",
       "       2023-01-05  103.719480  104.219480  102.219480  103.219480  103061.224490\n",
       "       2023-01-06  103.951248  104.451248  102.451248  103.451248  104081.632653"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create a date range\n",
    "dates = pd.to_datetime(pd.date_range(start='2023-01-01', periods=50, freq='B'))\n",
    "\n",
    "# Create data for Ticker 'TEST' with a clear upward trend and some noise\n",
    "price = 100 + np.linspace(0, 20, 50) + np.sin(np.arange(50)/2) * 2\n",
    "volume = 100000 + np.linspace(0, 50000, 50) # Strong positive volume slope\n",
    "df_test = pd.DataFrame({\n",
    "    'Ticker': 'TEST', 'Date': dates, \n",
    "    'Adj Open': price + 0.5, 'Adj High': price + 1, 'Adj Low': price - 1, 'Adj Close': price,\n",
    "    'Volume': volume\n",
    "})\n",
    "\n",
    "# Add a ticker with insufficient data to test edge cases\n",
    "df_short = pd.DataFrame({\n",
    "    'Ticker': 'SHORT', 'Date': dates[:5],\n",
    "    'Adj High': 10, 'Adj Low': 9, 'Adj Close': 9.5, 'Volume': 1000\n",
    "})\n",
    "\n",
    "\n",
    "# Combine and set index\n",
    "sample_df = pd.concat([df_test, df_short]).set_index(['Ticker', 'Date'])\n",
    "print(\"Sample DataFrame created.\")\n",
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from itertools import product\n",
    "\n",
    "def precompute_signals(df_ohlcv, config):\n",
    "    \"\"\"\n",
    "    Pre-computes signals, now generating Z-scores for all OHLCV columns\n",
    "    for maximum flexibility in strategy design.\n",
    "    \"\"\"\n",
    "    print(\"Pre-computing features for this parameter set...\")\n",
    "    \n",
    "    # --- 1. TREND ANALYSIS (No changes here) ---\n",
    "    trends = df_ohlcv.groupby(level='Ticker', group_keys=False).apply(\n",
    "        analyze_ticker_trends_log_vectorized, config['lookback_days']\n",
    "    )\n",
    "    \n",
    "    # --- 2. Z-SCORE ANALYSIS (KEY CHANGE HERE) ---\n",
    "    \n",
    "    # Define the full list of columns for which we want Z-scores.\n",
    "    # We use the 'Adj' prefix for price columns as is standard.\n",
    "    z_score_columns = ['Adj Open', 'Adj High', 'Adj Low', 'Adj Close', 'Volume']\n",
    "    \n",
    "    # Ensure all requested columns exist in the DataFrame to prevent errors.\n",
    "    # This makes the function more robust if 'Adj Open' isn't available, for example.\n",
    "    z_score_columns_exist = [col for col in z_score_columns if col in df_ohlcv.columns]\n",
    "    \n",
    "    print(f\"Calculating Z-scores for columns: {z_score_columns_exist}\")\n",
    "    \n",
    "    # The general function handles the list of columns perfectly.\n",
    "    z_scores = df_ohlcv.groupby(level='Ticker', group_keys=False).apply(\n",
    "        calculate_rolling_z_scores_general, \n",
    "        columns_to_process=z_score_columns_exist,\n",
    "        rolling_window=config['rolling_window']\n",
    "    )\n",
    "    \n",
    "    # --- 3. COMBINE AND FILTER ---\n",
    "    \n",
    "    features = trends.join(z_scores).dropna()\n",
    "\n",
    "    # The filter logic remains the same, but it uses the standardized \n",
    "    # column name 'z_score_Adj Low'.\n",
    "    signals = features[\n",
    "        (features['low_slope'] > config['slope_thresh']) &\n",
    "        (features['low_r_squared'] > config['r2_thresh']) &\n",
    "        (features['volume_slope'] > 0) &\n",
    "        (features['z_score_Adj Low'] < config['z_entry_thresh'])\n",
    "    ]\n",
    "    \n",
    "    return signals\n",
    "\n",
    "\n",
    "\n",
    "# --- A special version for \"white-box\" testing that returns the intermediate features ---\n",
    "def precompute_signals_for_testing(df_ohlcv, config):\n",
    "    \"\"\"A modified version that returns both the features and the final signals.\"\"\"\n",
    "    # (Code is identical to above, just the return statement is different)\n",
    "    trends = df_ohlcv.groupby(level='Ticker', group_keys=False).apply(analyze_ticker_trends_log_vectorized, config['lookback_days'])\n",
    "\n",
    "\n",
    "    # --- 2. Z-SCORE ANALYSIS (KEY CHANGE HERE) ---\n",
    "    \n",
    "    # Define the full list of columns for which we want Z-scores.\n",
    "    # We use the 'Adj' prefix for price columns as is standard.\n",
    "    z_score_columns = ['Adj Open', 'Adj High', 'Adj Low', 'Adj Close', 'Volume']\n",
    "    \n",
    "    # Ensure all requested columns exist in the DataFrame to prevent errors.\n",
    "    # This makes the function more robust if 'Adj Open' isn't available, for example.\n",
    "    z_score_columns_exist = [col for col in z_score_columns if col in df_ohlcv.columns]\n",
    "    \n",
    "    print(f\"Calculating Z-scores for columns: {z_score_columns_exist}\")\n",
    "\n",
    "\n",
    "\n",
    "    z_scores = df_ohlcv.groupby(level='Ticker', group_keys=False).apply(\n",
    "        calculate_rolling_z_scores_general, \n",
    "        columns_to_process=z_score_columns, \n",
    "        rolling_window=config['rolling_window']\n",
    "        )\n",
    "    \n",
    "    features = trends.join(z_scores).dropna()\n",
    "###########################    \n",
    "    # signals = features[\n",
    "    #     (features['low_slope'] > config['slope_thresh']) &\n",
    "    #     (features['low_r_squared'] > config['r2_thresh']) &\n",
    "    #     (features['volume_slope'] > 0) &\n",
    "    #     (features['z_score_Adj Low'] < config['z_entry_thresh'])\n",
    "    # ]\n",
    "    # signals = features[\n",
    "    #     ((features['low_slope'] > config['slope_thresh']) &\n",
    "    #     (features['low_slope'] < .008)) &    \n",
    "    #     (features['low_r_squared'] > config['r2_thresh']) &\n",
    "    #     (features['volume_slope'] > 0) &\n",
    "    #     (features['z_score_Adj Low'] < config['z_entry_thresh'])\n",
    "    # ]\n",
    "# --- same-column filter ------------------------------------------------------\n",
    "    low_slope_ok = (\n",
    "        # (features['low_slope'] > config['slope_thresh']) &\n",
    "        (features['high_slope'] > features['low_slope'])\n",
    "    )\n",
    "\n",
    "    # --- final signal mask -------------------------------------------------------\n",
    "    signals = features[\n",
    "        low_slope_ok\n",
    "        & (features['low_r_squared'] > config['r2_thresh'])\n",
    "        & (features['volume_slope'] > 0)\n",
    "        & (features['z_score_Adj Low'] < config['z_entry_thresh'])\n",
    "        & (features['z_score_Adj Low'] > 0)        \n",
    "    ]\n",
    "\n",
    "###########################          \n",
    "    return features, signals, trends # Return both for inspection\n",
    "\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "def apply_strategy_rules(features, rules, config):\n",
    "    \"\"\"\n",
    "    Applies a list of filtering rules to a features DataFrame.\n",
    "\n",
    "    Args:\n",
    "        features (pd.DataFrame): The DataFrame containing all calculated features.\n",
    "        rules (list): A list of dictionaries, where each dict defines a filtering rule.\n",
    "        config (dict): The configuration dictionary, used for dynamic thresholds.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: A boolean Series (mask) indicating which rows pass all rules.\n",
    "    \"\"\"\n",
    "    # Start with a mask that is True for all rows. We will progressively filter it.\n",
    "    final_mask = pd.Series(True, index=features.index)\n",
    "    \n",
    "    # Map operator strings to actual Python operator functions for flexibility\n",
    "    op_map = {\n",
    "        '>': operator.gt,\n",
    "        '<': operator.lt,\n",
    "        '>=': operator.ge,\n",
    "        '<=': operator.le,\n",
    "        '==': operator.eq,\n",
    "        '!=': operator.ne\n",
    "    }\n",
    "\n",
    "    for rule in rules:\n",
    "        op_func = op_map[rule['operator']]\n",
    "        \n",
    "        # --- Rule Type 1: Comparing two columns ---\n",
    "        if 'column_A' in rule and 'column_B' in rule:\n",
    "            mask = op_func(features[rule['column_A']], features[rule['column_B']])\n",
    "        \n",
    "        # --- Rule Type 2: Comparing a column to a value ---\n",
    "        elif 'column' in rule:\n",
    "            # Determine the value to compare against\n",
    "            if 'value' in rule:\n",
    "                value = rule['value']\n",
    "            elif 'value_from_config' in rule:\n",
    "                value = config[rule['value_from_config']]\n",
    "            else:\n",
    "                raise ValueError(f\"Rule missing 'value' or 'value_from_config': {rule}\")\n",
    "            \n",
    "            mask = op_func(features[rule['column']], value)\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f\"Invalid rule format: {rule}\")\n",
    "        \n",
    "        # Combine the mask for this rule with the final mask using a logical AND\n",
    "        final_mask &= mask\n",
    "            \n",
    "    return final_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_signals(df_ohlcv, config, rules):\n",
    "    \"\"\"\n",
    "    Pre-computes a rich feature set and then applies a dynamic set of rules\n",
    "    to generate the final trading signals.\n",
    "    \"\"\"\n",
    "    print(\"Pre-computing features for this parameter set...\")\n",
    "    \n",
    "    # --- 1. FEATURE GENERATION (No changes here) ---\n",
    "    trends = df_ohlcv.groupby(level='Ticker', group_keys=False).apply(\n",
    "        analyze_ticker_trends_log_vectorized, config['lookback_days']\n",
    "    )\n",
    "    \n",
    "    z_score_columns = ['Adj Open', 'Adj High', 'Adj Low', 'Adj Close', 'Volume']\n",
    "    z_score_columns_exist = [col for col in z_score_columns if col in df_ohlcv.columns]\n",
    "    z_scores = df_ohlcv.groupby(level='Ticker', group_keys=False).apply(\n",
    "        calculate_rolling_z_scores_general, \n",
    "        columns_to_process=z_score_columns_exist,\n",
    "        rolling_window=config['rolling_window']\n",
    "    )\n",
    "    \n",
    "    features = trends.join(z_scores).dropna()\n",
    "\n",
    "    # --- 2. DYNAMIC FILTERING (KEY CHANGE HERE) ---\n",
    "    print(\"Applying dynamic strategy rules...\")\n",
    "    # Delegate the filtering logic to our new, specialized function\n",
    "    signal_mask = apply_strategy_rules(features, rules, config)\n",
    "    \n",
    "    signals = features[signal_mask]\n",
    "\n",
    "######################################    \n",
    "    # return signals\n",
    "    return signals, trends, features, df_ohlcv\n",
    "######################################    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Define your configuration for thresholds ---\n",
    "config = {\n",
    "    'lookback_days': 30,\n",
    "    'rolling_window': 20,\n",
    "    'r2_thresh': 0.50,\n",
    "    'z_entry_thresh': -1.5,\n",
    "    # You can add new thresholds here anytime\n",
    "    'min_penalty_score': 0.01 \n",
    "}\n",
    "\n",
    "# --- 2. Define your strategy as a list of rules ---\n",
    "# This is where you can be creative and easily swap rules in and out.\n",
    "strategy_1_rules = [\n",
    "    # Static value comparison: volume trend must be positive\n",
    "    {'column': 'volume_slope', 'operator': '>', 'value': 0},\n",
    "    \n",
    "    # Dynamic threshold from config: R-squared must be high enough\n",
    "    {'column': 'low_r_squared', 'operator': '>', 'value_from_config': 'r2_thresh'},\n",
    "    \n",
    "    # Dynamic threshold from config: Look for a price dip\n",
    "    {'column': 'z_score_Adj Low', 'operator': '<', 'value_from_config': 'z_entry_thresh'},\n",
    "    \n",
    "    # Column-vs-column comparison: Highs are trending up faster than lows\n",
    "    {'column_A': 'high_slope', 'operator': '>', 'column_B': 'low_slope'}\n",
    "]\n",
    "\n",
    "# --- 3. Run the pre-computation with your chosen strategy ---\n",
    "# To test a new strategy, you would just pass a different list of rules!\n",
    "signals_df = precompute_signals(df_train, config, rules=strategy_1_rules)\n",
    "\n",
    "print(\"\\n--- Strategy Results ---\")\n",
    "print(f\"Found {len(signals_df)} signals using the dynamic rules.\")\n",
    "print(signals_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Z-scores for columns: ['Adj Open', 'Adj High', 'Adj Low', 'Adj Close', 'Volume']\n",
      "--- Test 1: Inspecting the full 'features' DataFrame ---\n",
      "Shape of the features DataFrame: (40, 22)\n",
      "Note: The first (lookback_days - 1) rows should be missing due to NaNs.\n",
      "The ticker 'SHORT' should not appear at all.\n",
      "\n",
      "Head of features:\n",
      "                   open_slope  open_r_squared  high_slope  high_r_squared  low_slope  low_r_squared  close_slope  close_r_squared  volume_slope  volume_r_squared  unified_std_dev_returns  volume_std_dev_returns  open_penalty_score  high_penalty_score  low_penalty_score  close_penalty_score  volume_penalty_score  z_score_Adj Open  z_score_Adj High  z_score_Adj Low  z_score_Adj Close  z_score_Volume\n",
      "Ticker Date                                                                                                                                                                                                                                                                                                                                                                                                     \n",
      "TEST   2023-01-16   -0.000587        0.064803   -0.000585        0.064806  -0.000596       0.064794    -0.000590         0.064800      0.009666          0.999850                 0.006645                0.000274            0.006214            0.006214           0.006214             0.006214          4.099047e-08          0.145474          0.145474         0.145474           0.145474        1.264911\n",
      "       2023-01-17   -0.000934        0.208933   -0.000929        0.208933  -0.000947       0.208935    -0.000938         0.208934      0.009574          0.999853                 0.005835                0.000269            0.004616            0.004616           0.004616             0.004616          3.943599e-08          1.649108          1.649108         1.649108           1.649108        1.264911\n",
      "       2023-01-18   -0.000085        0.001011   -0.000085        0.001010  -0.000087       0.001013    -0.000086         0.001011      0.009483          0.999856                 0.005961                0.000263            0.005955            0.005955           0.005955             0.005955          3.795460e-08          1.555584          1.555584         1.555584           1.555584        1.264911\n",
      "       2023-01-19    0.001735        0.189329    0.001727        0.189342   0.001761       0.189292     0.001744         0.189317      0.009394          0.999859                 0.006730                0.000258            0.005456            0.005456           0.005456             0.005456          3.654222e-08          1.417468          1.417468         1.417468           1.417468        1.264911\n",
      "       2023-01-20    0.004073        0.522997    0.004054        0.523005   0.004133       0.522972     0.004093         0.522989      0.009306          0.999861                 0.007284                0.000254            0.003474            0.003474           0.003474             0.003474          3.519501e-08          1.303355          1.303355         1.303355           1.303355        1.264911\n",
      "\n",
      "Tail of features:\n",
      "                   open_slope  open_r_squared  high_slope  high_r_squared  low_slope  low_r_squared  close_slope  close_r_squared  volume_slope  volume_r_squared  unified_std_dev_returns  volume_std_dev_returns  open_penalty_score  high_penalty_score  low_penalty_score  close_penalty_score  volume_penalty_score  z_score_Adj Open  z_score_Adj High  z_score_Adj Low  z_score_Adj Close  z_score_Volume\n",
      "Ticker Date                                                                                                                                                                                                                                                                                                                                                                                                     \n",
      "TEST   2023-03-06    0.004240        0.602378    0.004222        0.602374   0.004296       0.602388     0.004259         0.602381      0.007222          0.999916                 0.006703                0.000152            0.002665            0.002665           0.002665             0.002665          1.273438e-08         -1.471663         -1.471663        -1.471663          -1.471663        1.264911\n",
      "       2023-03-07    0.002090        0.284036    0.002081        0.284026   0.002117       0.284066     0.002099         0.284046      0.007170          0.999917                 0.006891                0.000150            0.004934            0.004934           0.004933             0.004933          1.237280e-08         -1.188063         -1.188063        -1.188063          -1.188063        1.264911\n",
      "       2023-03-08    0.000294        0.012688    0.000293        0.012686   0.000298       0.012696     0.000295         0.012691      0.007119          0.999919                 0.006364                0.000148            0.006283            0.006283           0.006283             0.006283          1.202400e-08         -0.732233         -0.732233        -0.732233          -0.732233        1.264911\n",
      "       2023-03-09   -0.000713        0.147832   -0.000710        0.147833  -0.000722       0.147827    -0.000716         0.147830      0.007069          0.999920                 0.005482                0.000146            0.004672            0.004672           0.004672             0.004672          1.168745e-08          0.815601          0.815601         0.815601           0.815601        1.264911\n",
      "       2023-03-10   -0.000695        0.138679   -0.000692        0.138677  -0.000704       0.138683    -0.000698         0.138680      0.007019          0.999921                 0.004997                0.000144            0.004304            0.004304           0.004304             0.004304          1.136264e-08          1.655423          1.655423         1.655423           1.655423        1.264911\n",
      "\n",
      "[SUCCESS]: Ticker with insufficient data was correctly ignored.\n",
      "[SUCCESS]: Features DataFrame contains no NaNs.\n",
      "\n",
      "--- Sanity Check Passed for Feature Generation ---\n"
     ]
    }
   ],
   "source": [
    "# --- Test Configuration ---\n",
    "test_config = {\n",
    "    'lookback_days': 10,\n",
    "    'rolling_window': 5,\n",
    "    'slope_thresh': 0, # Set low to ensure we get some signals\n",
    "    'r2_thresh': 0.3,\n",
    "    'z_entry_thresh': -0.5,\n",
    "    'volume_thresh': 0,\n",
    "}\n",
    "\n",
    "# --- Run the special testing function ---\n",
    "all_features, signals_from_test_func, trends = precompute_signals_for_testing(sample_df, test_config)\n",
    "\n",
    "\n",
    "print(\"--- Test 1: Inspecting the full 'features' DataFrame ---\")\n",
    "print(f\"Shape of the features DataFrame: {all_features.shape}\")\n",
    "print(\"Note: The first (lookback_days - 1) rows should be missing due to NaNs.\")\n",
    "print(\"The ticker 'SHORT' should not appear at all.\\n\")\n",
    "\n",
    "# Display the head and tail to check values\n",
    "print(\"Head of features:\")\n",
    "print(all_features.head())\n",
    "print(\"\\nTail of features:\")\n",
    "print(all_features.tail())\n",
    "\n",
    "# --- Verification Checks for 'features' ---\n",
    "assert 'SHORT' not in all_features.index.get_level_values('Ticker'), \"FAIL: Ticker with insufficient data was not filtered out.\"\n",
    "print(\"\\n[SUCCESS]: Ticker with insufficient data was correctly ignored.\")\n",
    "\n",
    "assert not all_features.isnull().values.any(), \"FAIL: The features DataFrame contains unexpected NaNs after dropna().\"\n",
    "print(\"[SUCCESS]: Features DataFrame contains no NaNs.\")\n",
    "\n",
    "print(\"\\n--- Sanity Check Passed for Feature Generation ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df.to_csv(r'C:\\Users\\ping\\Desktop\\sample_df.csv', index=True)\n",
    "trends.to_csv(r'C:\\Users\\ping\\Desktop\\trends.csv', index=True)\n",
    "all_features.to_csv(r'C:\\Users\\ping\\Desktop\\all_features.csv', index=True)\n",
    "signals_from_test_func.to_csv(r'C:\\Users\\ping\\Desktop\\signals_from_test_func.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signals = features[\n",
    "    (features['low_slope'] > config['slope_thresh']) &\n",
    "    (features['low_r_squared'] > config['r2_thresh']) &\n",
    "    (features['volume_slope'] > 0) &\n",
    "    (features['z_score_Adj Low'] < config['z_entry_thresh'])\n",
    "]\n",
    "\n",
    "test_config = {\n",
    "    'lookback_days': 10,\n",
    "    'rolling_window': 5,\n",
    "    'slope_thresh': 0.005, # Set low to ensure we get some signals\n",
    "    'r2_thresh': 0.3,\n",
    "    'z_entry_thresh': -0.5,\n",
    "    'volume_thresh': 0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'trends:\\n{trends}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'sample_df:\\n{sample_df}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: \"Black-Box\" Test - Verify the Final `signals` DataFrame\n",
    "\n",
    "Now we test the real function. Our goal is to prove that **every single row** in the final `signals_df` meets the filtering criteria defined in our `test_config`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Run the REAL function to get the final output ---\n",
    "signals_df = precompute_signals(sample_df, test_config)\n",
    "\n",
    "print(f\"\\n--- Test 2: Verifying the final 'signals' DataFrame ---\")\n",
    "print(f\"Found {len(signals_df)} potential signals to verify.\")\n",
    "\n",
    "# --- Programmatic Verification ---\n",
    "# For every signal found, we cross-reference it with the 'all_features' DataFrame\n",
    "# and assert that its values meet the criteria.\n",
    "\n",
    "for idx, signal_row in signals_df.iterrows():\n",
    "    # Find the original, unfiltered features for this specific signal\n",
    "    original_features = all_features.loc[idx]\n",
    "\n",
    "    # Assert that each condition is met\n",
    "    try:\n",
    "        assert original_features['low_slope'] > test_config['slope_thresh']\n",
    "        assert original_features['low_r_squared'] > test_config['r2_thresh']\n",
    "        assert original_features['volume_slope'] > test_config['volume_thresh']\n",
    "        assert original_features['z_score_Adj Low'] < test_config['z_entry_thresh']\n",
    "    except AssertionError as e:\n",
    "        print(f\"\\n[FAILURE]: Verification failed for signal at index {idx}!\")\n",
    "        print(\"Signal Row:\")\n",
    "        print(signal_row)\n",
    "        print(\"\\nThresholds:\")\n",
    "        print(test_config)\n",
    "        raise e\n",
    "\n",
    "if not signals_df.empty:\n",
    "    print(\"\\n[SUCCESS]: All rows in the final signals DataFrame correctly meet the filter criteria.\")\n",
    "else:\n",
    "    print(\"\\n[INFO]: No signals were generated with this config, test passed vacuously.\")\n",
    "\n",
    "print(\"\\n--- Sanity Check Passed for Signal Filtering ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'signals_df:\\n{signals_df}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# penalty_scores are not calculated???\n",
    "* penalty_score = (1 - r_squared) * (unified_std_dev + 1e-9)  \n",
    "* volume_penalty_score = (1 - volume_r_squared) * (volume_std_dev + 1e-9)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signals_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: New Encapsulated Helper Functions\n",
    "\n",
    "These new functions isolate the logic for performance analysis and the optimization loop itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_performance(trade_results):\n",
    "    \"\"\"\n",
    "    Calculates performance metrics from a DataFrame of trades.\n",
    "    \n",
    "    Returns a dictionary of key metrics.\n",
    "    \"\"\"\n",
    "    if trade_results.empty:\n",
    "        return {'num_trades': 0, 'win_rate': 0, 'avg_return': 0, 'total_return': 0}\n",
    "    \n",
    "    win_rate = (trade_results['return'] > 0).mean()\n",
    "    total_return = (1 + trade_results['return']).prod() - 1\n",
    "    avg_return = trade_results['return'].mean()\n",
    "    \n",
    "    return {\n",
    "        'num_trades': len(trade_results),\n",
    "        'win_rate': win_rate,\n",
    "        'avg_return': avg_return,\n",
    "        'total_return': total_return\n",
    "    }\n",
    "\n",
    "def run_parameter_optimization(df, param_grid, static_params):\n",
    "    \"\"\"\n",
    "    Orchestrates the entire parameter optimization process.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The OHLCV data.\n",
    "        param_grid (dict): Dictionary with lists of parameters to test.\n",
    "        static_params (dict): Dictionary of parameters that are not being optimized.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A summary of results for each parameter combination.\n",
    "    \"\"\"\n",
    "    results_log = []\n",
    "    \n",
    "    # Use itertools.product to create a clean generator for all combinations\n",
    "    keys, values = zip(*param_grid.items())\n",
    "    param_combinations = [dict(zip(keys, v)) for v in product(*values)]\n",
    "\n",
    "    print(f\"Starting optimization for {len(param_combinations)} combinations...\")\n",
    "    \n",
    "    for param_set in tqdm(param_combinations, desc=\"Optimization Progress\"):\n",
    "        # Combine static and dynamic parameters into a single config for this run\n",
    "        current_config = {**static_params, **param_set}\n",
    "        \n",
    "        # 1. Run the backtest with the current configuration\n",
    "        trade_results = run_backtest(df, current_config)\n",
    "        \n",
    "        # 2. Analyze the performance of this run\n",
    "        performance_metrics = analyze_performance(trade_results)\n",
    "        \n",
    "        # 3. Log the results\n",
    "        log_entry = {**param_set, **performance_metrics}\n",
    "        results_log.append(log_entry)\n",
    "        \n",
    "    return pd.DataFrame(results_log)\n",
    "\n",
    "def handle_entries_for_day(current_date, next_day_date, signals_today, open_positions, df_ohlcv):\n",
    "    \"\"\"\n",
    "    Processes entries and stores signal details in the open_positions dict.\n",
    "    \"\"\"\n",
    "    # --- KEY CHANGE: Loop through the signals DataFrame ---\n",
    "    for ticker, signal_row in signals_today.iterrows():\n",
    "        # The ticker is now in the index of signal_row, so we use its name\n",
    "        ticker_name = ticker[0] \n",
    "        \n",
    "        if ticker_name not in open_positions:\n",
    "            try:\n",
    "                entry_price = df_ohlcv.loc[(ticker_name, next_day_date), 'Adj High']\n",
    "                \n",
    "                # --- LOGGING: Store more info about the entry signal ---\n",
    "                open_positions[ticker_name] = {\n",
    "                    'entry_date': next_day_date,\n",
    "                    'entry_price': entry_price,\n",
    "                    'signal_date': current_date,\n",
    "                    'signal_features': signal_row.to_dict() # Store all features that triggered the signal\n",
    "                }\n",
    "            except KeyError:\n",
    "                pass\n",
    "                \n",
    "    return open_positions\n",
    "\n",
    "def handle_exits_for_day(current_date, next_day_date, open_positions, df_ohlcv, config):\n",
    "    \"\"\"\n",
    "    Checks for exits and logs detailed information about the exit trigger.\n",
    "    Corrected version with valid syntax for the if/elif chain.\n",
    "    \"\"\"\n",
    "    closed_trades = []\n",
    "    positions_to_close = []\n",
    "\n",
    "    for ticker, pos in open_positions.items():\n",
    "        try:\n",
    "            current_close_price = df_ohlcv.loc[(ticker, current_date), 'Adj Close']\n",
    "        except KeyError:\n",
    "            continue \n",
    "\n",
    "        exit_reason = None\n",
    "        exit_target_value = None \n",
    "        \n",
    "        # --- SYNTAX FIX: Calculate all threshold values *before* the conditional block ---\n",
    "        profit_target_price = pos['entry_price'] * (1 + config['profit_target'])\n",
    "        stop_loss_price = pos['entry_price'] * (1 - config['stop_loss'])\n",
    "        days_held = (current_date.to_pydatetime().date() - pos['entry_date'].to_pydatetime().date()).days\n",
    "        \n",
    "        # --- Now, check conditions in a contiguous if/elif/elif block ---\n",
    "        if current_close_price >= profit_target_price:\n",
    "            exit_reason = \"Profit Target\"\n",
    "            exit_target_value = profit_target_price \n",
    "\n",
    "        elif current_close_price <= stop_loss_price:\n",
    "            exit_reason = \"Stop-Loss\"\n",
    "            exit_target_value = stop_loss_price \n",
    "\n",
    "        elif days_held >= config['time_hold_days']:\n",
    "            exit_reason = \"Time Hold\"\n",
    "            exit_target_value = days_held \n",
    "\n",
    "        if exit_reason:\n",
    "            try:\n",
    "                exit_price = df_ohlcv.loc[(ticker, next_day_date), 'Adj Low']\n",
    "                trade_return = (exit_price - pos['entry_price']) / pos['entry_price']\n",
    "                \n",
    "                trade_log = {\n",
    "                    'ticker': ticker, \n",
    "                    'entry_date': pos['entry_date'], \n",
    "                    'exit_date': next_day_date,\n",
    "                    'return': trade_return, \n",
    "                    'reason': exit_reason,\n",
    "                    'signal_date': pos['signal_date'],\n",
    "                    'entry_signal_features': pos['signal_features'],\n",
    "                    'entry_price_actual': pos['entry_price'],\n",
    "                    'exit_signal_date': current_date,\n",
    "                    'exit_trigger_price': current_close_price,\n",
    "                    'exit_target_value': exit_target_value,\n",
    "                    'exit_price_actual': exit_price,\n",
    "                }\n",
    "                closed_trades.append(trade_log)\n",
    "                positions_to_close.append(ticker)\n",
    "            except KeyError:\n",
    "                pass\n",
    "                \n",
    "    for ticker in positions_to_close:\n",
    "        del open_positions[ticker]\n",
    "        \n",
    "    return closed_trades, open_positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: The New, Clean Top-Level Script\n",
    "\n",
    "Your main script is now incredibly simple and readable. It's all about configuration and orchestration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. DEFINE CONFIGURATION ---\n",
    "\n",
    "# Parameters to be optimized, defining the search space\n",
    "optimization_grid = {\n",
    "    'lookback_days': [30, 60, 90],\n",
    "    'rolling_window': [15, 20]\n",
    "}\n",
    "\n",
    "# Static strategy parameters that do not change during optimization\n",
    "strategy_params = {\n",
    "    'slope_thresh': 1.0,\n",
    "    'r2_thresh': 0.50,\n",
    "    'z_entry_thresh': 0,\n",
    "    'profit_target': 0.10,\n",
    "    'stop_loss': 0.05,\n",
    "    'time_hold_days': 20\n",
    "}\n",
    "\n",
    "\n",
    "# --- 2. RUN ORCHESTRATOR ---\n",
    "\n",
    "# The main call is now a single, descriptive function\n",
    "optimization_results = run_parameter_optimization(\n",
    "    df_train, optimization_grid, strategy_params\n",
    ")\n",
    "\n",
    "\n",
    "# --- 3. ANALYZE RESULTS ---\n",
    "\n",
    "print(\"\\n\\n--- Optimization Complete ---\")\n",
    "print(optimization_results.sort_values(by='total_return', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: The \"One-Trade\" Deep Dive\n",
    "\n",
    "The most powerful debugging technique is to isolate a single trade and follow it from signal generation to exit. If the logic holds for one trade, it's likely correct for all of them.\n",
    "\n",
    "1.  **Pick a Winning Trade and a Losing Trade:** Run one of the backtests again (e.g., the one with `lookback=30`, `rolling=20`) and save the `trade_results` DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Pick a configuration to analyze\n",
    "config_to_test = {\n",
    "    **strategy_params, \n",
    "    'lookback_days': 30, \n",
    "    'rolling_window': 20\n",
    "}\n",
    "\n",
    "# 2. Run the backtest to get the detailed trade log\n",
    "trade_log_df = run_backtest(df_train, config_to_test)\n",
    "\n",
    "# 3. Isolate and inspect a single trade\n",
    "if not trade_log_df.empty:\n",
    "    # Get the first losing trade\n",
    "    losing_trade = trade_log_df[trade_log_df['return'] < 0].iloc[0]\n",
    "\n",
    "    print(\"--- Detailed Log for a Single Losing Trade ---\")\n",
    "    # Using .T transposes the Series for easy vertical reading\n",
    "    print(losing_trade.T)\n",
    "else:\n",
    "    print(\"No trades were made for this configuration.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losing_trade.entry_signal_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df_trends = analyze_ticker_trends_vectorized(df_train, lookback_days=30)\n",
    "print(f'_df_trends:\\n{_df_trends}')\n",
    "# _df_trends.to_csv('C:\\\\Users\\\\ping\\\\Desktop\\\\_df_trends.csv', index=True)\n",
    "_df_trends.index.names = ['Ticker', 'Date']\n",
    "_df_trends.reset_index().to_csv(r'C:\\Users\\ping\\Desktop\\_df_trends.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.loc['MSFT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to CSV\n",
    "_df_trends.to_csv(r'C:\\Users\\ping\\Desktop\\_df_trends.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df_trends.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loss = trade_log_df[trade_log_df['return'] < 0]\n",
    "print(df_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_win = trade_log_df[trade_log_df['return'] > 0]\n",
    "print(df_win)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loss.to_csv('C:\\\\Users\\\\ping\\\\Desktop\\\\df_loss.csv', index=True)\n",
    "df_win.to_csv('C:\\\\Users\\\\ping\\\\Desktop\\\\df_win.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimization_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a configuration to analyze\n",
    "config_to_test = {**strategy_params, 'lookback_days': 30, 'rolling_window': 20}\n",
    "\n",
    "# Run a single backtest and get the detailed trade log\n",
    "single_run_trades = run_backtest(df_train, config_to_test)\n",
    "\n",
    "# Find a winning and a losing trade to investigate\n",
    "print(\"Sample winning trade:\")\n",
    "print(single_run_trades[single_run_trades['return'] > 0].head(1))\n",
    "\n",
    "print(\"\\nSample losing trade:\")\n",
    "print(single_run_trades[single_run_trades['return'] < 0].head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trends = analyze_ticker_trends_vectorized(df_train, lookback_days=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trends.loc['FIX', '2024-10-25']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
