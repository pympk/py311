{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and Configuration\n",
    "\n",
    "This cell contains all imports and user-configurable parameters for the analysis pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Path Configuration ---\n",
      "✅ Project Root: c:\\Users\\ping\\Files_win10\\python\\py311\\stocks\n",
      "✅ Data Dir:     c:\\Users\\ping\\Files_win10\\python\\py311\\stocks\\data\n",
      "✅ Source Dir:   c:\\Users\\ping\\Files_win10\\python\\py311\\stocks\\src\n",
      "\n",
      "--- Module Verification ---\n",
      "✅ Successfully imported 'utils' and 'plotting_utils'.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pprint\n",
    "import matplotlib.pyplot as plt # Import for plotting\n",
    "from IPython.display import display, Markdown\n",
    "from scipy.stats import linregress \n",
    "\n",
    "# --- 1. PANDAS & IPYTHON OPTIONS ---\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 3000)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# --- 2. PROJECT PATH CONFIGURATION ---\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "ROOT_DIR = NOTEBOOK_DIR.parent.parent  # Adjust if your notebook is in a 'notebooks' subdirectory\n",
    "DATA_DIR = ROOT_DIR / 'data'\n",
    "SRC_DIR = ROOT_DIR / 'src'\n",
    "\n",
    "# Add 'src' to the Python path to import custom modules\n",
    "if str(SRC_DIR) not in sys.path:\n",
    "    sys.path.append(str(SRC_DIR))\n",
    "\n",
    "# --- 3. IMPORT CUSTOM MODULES ---\n",
    "import utils\n",
    "import plotting_utils\n",
    "\n",
    "# --- 4. ANALYSIS & FILTERING CONFIGURATION ---\n",
    "\n",
    "# File searching parameters\n",
    "# FILE_PREFIX = ''  # e.g., '2024'\n",
    "FILE_CONTAINS_PATTERN = 'df_OHLCV_clean_stocks_etfs'\n",
    "\n",
    "# # Parameters defining the time windows for metric calculation\n",
    "PERIOD_PARAMS = {\n",
    "    'lookback_days': 22,\n",
    "    'recent_days': 0,\n",
    "}\n",
    "\n",
    "# This is not use for filtering, it's use to calculate metrics in SORT_ORDER\n",
    "# Parameters for filtering the calculated metrics to find candidates\n",
    "METRIC_FILTERS = {\n",
    "    'min_lookback_improvement': 0,\n",
    "    'current_rank_bracket_start': 1,\n",
    "    'current_rank_bracket_end': 1000,\n",
    "    # --- Select ONE mode by commenting out the others ---\n",
    "    # 'Reversal' Mode\n",
    "    'min_recent_bottom_to_recent_start': 0,\n",
    "    'min_recent_bottom_to_current': 0,    \n",
    "    # 'Dip' Mode\n",
    "    # 'min_current_to_recent_start': 10,\n",
    "}\n",
    "\n",
    "# --- 5. VERIFICATION ---\n",
    "print(\"--- Path Configuration ---\")\n",
    "print(f\"✅ Project Root: {ROOT_DIR}\")\n",
    "print(f\"✅ Data Dir:     {DATA_DIR}\")\n",
    "print(f\"✅ Source Dir:   {SRC_DIR}\")\n",
    "assert all([ROOT_DIR.exists(), DATA_DIR.exists(), SRC_DIR.exists()]), \"A key directory was not found!\"\n",
    "\n",
    "print(\"\\n--- Module Verification ---\")\n",
    "print(f\"✅ Successfully imported 'utils' and 'plotting_utils'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Loading latest consolidated Finviz data ---\n",
      "Info: Index is unnamed. Assuming it contains tickers and assigning the name 'Ticker'.\n",
      "✅ Successfully loaded: 2025-09-12_df_finviz_merged_stocks_etfs.parquet\n",
      "Shape: (1463, 139)\n",
      "        No.                Company               Index      Sector                   Industry Country Exchange                                   Info  MktCap AUM, M  Rank  Market Cap, M    P/E  Fwd P/E   PEG    P/S    P/B    P/C  P/FCF  Book/sh  Cash/sh  Dividend %  Dividend TTM Dividend Ex Date  Payout Ratio %    EPS  EPS next Q  EPS this Y %  EPS next Y %  EPS past 5Y %  EPS next 5Y %  Sales past 5Y %  Sales Q/Q %  EPS Q/Q %  EPS YoY TTM %  Sales YoY TTM %  Sales, M  Income, M  EPS Surprise %  Revenue Surprise %  Outstanding, M  Float, M  Float %  Insider Own %  Insider Trans %  Inst Own %  Inst Trans %  Short Float %  Short Ratio  Short Interest, M  ROA %   ROE %  ROIC %  Curr R  Quick R  LTDebt/Eq  Debt/Eq  Gross M %  Oper M %  Profit M %  Perf 3D %  Perf Week %  Perf Month %  Perf Quart %  Perf Half %  Perf Year %  Perf YTD %  Beta   ATR  ATR/Price %  Volatility W %  Volatility M %  SMA20 %  SMA50 %  SMA200 %  50D High %  50D Low %  52W High %  52W Low %        52W Range  All-Time High %  All-Time Low %    RSI  Earnings    IPO Date Optionable Shortable  Employees  Change from Open %  Gap %  Recom  Avg Volume, M  Rel Volume     Volume  Target Price  Prev Close    Open    High     Low   Price  Change % Single Category Asset Type  Expense %  Holdings  AUM, M  Flows 1M, M  Flows% 1M  Flows 3M, M  Flows% 3M  Flows YTD, M  Flows% YTD  Return% 1Y  Return% 3Y  Return% 5Y Tags   Sharpe 3d   Sortino 3d    Omega 3d  Sharpe 5d   Sortino 5d    Omega 5d  Sharpe 10d  Sortino 10d  Omega 10d  Sharpe 15d  Sortino 15d  Omega 15d  Sharpe 30d  Sortino 30d  Omega 30d  Sharpe 60d  Sortino 60d  Omega 60d  Sharpe 120d  Sortino 120d  Omega 120d  Sharpe 250d  Sortino 250d  Omega 250d\n",
      "Ticker                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
      "NVDA      1            NVIDIA Corp  DJIA, NDX, S&P 500  Technology             Semiconductors     USA     NASD             Technology, Semiconductors      4321030.0     1      4321030.0  50.61    27.89  1.45  26.15  43.24  76.09  60.00     4.11     2.34        0.02          0.04        9/11/2025            1.16   3.51        1.24           NaN           NaN            NaN            NaN              NaN        55.60      61.23          64.54            71.55  165220.0    86600.0            4.13                1.51         24350.0   23300.0    95.72           4.08            -0.80       67.99          1.73           0.84         1.13             196.70  76.65  109.42   78.42    4.21     3.60       0.10     0.11      69.85     58.09       52.41   4.140556         6.47         -2.08         22.63        53.85        64.50       32.41  2.13  4.84     2.721854            2.05            2.58     1.16     2.12     27.04       -3.61      16.25       -3.61     105.29   86.62 - 184.48            -3.61       533359.99  55.51  Aug 27/a   1/22/1999        Yes       Yes    36000.0               -0.07   0.43   1.35         174.66        0.71  123525769        217.50      177.17  177.94  178.60  176.45  177.82      0.37                                   NaN       NaN     NaN          NaN        NaN          NaN        NaN           NaN         NaN         NaN         NaN         NaN    -    6.229198    27.992689    3.493787  12.468970   436.235684   55.960530    1.936209     3.269837   1.423951   -0.060206    -0.085309   0.989437    0.747411     1.080169   1.139999    3.145329     5.218871   1.688755     1.867452      3.167447    1.441991     0.984842      1.415286    1.194336\n",
      "MSFT      2  Microsoft Corporation  DJIA, NDX, S&P 500  Technology  Software - Infrastructure     USA     NASD  Technology, Software - Infrastructure      3790170.0     2      3790170.0  37.38    27.87  2.25  13.45  11.04  40.08  52.93    46.20    12.72        0.68          3.32        8/21/2025           24.34  13.64        3.65           NaN           NaN            NaN            NaN              NaN        18.10      23.77          15.50            14.93  281720.0   101830.0            8.16                3.40          7430.0    7320.0    98.51           1.48            -0.26       73.31         -0.34           0.80         2.89              58.49  18.00   33.28   22.93    1.35     1.35       0.29     0.33      68.82     45.62       36.15   2.305331         3.01         -2.05          6.48        34.62        23.10       20.97  1.03  7.94     1.557168            1.25            1.39     0.82    -0.10     14.29       -8.20       4.34       -8.20      47.89  344.79 - 555.45            -8.20       639845.66  53.36  Jul 30/a   3/13/1986        Yes       Yes   228000.0                0.70   1.07   1.20          20.26        1.16   23597200        624.36      501.01  506.35  512.55  503.85  509.90      1.77                                   NaN       NaN     NaN          NaN        NaN          NaN        NaN           NaN         NaN         NaN         NaN         NaN    -   12.752518  3589.625439  320.789251  11.180249  5500.580410  694.007992    0.831814     1.107777   1.187282    0.421006     0.563132   1.080631   -1.562981    -2.083460   0.769051    1.429824     2.381983   1.289158     2.073508      4.119370    1.548125     0.682032      1.058333    1.142387\n",
      "AAPL      3              Apple Inc  DJIA, NDX, S&P 500  Technology       Consumer Electronics     USA     NASD       Technology, Consumer Electronics      3473690.0     3      3473690.0  35.58    29.35  4.03   8.50  52.83  62.73  36.12     4.43     3.73        0.44          1.02        8/11/2025           16.11   6.58        1.76           NaN           NaN            NaN            NaN              NaN         9.63      12.19           0.15             5.97  408620.0    99280.0            9.18                4.99         14860.0   14820.0    99.79           0.10            -2.09       64.55          1.15           0.77         2.08             113.58  29.94  149.81   66.96    0.87     0.83       1.25     1.54      46.68     31.87       24.30  -0.119479        -2.34          0.32         17.51        11.63         6.34       -6.53  1.08  4.74     2.025035            2.17            1.72     1.02     5.85      5.67       -3.00      16.16      -10.01      38.33  169.21 - 260.10           -10.01       367841.99  57.15  Jul 31/a  12/12/1980        Yes       Yes   164000.0                2.11  -0.34   2.08          54.68        1.01   55316953        242.54      230.03  229.24  234.51  229.02  234.07      1.76                                   NaN       NaN     NaN          NaN        NaN          NaN        NaN           NaN         NaN         NaN         NaN         NaN    -  108.022218  3589.625439  320.789251  -2.636360    -3.529707    0.664999    0.730838     1.189589   1.129122    1.842245     2.988792   1.379018    4.409409     9.676153   2.232092    3.118618     6.039111   1.772434     0.418330      0.651481    1.091682     0.209388      0.308779    1.042199\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Step 1: Loading latest consolidated Finviz data ---\")\n",
    "\n",
    "# Find the most recent file matching the pattern\n",
    "# This function is now understood to return List[str] (filenames), not List[Path].\n",
    "latest_finviz_filepaths = utils.get_recent_files(\n",
    "    directory_path=DATA_DIR,\n",
    "    extension='parquet',\n",
    "    prefix='202',\n",
    "    contains_pattern='df_finviz_merged_stocks_etfs',\n",
    "    count=1\n",
    ")\n",
    "\n",
    "if not latest_finviz_filepaths:\n",
    "    raise FileNotFoundError(f\"No files found in '{DATA_DIR}' with prefix '{FILE_PREFIX}' and pattern '{FILE_CONTAINS_PATTERN}'\")\n",
    "\n",
    "# Get the filename string from the list\n",
    "latest_filename = latest_finviz_filepaths[0]\n",
    "\n",
    "# Manually construct the full path before loading\n",
    "full_file_path = DATA_DIR / latest_filename\n",
    "df_finviz_latest = pd.read_parquet(full_file_path, engine='pyarrow')\n",
    "\n",
    "\n",
    "# --- Robust Index Setting (this logic remains correct) ---\n",
    "if df_finviz_latest.index.name == 'Ticker':\n",
    "    print(\"Info: 'Ticker' is already the index. No action needed.\")\n",
    "elif 'Ticker' in df_finviz_latest.columns:\n",
    "    print(\"Info: 'Ticker' column found. Setting it as the DataFrame index.\")\n",
    "    df_finviz_latest.set_index('Ticker', inplace=True)\n",
    "elif 'ticker' in df_finviz_latest.columns:\n",
    "    print(\"Info: 'ticker' column found. Renaming and setting as index.\")\n",
    "    df_finviz_latest.rename(columns={'ticker': 'Ticker'}, inplace=True)\n",
    "    df_finviz_latest.set_index('Ticker', inplace=True)\n",
    "elif df_finviz_latest.index.name is None:\n",
    "    print(\"Info: Index is unnamed. Assuming it contains tickers and assigning the name 'Ticker'.\")\n",
    "    df_finviz_latest.index.name = 'Ticker'\n",
    "else:\n",
    "    print(\"ERROR: Loaded DataFrame has an unexpected format.\")\n",
    "    print(f\"Columns: {df_finviz_latest.columns.tolist()}\")\n",
    "    print(f\"Index Name: '{df_finviz_latest.index.name}'\")\n",
    "    raise ValueError(\"Could not find a 'Ticker' column or a usable index to proceed.\")\n",
    "\n",
    "\n",
    "# Correct the print statement to work with the filename string\n",
    "print(f\"✅ Successfully loaded: {latest_filename}\")\n",
    "print(f\"Shape: {df_finviz_latest.shape}\")\n",
    "print(df_finviz_latest.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# DATA_DIR = r'c:\\Users\\ping\\Files_win10\\python\\py311\\stocks\\data'\n",
    "# Manually construct the full path before loading\n",
    "full_file_path = r'c:\\Users\\ping\\Files_win10\\python\\py311\\stocks\\data\\df_OHLCV_clean_stocks_etfs.parquet'\n",
    "df_OHLCV = pd.read_parquet(full_file_path, engine='pyarrow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 2: The Chronological Split Code\n",
    "\n",
    "This cell contains the logic to find the split date and create the `df_train` and `df_test` DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique trading dates in dataset: 250\n",
      "The data will be split on the date: 2025-05-28\n",
      "\n",
      "--- Verification ---\n",
      "Original DataFrame shape: (371000, 5)\n",
      "Training set shape:   (261184, 5)\n",
      "Testing set shape:    (109816, 5)\n",
      "\n",
      "Date Ranges:\n",
      "  Training: 2024-09-13 to 2025-05-28\n",
      "  Testing:  2025-05-29 to 2025-09-12\n",
      "\n",
      "Verification successful: There is no date overlap between train and test sets.\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Find the Chronological Split Point ---\n",
    "\n",
    "# Get all unique dates from the index and sort them\n",
    "unique_dates = df_OHLCV.index.get_level_values('Date').unique().sort_values()\n",
    "\n",
    "# Determine the index for the 70% split\n",
    "split_index = int(len(unique_dates) * 0.7)\n",
    "\n",
    "# Find the actual date at that split index\n",
    "split_date = unique_dates[split_index]\n",
    "\n",
    "print(f\"Total unique trading dates in dataset: {len(unique_dates)}\")\n",
    "print(f\"The data will be split on the date: {split_date.date()}\")\n",
    "\n",
    "# --- 2. Create the Training and Testing Sets ---\n",
    "\n",
    "# The training set includes all data UP TO and INCLUDING the split_date\n",
    "df_train = df_OHLCV[df_OHLCV.index.get_level_values('Date') <= split_date]\n",
    "\n",
    "# The testing set includes all data AFTER the split_date\n",
    "df_test = df_OHLCV[df_OHLCV.index.get_level_values('Date') > split_date]\n",
    "\n",
    "\n",
    "# --- 3. Verify the Split ---\n",
    "\n",
    "print(\"\\n--- Verification ---\")\n",
    "print(f\"Original DataFrame shape: {df_OHLCV.shape}\")\n",
    "print(f\"Training set shape:   {df_train.shape}\")\n",
    "print(f\"Testing set shape:    {df_test.shape}\")\n",
    "\n",
    "print(\"\\nDate Ranges:\")\n",
    "print(f\"  Training: {df_train.index.get_level_values('Date').min().date()} to {df_train.index.get_level_values('Date').max().date()}\")\n",
    "print(f\"  Testing:  {df_test.index.get_level_values('Date').min().date()} to {df_test.index.get_level_values('Date').max().date()}\")\n",
    "\n",
    "# Final check to ensure no overlap\n",
    "assert df_train.index.get_level_values('Date').max() < df_test.index.get_level_values('Date').min()\n",
    "print(\"\\nVerification successful: There is no date overlap between train and test sets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jupyter Notebook: Verifying `analyze_ticker_trends_vectorized`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cell 1: Setup and Imports\n",
    "\n",
    "First, let's import the necessary libraries and define both versions of the function we want to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verification functions are defined.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import linregress\n",
    "\n",
    "# --- Function 1: The Fast, Vectorized Version (The one we are verifying) ---\n",
    "\n",
    "def analyze_ticker_trends_vectorized(df_group, lookback_days=60):\n",
    "    \"\"\"\n",
    "    Vectorized analysis of trends for a ticker's price and volume.\n",
    "    Calculates linear regression metrics using rolling covariance and variance.\n",
    "    \"\"\"\n",
    "    if len(df_group) < lookback_days:\n",
    "        return None \n",
    "\n",
    "    time_index = pd.Series(np.arange(len(df_group)), index=df_group.index)\n",
    "    var_time = np.var(np.arange(lookback_days), ddof=0)\n",
    "    \n",
    "    series_to_analyze = {\n",
    "        'high': df_group['Adj High'],\n",
    "        'low': df_group['Adj Low'],\n",
    "        'volume': df_group['Volume'].astype(float)\n",
    "    }\n",
    "    \n",
    "    df_results = pd.DataFrame(index=df_group.index)\n",
    "\n",
    "    for name, series in series_to_analyze.items():\n",
    "        rolling_cov = time_index.rolling(window=lookback_days).cov(series, ddof=0)\n",
    "        rolling_var_series = series.rolling(window=lookback_days).var(ddof=0)\n",
    "        \n",
    "        df_results[f'{name}_slope'] = rolling_cov / var_time\n",
    "        denominator = (var_time * rolling_var_series) + 1e-9\n",
    "        df_results[f'{name}_r_squared'] = (rolling_cov**2) / denominator\n",
    "\n",
    "    yesterday_low = df_group['Adj Low'].shift(1)\n",
    "    worst_case_returns = (df_group['Adj High'] - yesterday_low) / yesterday_low\n",
    "    df_results['unified_std_dev_returns'] = worst_case_returns.rolling(window=lookback_days).std(ddof=0)\n",
    "    \n",
    "    volume_std_dev = df_group['Volume'].pct_change().rolling(window=lookback_days).std(ddof=0)\n",
    "    df_results['volume_std_dev_returns'] = volume_std_dev\n",
    "    \n",
    "    df_results['low_penalty_score'] = (1 - df_results['low_r_squared']) * (df_results['unified_std_dev_returns'] + 1e-9)\n",
    "    df_results['high_penalty_score'] = (1 - df_results['high_r_squared']) * (df_results['unified_std_dev_returns'] + 1e-9)\n",
    "    df_results['volume_penalty_score'] = (1 - df_results['volume_r_squared']) * (df_results['volume_std_dev_returns'] + 1e-9)\n",
    "    \n",
    "    return df_results\n",
    "\n",
    "# --- Function 2: The Original, Explicit Version (Our \"Ground Truth\") ---\n",
    "\n",
    "def analyze_ticker_trends_original(df_group, lookback_days=60):\n",
    "    \"\"\"\n",
    "    The original, non-vectorized version. We use this as our ground truth\n",
    "    because its logic is explicit and easy to reason about for a single window.\n",
    "    \"\"\"\n",
    "    \n",
    "    def _perform_price_regression(price_series, time_index):\n",
    "        \"\"\"Helper for High/Low regression.\"\"\"\n",
    "        # NOTE: The original function normalized the price. This is the key difference.\n",
    "        normalized_price = price_series / price_series.iloc[0] \n",
    "        try:\n",
    "            res = linregress(x=time_index, y=normalized_price)\n",
    "            return {'slope': res.slope, 'r_squared': res.rvalue**2}\n",
    "        except (ValueError, ZeroDivisionError):\n",
    "            return {'slope': 0.0, 'r_squared': np.nan}\n",
    "\n",
    "    window = df_group.tail(lookback_days)\n",
    "    if len(window) < lookback_days:\n",
    "        return pd.Series()\n",
    "\n",
    "    time_index = np.arange(len(window))\n",
    "    \n",
    "    # Analyze High/Low\n",
    "    high_metrics = _perform_price_regression(window['Adj High'], time_index)\n",
    "    low_metrics = _perform_price_regression(window['Adj Low'], time_index)\n",
    "\n",
    "    # Analyze Volume\n",
    "    start_volume = window['Volume'].iloc[0]\n",
    "    if start_volume > 0:\n",
    "        normalized_volume = window['Volume'] / start_volume\n",
    "        try:\n",
    "            vol_res = linregress(x=time_index, y=normalized_volume)\n",
    "            volume_slope, volume_r_squared = vol_res.slope, vol_res.rvalue**2\n",
    "        except ValueError:\n",
    "            volume_slope, volume_r_squared = 0.0, np.nan\n",
    "    else:\n",
    "        volume_slope, volume_r_squared = 0.0, np.nan\n",
    "        \n",
    "    final_results = {\n",
    "        'high_slope': high_metrics['slope'], 'high_r_squared': high_metrics['r_squared'],\n",
    "        'low_slope': low_metrics['slope'], 'low_r_squared': low_metrics['r_squared'],\n",
    "        'volume_slope': volume_slope, 'volume_r_squared': volume_r_squared\n",
    "    }\n",
    "    \n",
    "    return pd.Series(final_results)\n",
    "\n",
    "\n",
    "print(\"Verification functions are defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_ticker_trends(df_group, lookback_days=60):\n",
    "    \"\"\"\n",
    "    Analyzes the trends of a ticker's price channel (High/Low) and Volume.\n",
    "    \n",
    "    - Price channel analysis uses a unified \"worst-case\" volatility.\n",
    "    - Volume analysis uses its own standard pct_change volatility.\n",
    "    \"\"\"\n",
    "    \n",
    "    def _perform_price_regression(price_series, time_index, unified_std_dev):\n",
    "        \"\"\"Helper for High/Low regression, using the shared volatility metric.\"\"\"\n",
    "        normalized_price = price_series / price_series.iloc[0]\n",
    "        try:\n",
    "            res = linregress(x=time_index, y=normalized_price)\n",
    "            slope, r_squared = res.slope, res.rvalue**2\n",
    "        except (ValueError, ZeroDivisionError):\n",
    "            return {'slope': 0.0, 'r_squared': np.nan, 'penalty_score': np.nan}\n",
    "        penalty_score = (1 - r_squared) * (unified_std_dev + 1e-9)\n",
    "        return {'slope': slope, 'r_squared': r_squared, 'penalty_score': penalty_score}\n",
    "\n",
    "    # --- Main function logic ---\n",
    "    \n",
    "    # 1. Select the lookback window\n",
    "    window = df_group.tail(lookback_days)\n",
    "    if len(window) < lookback_days or len(window) < 10:\n",
    "        # Define all expected columns for a clean NaN return\n",
    "        cols = ['high_slope', 'high_r_squared', 'high_penalty_score',\n",
    "                'low_slope', 'low_r_squared', 'low_penalty_score',\n",
    "                'unified_std_dev_returns', 'volume_slope', 'volume_r_squared',\n",
    "                'volume_std_dev_returns', 'volume_penalty_score']\n",
    "        return pd.Series(dict.fromkeys(cols, np.nan))\n",
    "\n",
    "    # 2. Calculate the UNIFIED \"Worst-Case\" Volatility for PRICE\n",
    "    yesterday_low = window['Adj Low'].shift(1)\n",
    "    worst_case_returns = (window['Adj High'] - yesterday_low) / yesterday_low\n",
    "    unified_std_dev = worst_case_returns.std(ddof=0) # ddof=0 for population std dev\n",
    "    unified_std_dev = 0.0 if pd.isna(unified_std_dev) else unified_std_dev\n",
    "\n",
    "    # 3. Analyze Volume Trend and Volatility (SELF-CONTAINED)\n",
    "    time_index = np.arange(len(window))\n",
    "    start_volume = window['Volume'].iloc[0]\n",
    "    \n",
    "    if start_volume > 0:\n",
    "        normalized_volume = window['Volume'] / start_volume\n",
    "        try:\n",
    "            vol_res = linregress(x=time_index, y=normalized_volume)\n",
    "            volume_slope, volume_r_squared = vol_res.slope, vol_res.rvalue**2\n",
    "        except ValueError:\n",
    "            volume_slope, volume_r_squared = 0.0, np.nan\n",
    "    else: # Handle zero start volume edge case\n",
    "        volume_slope, volume_r_squared = 0.0, np.nan\n",
    "        \n",
    "    volume_std_dev = window['Volume'].pct_change().std(ddof=0)\n",
    "    volume_std_dev = 0.0 if pd.isna(volume_std_dev) else volume_std_dev\n",
    "    volume_penalty_score = (1 - volume_r_squared) * (volume_std_dev + 1e-9)\n",
    "\n",
    "    # 4. Analyze the High and Low series using the helper\n",
    "    high_metrics = _perform_price_regression(window['Adj High'], time_index, unified_std_dev)\n",
    "    low_metrics = _perform_price_regression(window['Adj Low'], time_index, unified_std_dev)\n",
    "    \n",
    "    # 5. Combine all results into a single Series\n",
    "    final_results = {\n",
    "        'high_slope': high_metrics['slope'], 'high_r_squared': high_metrics['r_squared'], 'high_penalty_score': high_metrics['penalty_score'],\n",
    "        'low_slope': low_metrics['slope'], 'low_r_squared': low_metrics['r_squared'], 'low_penalty_score': low_metrics['penalty_score'],\n",
    "        'unified_std_dev_returns': unified_std_dev,\n",
    "        'volume_slope': volume_slope, 'volume_r_squared': volume_r_squared,\n",
    "        'volume_std_dev_returns': volume_std_dev, 'volume_penalty_score': volume_penalty_score\n",
    "    }\n",
    "    \n",
    "    return pd.Series(final_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cell 2: Create Realistic Sample Data\n",
    "\n",
    "We need some sample data that mimics your real dataset to perform the check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample DataFrame created with shape: (400, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Adj High</th>\n",
       "      <th>Adj Low</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ticker</th>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">A</th>\n",
       "      <th>2023-01-02</th>\n",
       "      <td>98.727599</td>\n",
       "      <td>96.727599</td>\n",
       "      <td>101871.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-03</th>\n",
       "      <td>99.656278</td>\n",
       "      <td>97.656278</td>\n",
       "      <td>98425.668333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-04</th>\n",
       "      <td>100.397974</td>\n",
       "      <td>98.397974</td>\n",
       "      <td>107919.386616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-05</th>\n",
       "      <td>100.609764</td>\n",
       "      <td>98.609764</td>\n",
       "      <td>101937.404133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-06</th>\n",
       "      <td>102.545761</td>\n",
       "      <td>100.545761</td>\n",
       "      <td>103213.366846</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Adj High     Adj Low         Volume\n",
       "Ticker Date                                             \n",
       "A      2023-01-02   98.727599   96.727599  101871.000000\n",
       "       2023-01-03   99.656278   97.656278   98425.668333\n",
       "       2023-01-04  100.397974   98.397974  107919.386616\n",
       "       2023-01-05  100.609764   98.609764  101937.404133\n",
       "       2023-01-06  102.545761  100.545761  103213.366846"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a date range\n",
    "dates = pd.to_datetime(pd.date_range(start='2023-01-01', periods=200, freq='B'))\n",
    "\n",
    "# Create data for Ticker 'A' (upward trend)\n",
    "price_a = 100 + np.linspace(0, 50, 200) + np.random.randn(200) * 2\n",
    "volume_a = 100000 + np.sin(np.arange(200)/10) * 20000 + np.random.randint(-5000, 5000, 200)\n",
    "df_a = pd.DataFrame({\n",
    "    'Ticker': 'A',\n",
    "    'Date': dates,\n",
    "    'Adj High': price_a + 1,\n",
    "    'Adj Low': price_a - 1,\n",
    "    'Volume': volume_a\n",
    "})\n",
    "\n",
    "# Create data for Ticker 'B' (downward trend)\n",
    "price_b = 200 - np.linspace(0, 30, 200) + np.random.randn(200) * 3\n",
    "volume_b = 500000 + np.cos(np.arange(200)/5) * 50000 + np.random.randint(-10000, 10000, 200)\n",
    "df_b = pd.DataFrame({\n",
    "    'Ticker': 'B',\n",
    "    'Date': dates,\n",
    "    'Adj High': price_b + 1.5,\n",
    "    'Adj Low': price_b - 1.5,\n",
    "    'Volume': volume_b\n",
    "})\n",
    "\n",
    "# Combine and set the multi-index\n",
    "sample_df = pd.concat([df_a, df_b]).set_index(['Ticker', 'Date'])\n",
    "\n",
    "print(\"Sample DataFrame created with shape:\", sample_df.shape)\n",
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cell 3: The Verification Process\n",
    "\n",
    "Here, we'll pick a specific date and ticker, run both functions, and compare their outputs side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Verifying calculations for Ticker 'A' on 2023-06-21 with a 60-day lookback ---\n",
      "\n",
      "### Important Note on Slopes ###\n",
      "The 'Original' function normalized prices/volume before regression (e.g., price / first_price).\n",
      "The 'Vectorized' function does not, for performance reasons. This means:\n",
      "  - R-SQUARED values SHOULD MATCH PERFECTLY (as R^2 is scale-invariant).\n",
      "  - SLOPE magnitudes WILL NOT MATCH, but their SIGN (+ or -) SHOULD.\n",
      "\n",
      "### Side-by-Side Comparison ###\n",
      "                  Vectorized  Original (Normalized)    Difference Sign Match?\n",
      "high_r_squared      0.833543               0.833543 -1.434408e-13         NaN\n",
      "high_slope          0.250067               0.002111  2.479559e-01        True\n",
      "low_r_squared       0.833543               0.833543 -1.431077e-13         NaN\n",
      "low_slope           0.250067               0.002148  2.479196e-01        True\n",
      "volume_r_squared    0.675416               0.675416 -1.554312e-15         NaN\n",
      "volume_slope     -681.682635              -0.006553 -6.816761e+02        True\n",
      "\n",
      "[FAILURE]: R-Squared values do not match.\n",
      "Series are different\n",
      "\n",
      "Attribute \"name\" are different\n",
      "[left]:  Vectorized\n",
      "[right]: Original (Normalized)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ping\\AppData\\Local\\Temp\\ipykernel_16884\\3739976515.py:36: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[ True  True  True]' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  comparison_df.loc[slope_cols, 'Sign Match?'] = np.sign(comparison_df.loc[slope_cols, 'Vectorized']) == np.sign(comparison_df.loc[slope_cols, 'Original (Normalized)'])\n"
     ]
    }
   ],
   "source": [
    "# --- Parameters for our Test ---\n",
    "TICKER_TO_CHECK = 'A'\n",
    "DATE_TO_CHECK = pd.to_datetime('2023-06-21')\n",
    "LOOKBACK_DAYS = 60\n",
    "\n",
    "\n",
    "print(f\"--- Verifying calculations for Ticker '{TICKER_TO_CHECK}' on {DATE_TO_CHECK.date()} with a {LOOKBACK_DAYS}-day lookback ---\\n\")\n",
    "\n",
    "# --- 1. Run the FAST Vectorized function on the full history ---\n",
    "ticker_history = sample_df.loc[TICKER_TO_CHECK]\n",
    "vectorized_results_full = analyze_ticker_trends_vectorized(ticker_history, LOOKBACK_DAYS)\n",
    "\n",
    "# Get the specific result for our target date\n",
    "vectorized_result_today = vectorized_results_full.loc[DATE_TO_CHECK]\n",
    "\n",
    "# --- 2. Run the SLOW Original function on a sliced window (our ground truth) ---\n",
    "# Get all data up to and including our check date\n",
    "historical_slice = ticker_history.loc[:DATE_TO_CHECK]\n",
    "original_result_today = analyze_ticker_trends_original(historical_slice, LOOKBACK_DAYS)\n",
    "\n",
    "\n",
    "# --- 3. Compare the results ---\n",
    "# Create a comparison DataFrame for clarity\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Vectorized': vectorized_result_today,\n",
    "    'Original (Normalized)': original_result_today\n",
    "}).dropna()\n",
    "\n",
    "# Note on Slopes: The vectorized version doesn't normalize the input, so the slope magnitude will differ.\n",
    "# However, the R-squared value, which is scale-invariant, should be nearly identical.\n",
    "comparison_df['Difference'] = comparison_df['Vectorized'] - comparison_df['Original (Normalized)']\n",
    "\n",
    "# For slopes, the sign should be the same\n",
    "slope_cols = [col for col in comparison_df.index if 'slope' in col]\n",
    "comparison_df['Sign Match?'] = np.nan\n",
    "comparison_df.loc[slope_cols, 'Sign Match?'] = np.sign(comparison_df.loc[slope_cols, 'Vectorized']) == np.sign(comparison_df.loc[slope_cols, 'Original (Normalized)'])\n",
    "\n",
    "\n",
    "print(\"### Important Note on Slopes ###\")\n",
    "print(\"The 'Original' function normalized prices/volume before regression (e.g., price / first_price).\")\n",
    "print(\"The 'Vectorized' function does not, for performance reasons. This means:\")\n",
    "print(\"  - R-SQUARED values SHOULD MATCH PERFECTLY (as R^2 is scale-invariant).\")\n",
    "print(\"  - SLOPE magnitudes WILL NOT MATCH, but their SIGN (+ or -) SHOULD.\\n\")\n",
    "\n",
    "print(\"### Side-by-Side Comparison ###\")\n",
    "print(comparison_df)\n",
    "\n",
    "# --- 4. Programmatic Check for R-Squared values ---\n",
    "try:\n",
    "    r_squared_cols = [col for col in comparison_df.index if 'r_squared' in col]\n",
    "    pd.testing.assert_series_equal(\n",
    "        comparison_df.loc[r_squared_cols, 'Vectorized'],\n",
    "        comparison_df.loc[r_squared_cols, 'Original (Normalized)'],\n",
    "        atol=1e-9 # Use a small tolerance for floating point math\n",
    "    )\n",
    "    print(\"\\n[SUCCESS]: R-Squared values match perfectly!\")\n",
    "except AssertionError as e:\n",
    "    print(\"\\n[FAILURE]: R-Squared values do not match.\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Verifying calculations for Ticker 'A' on 2023-06-21 with a 60-day lookback ---\n",
      "\n",
      "### Important Note on Slopes ###\n",
      "The 'Original' function normalized prices/volume before regression (e.g., price / first_price).\n",
      "The 'Vectorized' function does not, for performance reasons. This means:\n",
      "  - R-SQUARED values SHOULD MATCH PERFECTLY (as R^2 is scale-invariant).\n",
      "  - SLOPE magnitudes WILL NOT MATCH, but their SIGN (+ or -) SHOULD.\n",
      "\n",
      "### Side-by-Side Comparison ###\n",
      "                         Vectorized  Original (Normalized)    Difference Sign Match?\n",
      "high_penalty_score         0.004176               0.004173  2.188011e-06         NaN\n",
      "high_r_squared             0.833543               0.833543 -1.434408e-13         NaN\n",
      "high_slope                 0.250067               0.002111  2.479559e-01        True\n",
      "low_penalty_score          0.004176               0.004173  2.188011e-06         NaN\n",
      "low_r_squared              0.833543               0.833543 -1.431077e-13         NaN\n",
      "low_slope                  0.250067               0.002148  2.479196e-01        True\n",
      "unified_std_dev_returns    0.025085               0.025072  1.314462e-05         NaN\n",
      "volume_penalty_score       0.014795               0.014613  1.816572e-04         NaN\n",
      "volume_r_squared           0.675416               0.675416 -1.554312e-15         NaN\n",
      "volume_slope            -681.682635              -0.006553 -6.816761e+02        True\n",
      "volume_std_dev_returns     0.045580               0.045021  5.596615e-04         NaN\n",
      "\n",
      "[FAILURE]: R-Squared values do not match.\n",
      "Series are different\n",
      "\n",
      "Attribute \"name\" are different\n",
      "[left]:  Vectorized\n",
      "[right]: Original (Normalized)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ping\\AppData\\Local\\Temp\\ipykernel_16884\\1313454612.py:38: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[ True  True  True]' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  comparison_df.loc[slope_cols, 'Sign Match?'] = np.sign(comparison_df.loc[slope_cols, 'Vectorized']) == np.sign(comparison_df.loc[slope_cols, 'Original (Normalized)'])\n"
     ]
    }
   ],
   "source": [
    "# --- Parameters for our Test ---\n",
    "TICKER_TO_CHECK = 'A'\n",
    "DATE_TO_CHECK = pd.to_datetime('2023-06-21')\n",
    "LOOKBACK_DAYS = 60\n",
    "\n",
    "\n",
    "print(f\"--- Verifying calculations for Ticker '{TICKER_TO_CHECK}' on {DATE_TO_CHECK.date()} with a {LOOKBACK_DAYS}-day lookback ---\\n\")\n",
    "\n",
    "# --- 1. Run the FAST Vectorized function on the full history ---\n",
    "ticker_history = sample_df.loc[TICKER_TO_CHECK]\n",
    "vectorized_results_full = analyze_ticker_trends_vectorized(ticker_history, LOOKBACK_DAYS)\n",
    "\n",
    "# Get the specific result for our target date\n",
    "vectorized_result_today = vectorized_results_full.loc[DATE_TO_CHECK]\n",
    "\n",
    "# --- 2. Run the SLOW Original function on a sliced window (our ground truth) ---\n",
    "# Get all data up to and including our check date\n",
    "historical_slice = ticker_history.loc[:DATE_TO_CHECK]\n",
    "\n",
    "\n",
    "# original_result_today = analyze_ticker_trends_original(historical_slice, LOOKBACK_DAYS)\n",
    "original_result_today = analyze_ticker_trends(historical_slice, LOOKBACK_DAYS)\n",
    "\n",
    "# --- 3. Compare the results ---\n",
    "# Create a comparison DataFrame for clarity\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Vectorized': vectorized_result_today,\n",
    "    'Original (Normalized)': original_result_today\n",
    "}).dropna()\n",
    "\n",
    "# Note on Slopes: The vectorized version doesn't normalize the input, so the slope magnitude will differ.\n",
    "# However, the R-squared value, which is scale-invariant, should be nearly identical.\n",
    "comparison_df['Difference'] = comparison_df['Vectorized'] - comparison_df['Original (Normalized)']\n",
    "\n",
    "# For slopes, the sign should be the same\n",
    "slope_cols = [col for col in comparison_df.index if 'slope' in col]\n",
    "comparison_df['Sign Match?'] = np.nan\n",
    "comparison_df.loc[slope_cols, 'Sign Match?'] = np.sign(comparison_df.loc[slope_cols, 'Vectorized']) == np.sign(comparison_df.loc[slope_cols, 'Original (Normalized)'])\n",
    "\n",
    "\n",
    "print(\"### Important Note on Slopes ###\")\n",
    "print(\"The 'Original' function normalized prices/volume before regression (e.g., price / first_price).\")\n",
    "print(\"The 'Vectorized' function does not, for performance reasons. This means:\")\n",
    "print(\"  - R-SQUARED values SHOULD MATCH PERFECTLY (as R^2 is scale-invariant).\")\n",
    "print(\"  - SLOPE magnitudes WILL NOT MATCH, but their SIGN (+ or -) SHOULD.\\n\")\n",
    "\n",
    "print(\"### Side-by-Side Comparison ###\")\n",
    "print(comparison_df)\n",
    "\n",
    "# --- 4. Programmatic Check for R-Squared values ---\n",
    "try:\n",
    "    r_squared_cols = [col for col in comparison_df.index if 'r_squared' in col]\n",
    "    pd.testing.assert_series_equal(\n",
    "        comparison_df.loc[r_squared_cols, 'Vectorized'],\n",
    "        comparison_df.loc[r_squared_cols, 'Original (Normalized)'],\n",
    "        atol=1e-9 # Use a small tolerance for floating point math\n",
    "    )\n",
    "    print(\"\\n[SUCCESS]: R-Squared values match perfectly!\")\n",
    "except AssertionError as e:\n",
    "    print(\"\\n[FAILURE]: R-Squared values do not match.\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refactored Code\n",
    "\n",
    "Here is the complete, refactored solution. I've included the previously refactored functions with slight modifications to accept the new configuration structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def calculate_rolling_z_scores(df_group, rolling_window=20):\n",
    "    \"\"\"\n",
    "    Calculates a rolling Z-score for the 'Adj Low' price for an entire ticker history.\n",
    "    \n",
    "    This function is designed to be used with pandas' groupby().apply().\n",
    "    \n",
    "    Args:\n",
    "        df_group (pd.DataFrame): The DataFrame for a single ticker.\n",
    "        rolling_window (int): The lookback window for calculating mean and std.\n",
    "        \n",
    "    Returns:\n",
    "        pd.Series: A Series of Z-scores with the same index as the input df_group.\n",
    "    \"\"\"\n",
    "    # Use the column directly, which is more efficient.\n",
    "    low_price = df_group['Adj Low']\n",
    "    \n",
    "    # Calculate rolling statistics. min_periods=1 ensures we get values even at the start.\n",
    "    rolling_mean = low_price.rolling(window=rolling_window, min_periods=1).mean()\n",
    "    rolling_std = low_price.rolling(window=rolling_window, min_periods=1).std()\n",
    "    \n",
    "    # Calculate the Z-score for all dates at once.\n",
    "    z_score = (low_price - rolling_mean) / rolling_std\n",
    "    \n",
    "    # --- ROBUSTNESS IMPROVEMENTS ---\n",
    "    # 1. Handle division by zero: if std is 0, z_score is inf. Replace with 0.\n",
    "    z_score = z_score.replace([np.inf, -np.inf], 0)\n",
    "    \n",
    "    # 2. Fill any initial NaNs (from std dev calculation) with 0.\n",
    "    z_score = z_score.fillna(0)\n",
    "    \n",
    "    # Return the entire series, renamed appropriately.\n",
    "    return z_score.rename('low_rolling_z_score')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import linregress # We no longer need this for the vectorized function\n",
    "\n",
    "def analyze_ticker_trends_vectorized(df_group, lookback_days=60):\n",
    "    \"\"\"\n",
    "    Vectorized analysis of trends for a ticker's price and volume.\n",
    "    \n",
    "    This version calculates linear regression metrics using rolling covariance\n",
    "    and variance, avoiding the limitations of .apply() and maximizing performance.\n",
    "    \"\"\"\n",
    "    if len(df_group) < lookback_days:\n",
    "        return None # groupby().apply() will correctly handle None returns by skipping the group\n",
    "\n",
    "    # --- 1. Setup for Vectorized Regression ---\n",
    "    \n",
    "    # Create a time-series index [0, 1, 2, ...] that aligns with the data\n",
    "    time_index = pd.Series(np.arange(len(df_group)), index=df_group.index)\n",
    "    \n",
    "    # The variance of a sequence [0, 1, ..., n-1] is constant. Pre-calculate it.\n",
    "    var_time = np.var(np.arange(lookback_days), ddof=0)\n",
    "    \n",
    "    # --- 2. Calculate Rolling Metrics for Each Column ---\n",
    "\n",
    "    # We'll work with a dictionary of the series we want to analyze\n",
    "    series_to_analyze = {\n",
    "        'high': df_group['Adj High'],\n",
    "        'low': df_group['Adj Low'],\n",
    "        'volume': df_group['Volume'].astype(float) # Ensure volume is float\n",
    "    }\n",
    "    \n",
    "    df_results = pd.DataFrame(index=df_group.index)\n",
    "\n",
    "    for name, series in series_to_analyze.items():\n",
    "        # Rolling covariance between the series and the time index\n",
    "        rolling_cov = time_index.rolling(window=lookback_days).cov(series, ddof=0)\n",
    "        \n",
    "        # Rolling variance of the series itself\n",
    "        rolling_var_series = series.rolling(window=lookback_days).var(ddof=0)\n",
    "        \n",
    "        # Slope = cov(t, y) / var(t)\n",
    "        df_results[f'{name}_slope'] = rolling_cov / var_time\n",
    "        \n",
    "        # R-squared = cov(t, y)^2 / (var(t) * var(y))\n",
    "        # Add a small epsilon to prevent division by zero\n",
    "        denominator = (var_time * rolling_var_series) + 1e-9\n",
    "        df_results[f'{name}_r_squared'] = (rolling_cov**2) / denominator\n",
    "\n",
    "    # --- 3. Calculate Volatility and Penalty Scores (as before) ---\n",
    "    \n",
    "    yesterday_low = df_group['Adj Low'].shift(1)\n",
    "    worst_case_returns = (df_group['Adj High'] - yesterday_low) / yesterday_low\n",
    "    df_results['unified_std_dev_returns'] = worst_case_returns.rolling(window=lookback_days).std(ddof=0)\n",
    "    \n",
    "    volume_std_dev = df_group['Volume'].pct_change().rolling(window=lookback_days).std(ddof=0)\n",
    "    df_results['volume_std_dev_returns'] = volume_std_dev\n",
    "    \n",
    "    df_results['low_penalty_score'] = (1 - df_results['low_r_squared']) * (df_results['unified_std_dev_returns'] + 1e-9)\n",
    "    df_results['high_penalty_score'] = (1 - df_results['high_r_squared']) * (df_results['unified_std_dev_returns'] + 1e-9)\n",
    "    df_results['volume_penalty_score'] = (1 - df_results['volume_r_squared']) * (df_results['volume_std_dev_returns'] + 1e-9)\n",
    "    \n",
    "    return df_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Refined Core Backtesting Functions\n",
    "\n",
    "We'll modify the function signatures to accept a single `config` dictionary. This makes them more modular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from itertools import product\n",
    "\n",
    "# Assume analyze_ticker_trends_vectorized and calculate_rolling_z_scores are defined elsewhere\n",
    "\n",
    "def precompute_signals(df_ohlcv, config):\n",
    "    \"\"\"\n",
    "    Pre-computes all trading signals and returns the full features DataFrame\n",
    "    for the dates where signals were triggered.\n",
    "    \"\"\"\n",
    "    print(\"Pre-computing features for this parameter set...\")\n",
    "    \n",
    "    trends = df_ohlcv.groupby(level='Ticker', group_keys=False).apply(\n",
    "        analyze_ticker_trends_vectorized, config['lookback_days']\n",
    "    )\n",
    "    \n",
    "    z_scores = df_ohlcv.groupby(level='Ticker', group_keys=False).apply(\n",
    "        calculate_rolling_z_scores, config['rolling_window']\n",
    "    )\n",
    "    \n",
    "    features = trends.join(z_scores).dropna()\n",
    "\n",
    "    signals = features[\n",
    "        (features['low_slope'] > config['slope_thresh']) &\n",
    "        (features['low_r_squared'] > config['r2_thresh']) &\n",
    "        (features['volume_slope'] > 0) &\n",
    "        (features['low_rolling_z_score'] < config['z_entry_thresh'])\n",
    "    ]\n",
    "    \n",
    "    # --- KEY CHANGE: Return the full signals DataFrame, not just the index ---\n",
    "    return signals\n",
    "\n",
    "def run_backtest(df_ohlcv, config):\n",
    "    \"\"\"\n",
    "    Orchestrates the backtesting process with enhanced logging.\n",
    "    \"\"\"\n",
    "    # Now returns a DataFrame of features for triggered signals\n",
    "    entry_signals_features = precompute_signals(df_ohlcv, config)\n",
    "    \n",
    "    trades = []\n",
    "    open_positions = {}\n",
    "    \n",
    "    all_dates = df_ohlcv.index.get_level_values('Date').unique().sort_values()\n",
    "    start_index = max(config['lookback_days'], config['rolling_window'])\n",
    "\n",
    "    for i in tqdm(range(start_index, len(all_dates) - 1), desc=\"Backtesting\"):\n",
    "        current_date = all_dates[i]\n",
    "        next_day_date = all_dates[i+1]\n",
    "\n",
    "        closed_trades, open_positions = handle_exits_for_day(\n",
    "            current_date, next_day_date, open_positions, df_ohlcv, config\n",
    "        )\n",
    "        trades.extend(closed_trades)\n",
    "\n",
    "        # --- KEY CHANGE: Filter the features DataFrame for today's signals ---\n",
    "        signals_today = entry_signals_features[\n",
    "            entry_signals_features.index.get_level_values('Date') == current_date\n",
    "        ]\n",
    "        \n",
    "        # Pass the full signals_today DataFrame to the handler\n",
    "        open_positions = handle_entries_for_day(\n",
    "            current_date, next_day_date, signals_today, open_positions, df_ohlcv\n",
    "        )\n",
    "                \n",
    "    # --- Create the final DataFrame and reorder columns for clarity ---\n",
    "    if not trades:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    final_trades_df = pd.DataFrame(trades)\n",
    "    log_columns = [\n",
    "        'ticker', 'signal_date', 'entry_date', 'exit_signal_date', 'exit_date', 'reason',\n",
    "        'return', 'entry_price_actual', 'exit_price_actual', 'exit_trigger_price', \n",
    "        'exit_target_value', 'entry_signal_features'\n",
    "    ]\n",
    "    # Ensure all columns exist, fill missing with None\n",
    "    for col in log_columns:\n",
    "        if col not in final_trades_df.columns:\n",
    "            final_trades_df[col] = None\n",
    "            \n",
    "    return final_trades_df[log_columns]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: New Encapsulated Helper Functions\n",
    "\n",
    "These new functions isolate the logic for performance analysis and the optimization loop itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_performance(trade_results):\n",
    "    \"\"\"\n",
    "    Calculates performance metrics from a DataFrame of trades.\n",
    "    \n",
    "    Returns a dictionary of key metrics.\n",
    "    \"\"\"\n",
    "    if trade_results.empty:\n",
    "        return {'num_trades': 0, 'win_rate': 0, 'avg_return': 0, 'total_return': 0}\n",
    "    \n",
    "    win_rate = (trade_results['return'] > 0).mean()\n",
    "    total_return = (1 + trade_results['return']).prod() - 1\n",
    "    avg_return = trade_results['return'].mean()\n",
    "    \n",
    "    return {\n",
    "        'num_trades': len(trade_results),\n",
    "        'win_rate': win_rate,\n",
    "        'avg_return': avg_return,\n",
    "        'total_return': total_return\n",
    "    }\n",
    "\n",
    "def run_parameter_optimization(df, param_grid, static_params):\n",
    "    \"\"\"\n",
    "    Orchestrates the entire parameter optimization process.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The OHLCV data.\n",
    "        param_grid (dict): Dictionary with lists of parameters to test.\n",
    "        static_params (dict): Dictionary of parameters that are not being optimized.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A summary of results for each parameter combination.\n",
    "    \"\"\"\n",
    "    results_log = []\n",
    "    \n",
    "    # Use itertools.product to create a clean generator for all combinations\n",
    "    keys, values = zip(*param_grid.items())\n",
    "    param_combinations = [dict(zip(keys, v)) for v in product(*values)]\n",
    "\n",
    "    print(f\"Starting optimization for {len(param_combinations)} combinations...\")\n",
    "    \n",
    "    for param_set in tqdm(param_combinations, desc=\"Optimization Progress\"):\n",
    "        # Combine static and dynamic parameters into a single config for this run\n",
    "        current_config = {**static_params, **param_set}\n",
    "        \n",
    "        # 1. Run the backtest with the current configuration\n",
    "        trade_results = run_backtest(df, current_config)\n",
    "        \n",
    "        # 2. Analyze the performance of this run\n",
    "        performance_metrics = analyze_performance(trade_results)\n",
    "        \n",
    "        # 3. Log the results\n",
    "        log_entry = {**param_set, **performance_metrics}\n",
    "        results_log.append(log_entry)\n",
    "        \n",
    "    return pd.DataFrame(results_log)\n",
    "\n",
    "def handle_entries_for_day(current_date, next_day_date, signals_today, open_positions, df_ohlcv):\n",
    "    \"\"\"\n",
    "    Processes entries and stores signal details in the open_positions dict.\n",
    "    \"\"\"\n",
    "    # --- KEY CHANGE: Loop through the signals DataFrame ---\n",
    "    for ticker, signal_row in signals_today.iterrows():\n",
    "        # The ticker is now in the index of signal_row, so we use its name\n",
    "        ticker_name = ticker[0] \n",
    "        \n",
    "        if ticker_name not in open_positions:\n",
    "            try:\n",
    "                entry_price = df_ohlcv.loc[(ticker_name, next_day_date), 'Adj High']\n",
    "                \n",
    "                # --- LOGGING: Store more info about the entry signal ---\n",
    "                open_positions[ticker_name] = {\n",
    "                    'entry_date': next_day_date,\n",
    "                    'entry_price': entry_price,\n",
    "                    'signal_date': current_date,\n",
    "                    'signal_features': signal_row.to_dict() # Store all features that triggered the signal\n",
    "                }\n",
    "            except KeyError:\n",
    "                pass\n",
    "                \n",
    "    return open_positions\n",
    "\n",
    "def handle_exits_for_day(current_date, next_day_date, open_positions, df_ohlcv, config):\n",
    "    \"\"\"\n",
    "    Checks for exits and logs detailed information about the exit trigger.\n",
    "    Corrected version with valid syntax for the if/elif chain.\n",
    "    \"\"\"\n",
    "    closed_trades = []\n",
    "    positions_to_close = []\n",
    "\n",
    "    for ticker, pos in open_positions.items():\n",
    "        try:\n",
    "            current_close_price = df_ohlcv.loc[(ticker, current_date), 'Adj Close']\n",
    "        except KeyError:\n",
    "            continue \n",
    "\n",
    "        exit_reason = None\n",
    "        exit_target_value = None \n",
    "        \n",
    "        # --- SYNTAX FIX: Calculate all threshold values *before* the conditional block ---\n",
    "        profit_target_price = pos['entry_price'] * (1 + config['profit_target'])\n",
    "        stop_loss_price = pos['entry_price'] * (1 - config['stop_loss'])\n",
    "        days_held = (current_date.to_pydatetime().date() - pos['entry_date'].to_pydatetime().date()).days\n",
    "        \n",
    "        # --- Now, check conditions in a contiguous if/elif/elif block ---\n",
    "        if current_close_price >= profit_target_price:\n",
    "            exit_reason = \"Profit Target\"\n",
    "            exit_target_value = profit_target_price \n",
    "\n",
    "        elif current_close_price <= stop_loss_price:\n",
    "            exit_reason = \"Stop-Loss\"\n",
    "            exit_target_value = stop_loss_price \n",
    "\n",
    "        elif days_held >= config['time_hold_days']:\n",
    "            exit_reason = \"Time Hold\"\n",
    "            exit_target_value = days_held \n",
    "\n",
    "        if exit_reason:\n",
    "            try:\n",
    "                exit_price = df_ohlcv.loc[(ticker, next_day_date), 'Adj Low']\n",
    "                trade_return = (exit_price - pos['entry_price']) / pos['entry_price']\n",
    "                \n",
    "                trade_log = {\n",
    "                    'ticker': ticker, \n",
    "                    'entry_date': pos['entry_date'], \n",
    "                    'exit_date': next_day_date,\n",
    "                    'return': trade_return, \n",
    "                    'reason': exit_reason,\n",
    "                    'signal_date': pos['signal_date'],\n",
    "                    'entry_signal_features': pos['signal_features'],\n",
    "                    'entry_price_actual': pos['entry_price'],\n",
    "                    'exit_signal_date': current_date,\n",
    "                    'exit_trigger_price': current_close_price,\n",
    "                    'exit_target_value': exit_target_value,\n",
    "                    'exit_price_actual': exit_price,\n",
    "                }\n",
    "                closed_trades.append(trade_log)\n",
    "                positions_to_close.append(ticker)\n",
    "            except KeyError:\n",
    "                pass\n",
    "                \n",
    "    for ticker in positions_to_close:\n",
    "        del open_positions[ticker]\n",
    "        \n",
    "    return closed_trades, open_positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: The New, Clean Top-Level Script\n",
    "\n",
    "Your main script is now incredibly simple and readable. It's all about configuration and orchestration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. DEFINE CONFIGURATION ---\n",
    "\n",
    "# Parameters to be optimized, defining the search space\n",
    "optimization_grid = {\n",
    "    'lookback_days': [30, 60, 90],\n",
    "    'rolling_window': [15, 20]\n",
    "}\n",
    "\n",
    "# Static strategy parameters that do not change during optimization\n",
    "strategy_params = {\n",
    "    'slope_thresh': 1.0,\n",
    "    'r2_thresh': 0.50,\n",
    "    'z_entry_thresh': 0,\n",
    "    'profit_target': 0.10,\n",
    "    'stop_loss': 0.05,\n",
    "    'time_hold_days': 20\n",
    "}\n",
    "\n",
    "\n",
    "# --- 2. RUN ORCHESTRATOR ---\n",
    "\n",
    "# The main call is now a single, descriptive function\n",
    "optimization_results = run_parameter_optimization(\n",
    "    df_train, optimization_grid, strategy_params\n",
    ")\n",
    "\n",
    "\n",
    "# --- 3. ANALYZE RESULTS ---\n",
    "\n",
    "print(\"\\n\\n--- Optimization Complete ---\")\n",
    "print(optimization_results.sort_values(by='total_return', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: The \"One-Trade\" Deep Dive\n",
    "\n",
    "The most powerful debugging technique is to isolate a single trade and follow it from signal generation to exit. If the logic holds for one trade, it's likely correct for all of them.\n",
    "\n",
    "1.  **Pick a Winning Trade and a Losing Trade:** Run one of the backtests again (e.g., the one with `lookback=30`, `rolling=20`) and save the `trade_results` DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Pick a configuration to analyze\n",
    "config_to_test = {\n",
    "    **strategy_params, \n",
    "    'lookback_days': 30, \n",
    "    'rolling_window': 20\n",
    "}\n",
    "\n",
    "# 2. Run the backtest to get the detailed trade log\n",
    "trade_log_df = run_backtest(df_train, config_to_test)\n",
    "\n",
    "# 3. Isolate and inspect a single trade\n",
    "if not trade_log_df.empty:\n",
    "    # Get the first losing trade\n",
    "    losing_trade = trade_log_df[trade_log_df['return'] < 0].iloc[0]\n",
    "\n",
    "    print(\"--- Detailed Log for a Single Losing Trade ---\")\n",
    "    # Using .T transposes the Series for easy vertical reading\n",
    "    print(losing_trade.T)\n",
    "else:\n",
    "    print(\"No trades were made for this configuration.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losing_trade.entry_signal_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df_trends = analyze_ticker_trends_vectorized(df_train, lookback_days=30)\n",
    "print(f'_df_trends:\\n{_df_trends}')\n",
    "# _df_trends.to_csv('C:\\\\Users\\\\ping\\\\Desktop\\\\_df_trends.csv', index=True)\n",
    "_df_trends.index.names = ['Ticker', 'Date']\n",
    "_df_trends.reset_index().to_csv(r'C:\\Users\\ping\\Desktop\\_df_trends.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.loc['MSFT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to CSV\n",
    "_df_trends.to_csv(r'C:\\Users\\ping\\Desktop\\_df_trends.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df_trends.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loss = trade_log_df[trade_log_df['return'] < 0]\n",
    "print(df_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_win = trade_log_df[trade_log_df['return'] > 0]\n",
    "print(df_win)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loss.to_csv('C:\\\\Users\\\\ping\\\\Desktop\\\\df_loss.csv', index=True)\n",
    "df_win.to_csv('C:\\\\Users\\\\ping\\\\Desktop\\\\df_win.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimization_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a configuration to analyze\n",
    "config_to_test = {**strategy_params, 'lookback_days': 30, 'rolling_window': 20}\n",
    "\n",
    "# Run a single backtest and get the detailed trade log\n",
    "single_run_trades = run_backtest(df_train, config_to_test)\n",
    "\n",
    "# Find a winning and a losing trade to investigate\n",
    "print(\"Sample winning trade:\")\n",
    "print(single_run_trades[single_run_trades['return'] > 0].head(1))\n",
    "\n",
    "print(\"\\nSample losing trade:\")\n",
    "print(single_run_trades[single_run_trades['return'] < 0].head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trends = analyze_ticker_trends_vectorized(df_train, lookback_days=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trends.loc['FIX', '2024-10-25']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
