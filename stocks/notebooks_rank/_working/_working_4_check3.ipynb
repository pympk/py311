{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and Configuration\n",
    "\n",
    "This cell contains all imports and user-configurable parameters for the analysis pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pprint\n",
    "import matplotlib.pyplot as plt # Import for plotting\n",
    "from IPython.display import display, Markdown\n",
    "from scipy.stats import linregress \n",
    "\n",
    "# --- 1. PANDAS & IPYTHON OPTIONS ---\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 3000)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# --- 2. PROJECT PATH CONFIGURATION ---\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "ROOT_DIR = NOTEBOOK_DIR.parent.parent  # Adjust if your notebook is in a 'notebooks' subdirectory\n",
    "DATA_DIR = ROOT_DIR / 'data'\n",
    "SRC_DIR = ROOT_DIR / 'src'\n",
    "\n",
    "# Add 'src' to the Python path to import custom modules\n",
    "if str(SRC_DIR) not in sys.path:\n",
    "    sys.path.append(str(SRC_DIR))\n",
    "\n",
    "# --- 3. IMPORT CUSTOM MODULES ---\n",
    "import utils\n",
    "import plotting_utils\n",
    "\n",
    "# --- 4. ANALYSIS & FILTERING CONFIGURATION ---\n",
    "\n",
    "# File searching parameters\n",
    "# FILE_PREFIX = ''  # e.g., '2024'\n",
    "FILE_CONTAINS_PATTERN = 'df_OHLCV_clean_stocks_etfs'\n",
    "\n",
    "# # Parameters defining the time windows for metric calculation\n",
    "PERIOD_PARAMS = {\n",
    "    'lookback_days': 22,\n",
    "    'recent_days': 0,\n",
    "}\n",
    "\n",
    "# This is not use for filtering, it's use to calculate metrics in SORT_ORDER\n",
    "# Parameters for filtering the calculated metrics to find candidates\n",
    "METRIC_FILTERS = {\n",
    "    'min_lookback_improvement': 0,\n",
    "    'current_rank_bracket_start': 1,\n",
    "    'current_rank_bracket_end': 1000,\n",
    "    # --- Select ONE mode by commenting out the others ---\n",
    "    # 'Reversal' Mode\n",
    "    'min_recent_bottom_to_recent_start': 0,\n",
    "    'min_recent_bottom_to_current': 0,    \n",
    "    # 'Dip' Mode\n",
    "    # 'min_current_to_recent_start': 10,\n",
    "}\n",
    "\n",
    "# --- 5. VERIFICATION ---\n",
    "print(\"--- Path Configuration ---\")\n",
    "print(f\"✅ Project Root: {ROOT_DIR}\")\n",
    "print(f\"✅ Data Dir:     {DATA_DIR}\")\n",
    "print(f\"✅ Source Dir:   {SRC_DIR}\")\n",
    "assert all([ROOT_DIR.exists(), DATA_DIR.exists(), SRC_DIR.exists()]), \"A key directory was not found!\"\n",
    "\n",
    "print(\"\\n--- Module Verification ---\")\n",
    "print(f\"✅ Successfully imported 'utils' and 'plotting_utils'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Step 1: Loading latest consolidated Finviz data ---\")\n",
    "\n",
    "# Find the most recent file matching the pattern\n",
    "# This function is now understood to return List[str] (filenames), not List[Path].\n",
    "latest_finviz_filepaths = utils.get_recent_files(\n",
    "    directory_path=DATA_DIR,\n",
    "    extension='parquet',\n",
    "    prefix='202',\n",
    "    contains_pattern='df_finviz_merged_stocks_etfs',\n",
    "    count=1\n",
    ")\n",
    "\n",
    "if not latest_finviz_filepaths:\n",
    "    raise FileNotFoundError(f\"No files found in '{DATA_DIR}' with prefix '{FILE_PREFIX}' and pattern '{FILE_CONTAINS_PATTERN}'\")\n",
    "\n",
    "# Get the filename string from the list\n",
    "latest_filename = latest_finviz_filepaths[0]\n",
    "\n",
    "# Manually construct the full path before loading\n",
    "full_file_path = DATA_DIR / latest_filename\n",
    "df_finviz_latest = pd.read_parquet(full_file_path, engine='pyarrow')\n",
    "\n",
    "\n",
    "# --- Robust Index Setting (this logic remains correct) ---\n",
    "if df_finviz_latest.index.name == 'Ticker':\n",
    "    print(\"Info: 'Ticker' is already the index. No action needed.\")\n",
    "elif 'Ticker' in df_finviz_latest.columns:\n",
    "    print(\"Info: 'Ticker' column found. Setting it as the DataFrame index.\")\n",
    "    df_finviz_latest.set_index('Ticker', inplace=True)\n",
    "elif 'ticker' in df_finviz_latest.columns:\n",
    "    print(\"Info: 'ticker' column found. Renaming and setting as index.\")\n",
    "    df_finviz_latest.rename(columns={'ticker': 'Ticker'}, inplace=True)\n",
    "    df_finviz_latest.set_index('Ticker', inplace=True)\n",
    "elif df_finviz_latest.index.name is None:\n",
    "    print(\"Info: Index is unnamed. Assuming it contains tickers and assigning the name 'Ticker'.\")\n",
    "    df_finviz_latest.index.name = 'Ticker'\n",
    "else:\n",
    "    print(\"ERROR: Loaded DataFrame has an unexpected format.\")\n",
    "    print(f\"Columns: {df_finviz_latest.columns.tolist()}\")\n",
    "    print(f\"Index Name: '{df_finviz_latest.index.name}'\")\n",
    "    raise ValueError(\"Could not find a 'Ticker' column or a usable index to proceed.\")\n",
    "\n",
    "\n",
    "# Correct the print statement to work with the filename string\n",
    "print(f\"✅ Successfully loaded: {latest_filename}\")\n",
    "print(f\"Shape: {df_finviz_latest.shape}\")\n",
    "print(df_finviz_latest.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 3000)\n",
    "\n",
    "# DATA_DIR = r'c:\\Users\\ping\\Files_win10\\python\\py311\\stocks\\data'\n",
    "# Manually construct the full path before loading\n",
    "full_file_path = r'c:\\Users\\ping\\Files_win10\\python\\py311\\stocks\\data\\df_OHLCV_clean_stocks_etfs.parquet'\n",
    "df_OHLCV = pd.read_parquet(full_file_path, engine='pyarrow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 2: The Chronological Split Code\n",
    "\n",
    "This cell contains the logic to find the split date and create the `df_train` and `df_test` DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique trading dates in dataset: 250\n",
      "The data will be split on the date: 2025-05-28\n",
      "\n",
      "--- Verification ---\n",
      "Original DataFrame shape: (371000, 5)\n",
      "Training set shape:   (261184, 5)\n",
      "Testing set shape:    (109816, 5)\n",
      "\n",
      "Date Ranges:\n",
      "  Training: 2024-09-13 to 2025-05-28\n",
      "  Testing:  2025-05-29 to 2025-09-12\n",
      "\n",
      "Verification successful: There is no date overlap between train and test sets.\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Find the Chronological Split Point ---\n",
    "\n",
    "# Get all unique dates from the index and sort them\n",
    "unique_dates = df_OHLCV.index.get_level_values('Date').unique().sort_values()\n",
    "\n",
    "# Determine the index for the 70% split\n",
    "split_index = int(len(unique_dates) * 0.7)\n",
    "\n",
    "# Find the actual date at that split index\n",
    "split_date = unique_dates[split_index]\n",
    "\n",
    "print(f\"Total unique trading dates in dataset: {len(unique_dates)}\")\n",
    "print(f\"The data will be split on the date: {split_date.date()}\")\n",
    "\n",
    "# --- 2. Create the Training and Testing Sets ---\n",
    "\n",
    "# The training set includes all data UP TO and INCLUDING the split_date\n",
    "df_train = df_OHLCV[df_OHLCV.index.get_level_values('Date') <= split_date]\n",
    "\n",
    "# The testing set includes all data AFTER the split_date\n",
    "df_test = df_OHLCV[df_OHLCV.index.get_level_values('Date') > split_date]\n",
    "\n",
    "\n",
    "# --- 3. Verify the Split ---\n",
    "\n",
    "print(\"\\n--- Verification ---\")\n",
    "print(f\"Original DataFrame shape: {df_OHLCV.shape}\")\n",
    "print(f\"Training set shape:   {df_train.shape}\")\n",
    "print(f\"Testing set shape:    {df_test.shape}\")\n",
    "\n",
    "print(\"\\nDate Ranges:\")\n",
    "print(f\"  Training: {df_train.index.get_level_values('Date').min().date()} to {df_train.index.get_level_values('Date').max().date()}\")\n",
    "print(f\"  Testing:  {df_test.index.get_level_values('Date').min().date()} to {df_test.index.get_level_values('Date').max().date()}\")\n",
    "\n",
    "# Final check to ensure no overlap\n",
    "assert df_train.index.get_level_values('Date').max() < df_test.index.get_level_values('Date').min()\n",
    "print(\"\\nVerification successful: There is no date overlap between train and test sets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jupyter Notebook: Verifying `analyze_ticker_trends_vectorized`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cell 1: Setup and Imports\n",
    "\n",
    "First, let's import the necessary libraries and define both versions of the function we want to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verification functions are defined.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import linregress\n",
    "\n",
    "# --- Function 1: The NEW, Correct, Log-Vectorized Version (The one we are verifying) ---\n",
    "\n",
    "def analyze_ticker_trends_log_vectorized(df_group, lookback_days=60):\n",
    "    \"\"\"\n",
    "    This function output has been verified. \n",
    "\n",
    "    Vectorized analysis of trends using LOG-TRANSFORMED data.\n",
    "    \n",
    "    This is the CORRECTED version. The slope of a log-transformed series represents\n",
    "    the average percentage change, making it comparable across different stocks.\n",
    "\n",
    "    The first n lookback_days rows will be all NaN.\n",
    "    'unified_std_dev_returns' and 'volume_std_dev_returns' will have NaN on \n",
    "    lookback_days + 1 row.\n",
    "    \"\"\"\n",
    "    if len(df_group) < lookback_days:\n",
    "        return None \n",
    "\n",
    "    time_index = pd.Series(np.arange(len(df_group)), index=df_group.index)\n",
    "    var_time = np.var(np.arange(lookback_days), ddof=0)\n",
    "    \n",
    "    series_to_analyze = {\n",
    "        # Uses natural log \n",
    "        'high': np.log(df_group['Adj High']),\n",
    "        'low': np.log(df_group['Adj Low']),\n",
    "        'volume': np.log(df_group['Volume'].astype(float) + 1)\n",
    "    }\n",
    "    \n",
    "    df_results = pd.DataFrame(index=df_group.index)\n",
    "\n",
    "    for name, log_series in series_to_analyze.items():\n",
    "        # Uses covariance and variance of population\n",
    "        rolling_cov = time_index.rolling(window=lookback_days).cov(log_series, ddof=0)\n",
    "        rolling_var_series = log_series.rolling(window=lookback_days).var(ddof=0)\n",
    "        \n",
    "        df_results[f'{name}_slope'] = rolling_cov / var_time\n",
    "        denominator = (var_time * rolling_var_series) + 1e-9\n",
    "        df_results[f'{name}_r_squared'] = (rolling_cov**2) / denominator\n",
    "\n",
    "    yesterday_low = df_group['Adj Low'].shift(1)\n",
    "    worst_case_returns = (df_group['Adj High'] - yesterday_low) / yesterday_low\n",
    "    df_results['unified_std_dev_returns'] = worst_case_returns.rolling(window=lookback_days).std(ddof=0)\n",
    "    \n",
    "    volume_std_dev = df_group['Volume'].pct_change().rolling(window=lookback_days).std(ddof=0)\n",
    "    df_results['volume_std_dev_returns'] = volume_std_dev\n",
    "    \n",
    "    return df_results # Penalty scores omitted for verification clarity\n",
    "\n",
    "print(\"Verification functions are defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cell 2: Create Realistic Sample Data\n",
    "\n",
    "We need some sample data that mimics your real dataset to perform the check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a date range\n",
    "dates = pd.to_datetime(pd.date_range(start='2023-01-01', periods=200, freq='B'))\n",
    "\n",
    "# Create data for Ticker 'A' (upward trend)\n",
    "price_a = 100 + np.linspace(0, 50, 200) + np.random.randn(200) * 2\n",
    "volume_a = 100000 + np.sin(np.arange(200)/10) * 20000 + np.random.randint(-5000, 5000, 200)\n",
    "df_a = pd.DataFrame({\n",
    "    'Ticker': 'A',\n",
    "    'Date': dates,\n",
    "    'Adj High': price_a + 1,\n",
    "    'Adj Low': price_a - 1,\n",
    "    'Volume': volume_a\n",
    "})\n",
    "\n",
    "# Create data for Ticker 'B' (downward trend)\n",
    "price_b = 200 - np.linspace(0, 30, 200) + np.random.randn(200) * 3\n",
    "volume_b = 500000 + np.cos(np.arange(200)/5) * 50000 + np.random.randint(-10000, 10000, 200)\n",
    "df_b = pd.DataFrame({\n",
    "    'Ticker': 'B',\n",
    "    'Date': dates,\n",
    "    'Adj High': price_b + 1.5,\n",
    "    'Adj Low': price_b - 1.5,\n",
    "    'Volume': volume_b\n",
    "})\n",
    "\n",
    "# Combine and set the multi-index\n",
    "sample_df = pd.concat([df_a, df_b]).set_index(['Ticker', 'Date'])\n",
    "\n",
    "print(\"Sample DataFrame created with shape:\", sample_df.shape)\n",
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSFT.head(3):\n",
      "            Adj Open  Adj High  Adj Low  Adj Close    Volume\n",
      "Date                                                        \n",
      "2024-09-13   422.657   428.612  422.290    427.381  15993778\n",
      "2024-09-16   427.391   430.300  425.029    428.126  13938564\n",
      "2024-09-17   436.950   438.558  429.049    431.907  19015898\n",
      "\n",
      "MSFT.tail(3):\n",
      "            Adj Open  Adj High  Adj Low  Adj Close    Volume\n",
      "Date                                                        \n",
      "2025-05-23   449.242   452.945  448.173    449.441  16911254\n",
      "2025-05-27   455.731   460.193  455.371    459.934  21008780\n",
      "2025-05-28   460.463   461.761  456.180    456.609  17114387\n",
      "\n",
      "len(MSFT): 176\n"
     ]
    }
   ],
   "source": [
    "MSFT = df_train.loc['MSFT'].copy()\n",
    "print(f'MSFT.head(3):\\n{MSFT.head(3)}')\n",
    "print(f'\\nMSFT.tail(3):\\n{MSFT.tail(3)}')\n",
    "print(f'\\nlen(MSFT): {len(MSFT)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cell 3: The New Verification\n",
    "\n",
    "Now, we will compare the log-vectorized results against the original normalized results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSFT.to_csv(r'C:\\Users\\ping\\Desktop\\MSFT.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Parameters for our Test ---\n",
    "TICKER_TO_CHECK = 'MSFT'\n",
    "DATE_TO_CHECK = pd.to_datetime('2024-09-26')\n",
    "LOOKBACK_DAYS = 10\n",
    "\n",
    "print(f\"--- Verifying LOG-TRANSFORMED calculations for '{TICKER_TO_CHECK}' on {DATE_TO_CHECK.date()} ---\\n\")\n",
    "\n",
    "# --- 1. Run the FAST Log-Vectorized function ---\n",
    "ticker_history = df_train.loc[TICKER_TO_CHECK]\n",
    "vectorized_results_full = analyze_ticker_trends_log_vectorized(ticker_history, LOOKBACK_DAYS)\n",
    "vectorized_result_today = vectorized_results_full.loc[DATE_TO_CHECK]\n",
    "\n",
    "# --- 2. Run the SLOW Original (Normalized) function ---\n",
    "historical_slice = ticker_history.loc[:DATE_TO_CHECK]\n",
    "original_result_today = analyze_ticker_trends_original(historical_slice, LOOKBACK_DAYS)\n",
    "\n",
    "# --- 3. Compare the results ---\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Log-Vectorized': vectorized_result_today,\n",
    "    'Original (Normalized)': original_result_today\n",
    "}).dropna()\n",
    "\n",
    "print(\"### Important Note ###\")\n",
    "print(\"The slope of a log(price) series is mathematically very similar to the slope of a normalized (price/price_0) series.\")\n",
    "print(\"Therefore, both the SLOPES and R-SQUARED values should now be very close.\\n\")\n",
    "\n",
    "print(\"### Side-by-Side Comparison ###\")\n",
    "print(comparison_df)\n",
    "\n",
    "# --- 4. Programmatic Check for ALL values ---\n",
    "try:\n",
    "    # We now test BOTH slope and r_squared\n",
    "    pd.testing.assert_series_equal(\n",
    "        comparison_df['Log-Vectorized'],\n",
    "        comparison_df['Original (Normalized)'],\n",
    "        atol=0.05 # Use a slightly larger tolerance for slope approximation\n",
    "    )\n",
    "    print(\"\\n[SUCCESS]: Log-vectorized results closely match the original normalized results!\")\n",
    "except AssertionError as e:\n",
    "    print(\"\\n[FAILURE]: Results do not match.\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_results_full.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_results_full.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refactored Code\n",
    "\n",
    "Here is the complete, refactored solution. I've included the previously refactored functions with slight modifications to accept the new configuration structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def calculate_rolling_z_scores_general(df_group, columns_to_process, rolling_window=20):\n",
    "    \"\"\"\n",
    "    This function output has been verified. \n",
    "\n",
    "    Calculates rolling Z-scores for a list of specified columns.\n",
    "    \n",
    "    This is a flexible, reusable, and efficient version.\n",
    "    \n",
    "    Args:\n",
    "        df_group (pd.DataFrame): The DataFrame for a single ticker.\n",
    "        columns_to_process (list): A list of column names to calculate Z-scores for.\n",
    "        rolling_window (int): The lookback window.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with Z-score columns, prefixed with 'z_score_'.\n",
    "                      Returns NaNs for non-computable values.\n",
    "    \"\"\"\n",
    "    if df_group.empty or len(df_group) < rolling_window:\n",
    "        # Return an empty DataFrame with the expected column names for consistency\n",
    "        return pd.DataFrame(columns=[f\"z_score_{col}\" for col in columns_to_process])\n",
    "\n",
    "    # Select the subset of data to work on\n",
    "    data_subset = df_group[columns_to_process]\n",
    "    \n",
    "    # Calculate rolling stats for all columns at once.\n",
    "    # This correctly produces NaNs for the initial, incomplete windows.\n",
    "    rolling_mean = data_subset.rolling(window=rolling_window).mean()\n",
    "    rolling_std = data_subset.rolling(window=rolling_window).std()\n",
    "    \n",
    "    # Calculate Z-scores for all columns in one vectorized operation.\n",
    "    z_scores_df = (data_subset - rolling_mean) / rolling_std\n",
    "    \n",
    "    # Handle true division-by-zero errors (where std is 0)\n",
    "    z_scores_df = z_scores_df.replace([np.inf, -np.inf], 0)\n",
    "    \n",
    "    # Add a descriptive prefix to the column names (e.g., 'Adj Low' -> 'z_score_Adj Low')\n",
    "    return z_scores_df.add_prefix('z_score_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_scores = df_train.groupby(level='Ticker', group_keys=False).apply(\n",
    "    calculate_rolling_z_scores_general, df_train.columns, rolling_window=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_MSFT_w10 = z_scores.loc['MSFT']\n",
    "z_MSFT_w10.to_csv(r'C:\\Users\\ping\\Desktop\\z_MSFT_w10.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Refined Core Backtesting Functions\n",
    "\n",
    "We'll modify the function signatures to accept a single `config` dictionary. This makes them more modular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from itertools import product\n",
    "\n",
    "def precompute_signals(df_ohlcv, config):\n",
    "    \"\"\"Pre-computes trading signals using the general-purpose Z-score function.\"\"\"\n",
    "    print(\"Pre-computing features for this parameter set...\")\n",
    "    \n",
    "    trends = df_ohlcv.groupby(level='Ticker', group_keys=False).apply(\n",
    "        analyze_ticker_trends_log_vectorized, config['lookback_days']\n",
    "    )\n",
    "    \n",
    "    # --- KEY CHANGE: Define which Z-scores you want to calculate ---\n",
    "    z_score_columns = ['Adj Open', 'Adj High', 'Adj Low', 'Adj Close', 'Volume'] # You can now easily add 'Adj Close', etc.\n",
    "    \n",
    "    # --- Use the new general function ---\n",
    "    z_scores = df_ohlcv.groupby(level='Ticker', group_keys=False).apply(\n",
    "        calculate_rolling_z_scores_general, \n",
    "        columns_to_process=z_score_columns,\n",
    "        rolling_window=config['rolling_window']\n",
    "    )\n",
    "    \n",
    "    features = trends.join(z_scores).dropna()\n",
    "\n",
    "    # --- KEY CHANGE: Update the signal condition to use the new column name ---\n",
    "    # The column is now 'z_score_Adj Low', not 'low_rolling_z_score'.\n",
    "    signals = features[\n",
    "        (features['low_slope'] > config['slope_thresh']) &\n",
    "        (features['low_r_squared'] > config['r2_thresh']) &\n",
    "        (features['volume_slope'] > 0) &\n",
    "        (features['z_score_Adj Low'] < config['z_entry_thresh'])\n",
    "        # You could now add another condition like:\n",
    "        # & (features['z_score_Volume'] > 1.0) \n",
    "    ]\n",
    "\n",
    "\n",
    "####################################\n",
    "    # return signals\n",
    "    return signals, features\n",
    "####################################\n",
    "\n",
    "\n",
    "def run_backtest(df_ohlcv, config):\n",
    "    \"\"\"\n",
    "    Orchestrates the backtesting process with enhanced logging.\n",
    "    \"\"\"\n",
    "    # Now returns a DataFrame of features for triggered signals\n",
    "    entry_signals_features = precompute_signals(df_ohlcv, config)\n",
    "    \n",
    "    trades = []\n",
    "    open_positions = {}\n",
    "    \n",
    "    all_dates = df_ohlcv.index.get_level_values('Date').unique().sort_values()\n",
    "    start_index = max(config['lookback_days'], config['rolling_window'])\n",
    "\n",
    "    for i in tqdm(range(start_index, len(all_dates) - 1), desc=\"Backtesting\"):\n",
    "        current_date = all_dates[i]\n",
    "        next_day_date = all_dates[i+1]\n",
    "\n",
    "        closed_trades, open_positions = handle_exits_for_day(\n",
    "            current_date, next_day_date, open_positions, df_ohlcv, config\n",
    "        )\n",
    "        trades.extend(closed_trades)\n",
    "\n",
    "        # --- KEY CHANGE: Filter the features DataFrame for today's signals ---\n",
    "        signals_today = entry_signals_features[\n",
    "            entry_signals_features.index.get_level_values('Date') == current_date\n",
    "        ]\n",
    "        \n",
    "        # Pass the full signals_today DataFrame to the handler\n",
    "        open_positions = handle_entries_for_day(\n",
    "            current_date, next_day_date, signals_today, open_positions, df_ohlcv\n",
    "        )\n",
    "                \n",
    "    # --- Create the final DataFrame and reorder columns for clarity ---\n",
    "    if not trades:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    final_trades_df = pd.DataFrame(trades)\n",
    "    log_columns = [\n",
    "        'ticker', 'signal_date', 'entry_date', 'exit_signal_date', 'exit_date', 'reason',\n",
    "        'return', 'entry_price_actual', 'exit_price_actual', 'exit_trigger_price', \n",
    "        'exit_target_value', 'entry_signal_features'\n",
    "    ]\n",
    "    # Ensure all columns exist, fill missing with None\n",
    "    for col in log_columns:\n",
    "        if col not in final_trades_df.columns:\n",
    "            final_trades_df[col] = None\n",
    "            \n",
    "    return final_trades_df[log_columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-computing features for this parameter set...\n"
     ]
    }
   ],
   "source": [
    "# 1. Define the full configuration\n",
    "config = {\n",
    "    'lookback_days': 30,\n",
    "    'rolling_window': 10,  # Set the value you want to test\n",
    "    'slope_thresh': 0.05,\n",
    "    'r2_thresh': 0.50,\n",
    "    'z_entry_thresh': -1.5,\n",
    "    # ... other params if needed\n",
    "}\n",
    "\n",
    "# 2. Call the function correctly\n",
    "signals_df, features_df = precompute_signals(df_train, config) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jupyter Notebook: Testing `precompute_signals`\n",
    "\n",
    "#### Cell 1: Setup and Test Functions\n",
    "\n",
    "First, we need the function definitions and a small, predictable sample dataset. Testing on the full `df_train` is too difficult to verify manually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Paste your verified helper functions here ---\n",
    "# analyze_ticker_trends_log_vectorized(...)\n",
    "# calculate_rolling_z_scores_general(...)\n",
    "\n",
    "# --- The function we are testing ---\n",
    "def precompute_signals(df_ohlcv, config):\n",
    "    \"\"\"Pre-computes trading signals using the general-purpose Z-score function.\"\"\"\n",
    "    print(\"Pre-computing features for this parameter set...\")\n",
    "    \n",
    "    trends = df_ohlcv.groupby(level='Ticker', group_keys=False).apply(\n",
    "        analyze_ticker_trends_log_vectorized, config['lookback_days']\n",
    "    )\n",
    "    \n",
    "    z_score_columns = ['Adj Low', 'Volume']\n",
    "    z_scores = df_ohlcv.groupby(level='Ticker', group_keys=False).apply(\n",
    "        calculate_rolling_z_scores_general, \n",
    "        columns_to_process=z_score_columns,\n",
    "        rolling_window=config['rolling_window']\n",
    "    )\n",
    "    \n",
    "    features = trends.join(z_scores).dropna()\n",
    "\n",
    "    signals = features[\n",
    "        (features['low_slope'] > config['slope_thresh']) &\n",
    "        (features['low_r_squared'] > config['r2_thresh']) &\n",
    "        (features['volume_slope'] > 0) &\n",
    "        (features['z_score_Adj Low'] < config['z_entry_thresh'])\n",
    "    ]\n",
    "    return signals\n",
    "\n",
    "# --- A special version for \"white-box\" testing that returns the intermediate features ---\n",
    "def precompute_signals_for_testing(df_ohlcv, config):\n",
    "    \"\"\"A modified version that returns both the features and the final signals.\"\"\"\n",
    "    # (Code is identical to above, just the return statement is different)\n",
    "    trends = df_ohlcv.groupby(level='Ticker', group_keys=False).apply(analyze_ticker_trends_log_vectorized, config['lookback_days'])\n",
    "\n",
    "###########################  \n",
    "    # z_score_columns = ['Adj Low', 'Volume']\n",
    "    z_score_columns = df_ohlcv.columns\n",
    "###########################  \n",
    "\n",
    "    z_scores = df_ohlcv.groupby(level='Ticker', group_keys=False).apply(calculate_rolling_z_scores_general, columns_to_process=z_score_columns, rolling_window=config['rolling_window'])\n",
    "    features = trends.join(z_scores).dropna()\n",
    "###########################    \n",
    "    # signals = features[\n",
    "    #     (features['low_slope'] > config['slope_thresh']) &\n",
    "    #     (features['low_r_squared'] > config['r2_thresh']) &\n",
    "    #     (features['volume_slope'] > 0) &\n",
    "    #     (features['z_score_Adj Low'] < config['z_entry_thresh'])\n",
    "    # ]\n",
    "    # signals = features[\n",
    "    #     ((features['low_slope'] > config['slope_thresh']) &\n",
    "    #     (features['low_slope'] < .008)) &    \n",
    "    #     (features['low_r_squared'] > config['r2_thresh']) &\n",
    "    #     (features['volume_slope'] > 0) &\n",
    "    #     (features['z_score_Adj Low'] < config['z_entry_thresh'])\n",
    "    # ]\n",
    "# --- same-column filter ------------------------------------------------------\n",
    "    low_slope_ok = (\n",
    "        # (features['low_slope'] > config['slope_thresh']) &\n",
    "        (features['high_slope'] > features['low_slope'])\n",
    "    )\n",
    "\n",
    "    # --- final signal mask -------------------------------------------------------\n",
    "    signals = features[\n",
    "        low_slope_ok\n",
    "        & (features['low_r_squared'] > config['r2_thresh'])\n",
    "        & (features['volume_slope'] > 0)\n",
    "        & (features['z_score_Adj Low'] < config['z_entry_thresh'])\n",
    "        & (features['volume_slope'] > 0)\n",
    "        & (features['z_score_Adj Low'] > 0)        \n",
    "    ]\n",
    "\n",
    "###########################          \n",
    "    return features, signals, trends # Return both for inspection\n",
    "\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cell 2: Create a Controlled Sample Dataset\n",
    "\n",
    "We'll create a small DataFrame with one ticker that has a predictable trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample DataFrame created.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Adj High</th>\n",
       "      <th>Adj Low</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ticker</th>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">TEST</th>\n",
       "      <th>2023-01-02</th>\n",
       "      <td>101.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-03</th>\n",
       "      <td>102.367014</td>\n",
       "      <td>100.367014</td>\n",
       "      <td>101.367014</td>\n",
       "      <td>101020.408163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-04</th>\n",
       "      <td>103.499269</td>\n",
       "      <td>101.499269</td>\n",
       "      <td>102.499269</td>\n",
       "      <td>102040.816327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-05</th>\n",
       "      <td>104.219480</td>\n",
       "      <td>102.219480</td>\n",
       "      <td>103.219480</td>\n",
       "      <td>103061.224490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-06</th>\n",
       "      <td>104.451248</td>\n",
       "      <td>102.451248</td>\n",
       "      <td>103.451248</td>\n",
       "      <td>104081.632653</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Adj High     Adj Low   Adj Close         Volume\n",
       "Ticker Date                                                         \n",
       "TEST   2023-01-02  101.000000   99.000000  100.000000  100000.000000\n",
       "       2023-01-03  102.367014  100.367014  101.367014  101020.408163\n",
       "       2023-01-04  103.499269  101.499269  102.499269  102040.816327\n",
       "       2023-01-05  104.219480  102.219480  103.219480  103061.224490\n",
       "       2023-01-06  104.451248  102.451248  103.451248  104081.632653"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a date range\n",
    "dates = pd.to_datetime(pd.date_range(start='2023-01-01', periods=50, freq='B'))\n",
    "\n",
    "# Create data for Ticker 'TEST' with a clear upward trend and some noise\n",
    "price = 100 + np.linspace(0, 20, 50) + np.sin(np.arange(50)/2) * 2\n",
    "volume = 100000 + np.linspace(0, 50000, 50) # Strong positive volume slope\n",
    "df_test = pd.DataFrame({\n",
    "    'Ticker': 'TEST', 'Date': dates,\n",
    "    'Adj High': price + 1, 'Adj Low': price - 1, 'Adj Close': price,\n",
    "    'Volume': volume\n",
    "})\n",
    "\n",
    "# Add a ticker with insufficient data to test edge cases\n",
    "df_short = pd.DataFrame({\n",
    "    'Ticker': 'SHORT', 'Date': dates[:5],\n",
    "    'Adj High': 10, 'Adj Low': 9, 'Adj Close': 9.5, 'Volume': 1000\n",
    "})\n",
    "\n",
    "\n",
    "# Combine and set index\n",
    "sample_df = pd.concat([df_test, df_short]).set_index(['Ticker', 'Date'])\n",
    "print(\"Sample DataFrame created.\")\n",
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df.to_csv(r'C:\\Users\\ping\\Desktop\\sample_df.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_df:\n",
      "                     Adj High     Adj Low   Adj Close         Volume\n",
      "Ticker Date                                                         \n",
      "TEST   2023-01-02  101.000000   99.000000  100.000000  100000.000000\n",
      "       2023-01-03  102.367014  100.367014  101.367014  101020.408163\n",
      "       2023-01-04  103.499269  101.499269  102.499269  102040.816327\n",
      "       2023-01-05  104.219480  102.219480  103.219480  103061.224490\n",
      "       2023-01-06  104.451248  102.451248  103.451248  104081.632653\n",
      "       2023-01-09  104.237761  102.237761  103.237761  105102.040816\n",
      "       2023-01-10  103.731220  101.731220  102.731220  106122.448980\n",
      "       2023-01-11  103.155576  101.155576  102.155576  107142.857143\n",
      "       2023-01-12  102.751701  100.751701  101.751701  108163.265306\n",
      "       2023-01-13  102.718409  100.718409  101.718409  109183.673469\n",
      "       2023-01-16  103.163784  101.163784  102.163784  110204.081633\n",
      "       2023-01-17  104.078715  102.078715  103.078715  111224.489796\n",
      "       2023-01-18  105.339128  103.339128  104.339128  112244.897959\n",
      "       2023-01-19  106.736362  104.736362  105.736362  113265.306122\n",
      "       2023-01-20  108.028259  106.028259  107.028259  114285.714286\n",
      "       2023-01-23  108.998449  106.998449  107.998449  115306.122449\n",
      "       2023-01-24  109.509329  107.509329  108.509329  116326.530612\n",
      "       2023-01-25  109.535750  107.535750  108.535750  117346.938776\n",
      "       2023-01-26  109.171176  107.171176  108.171176  118367.346939\n",
      "       2023-01-27  108.604800  106.604800  107.604800  119387.755102\n",
      "       2023-01-30  108.075223  106.075223  107.075223  120408.163265\n",
      "       2023-01-31  107.812037  105.812037  106.812037  121428.571429\n",
      "       2023-02-01  107.979611  105.979611  106.979611  122448.979592\n",
      "       2023-02-02  108.636851  106.636851  107.636851  123469.387755\n",
      "       2023-02-03  109.722773  107.722773  108.722773  124489.795918\n",
      "       2023-02-06  111.071438  109.071438  110.071438  125510.204082\n",
      "       2023-02-07  112.452579  110.452579  111.452579  126530.612245\n",
      "       2023-02-08  113.627977  111.627977  112.627977  127551.020408\n",
      "       2023-02-09  114.409786  112.409786  113.409786  128571.428571\n",
      "       2023-02-10  114.706525  112.706525  113.706525  129591.836735\n",
      "       2023-02-13  114.545474  112.545474  113.545474  130612.244898\n",
      "       2023-02-14  114.065996  112.065996  113.065996  131632.653061\n",
      "       2023-02-15  113.485418  111.485418  112.485418  132653.061224\n",
      "       2023-02-16  113.045817  111.045817  112.045817  133673.469388\n",
      "       2023-02-17  112.954756  110.954756  111.954756  134693.877551\n",
      "       2023-02-20  113.334462  111.334462  112.334462  135714.285714\n",
      "       2023-02-21  114.191903  112.191903  113.191903  136734.693878\n",
      "       2023-02-22  115.417080  113.417080  114.417080  137755.102041\n",
      "       2023-02-23  116.809959  114.809959  115.809959  138775.510204\n",
      "       2023-02-24  118.129447  116.129447  117.129447  139795.918367\n",
      "       2023-02-27  119.152421  117.152421  118.152421  140816.326531\n",
      "       2023-02-28  119.728353  117.728353  118.728353  141836.734694\n",
      "       2023-03-01  119.816168  117.816168  118.816168  142857.142857\n",
      "       2023-03-02  119.494298  117.494298  118.494298  143877.551020\n",
      "       2023-03-03  118.941481  116.941481  117.941481  144897.959184\n",
      "       2023-03-06  118.392998  116.392998  117.392998  145918.367347\n",
      "       2023-03-07  118.083069  116.083069  117.083069  146938.775510\n",
      "       2023-03-08  118.187509  116.187509  117.187509  147959.183673\n",
      "       2023-03-09  118.780680  116.780680  117.780680  148979.591837\n",
      "       2023-03-10  119.817285  117.817285  118.817285  150000.000000\n",
      "SHORT  2023-01-02   10.000000    9.000000    9.500000    1000.000000\n",
      "       2023-01-03   10.000000    9.000000    9.500000    1000.000000\n",
      "       2023-01-04   10.000000    9.000000    9.500000    1000.000000\n",
      "       2023-01-05   10.000000    9.000000    9.500000    1000.000000\n",
      "       2023-01-06   10.000000    9.000000    9.500000    1000.000000\n"
     ]
    }
   ],
   "source": [
    "print(f'sample_df:\\n{sample_df}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: \"White-Box\" Test - Inspect the Intermediate `features`\n",
    "\n",
    "This is the most important step. We use our special testing function to look \"inside\" and see if the features are being generated correctly before the filtering logic is applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Test 1: Inspecting the full 'features' DataFrame ---\n",
      "Shape of the features DataFrame: (40, 12)\n",
      "Note: The first (lookback_days - 1) rows should be missing due to NaNs.\n",
      "The ticker 'SHORT' should not appear at all.\n",
      "\n",
      "Head of features:\n",
      "                   high_slope  high_r_squared  low_slope  low_r_squared  volume_slope  volume_r_squared  unified_std_dev_returns  volume_std_dev_returns  z_score_Adj High  z_score_Adj Low  z_score_Adj Close  z_score_Volume\n",
      "Ticker Date                                                                                                                                                                                                                   \n",
      "TEST   2023-01-16   -0.000585        0.064806  -0.000596       0.064794      0.009666          0.999850                 0.006645                0.000274          0.145474         0.145474           0.145474        1.264911\n",
      "       2023-01-17   -0.000929        0.208933  -0.000947       0.208935      0.009574          0.999853                 0.005835                0.000269          1.649108         1.649108           1.649108        1.264911\n",
      "       2023-01-18   -0.000085        0.001010  -0.000087       0.001013      0.009483          0.999856                 0.005961                0.000263          1.555584         1.555584           1.555584        1.264911\n",
      "       2023-01-19    0.001727        0.189342   0.001761       0.189292      0.009394          0.999859                 0.006730                0.000258          1.417468         1.417468           1.417468        1.264911\n",
      "       2023-01-20    0.004054        0.523005   0.004133       0.522972      0.009306          0.999861                 0.007284                0.000254          1.303355         1.303355           1.303355        1.264911\n",
      "\n",
      "Tail of features:\n",
      "                   high_slope  high_r_squared  low_slope  low_r_squared  volume_slope  volume_r_squared  unified_std_dev_returns  volume_std_dev_returns  z_score_Adj High  z_score_Adj Low  z_score_Adj Close  z_score_Volume\n",
      "Ticker Date                                                                                                                                                                                                                   \n",
      "TEST   2023-03-06    0.004222        0.602374   0.004296       0.602388      0.007222          0.999916                 0.006703                0.000152         -1.471663        -1.471663          -1.471663        1.264911\n",
      "       2023-03-07    0.002081        0.284026   0.002117       0.284066      0.007170          0.999917                 0.006891                0.000150         -1.188063        -1.188063          -1.188063        1.264911\n",
      "       2023-03-08    0.000293        0.012686   0.000298       0.012696      0.007119          0.999919                 0.006364                0.000148         -0.732233        -0.732233          -0.732233        1.264911\n",
      "       2023-03-09   -0.000710        0.147833  -0.000722       0.147827      0.007069          0.999920                 0.005482                0.000146          0.815601         0.815601           0.815601        1.264911\n",
      "       2023-03-10   -0.000692        0.138677  -0.000704       0.138683      0.007019          0.999921                 0.004997                0.000144          1.655423         1.655423           1.655423        1.264911\n",
      "\n",
      "[SUCCESS]: Ticker with insufficient data was correctly ignored.\n",
      "[SUCCESS]: Features DataFrame contains no NaNs.\n",
      "\n",
      "--- Sanity Check Passed for Feature Generation ---\n"
     ]
    }
   ],
   "source": [
    "# --- Test Configuration ---\n",
    "test_config = {\n",
    "    'lookback_days': 10,\n",
    "    'rolling_window': 5,\n",
    "    'slope_thresh': 0, # Set low to ensure we get some signals\n",
    "    'r2_thresh': 0.3,\n",
    "    'z_entry_thresh': -0.5,\n",
    "    'volume_thresh': 0,\n",
    "}\n",
    "\n",
    "# --- Run the special testing function ---\n",
    "all_features, signals_from_test_func, trends = precompute_signals_for_testing(sample_df, test_config)\n",
    "\n",
    "\n",
    "print(\"--- Test 1: Inspecting the full 'features' DataFrame ---\")\n",
    "print(f\"Shape of the features DataFrame: {all_features.shape}\")\n",
    "print(\"Note: The first (lookback_days - 1) rows should be missing due to NaNs.\")\n",
    "print(\"The ticker 'SHORT' should not appear at all.\\n\")\n",
    "\n",
    "# Display the head and tail to check values\n",
    "print(\"Head of features:\")\n",
    "print(all_features.head())\n",
    "print(\"\\nTail of features:\")\n",
    "print(all_features.tail())\n",
    "\n",
    "# --- Verification Checks for 'features' ---\n",
    "assert 'SHORT' not in all_features.index.get_level_values('Ticker'), \"FAIL: Ticker with insufficient data was not filtered out.\"\n",
    "print(\"\\n[SUCCESS]: Ticker with insufficient data was correctly ignored.\")\n",
    "\n",
    "assert not all_features.isnull().values.any(), \"FAIL: The features DataFrame contains unexpected NaNs after dropna().\"\n",
    "print(\"[SUCCESS]: Features DataFrame contains no NaNs.\")\n",
    "\n",
    "print(\"\\n--- Sanity Check Passed for Feature Generation ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "trends.to_csv(r'C:\\Users\\ping\\Desktop\\trends.csv', index=True)\n",
    "all_features.to_csv(r'C:\\Users\\ping\\Desktop\\all_features.csv', index=True)\n",
    "signals_from_test_func.to_csv(r'C:\\Users\\ping\\Desktop\\signals_from_test_func.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signals = features[\n",
    "    (features['low_slope'] > config['slope_thresh']) &\n",
    "    (features['low_r_squared'] > config['r2_thresh']) &\n",
    "    (features['volume_slope'] > 0) &\n",
    "    (features['z_score_Adj Low'] < config['z_entry_thresh'])\n",
    "]\n",
    "\n",
    "test_config = {\n",
    "    'lookback_days': 10,\n",
    "    'rolling_window': 5,\n",
    "    'slope_thresh': 0.005, # Set low to ensure we get some signals\n",
    "    'r2_thresh': 0.3,\n",
    "    'z_entry_thresh': -0.5,\n",
    "    'volume_thresh': 0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trends:\n",
      "                   high_slope  high_r_squared  low_slope  low_r_squared  volume_slope  volume_r_squared  unified_std_dev_returns  volume_std_dev_returns\n",
      "Ticker Date                                                                                                                                             \n",
      "TEST   2023-01-02         NaN             NaN        NaN            NaN           NaN               NaN                      NaN                     NaN\n",
      "       2023-01-03         NaN             NaN        NaN            NaN           NaN               NaN                      NaN                     NaN\n",
      "       2023-01-04         NaN             NaN        NaN            NaN           NaN               NaN                      NaN                     NaN\n",
      "       2023-01-05         NaN             NaN        NaN            NaN           NaN               NaN                      NaN                     NaN\n",
      "       2023-01-06         NaN             NaN        NaN            NaN           NaN               NaN                      NaN                     NaN\n",
      "       2023-01-09         NaN             NaN        NaN            NaN           NaN               NaN                      NaN                     NaN\n",
      "       2023-01-10         NaN             NaN        NaN            NaN           NaN               NaN                      NaN                     NaN\n",
      "       2023-01-11         NaN             NaN        NaN            NaN           NaN               NaN                      NaN                     NaN\n",
      "       2023-01-12         NaN             NaN        NaN            NaN           NaN               NaN                      NaN                     NaN\n",
      "       2023-01-13    0.000881        0.067534   0.000898       0.067566      0.009761          0.999847                      NaN                     NaN\n",
      "       2023-01-16   -0.000585        0.064806  -0.000596       0.064794      0.009666          0.999850                 0.006645                0.000274\n",
      "       2023-01-17   -0.000929        0.208933  -0.000947       0.208935      0.009574          0.999853                 0.005835                0.000269\n",
      "       2023-01-18   -0.000085        0.001010  -0.000087       0.001013      0.009483          0.999856                 0.005961                0.000263\n",
      "       2023-01-19    0.001727        0.189342   0.001761       0.189292      0.009394          0.999859                 0.006730                0.000258\n",
      "       2023-01-20    0.004054        0.523005   0.004133       0.522972      0.009306          0.999861                 0.007284                0.000254\n",
      "       2023-01-23    0.006324        0.773582   0.006446       0.773590      0.009220          0.999864                 0.007156                0.000249\n",
      "       2023-01-24    0.007983        0.914541   0.008137       0.914560      0.009136          0.999866                 0.006435                0.000244\n",
      "       2023-01-25    0.008628        0.961420   0.008793       0.961421      0.009053          0.999869                 0.005731                0.000240\n",
      "       2023-01-26    0.008101        0.923795   0.008256       0.923773      0.008972          0.999871                 0.005830                0.000236\n",
      "       2023-01-27    0.006534        0.796915   0.006659       0.796899      0.008892          0.999873                 0.006682                0.000231\n",
      "       2023-01-30    0.004314        0.565259   0.004397       0.565282      0.008814          0.999875                 0.007463                0.000227\n",
      "       2023-01-31    0.001986        0.238794   0.002024       0.238842      0.008737          0.999878                 0.007556                0.000223\n",
      "       2023-02-01    0.000118        0.001912   0.000120       0.001917      0.008661          0.999880                 0.006881                0.000219\n",
      "       2023-02-02   -0.000841        0.182076  -0.000857       0.182072      0.008587          0.999882                 0.005908                0.000216\n",
      "       2023-02-03   -0.000669        0.101311  -0.000682       0.101321      0.008514          0.999884                 0.005483                0.000212\n",
      "       2023-02-06    0.000577        0.036477   0.000587       0.036456      0.008442          0.999886                 0.005968                0.000208\n",
      "       2023-02-07    0.002580        0.337143   0.002627       0.337097      0.008371          0.999888                 0.006704                0.000205\n",
      "       2023-02-08    0.004844        0.645597   0.004934       0.645584      0.008301          0.999890                 0.006977                0.000201\n",
      "       2023-02-09    0.006816        0.847305   0.006942       0.847321      0.008233          0.999891                 0.006571                0.000198\n",
      "       2023-02-10    0.008015        0.945527   0.008162       0.945539      0.008166          0.999893                 0.005792                0.000195\n",
      "       2023-02-13    0.008148        0.955441   0.008297       0.955432      0.008100          0.999895                 0.005379                0.000192\n",
      "       2023-02-14    0.007185        0.880585   0.007317       0.880564      0.008035          0.999896                 0.005850                0.000189\n",
      "       2023-02-15    0.005364        0.710198   0.005462       0.710196      0.007971          0.999898                 0.006731                0.000186\n",
      "       2023-02-16    0.003133        0.431215   0.003190       0.431251      0.007907          0.999900                 0.007235                0.000183\n",
      "       2023-02-17    0.001039        0.101787   0.001058       0.101820      0.007845          0.999901                 0.006989                0.000180\n",
      "       2023-02-20   -0.000409        0.035195  -0.000417       0.035185      0.007784          0.999903                 0.006144                0.000177\n",
      "       2023-02-21   -0.000867        0.224121  -0.000883       0.224122      0.007724          0.999904                 0.005347                0.000174\n",
      "       2023-02-22   -0.000236        0.010470  -0.000241       0.010477      0.007665          0.999906                 0.005347                0.000171\n",
      "       2023-02-23    0.001317        0.147264   0.001340       0.147227      0.007607          0.999907                 0.006022                0.000169\n",
      "       2023-02-24    0.003404        0.481772   0.003464       0.481740      0.007549          0.999909                 0.006594                0.000166\n",
      "       2023-02-27    0.005511        0.746850   0.005608       0.746853      0.007493          0.999910                 0.006572                0.000164\n",
      "       2023-02-28    0.007124        0.901504   0.007249       0.901521      0.007437          0.999911                 0.005966                0.000161\n",
      "       2023-03-01    0.007849        0.960115   0.007987       0.960118      0.007382          0.999913                 0.005271                0.000159\n",
      "       2023-03-02    0.007511        0.933761   0.007642       0.933745      0.007328          0.999914                 0.005228                0.000157\n",
      "       2023-03-03    0.006194        0.819506   0.006302       0.819491      0.007275          0.999915                 0.005943                0.000154\n",
      "       2023-03-06    0.004222        0.602374   0.004296       0.602388      0.007222          0.999916                 0.006703                0.000152\n",
      "       2023-03-07    0.002081        0.284026   0.002117       0.284066      0.007170          0.999917                 0.006891                0.000150\n",
      "       2023-03-08    0.000293        0.012686   0.000298       0.012696      0.007119          0.999919                 0.006364                0.000148\n",
      "       2023-03-09   -0.000710        0.147833  -0.000722       0.147827      0.007069          0.999920                 0.005482                0.000146\n",
      "       2023-03-10   -0.000692        0.138677  -0.000704       0.138683      0.007019          0.999921                 0.004997                0.000144\n"
     ]
    }
   ],
   "source": [
    "print(f'trends:\\n{trends}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_df:\n",
      "                     Adj High     Adj Low   Adj Close         Volume\n",
      "Ticker Date                                                         \n",
      "TEST   2023-01-02  101.000000   99.000000  100.000000  100000.000000\n",
      "       2023-01-03  102.367014  100.367014  101.367014  101020.408163\n",
      "       2023-01-04  103.499269  101.499269  102.499269  102040.816327\n",
      "       2023-01-05  104.219480  102.219480  103.219480  103061.224490\n",
      "       2023-01-06  104.451248  102.451248  103.451248  104081.632653\n",
      "       2023-01-09  104.237761  102.237761  103.237761  105102.040816\n",
      "       2023-01-10  103.731220  101.731220  102.731220  106122.448980\n",
      "       2023-01-11  103.155576  101.155576  102.155576  107142.857143\n",
      "       2023-01-12  102.751701  100.751701  101.751701  108163.265306\n",
      "       2023-01-13  102.718409  100.718409  101.718409  109183.673469\n",
      "       2023-01-16  103.163784  101.163784  102.163784  110204.081633\n",
      "       2023-01-17  104.078715  102.078715  103.078715  111224.489796\n",
      "       2023-01-18  105.339128  103.339128  104.339128  112244.897959\n",
      "       2023-01-19  106.736362  104.736362  105.736362  113265.306122\n",
      "       2023-01-20  108.028259  106.028259  107.028259  114285.714286\n",
      "       2023-01-23  108.998449  106.998449  107.998449  115306.122449\n",
      "       2023-01-24  109.509329  107.509329  108.509329  116326.530612\n",
      "       2023-01-25  109.535750  107.535750  108.535750  117346.938776\n",
      "       2023-01-26  109.171176  107.171176  108.171176  118367.346939\n",
      "       2023-01-27  108.604800  106.604800  107.604800  119387.755102\n",
      "       2023-01-30  108.075223  106.075223  107.075223  120408.163265\n",
      "       2023-01-31  107.812037  105.812037  106.812037  121428.571429\n",
      "       2023-02-01  107.979611  105.979611  106.979611  122448.979592\n",
      "       2023-02-02  108.636851  106.636851  107.636851  123469.387755\n",
      "       2023-02-03  109.722773  107.722773  108.722773  124489.795918\n",
      "       2023-02-06  111.071438  109.071438  110.071438  125510.204082\n",
      "       2023-02-07  112.452579  110.452579  111.452579  126530.612245\n",
      "       2023-02-08  113.627977  111.627977  112.627977  127551.020408\n",
      "       2023-02-09  114.409786  112.409786  113.409786  128571.428571\n",
      "       2023-02-10  114.706525  112.706525  113.706525  129591.836735\n",
      "       2023-02-13  114.545474  112.545474  113.545474  130612.244898\n",
      "       2023-02-14  114.065996  112.065996  113.065996  131632.653061\n",
      "       2023-02-15  113.485418  111.485418  112.485418  132653.061224\n",
      "       2023-02-16  113.045817  111.045817  112.045817  133673.469388\n",
      "       2023-02-17  112.954756  110.954756  111.954756  134693.877551\n",
      "       2023-02-20  113.334462  111.334462  112.334462  135714.285714\n",
      "       2023-02-21  114.191903  112.191903  113.191903  136734.693878\n",
      "       2023-02-22  115.417080  113.417080  114.417080  137755.102041\n",
      "       2023-02-23  116.809959  114.809959  115.809959  138775.510204\n",
      "       2023-02-24  118.129447  116.129447  117.129447  139795.918367\n",
      "       2023-02-27  119.152421  117.152421  118.152421  140816.326531\n",
      "       2023-02-28  119.728353  117.728353  118.728353  141836.734694\n",
      "       2023-03-01  119.816168  117.816168  118.816168  142857.142857\n",
      "       2023-03-02  119.494298  117.494298  118.494298  143877.551020\n",
      "       2023-03-03  118.941481  116.941481  117.941481  144897.959184\n",
      "       2023-03-06  118.392998  116.392998  117.392998  145918.367347\n",
      "       2023-03-07  118.083069  116.083069  117.083069  146938.775510\n",
      "       2023-03-08  118.187509  116.187509  117.187509  147959.183673\n",
      "       2023-03-09  118.780680  116.780680  117.780680  148979.591837\n",
      "       2023-03-10  119.817285  117.817285  118.817285  150000.000000\n",
      "SHORT  2023-01-02   10.000000    9.000000    9.500000    1000.000000\n",
      "       2023-01-03   10.000000    9.000000    9.500000    1000.000000\n",
      "       2023-01-04   10.000000    9.000000    9.500000    1000.000000\n",
      "       2023-01-05   10.000000    9.000000    9.500000    1000.000000\n",
      "       2023-01-06   10.000000    9.000000    9.500000    1000.000000\n"
     ]
    }
   ],
   "source": [
    "print(f'sample_df:\\n{sample_df}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: \"Black-Box\" Test - Verify the Final `signals` DataFrame\n",
    "\n",
    "Now we test the real function. Our goal is to prove that **every single row** in the final `signals_df` meets the filtering criteria defined in our `test_config`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-computing features for this parameter set...\n",
      "\n",
      "--- Test 2: Verifying the final 'signals' DataFrame ---\n",
      "Found 3 potential signals to verify.\n",
      "\n",
      "[SUCCESS]: All rows in the final signals DataFrame correctly meet the filter criteria.\n",
      "\n",
      "--- Sanity Check Passed for Signal Filtering ---\n"
     ]
    }
   ],
   "source": [
    "# --- Run the REAL function to get the final output ---\n",
    "signals_df = precompute_signals(sample_df, test_config)\n",
    "\n",
    "print(f\"\\n--- Test 2: Verifying the final 'signals' DataFrame ---\")\n",
    "print(f\"Found {len(signals_df)} potential signals to verify.\")\n",
    "\n",
    "# --- Programmatic Verification ---\n",
    "# For every signal found, we cross-reference it with the 'all_features' DataFrame\n",
    "# and assert that its values meet the criteria.\n",
    "\n",
    "for idx, signal_row in signals_df.iterrows():\n",
    "    # Find the original, unfiltered features for this specific signal\n",
    "    original_features = all_features.loc[idx]\n",
    "\n",
    "    # Assert that each condition is met\n",
    "    try:\n",
    "        assert original_features['low_slope'] > test_config['slope_thresh']\n",
    "        assert original_features['low_r_squared'] > test_config['r2_thresh']\n",
    "        assert original_features['volume_slope'] > test_config['volume_thresh']\n",
    "        assert original_features['z_score_Adj Low'] < test_config['z_entry_thresh']\n",
    "    except AssertionError as e:\n",
    "        print(f\"\\n[FAILURE]: Verification failed for signal at index {idx}!\")\n",
    "        print(\"Signal Row:\")\n",
    "        print(signal_row)\n",
    "        print(\"\\nThresholds:\")\n",
    "        print(test_config)\n",
    "        raise e\n",
    "\n",
    "if not signals_df.empty:\n",
    "    print(\"\\n[SUCCESS]: All rows in the final signals DataFrame correctly meet the filter criteria.\")\n",
    "else:\n",
    "    print(\"\\n[INFO]: No signals were generated with this config, test passed vacuously.\")\n",
    "\n",
    "print(\"\\n--- Sanity Check Passed for Signal Filtering ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "signals_df:\n",
      "                   high_slope  high_r_squared  low_slope  low_r_squared  volume_slope  volume_r_squared  unified_std_dev_returns  volume_std_dev_returns  z_score_Adj Low  z_score_Volume\n",
      "Ticker Date                                                                                                                                                                              \n",
      "TEST   2023-01-27    0.006534        0.796915   0.006659       0.796899      0.008892          0.999873                 0.006682                0.000231        -1.446472        1.264911\n",
      "       2023-02-15    0.005364        0.710198   0.005462       0.710196      0.007971          0.999898                 0.006731                0.000186        -1.562509        1.264911\n",
      "       2023-03-03    0.006194        0.819506   0.006302       0.819491      0.007275          0.999915                 0.005943                0.000154        -1.298805        1.264911\n"
     ]
    }
   ],
   "source": [
    "print(f'signals_df:\\n{signals_df}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# penalty_scores are not calculated???\n",
    "* penalty_score = (1 - r_squared) * (unified_std_dev + 1e-9)  \n",
    "* volume_penalty_score = (1 - volume_r_squared) * (volume_std_dev + 1e-9)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>high_slope</th>\n",
       "      <th>high_r_squared</th>\n",
       "      <th>low_slope</th>\n",
       "      <th>low_r_squared</th>\n",
       "      <th>volume_slope</th>\n",
       "      <th>volume_r_squared</th>\n",
       "      <th>unified_std_dev_returns</th>\n",
       "      <th>volume_std_dev_returns</th>\n",
       "      <th>z_score_Adj Open</th>\n",
       "      <th>z_score_Adj High</th>\n",
       "      <th>z_score_Adj Low</th>\n",
       "      <th>z_score_Adj Close</th>\n",
       "      <th>z_score_Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ticker</th>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [high_slope, high_r_squared, low_slope, low_r_squared, volume_slope, volume_r_squared, unified_std_dev_returns, volume_std_dev_returns, z_score_Adj Open, z_score_Adj High, z_score_Adj Low, z_score_Adj Close, z_score_Volume]\n",
       "Index: []"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signals_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>high_slope</th>\n",
       "      <th>high_r_squared</th>\n",
       "      <th>low_slope</th>\n",
       "      <th>low_r_squared</th>\n",
       "      <th>volume_slope</th>\n",
       "      <th>volume_r_squared</th>\n",
       "      <th>unified_std_dev_returns</th>\n",
       "      <th>volume_std_dev_returns</th>\n",
       "      <th>z_score_Adj Open</th>\n",
       "      <th>z_score_Adj High</th>\n",
       "      <th>z_score_Adj Low</th>\n",
       "      <th>z_score_Adj Close</th>\n",
       "      <th>z_score_Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ticker</th>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <th>2024-10-25</th>\n",
       "      <td>-0.001549</td>\n",
       "      <td>0.170512</td>\n",
       "      <td>-0.001499</td>\n",
       "      <td>0.147198</td>\n",
       "      <td>-0.002818</td>\n",
       "      <td>0.005825</td>\n",
       "      <td>0.016570</td>\n",
       "      <td>0.378456</td>\n",
       "      <td>-1.439517</td>\n",
       "      <td>-1.377791</td>\n",
       "      <td>-1.290638</td>\n",
       "      <td>-1.307466</td>\n",
       "      <td>-0.825383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAL</th>\n",
       "      <th>2024-10-25</th>\n",
       "      <td>0.007047</td>\n",
       "      <td>0.794413</td>\n",
       "      <td>0.006360</td>\n",
       "      <td>0.785938</td>\n",
       "      <td>-0.006610</td>\n",
       "      <td>0.019615</td>\n",
       "      <td>0.029808</td>\n",
       "      <td>0.818874</td>\n",
       "      <td>0.586457</td>\n",
       "      <td>0.966663</td>\n",
       "      <td>0.795993</td>\n",
       "      <td>0.950780</td>\n",
       "      <td>0.522081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAON</th>\n",
       "      <th>2024-10-25</th>\n",
       "      <td>0.003277</td>\n",
       "      <td>0.514929</td>\n",
       "      <td>0.003696</td>\n",
       "      <td>0.542243</td>\n",
       "      <td>-0.014090</td>\n",
       "      <td>0.073470</td>\n",
       "      <td>0.019885</td>\n",
       "      <td>0.625558</td>\n",
       "      <td>-0.494063</td>\n",
       "      <td>0.120329</td>\n",
       "      <td>-0.096700</td>\n",
       "      <td>0.783208</td>\n",
       "      <td>0.825011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAPL</th>\n",
       "      <th>2024-10-25</th>\n",
       "      <td>0.001725</td>\n",
       "      <td>0.486748</td>\n",
       "      <td>0.001832</td>\n",
       "      <td>0.530640</td>\n",
       "      <td>-0.021447</td>\n",
       "      <td>0.178248</td>\n",
       "      <td>0.013944</td>\n",
       "      <td>0.748888</td>\n",
       "      <td>-1.164071</td>\n",
       "      <td>-0.487396</td>\n",
       "      <td>-0.518780</td>\n",
       "      <td>-0.685118</td>\n",
       "      <td>-0.265323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABBV</th>\n",
       "      <th>2024-10-25</th>\n",
       "      <td>-0.000732</td>\n",
       "      <td>0.310288</td>\n",
       "      <td>-0.000643</td>\n",
       "      <td>0.243094</td>\n",
       "      <td>0.003946</td>\n",
       "      <td>0.014364</td>\n",
       "      <td>0.008131</td>\n",
       "      <td>0.455564</td>\n",
       "      <td>-0.102501</td>\n",
       "      <td>-0.515262</td>\n",
       "      <td>-0.603197</td>\n",
       "      <td>-0.728499</td>\n",
       "      <td>-0.860770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZM</th>\n",
       "      <th>2025-05-28</th>\n",
       "      <td>0.005305</td>\n",
       "      <td>0.784938</td>\n",
       "      <td>0.005267</td>\n",
       "      <td>0.720825</td>\n",
       "      <td>0.031592</td>\n",
       "      <td>0.508399</td>\n",
       "      <td>0.012891</td>\n",
       "      <td>0.310738</td>\n",
       "      <td>-1.643184</td>\n",
       "      <td>-1.725532</td>\n",
       "      <td>-0.971138</td>\n",
       "      <td>-1.160761</td>\n",
       "      <td>-0.477397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZS</th>\n",
       "      <th>2025-05-28</th>\n",
       "      <td>0.009303</td>\n",
       "      <td>0.935603</td>\n",
       "      <td>0.009805</td>\n",
       "      <td>0.931124</td>\n",
       "      <td>0.006610</td>\n",
       "      <td>0.069734</td>\n",
       "      <td>0.018673</td>\n",
       "      <td>0.209714</td>\n",
       "      <td>1.409447</td>\n",
       "      <td>0.686478</td>\n",
       "      <td>1.079823</td>\n",
       "      <td>0.604444</td>\n",
       "      <td>-0.296513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZTO</th>\n",
       "      <th>2025-05-28</th>\n",
       "      <td>-0.003039</td>\n",
       "      <td>0.464491</td>\n",
       "      <td>-0.003090</td>\n",
       "      <td>0.465276</td>\n",
       "      <td>0.039047</td>\n",
       "      <td>0.346424</td>\n",
       "      <td>0.019013</td>\n",
       "      <td>0.861881</td>\n",
       "      <td>-0.843505</td>\n",
       "      <td>-0.516990</td>\n",
       "      <td>-0.582071</td>\n",
       "      <td>-0.233126</td>\n",
       "      <td>0.003116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZTS</th>\n",
       "      <th>2025-05-28</th>\n",
       "      <td>0.003754</td>\n",
       "      <td>0.893635</td>\n",
       "      <td>0.003965</td>\n",
       "      <td>0.842693</td>\n",
       "      <td>-0.002221</td>\n",
       "      <td>0.004114</td>\n",
       "      <td>0.015717</td>\n",
       "      <td>0.313638</td>\n",
       "      <td>1.667782</td>\n",
       "      <td>1.342668</td>\n",
       "      <td>1.245380</td>\n",
       "      <td>0.965542</td>\n",
       "      <td>-1.348780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZWS</th>\n",
       "      <th>2025-05-28</th>\n",
       "      <td>0.007280</td>\n",
       "      <td>0.760872</td>\n",
       "      <td>0.007837</td>\n",
       "      <td>0.784573</td>\n",
       "      <td>-0.037543</td>\n",
       "      <td>0.488119</td>\n",
       "      <td>0.030238</td>\n",
       "      <td>0.378436</td>\n",
       "      <td>0.681230</td>\n",
       "      <td>0.088664</td>\n",
       "      <td>-0.113297</td>\n",
       "      <td>-0.490508</td>\n",
       "      <td>-0.376315</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>216541 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   high_slope  high_r_squared  low_slope  low_r_squared  \\\n",
       "Ticker Date                                                               \n",
       "A      2024-10-25   -0.001549        0.170512  -0.001499       0.147198   \n",
       "AAL    2024-10-25    0.007047        0.794413   0.006360       0.785938   \n",
       "AAON   2024-10-25    0.003277        0.514929   0.003696       0.542243   \n",
       "AAPL   2024-10-25    0.001725        0.486748   0.001832       0.530640   \n",
       "ABBV   2024-10-25   -0.000732        0.310288  -0.000643       0.243094   \n",
       "...                       ...             ...        ...            ...   \n",
       "ZM     2025-05-28    0.005305        0.784938   0.005267       0.720825   \n",
       "ZS     2025-05-28    0.009303        0.935603   0.009805       0.931124   \n",
       "ZTO    2025-05-28   -0.003039        0.464491  -0.003090       0.465276   \n",
       "ZTS    2025-05-28    0.003754        0.893635   0.003965       0.842693   \n",
       "ZWS    2025-05-28    0.007280        0.760872   0.007837       0.784573   \n",
       "\n",
       "                   volume_slope  volume_r_squared  unified_std_dev_returns  \\\n",
       "Ticker Date                                                                  \n",
       "A      2024-10-25     -0.002818          0.005825                 0.016570   \n",
       "AAL    2024-10-25     -0.006610          0.019615                 0.029808   \n",
       "AAON   2024-10-25     -0.014090          0.073470                 0.019885   \n",
       "AAPL   2024-10-25     -0.021447          0.178248                 0.013944   \n",
       "ABBV   2024-10-25      0.003946          0.014364                 0.008131   \n",
       "...                         ...               ...                      ...   \n",
       "ZM     2025-05-28      0.031592          0.508399                 0.012891   \n",
       "ZS     2025-05-28      0.006610          0.069734                 0.018673   \n",
       "ZTO    2025-05-28      0.039047          0.346424                 0.019013   \n",
       "ZTS    2025-05-28     -0.002221          0.004114                 0.015717   \n",
       "ZWS    2025-05-28     -0.037543          0.488119                 0.030238   \n",
       "\n",
       "                   volume_std_dev_returns  z_score_Adj Open  z_score_Adj High  \\\n",
       "Ticker Date                                                                     \n",
       "A      2024-10-25                0.378456         -1.439517         -1.377791   \n",
       "AAL    2024-10-25                0.818874          0.586457          0.966663   \n",
       "AAON   2024-10-25                0.625558         -0.494063          0.120329   \n",
       "AAPL   2024-10-25                0.748888         -1.164071         -0.487396   \n",
       "ABBV   2024-10-25                0.455564         -0.102501         -0.515262   \n",
       "...                                   ...               ...               ...   \n",
       "ZM     2025-05-28                0.310738         -1.643184         -1.725532   \n",
       "ZS     2025-05-28                0.209714          1.409447          0.686478   \n",
       "ZTO    2025-05-28                0.861881         -0.843505         -0.516990   \n",
       "ZTS    2025-05-28                0.313638          1.667782          1.342668   \n",
       "ZWS    2025-05-28                0.378436          0.681230          0.088664   \n",
       "\n",
       "                   z_score_Adj Low  z_score_Adj Close  z_score_Volume  \n",
       "Ticker Date                                                            \n",
       "A      2024-10-25        -1.290638          -1.307466       -0.825383  \n",
       "AAL    2024-10-25         0.795993           0.950780        0.522081  \n",
       "AAON   2024-10-25        -0.096700           0.783208        0.825011  \n",
       "AAPL   2024-10-25        -0.518780          -0.685118       -0.265323  \n",
       "ABBV   2024-10-25        -0.603197          -0.728499       -0.860770  \n",
       "...                            ...                ...             ...  \n",
       "ZM     2025-05-28        -0.971138          -1.160761       -0.477397  \n",
       "ZS     2025-05-28         1.079823           0.604444       -0.296513  \n",
       "ZTO    2025-05-28        -0.582071          -0.233126        0.003116  \n",
       "ZTS    2025-05-28         1.245380           0.965542       -1.348780  \n",
       "ZWS    2025-05-28        -0.113297          -0.490508       -0.376315  \n",
       "\n",
       "[216541 rows x 13 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: New Encapsulated Helper Functions\n",
    "\n",
    "These new functions isolate the logic for performance analysis and the optimization loop itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_performance(trade_results):\n",
    "    \"\"\"\n",
    "    Calculates performance metrics from a DataFrame of trades.\n",
    "    \n",
    "    Returns a dictionary of key metrics.\n",
    "    \"\"\"\n",
    "    if trade_results.empty:\n",
    "        return {'num_trades': 0, 'win_rate': 0, 'avg_return': 0, 'total_return': 0}\n",
    "    \n",
    "    win_rate = (trade_results['return'] > 0).mean()\n",
    "    total_return = (1 + trade_results['return']).prod() - 1\n",
    "    avg_return = trade_results['return'].mean()\n",
    "    \n",
    "    return {\n",
    "        'num_trades': len(trade_results),\n",
    "        'win_rate': win_rate,\n",
    "        'avg_return': avg_return,\n",
    "        'total_return': total_return\n",
    "    }\n",
    "\n",
    "def run_parameter_optimization(df, param_grid, static_params):\n",
    "    \"\"\"\n",
    "    Orchestrates the entire parameter optimization process.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The OHLCV data.\n",
    "        param_grid (dict): Dictionary with lists of parameters to test.\n",
    "        static_params (dict): Dictionary of parameters that are not being optimized.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A summary of results for each parameter combination.\n",
    "    \"\"\"\n",
    "    results_log = []\n",
    "    \n",
    "    # Use itertools.product to create a clean generator for all combinations\n",
    "    keys, values = zip(*param_grid.items())\n",
    "    param_combinations = [dict(zip(keys, v)) for v in product(*values)]\n",
    "\n",
    "    print(f\"Starting optimization for {len(param_combinations)} combinations...\")\n",
    "    \n",
    "    for param_set in tqdm(param_combinations, desc=\"Optimization Progress\"):\n",
    "        # Combine static and dynamic parameters into a single config for this run\n",
    "        current_config = {**static_params, **param_set}\n",
    "        \n",
    "        # 1. Run the backtest with the current configuration\n",
    "        trade_results = run_backtest(df, current_config)\n",
    "        \n",
    "        # 2. Analyze the performance of this run\n",
    "        performance_metrics = analyze_performance(trade_results)\n",
    "        \n",
    "        # 3. Log the results\n",
    "        log_entry = {**param_set, **performance_metrics}\n",
    "        results_log.append(log_entry)\n",
    "        \n",
    "    return pd.DataFrame(results_log)\n",
    "\n",
    "def handle_entries_for_day(current_date, next_day_date, signals_today, open_positions, df_ohlcv):\n",
    "    \"\"\"\n",
    "    Processes entries and stores signal details in the open_positions dict.\n",
    "    \"\"\"\n",
    "    # --- KEY CHANGE: Loop through the signals DataFrame ---\n",
    "    for ticker, signal_row in signals_today.iterrows():\n",
    "        # The ticker is now in the index of signal_row, so we use its name\n",
    "        ticker_name = ticker[0] \n",
    "        \n",
    "        if ticker_name not in open_positions:\n",
    "            try:\n",
    "                entry_price = df_ohlcv.loc[(ticker_name, next_day_date), 'Adj High']\n",
    "                \n",
    "                # --- LOGGING: Store more info about the entry signal ---\n",
    "                open_positions[ticker_name] = {\n",
    "                    'entry_date': next_day_date,\n",
    "                    'entry_price': entry_price,\n",
    "                    'signal_date': current_date,\n",
    "                    'signal_features': signal_row.to_dict() # Store all features that triggered the signal\n",
    "                }\n",
    "            except KeyError:\n",
    "                pass\n",
    "                \n",
    "    return open_positions\n",
    "\n",
    "def handle_exits_for_day(current_date, next_day_date, open_positions, df_ohlcv, config):\n",
    "    \"\"\"\n",
    "    Checks for exits and logs detailed information about the exit trigger.\n",
    "    Corrected version with valid syntax for the if/elif chain.\n",
    "    \"\"\"\n",
    "    closed_trades = []\n",
    "    positions_to_close = []\n",
    "\n",
    "    for ticker, pos in open_positions.items():\n",
    "        try:\n",
    "            current_close_price = df_ohlcv.loc[(ticker, current_date), 'Adj Close']\n",
    "        except KeyError:\n",
    "            continue \n",
    "\n",
    "        exit_reason = None\n",
    "        exit_target_value = None \n",
    "        \n",
    "        # --- SYNTAX FIX: Calculate all threshold values *before* the conditional block ---\n",
    "        profit_target_price = pos['entry_price'] * (1 + config['profit_target'])\n",
    "        stop_loss_price = pos['entry_price'] * (1 - config['stop_loss'])\n",
    "        days_held = (current_date.to_pydatetime().date() - pos['entry_date'].to_pydatetime().date()).days\n",
    "        \n",
    "        # --- Now, check conditions in a contiguous if/elif/elif block ---\n",
    "        if current_close_price >= profit_target_price:\n",
    "            exit_reason = \"Profit Target\"\n",
    "            exit_target_value = profit_target_price \n",
    "\n",
    "        elif current_close_price <= stop_loss_price:\n",
    "            exit_reason = \"Stop-Loss\"\n",
    "            exit_target_value = stop_loss_price \n",
    "\n",
    "        elif days_held >= config['time_hold_days']:\n",
    "            exit_reason = \"Time Hold\"\n",
    "            exit_target_value = days_held \n",
    "\n",
    "        if exit_reason:\n",
    "            try:\n",
    "                exit_price = df_ohlcv.loc[(ticker, next_day_date), 'Adj Low']\n",
    "                trade_return = (exit_price - pos['entry_price']) / pos['entry_price']\n",
    "                \n",
    "                trade_log = {\n",
    "                    'ticker': ticker, \n",
    "                    'entry_date': pos['entry_date'], \n",
    "                    'exit_date': next_day_date,\n",
    "                    'return': trade_return, \n",
    "                    'reason': exit_reason,\n",
    "                    'signal_date': pos['signal_date'],\n",
    "                    'entry_signal_features': pos['signal_features'],\n",
    "                    'entry_price_actual': pos['entry_price'],\n",
    "                    'exit_signal_date': current_date,\n",
    "                    'exit_trigger_price': current_close_price,\n",
    "                    'exit_target_value': exit_target_value,\n",
    "                    'exit_price_actual': exit_price,\n",
    "                }\n",
    "                closed_trades.append(trade_log)\n",
    "                positions_to_close.append(ticker)\n",
    "            except KeyError:\n",
    "                pass\n",
    "                \n",
    "    for ticker in positions_to_close:\n",
    "        del open_positions[ticker]\n",
    "        \n",
    "    return closed_trades, open_positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: The New, Clean Top-Level Script\n",
    "\n",
    "Your main script is now incredibly simple and readable. It's all about configuration and orchestration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. DEFINE CONFIGURATION ---\n",
    "\n",
    "# Parameters to be optimized, defining the search space\n",
    "optimization_grid = {\n",
    "    'lookback_days': [30, 60, 90],\n",
    "    'rolling_window': [15, 20]\n",
    "}\n",
    "\n",
    "# Static strategy parameters that do not change during optimization\n",
    "strategy_params = {\n",
    "    'slope_thresh': 1.0,\n",
    "    'r2_thresh': 0.50,\n",
    "    'z_entry_thresh': 0,\n",
    "    'profit_target': 0.10,\n",
    "    'stop_loss': 0.05,\n",
    "    'time_hold_days': 20\n",
    "}\n",
    "\n",
    "\n",
    "# --- 2. RUN ORCHESTRATOR ---\n",
    "\n",
    "# The main call is now a single, descriptive function\n",
    "optimization_results = run_parameter_optimization(\n",
    "    df_train, optimization_grid, strategy_params\n",
    ")\n",
    "\n",
    "\n",
    "# --- 3. ANALYZE RESULTS ---\n",
    "\n",
    "print(\"\\n\\n--- Optimization Complete ---\")\n",
    "print(optimization_results.sort_values(by='total_return', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: The \"One-Trade\" Deep Dive\n",
    "\n",
    "The most powerful debugging technique is to isolate a single trade and follow it from signal generation to exit. If the logic holds for one trade, it's likely correct for all of them.\n",
    "\n",
    "1.  **Pick a Winning Trade and a Losing Trade:** Run one of the backtests again (e.g., the one with `lookback=30`, `rolling=20`) and save the `trade_results` DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Pick a configuration to analyze\n",
    "config_to_test = {\n",
    "    **strategy_params, \n",
    "    'lookback_days': 30, \n",
    "    'rolling_window': 20\n",
    "}\n",
    "\n",
    "# 2. Run the backtest to get the detailed trade log\n",
    "trade_log_df = run_backtest(df_train, config_to_test)\n",
    "\n",
    "# 3. Isolate and inspect a single trade\n",
    "if not trade_log_df.empty:\n",
    "    # Get the first losing trade\n",
    "    losing_trade = trade_log_df[trade_log_df['return'] < 0].iloc[0]\n",
    "\n",
    "    print(\"--- Detailed Log for a Single Losing Trade ---\")\n",
    "    # Using .T transposes the Series for easy vertical reading\n",
    "    print(losing_trade.T)\n",
    "else:\n",
    "    print(\"No trades were made for this configuration.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losing_trade.entry_signal_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df_trends = analyze_ticker_trends_vectorized(df_train, lookback_days=30)\n",
    "print(f'_df_trends:\\n{_df_trends}')\n",
    "# _df_trends.to_csv('C:\\\\Users\\\\ping\\\\Desktop\\\\_df_trends.csv', index=True)\n",
    "_df_trends.index.names = ['Ticker', 'Date']\n",
    "_df_trends.reset_index().to_csv(r'C:\\Users\\ping\\Desktop\\_df_trends.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.loc['MSFT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to CSV\n",
    "_df_trends.to_csv(r'C:\\Users\\ping\\Desktop\\_df_trends.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df_trends.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loss = trade_log_df[trade_log_df['return'] < 0]\n",
    "print(df_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_win = trade_log_df[trade_log_df['return'] > 0]\n",
    "print(df_win)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loss.to_csv('C:\\\\Users\\\\ping\\\\Desktop\\\\df_loss.csv', index=True)\n",
    "df_win.to_csv('C:\\\\Users\\\\ping\\\\Desktop\\\\df_win.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimization_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a configuration to analyze\n",
    "config_to_test = {**strategy_params, 'lookback_days': 30, 'rolling_window': 20}\n",
    "\n",
    "# Run a single backtest and get the detailed trade log\n",
    "single_run_trades = run_backtest(df_train, config_to_test)\n",
    "\n",
    "# Find a winning and a losing trade to investigate\n",
    "print(\"Sample winning trade:\")\n",
    "print(single_run_trades[single_run_trades['return'] > 0].head(1))\n",
    "\n",
    "print(\"\\nSample losing trade:\")\n",
    "print(single_run_trades[single_run_trades['return'] < 0].head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trends = analyze_ticker_trends_vectorized(df_train, lookback_days=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trends.loc['FIX', '2024-10-25']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
