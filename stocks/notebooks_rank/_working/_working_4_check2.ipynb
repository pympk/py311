{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and Configuration\n",
    "\n",
    "This cell contains all imports and user-configurable parameters for the analysis pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pprint\n",
    "import matplotlib.pyplot as plt # Import for plotting\n",
    "from IPython.display import display, Markdown\n",
    "from scipy.stats import linregress \n",
    "\n",
    "# --- 1. PANDAS & IPYTHON OPTIONS ---\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 3000)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# --- 2. PROJECT PATH CONFIGURATION ---\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "ROOT_DIR = NOTEBOOK_DIR.parent.parent  # Adjust if your notebook is in a 'notebooks' subdirectory\n",
    "DATA_DIR = ROOT_DIR / 'data'\n",
    "SRC_DIR = ROOT_DIR / 'src'\n",
    "\n",
    "# Add 'src' to the Python path to import custom modules\n",
    "if str(SRC_DIR) not in sys.path:\n",
    "    sys.path.append(str(SRC_DIR))\n",
    "\n",
    "# --- 3. IMPORT CUSTOM MODULES ---\n",
    "import utils\n",
    "import plotting_utils\n",
    "\n",
    "# --- 4. ANALYSIS & FILTERING CONFIGURATION ---\n",
    "\n",
    "# File searching parameters\n",
    "# FILE_PREFIX = ''  # e.g., '2024'\n",
    "FILE_CONTAINS_PATTERN = 'df_OHLCV_clean_stocks_etfs'\n",
    "\n",
    "# # Parameters defining the time windows for metric calculation\n",
    "PERIOD_PARAMS = {\n",
    "    'lookback_days': 22,\n",
    "    'recent_days': 0,\n",
    "}\n",
    "\n",
    "# This is not use for filtering, it's use to calculate metrics in SORT_ORDER\n",
    "# Parameters for filtering the calculated metrics to find candidates\n",
    "METRIC_FILTERS = {\n",
    "    'min_lookback_improvement': 0,\n",
    "    'current_rank_bracket_start': 1,\n",
    "    'current_rank_bracket_end': 1000,\n",
    "    # --- Select ONE mode by commenting out the others ---\n",
    "    # 'Reversal' Mode\n",
    "    'min_recent_bottom_to_recent_start': 0,\n",
    "    'min_recent_bottom_to_current': 0,    \n",
    "    # 'Dip' Mode\n",
    "    # 'min_current_to_recent_start': 10,\n",
    "}\n",
    "\n",
    "# --- 5. VERIFICATION ---\n",
    "print(\"--- Path Configuration ---\")\n",
    "print(f\"✅ Project Root: {ROOT_DIR}\")\n",
    "print(f\"✅ Data Dir:     {DATA_DIR}\")\n",
    "print(f\"✅ Source Dir:   {SRC_DIR}\")\n",
    "assert all([ROOT_DIR.exists(), DATA_DIR.exists(), SRC_DIR.exists()]), \"A key directory was not found!\"\n",
    "\n",
    "print(\"\\n--- Module Verification ---\")\n",
    "print(f\"✅ Successfully imported 'utils' and 'plotting_utils'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Step 1: Loading latest consolidated Finviz data ---\")\n",
    "\n",
    "# Find the most recent file matching the pattern\n",
    "# This function is now understood to return List[str] (filenames), not List[Path].\n",
    "latest_finviz_filepaths = utils.get_recent_files(\n",
    "    directory_path=DATA_DIR,\n",
    "    extension='parquet',\n",
    "    prefix='202',\n",
    "    contains_pattern='df_finviz_merged_stocks_etfs',\n",
    "    count=1\n",
    ")\n",
    "\n",
    "if not latest_finviz_filepaths:\n",
    "    raise FileNotFoundError(f\"No files found in '{DATA_DIR}' with prefix '{FILE_PREFIX}' and pattern '{FILE_CONTAINS_PATTERN}'\")\n",
    "\n",
    "# Get the filename string from the list\n",
    "latest_filename = latest_finviz_filepaths[0]\n",
    "\n",
    "# Manually construct the full path before loading\n",
    "full_file_path = DATA_DIR / latest_filename\n",
    "df_finviz_latest = pd.read_parquet(full_file_path, engine='pyarrow')\n",
    "\n",
    "\n",
    "# --- Robust Index Setting (this logic remains correct) ---\n",
    "if df_finviz_latest.index.name == 'Ticker':\n",
    "    print(\"Info: 'Ticker' is already the index. No action needed.\")\n",
    "elif 'Ticker' in df_finviz_latest.columns:\n",
    "    print(\"Info: 'Ticker' column found. Setting it as the DataFrame index.\")\n",
    "    df_finviz_latest.set_index('Ticker', inplace=True)\n",
    "elif 'ticker' in df_finviz_latest.columns:\n",
    "    print(\"Info: 'ticker' column found. Renaming and setting as index.\")\n",
    "    df_finviz_latest.rename(columns={'ticker': 'Ticker'}, inplace=True)\n",
    "    df_finviz_latest.set_index('Ticker', inplace=True)\n",
    "elif df_finviz_latest.index.name is None:\n",
    "    print(\"Info: Index is unnamed. Assuming it contains tickers and assigning the name 'Ticker'.\")\n",
    "    df_finviz_latest.index.name = 'Ticker'\n",
    "else:\n",
    "    print(\"ERROR: Loaded DataFrame has an unexpected format.\")\n",
    "    print(f\"Columns: {df_finviz_latest.columns.tolist()}\")\n",
    "    print(f\"Index Name: '{df_finviz_latest.index.name}'\")\n",
    "    raise ValueError(\"Could not find a 'Ticker' column or a usable index to proceed.\")\n",
    "\n",
    "\n",
    "# Correct the print statement to work with the filename string\n",
    "print(f\"✅ Successfully loaded: {latest_filename}\")\n",
    "print(f\"Shape: {df_finviz_latest.shape}\")\n",
    "print(df_finviz_latest.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# DATA_DIR = r'c:\\Users\\ping\\Files_win10\\python\\py311\\stocks\\data'\n",
    "# Manually construct the full path before loading\n",
    "full_file_path = r'c:\\Users\\ping\\Files_win10\\python\\py311\\stocks\\data\\df_OHLCV_clean_stocks_etfs.parquet'\n",
    "df_OHLCV = pd.read_parquet(full_file_path, engine='pyarrow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 2: The Chronological Split Code\n",
    "\n",
    "This cell contains the logic to find the split date and create the `df_train` and `df_test` DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique trading dates in dataset: 250\n",
      "The data will be split on the date: 2025-05-28\n",
      "\n",
      "--- Verification ---\n",
      "Original DataFrame shape: (371000, 5)\n",
      "Training set shape:   (261184, 5)\n",
      "Testing set shape:    (109816, 5)\n",
      "\n",
      "Date Ranges:\n",
      "  Training: 2024-09-13 to 2025-05-28\n",
      "  Testing:  2025-05-29 to 2025-09-12\n",
      "\n",
      "Verification successful: There is no date overlap between train and test sets.\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Find the Chronological Split Point ---\n",
    "\n",
    "# Get all unique dates from the index and sort them\n",
    "unique_dates = df_OHLCV.index.get_level_values('Date').unique().sort_values()\n",
    "\n",
    "# Determine the index for the 70% split\n",
    "split_index = int(len(unique_dates) * 0.7)\n",
    "\n",
    "# Find the actual date at that split index\n",
    "split_date = unique_dates[split_index]\n",
    "\n",
    "print(f\"Total unique trading dates in dataset: {len(unique_dates)}\")\n",
    "print(f\"The data will be split on the date: {split_date.date()}\")\n",
    "\n",
    "# --- 2. Create the Training and Testing Sets ---\n",
    "\n",
    "# The training set includes all data UP TO and INCLUDING the split_date\n",
    "df_train = df_OHLCV[df_OHLCV.index.get_level_values('Date') <= split_date]\n",
    "\n",
    "# The testing set includes all data AFTER the split_date\n",
    "df_test = df_OHLCV[df_OHLCV.index.get_level_values('Date') > split_date]\n",
    "\n",
    "\n",
    "# --- 3. Verify the Split ---\n",
    "\n",
    "print(\"\\n--- Verification ---\")\n",
    "print(f\"Original DataFrame shape: {df_OHLCV.shape}\")\n",
    "print(f\"Training set shape:   {df_train.shape}\")\n",
    "print(f\"Testing set shape:    {df_test.shape}\")\n",
    "\n",
    "print(\"\\nDate Ranges:\")\n",
    "print(f\"  Training: {df_train.index.get_level_values('Date').min().date()} to {df_train.index.get_level_values('Date').max().date()}\")\n",
    "print(f\"  Testing:  {df_test.index.get_level_values('Date').min().date()} to {df_test.index.get_level_values('Date').max().date()}\")\n",
    "\n",
    "# Final check to ensure no overlap\n",
    "assert df_train.index.get_level_values('Date').max() < df_test.index.get_level_values('Date').min()\n",
    "print(\"\\nVerification successful: There is no date overlap between train and test sets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jupyter Notebook: Verifying `analyze_ticker_trends_vectorized`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cell 1: Setup and Imports\n",
    "\n",
    "First, let's import the necessary libraries and define both versions of the function we want to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verification functions are defined.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import linregress\n",
    "\n",
    "# --- Function 1: The NEW, Correct, Log-Vectorized Version (The one we are verifying) ---\n",
    "\n",
    "def analyze_ticker_trends_log_vectorized(df_group, lookback_days=60):\n",
    "    \"\"\"\n",
    "    This function output has been verified. \n",
    "\n",
    "    Vectorized analysis of trends using LOG-TRANSFORMED data.\n",
    "    \n",
    "    This is the CORRECTED version. The slope of a log-transformed series represents\n",
    "    the average percentage change, making it comparable across different stocks.\n",
    "    \"\"\"\n",
    "    if len(df_group) < lookback_days:\n",
    "        return None \n",
    "\n",
    "    time_index = pd.Series(np.arange(len(df_group)), index=df_group.index)\n",
    "    var_time = np.var(np.arange(lookback_days), ddof=0)\n",
    "    \n",
    "    series_to_analyze = {\n",
    "        # Uses natural log \n",
    "        'high': np.log(df_group['Adj High']),\n",
    "        'low': np.log(df_group['Adj Low']),\n",
    "        'volume': np.log(df_group['Volume'].astype(float) + 1)\n",
    "    }\n",
    "    \n",
    "    df_results = pd.DataFrame(index=df_group.index)\n",
    "\n",
    "    for name, log_series in series_to_analyze.items():\n",
    "        # Uses covariance and variance of population\n",
    "        rolling_cov = time_index.rolling(window=lookback_days).cov(log_series, ddof=0)\n",
    "        rolling_var_series = log_series.rolling(window=lookback_days).var(ddof=0)\n",
    "        \n",
    "        df_results[f'{name}_slope'] = rolling_cov / var_time\n",
    "        denominator = (var_time * rolling_var_series) + 1e-9\n",
    "        df_results[f'{name}_r_squared'] = (rolling_cov**2) / denominator\n",
    "\n",
    "    yesterday_low = df_group['Adj Low'].shift(1)\n",
    "    worst_case_returns = (df_group['Adj High'] - yesterday_low) / yesterday_low\n",
    "    df_results['unified_std_dev_returns'] = worst_case_returns.rolling(window=lookback_days).std(ddof=0)\n",
    "    \n",
    "    volume_std_dev = df_group['Volume'].pct_change().rolling(window=lookback_days).std(ddof=0)\n",
    "    df_results['volume_std_dev_returns'] = volume_std_dev\n",
    "    \n",
    "    return df_results # Penalty scores omitted for verification clarity\n",
    "\n",
    "print(\"Verification functions are defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cell 2: Create Realistic Sample Data\n",
    "\n",
    "We need some sample data that mimics your real dataset to perform the check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a date range\n",
    "dates = pd.to_datetime(pd.date_range(start='2023-01-01', periods=200, freq='B'))\n",
    "\n",
    "# Create data for Ticker 'A' (upward trend)\n",
    "price_a = 100 + np.linspace(0, 50, 200) + np.random.randn(200) * 2\n",
    "volume_a = 100000 + np.sin(np.arange(200)/10) * 20000 + np.random.randint(-5000, 5000, 200)\n",
    "df_a = pd.DataFrame({\n",
    "    'Ticker': 'A',\n",
    "    'Date': dates,\n",
    "    'Adj High': price_a + 1,\n",
    "    'Adj Low': price_a - 1,\n",
    "    'Volume': volume_a\n",
    "})\n",
    "\n",
    "# Create data for Ticker 'B' (downward trend)\n",
    "price_b = 200 - np.linspace(0, 30, 200) + np.random.randn(200) * 3\n",
    "volume_b = 500000 + np.cos(np.arange(200)/5) * 50000 + np.random.randint(-10000, 10000, 200)\n",
    "df_b = pd.DataFrame({\n",
    "    'Ticker': 'B',\n",
    "    'Date': dates,\n",
    "    'Adj High': price_b + 1.5,\n",
    "    'Adj Low': price_b - 1.5,\n",
    "    'Volume': volume_b\n",
    "})\n",
    "\n",
    "# Combine and set the multi-index\n",
    "sample_df = pd.concat([df_a, df_b]).set_index(['Ticker', 'Date'])\n",
    "\n",
    "print(\"Sample DataFrame created with shape:\", sample_df.shape)\n",
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSFT.head(3):\n",
      "            Adj Open  Adj High  Adj Low  Adj Close    Volume\n",
      "Date                                                        \n",
      "2024-09-13   422.657   428.612  422.290    427.381  15993778\n",
      "2024-09-16   427.391   430.300  425.029    428.126  13938564\n",
      "2024-09-17   436.950   438.558  429.049    431.907  19015898\n",
      "\n",
      "MSFT.tail(3):\n",
      "            Adj Open  Adj High  Adj Low  Adj Close    Volume\n",
      "Date                                                        \n",
      "2025-05-23   449.242   452.945  448.173    449.441  16911254\n",
      "2025-05-27   455.731   460.193  455.371    459.934  21008780\n",
      "2025-05-28   460.463   461.761  456.180    456.609  17114387\n",
      "\n",
      "len(MSFT): 176\n"
     ]
    }
   ],
   "source": [
    "MSFT = df_train.loc['MSFT'].copy()\n",
    "print(f'MSFT.head(3):\\n{MSFT.head(3)}')\n",
    "print(f'\\nMSFT.tail(3):\\n{MSFT.tail(3)}')\n",
    "print(f'\\nlen(MSFT): {len(MSFT)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cell 3: The New Verification\n",
    "\n",
    "Now, we will compare the log-vectorized results against the original normalized results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSFT.to_csv(r'C:\\Users\\ping\\Desktop\\MSFT.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Parameters for our Test ---\n",
    "TICKER_TO_CHECK = 'MSFT'\n",
    "DATE_TO_CHECK = pd.to_datetime('2024-09-26')\n",
    "LOOKBACK_DAYS = 10\n",
    "\n",
    "print(f\"--- Verifying LOG-TRANSFORMED calculations for '{TICKER_TO_CHECK}' on {DATE_TO_CHECK.date()} ---\\n\")\n",
    "\n",
    "# --- 1. Run the FAST Log-Vectorized function ---\n",
    "ticker_history = df_train.loc[TICKER_TO_CHECK]\n",
    "vectorized_results_full = analyze_ticker_trends_log_vectorized(ticker_history, LOOKBACK_DAYS)\n",
    "vectorized_result_today = vectorized_results_full.loc[DATE_TO_CHECK]\n",
    "\n",
    "# --- 2. Run the SLOW Original (Normalized) function ---\n",
    "historical_slice = ticker_history.loc[:DATE_TO_CHECK]\n",
    "original_result_today = analyze_ticker_trends_original(historical_slice, LOOKBACK_DAYS)\n",
    "\n",
    "# --- 3. Compare the results ---\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Log-Vectorized': vectorized_result_today,\n",
    "    'Original (Normalized)': original_result_today\n",
    "}).dropna()\n",
    "\n",
    "print(\"### Important Note ###\")\n",
    "print(\"The slope of a log(price) series is mathematically very similar to the slope of a normalized (price/price_0) series.\")\n",
    "print(\"Therefore, both the SLOPES and R-SQUARED values should now be very close.\\n\")\n",
    "\n",
    "print(\"### Side-by-Side Comparison ###\")\n",
    "print(comparison_df)\n",
    "\n",
    "# --- 4. Programmatic Check for ALL values ---\n",
    "try:\n",
    "    # We now test BOTH slope and r_squared\n",
    "    pd.testing.assert_series_equal(\n",
    "        comparison_df['Log-Vectorized'],\n",
    "        comparison_df['Original (Normalized)'],\n",
    "        atol=0.05 # Use a slightly larger tolerance for slope approximation\n",
    "    )\n",
    "    print(\"\\n[SUCCESS]: Log-vectorized results closely match the original normalized results!\")\n",
    "except AssertionError as e:\n",
    "    print(\"\\n[FAILURE]: Results do not match.\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_results_full.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_results_full.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refactored Code\n",
    "\n",
    "Here is the complete, refactored solution. I've included the previously refactored functions with slight modifications to accept the new configuration structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def calculate_rolling_z_scores_general(df_group, columns_to_process, rolling_window=20):\n",
    "    \"\"\"\n",
    "    This function output has been verified. \n",
    "\n",
    "    Calculates rolling Z-scores for a list of specified columns.\n",
    "    \n",
    "    This is a flexible, reusable, and efficient version.\n",
    "    \n",
    "    Args:\n",
    "        df_group (pd.DataFrame): The DataFrame for a single ticker.\n",
    "        columns_to_process (list): A list of column names to calculate Z-scores for.\n",
    "        rolling_window (int): The lookback window.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with Z-score columns, prefixed with 'z_score_'.\n",
    "                      Returns NaNs for non-computable values.\n",
    "    \"\"\"\n",
    "    if df_group.empty or len(df_group) < rolling_window:\n",
    "        # Return an empty DataFrame with the expected column names for consistency\n",
    "        return pd.DataFrame(columns=[f\"z_score_{col}\" for col in columns_to_process])\n",
    "\n",
    "    # Select the subset of data to work on\n",
    "    data_subset = df_group[columns_to_process]\n",
    "    \n",
    "    # Calculate rolling stats for all columns at once.\n",
    "    # This correctly produces NaNs for the initial, incomplete windows.\n",
    "    rolling_mean = data_subset.rolling(window=rolling_window).mean()\n",
    "    rolling_std = data_subset.rolling(window=rolling_window).std()\n",
    "    \n",
    "    # Calculate Z-scores for all columns in one vectorized operation.\n",
    "    z_scores_df = (data_subset - rolling_mean) / rolling_std\n",
    "    \n",
    "    # Handle true division-by-zero errors (where std is 0)\n",
    "    z_scores_df = z_scores_df.replace([np.inf, -np.inf], 0)\n",
    "    \n",
    "    # Add a descriptive prefix to the column names (e.g., 'Adj Low' -> 'z_score_Adj Low')\n",
    "    return z_scores_df.add_prefix('z_score_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_scores = df_train.groupby(level='Ticker', group_keys=False).apply(\n",
    "    calculate_rolling_z_scores_general, df_train.columns, rolling_window=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_MSFT_w10 = z_scores.loc['MSFT']\n",
    "z_MSFT_w10.to_csv(r'C:\\Users\\ping\\Desktop\\z_MSFT_w10.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Refined Core Backtesting Functions\n",
    "\n",
    "We'll modify the function signatures to accept a single `config` dictionary. This makes them more modular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m _signals, _features = precompute_signals(df_train, \u001b[43mconfig\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mrolling_window\u001b[39m\u001b[33m'\u001b[39m]==\u001b[32m10\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'config' is not defined"
     ]
    }
   ],
   "source": [
    "_signals, _features = precompute_signals(df_train, config['rolling_window']=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from itertools import product\n",
    "\n",
    "# Assume analyze_ticker_trends_vectorized and calculate_rolling_z_scores are defined elsewhere\n",
    "\n",
    "# def precompute_signals(df_ohlcv, config):\n",
    "#     \"\"\"\n",
    "#     Pre-computes all trading signals and returns the full features DataFrame\n",
    "#     for the dates where signals were triggered.\n",
    "#     \"\"\"\n",
    "#     print(\"Pre-computing features for this parameter set...\")\n",
    "    \n",
    "#     trends = df_ohlcv.groupby(level='Ticker', group_keys=False).apply(\n",
    "#         analyze_ticker_trends_log_vectorized, config['lookback_days']\n",
    "#     )\n",
    "    \n",
    "#     z_scores = df_ohlcv.groupby(level='Ticker', group_keys=False).apply(\n",
    "#         calculate_rolling_z_scores, config['rolling_window']\n",
    "#     )\n",
    "    \n",
    "#     features = trends.join(z_scores).dropna()\n",
    "\n",
    "#     signals = features[\n",
    "#         (features['low_slope'] > config['slope_thresh']) &\n",
    "#         (features['low_r_squared'] > config['r2_thresh']) &\n",
    "#         (features['volume_slope'] > 0) &\n",
    "#         (features['low_rolling_z_score'] < config['z_entry_thresh'])\n",
    "#     ]\n",
    "    \n",
    "#     # --- KEY CHANGE: Return the full signals DataFrame, not just the index ---\n",
    "#     return signals\n",
    "\n",
    "def precompute_signals(df_ohlcv, config):\n",
    "    \"\"\"Pre-computes trading signals using the general-purpose Z-score function.\"\"\"\n",
    "    print(\"Pre-computing features for this parameter set...\")\n",
    "    \n",
    "    trends = df_ohlcv.groupby(level='Ticker', group_keys=False).apply(\n",
    "        analyze_ticker_trends_log_vectorized, config['lookback_days']\n",
    "    )\n",
    "    \n",
    "    # --- KEY CHANGE: Define which Z-scores you want to calculate ---\n",
    "    z_score_columns = ['Adj Open', 'Adj High', 'Adj Low', 'Adj Close', 'Volume'] # You can now easily add 'Adj Close', etc.\n",
    "    \n",
    "    # --- Use the new general function ---\n",
    "    z_scores = df_ohlcv.groupby(level='Ticker', group_keys=False).apply(\n",
    "        calculate_rolling_z_scores_general, \n",
    "        columns_to_process=z_score_columns,\n",
    "        rolling_window=config['rolling_window']\n",
    "    )\n",
    "    \n",
    "    features = trends.join(z_scores).dropna()\n",
    "\n",
    "    # --- KEY CHANGE: Update the signal condition to use the new column name ---\n",
    "    # The column is now 'z_score_Adj Low', not 'low_rolling_z_score'.\n",
    "    signals = features[\n",
    "        (features['low_slope'] > config['slope_thresh']) &\n",
    "        (features['low_r_squared'] > config['r2_thresh']) &\n",
    "        (features['volume_slope'] > 0) &\n",
    "        (features['z_score_Adj Low'] < config['z_entry_thresh'])\n",
    "        # You could now add another condition like:\n",
    "        # & (features['z_score_Volume'] > 1.0) \n",
    "    ]\n",
    "\n",
    "\n",
    "####################################\n",
    "    # return signals\n",
    "    return signals, features\n",
    "####################################\n",
    "\n",
    "\n",
    "def run_backtest(df_ohlcv, config):\n",
    "    \"\"\"\n",
    "    Orchestrates the backtesting process with enhanced logging.\n",
    "    \"\"\"\n",
    "    # Now returns a DataFrame of features for triggered signals\n",
    "    entry_signals_features = precompute_signals(df_ohlcv, config)\n",
    "    \n",
    "    trades = []\n",
    "    open_positions = {}\n",
    "    \n",
    "    all_dates = df_ohlcv.index.get_level_values('Date').unique().sort_values()\n",
    "    start_index = max(config['lookback_days'], config['rolling_window'])\n",
    "\n",
    "    for i in tqdm(range(start_index, len(all_dates) - 1), desc=\"Backtesting\"):\n",
    "        current_date = all_dates[i]\n",
    "        next_day_date = all_dates[i+1]\n",
    "\n",
    "        closed_trades, open_positions = handle_exits_for_day(\n",
    "            current_date, next_day_date, open_positions, df_ohlcv, config\n",
    "        )\n",
    "        trades.extend(closed_trades)\n",
    "\n",
    "        # --- KEY CHANGE: Filter the features DataFrame for today's signals ---\n",
    "        signals_today = entry_signals_features[\n",
    "            entry_signals_features.index.get_level_values('Date') == current_date\n",
    "        ]\n",
    "        \n",
    "        # Pass the full signals_today DataFrame to the handler\n",
    "        open_positions = handle_entries_for_day(\n",
    "            current_date, next_day_date, signals_today, open_positions, df_ohlcv\n",
    "        )\n",
    "                \n",
    "    # --- Create the final DataFrame and reorder columns for clarity ---\n",
    "    if not trades:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    final_trades_df = pd.DataFrame(trades)\n",
    "    log_columns = [\n",
    "        'ticker', 'signal_date', 'entry_date', 'exit_signal_date', 'exit_date', 'reason',\n",
    "        'return', 'entry_price_actual', 'exit_price_actual', 'exit_trigger_price', \n",
    "        'exit_target_value', 'entry_signal_features'\n",
    "    ]\n",
    "    # Ensure all columns exist, fill missing with None\n",
    "    for col in log_columns:\n",
    "        if col not in final_trades_df.columns:\n",
    "            final_trades_df[col] = None\n",
    "            \n",
    "    return final_trades_df[log_columns]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: New Encapsulated Helper Functions\n",
    "\n",
    "These new functions isolate the logic for performance analysis and the optimization loop itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_performance(trade_results):\n",
    "    \"\"\"\n",
    "    Calculates performance metrics from a DataFrame of trades.\n",
    "    \n",
    "    Returns a dictionary of key metrics.\n",
    "    \"\"\"\n",
    "    if trade_results.empty:\n",
    "        return {'num_trades': 0, 'win_rate': 0, 'avg_return': 0, 'total_return': 0}\n",
    "    \n",
    "    win_rate = (trade_results['return'] > 0).mean()\n",
    "    total_return = (1 + trade_results['return']).prod() - 1\n",
    "    avg_return = trade_results['return'].mean()\n",
    "    \n",
    "    return {\n",
    "        'num_trades': len(trade_results),\n",
    "        'win_rate': win_rate,\n",
    "        'avg_return': avg_return,\n",
    "        'total_return': total_return\n",
    "    }\n",
    "\n",
    "def run_parameter_optimization(df, param_grid, static_params):\n",
    "    \"\"\"\n",
    "    Orchestrates the entire parameter optimization process.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The OHLCV data.\n",
    "        param_grid (dict): Dictionary with lists of parameters to test.\n",
    "        static_params (dict): Dictionary of parameters that are not being optimized.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A summary of results for each parameter combination.\n",
    "    \"\"\"\n",
    "    results_log = []\n",
    "    \n",
    "    # Use itertools.product to create a clean generator for all combinations\n",
    "    keys, values = zip(*param_grid.items())\n",
    "    param_combinations = [dict(zip(keys, v)) for v in product(*values)]\n",
    "\n",
    "    print(f\"Starting optimization for {len(param_combinations)} combinations...\")\n",
    "    \n",
    "    for param_set in tqdm(param_combinations, desc=\"Optimization Progress\"):\n",
    "        # Combine static and dynamic parameters into a single config for this run\n",
    "        current_config = {**static_params, **param_set}\n",
    "        \n",
    "        # 1. Run the backtest with the current configuration\n",
    "        trade_results = run_backtest(df, current_config)\n",
    "        \n",
    "        # 2. Analyze the performance of this run\n",
    "        performance_metrics = analyze_performance(trade_results)\n",
    "        \n",
    "        # 3. Log the results\n",
    "        log_entry = {**param_set, **performance_metrics}\n",
    "        results_log.append(log_entry)\n",
    "        \n",
    "    return pd.DataFrame(results_log)\n",
    "\n",
    "def handle_entries_for_day(current_date, next_day_date, signals_today, open_positions, df_ohlcv):\n",
    "    \"\"\"\n",
    "    Processes entries and stores signal details in the open_positions dict.\n",
    "    \"\"\"\n",
    "    # --- KEY CHANGE: Loop through the signals DataFrame ---\n",
    "    for ticker, signal_row in signals_today.iterrows():\n",
    "        # The ticker is now in the index of signal_row, so we use its name\n",
    "        ticker_name = ticker[0] \n",
    "        \n",
    "        if ticker_name not in open_positions:\n",
    "            try:\n",
    "                entry_price = df_ohlcv.loc[(ticker_name, next_day_date), 'Adj High']\n",
    "                \n",
    "                # --- LOGGING: Store more info about the entry signal ---\n",
    "                open_positions[ticker_name] = {\n",
    "                    'entry_date': next_day_date,\n",
    "                    'entry_price': entry_price,\n",
    "                    'signal_date': current_date,\n",
    "                    'signal_features': signal_row.to_dict() # Store all features that triggered the signal\n",
    "                }\n",
    "            except KeyError:\n",
    "                pass\n",
    "                \n",
    "    return open_positions\n",
    "\n",
    "def handle_exits_for_day(current_date, next_day_date, open_positions, df_ohlcv, config):\n",
    "    \"\"\"\n",
    "    Checks for exits and logs detailed information about the exit trigger.\n",
    "    Corrected version with valid syntax for the if/elif chain.\n",
    "    \"\"\"\n",
    "    closed_trades = []\n",
    "    positions_to_close = []\n",
    "\n",
    "    for ticker, pos in open_positions.items():\n",
    "        try:\n",
    "            current_close_price = df_ohlcv.loc[(ticker, current_date), 'Adj Close']\n",
    "        except KeyError:\n",
    "            continue \n",
    "\n",
    "        exit_reason = None\n",
    "        exit_target_value = None \n",
    "        \n",
    "        # --- SYNTAX FIX: Calculate all threshold values *before* the conditional block ---\n",
    "        profit_target_price = pos['entry_price'] * (1 + config['profit_target'])\n",
    "        stop_loss_price = pos['entry_price'] * (1 - config['stop_loss'])\n",
    "        days_held = (current_date.to_pydatetime().date() - pos['entry_date'].to_pydatetime().date()).days\n",
    "        \n",
    "        # --- Now, check conditions in a contiguous if/elif/elif block ---\n",
    "        if current_close_price >= profit_target_price:\n",
    "            exit_reason = \"Profit Target\"\n",
    "            exit_target_value = profit_target_price \n",
    "\n",
    "        elif current_close_price <= stop_loss_price:\n",
    "            exit_reason = \"Stop-Loss\"\n",
    "            exit_target_value = stop_loss_price \n",
    "\n",
    "        elif days_held >= config['time_hold_days']:\n",
    "            exit_reason = \"Time Hold\"\n",
    "            exit_target_value = days_held \n",
    "\n",
    "        if exit_reason:\n",
    "            try:\n",
    "                exit_price = df_ohlcv.loc[(ticker, next_day_date), 'Adj Low']\n",
    "                trade_return = (exit_price - pos['entry_price']) / pos['entry_price']\n",
    "                \n",
    "                trade_log = {\n",
    "                    'ticker': ticker, \n",
    "                    'entry_date': pos['entry_date'], \n",
    "                    'exit_date': next_day_date,\n",
    "                    'return': trade_return, \n",
    "                    'reason': exit_reason,\n",
    "                    'signal_date': pos['signal_date'],\n",
    "                    'entry_signal_features': pos['signal_features'],\n",
    "                    'entry_price_actual': pos['entry_price'],\n",
    "                    'exit_signal_date': current_date,\n",
    "                    'exit_trigger_price': current_close_price,\n",
    "                    'exit_target_value': exit_target_value,\n",
    "                    'exit_price_actual': exit_price,\n",
    "                }\n",
    "                closed_trades.append(trade_log)\n",
    "                positions_to_close.append(ticker)\n",
    "            except KeyError:\n",
    "                pass\n",
    "                \n",
    "    for ticker in positions_to_close:\n",
    "        del open_positions[ticker]\n",
    "        \n",
    "    return closed_trades, open_positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: The New, Clean Top-Level Script\n",
    "\n",
    "Your main script is now incredibly simple and readable. It's all about configuration and orchestration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. DEFINE CONFIGURATION ---\n",
    "\n",
    "# Parameters to be optimized, defining the search space\n",
    "optimization_grid = {\n",
    "    'lookback_days': [30, 60, 90],\n",
    "    'rolling_window': [15, 20]\n",
    "}\n",
    "\n",
    "# Static strategy parameters that do not change during optimization\n",
    "strategy_params = {\n",
    "    'slope_thresh': 1.0,\n",
    "    'r2_thresh': 0.50,\n",
    "    'z_entry_thresh': 0,\n",
    "    'profit_target': 0.10,\n",
    "    'stop_loss': 0.05,\n",
    "    'time_hold_days': 20\n",
    "}\n",
    "\n",
    "\n",
    "# --- 2. RUN ORCHESTRATOR ---\n",
    "\n",
    "# The main call is now a single, descriptive function\n",
    "optimization_results = run_parameter_optimization(\n",
    "    df_train, optimization_grid, strategy_params\n",
    ")\n",
    "\n",
    "\n",
    "# --- 3. ANALYZE RESULTS ---\n",
    "\n",
    "print(\"\\n\\n--- Optimization Complete ---\")\n",
    "print(optimization_results.sort_values(by='total_return', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: The \"One-Trade\" Deep Dive\n",
    "\n",
    "The most powerful debugging technique is to isolate a single trade and follow it from signal generation to exit. If the logic holds for one trade, it's likely correct for all of them.\n",
    "\n",
    "1.  **Pick a Winning Trade and a Losing Trade:** Run one of the backtests again (e.g., the one with `lookback=30`, `rolling=20`) and save the `trade_results` DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Pick a configuration to analyze\n",
    "config_to_test = {\n",
    "    **strategy_params, \n",
    "    'lookback_days': 30, \n",
    "    'rolling_window': 20\n",
    "}\n",
    "\n",
    "# 2. Run the backtest to get the detailed trade log\n",
    "trade_log_df = run_backtest(df_train, config_to_test)\n",
    "\n",
    "# 3. Isolate and inspect a single trade\n",
    "if not trade_log_df.empty:\n",
    "    # Get the first losing trade\n",
    "    losing_trade = trade_log_df[trade_log_df['return'] < 0].iloc[0]\n",
    "\n",
    "    print(\"--- Detailed Log for a Single Losing Trade ---\")\n",
    "    # Using .T transposes the Series for easy vertical reading\n",
    "    print(losing_trade.T)\n",
    "else:\n",
    "    print(\"No trades were made for this configuration.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losing_trade.entry_signal_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df_trends = analyze_ticker_trends_vectorized(df_train, lookback_days=30)\n",
    "print(f'_df_trends:\\n{_df_trends}')\n",
    "# _df_trends.to_csv('C:\\\\Users\\\\ping\\\\Desktop\\\\_df_trends.csv', index=True)\n",
    "_df_trends.index.names = ['Ticker', 'Date']\n",
    "_df_trends.reset_index().to_csv(r'C:\\Users\\ping\\Desktop\\_df_trends.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.loc['MSFT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to CSV\n",
    "_df_trends.to_csv(r'C:\\Users\\ping\\Desktop\\_df_trends.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df_trends.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loss = trade_log_df[trade_log_df['return'] < 0]\n",
    "print(df_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_win = trade_log_df[trade_log_df['return'] > 0]\n",
    "print(df_win)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loss.to_csv('C:\\\\Users\\\\ping\\\\Desktop\\\\df_loss.csv', index=True)\n",
    "df_win.to_csv('C:\\\\Users\\\\ping\\\\Desktop\\\\df_win.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimization_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a configuration to analyze\n",
    "config_to_test = {**strategy_params, 'lookback_days': 30, 'rolling_window': 20}\n",
    "\n",
    "# Run a single backtest and get the detailed trade log\n",
    "single_run_trades = run_backtest(df_train, config_to_test)\n",
    "\n",
    "# Find a winning and a losing trade to investigate\n",
    "print(\"Sample winning trade:\")\n",
    "print(single_run_trades[single_run_trades['return'] > 0].head(1))\n",
    "\n",
    "print(\"\\nSample losing trade:\")\n",
    "print(single_run_trades[single_run_trades['return'] < 0].head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trends = analyze_ticker_trends_vectorized(df_train, lookback_days=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trends.loc['FIX', '2024-10-25']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
