{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "232740e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 4423522 entries, ('A', Timestamp('1999-11-18 00:00:00')) to ('ZWS', Timestamp('2025-10-09 00:00:00'))\n",
      "Data columns (total 5 columns):\n",
      " #   Column     Dtype  \n",
      "---  ------     -----  \n",
      " 0   Adj Open   float64\n",
      " 1   Adj High   float64\n",
      " 2   Adj Low    float64\n",
      " 3   Adj Close  float64\n",
      " 4   Volume     int64  \n",
      "dtypes: float64(4), int64(1)\n",
      "memory usage: 186.3+ MB\n",
      "df_ohlcv.info():\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Adj Open</th>\n",
       "      <th>Adj High</th>\n",
       "      <th>Adj Low</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ticker</th>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">A</th>\n",
       "      <th>1999-11-18</th>\n",
       "      <td>27.2452</td>\n",
       "      <td>29.9398</td>\n",
       "      <td>23.9518</td>\n",
       "      <td>26.3470</td>\n",
       "      <td>74716417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999-11-19</th>\n",
       "      <td>25.7108</td>\n",
       "      <td>25.7482</td>\n",
       "      <td>23.8396</td>\n",
       "      <td>24.1764</td>\n",
       "      <td>18198348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999-11-22</th>\n",
       "      <td>24.7378</td>\n",
       "      <td>26.3470</td>\n",
       "      <td>23.9893</td>\n",
       "      <td>26.3470</td>\n",
       "      <td>7857766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999-11-23</th>\n",
       "      <td>25.4488</td>\n",
       "      <td>26.1225</td>\n",
       "      <td>23.9518</td>\n",
       "      <td>23.9518</td>\n",
       "      <td>7138322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999-11-24</th>\n",
       "      <td>24.0267</td>\n",
       "      <td>25.1120</td>\n",
       "      <td>23.9518</td>\n",
       "      <td>24.5881</td>\n",
       "      <td>5785607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">ZWS</th>\n",
       "      <th>2025-10-03</th>\n",
       "      <td>46.8700</td>\n",
       "      <td>47.3700</td>\n",
       "      <td>46.6200</td>\n",
       "      <td>46.8400</td>\n",
       "      <td>780500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-10-06</th>\n",
       "      <td>47.1600</td>\n",
       "      <td>47.3700</td>\n",
       "      <td>46.6700</td>\n",
       "      <td>47.2900</td>\n",
       "      <td>661000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-10-07</th>\n",
       "      <td>47.1800</td>\n",
       "      <td>47.7000</td>\n",
       "      <td>46.7300</td>\n",
       "      <td>47.2200</td>\n",
       "      <td>862300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-10-08</th>\n",
       "      <td>47.4600</td>\n",
       "      <td>47.7600</td>\n",
       "      <td>47.0900</td>\n",
       "      <td>47.3300</td>\n",
       "      <td>601600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-10-09</th>\n",
       "      <td>47.3900</td>\n",
       "      <td>47.4200</td>\n",
       "      <td>46.6400</td>\n",
       "      <td>46.7600</td>\n",
       "      <td>454200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4423522 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Adj Open  Adj High  Adj Low  Adj Close    Volume\n",
       "Ticker Date                                                        \n",
       "A      1999-11-18   27.2452   29.9398  23.9518    26.3470  74716417\n",
       "       1999-11-19   25.7108   25.7482  23.8396    24.1764  18198348\n",
       "       1999-11-22   24.7378   26.3470  23.9893    26.3470   7857766\n",
       "       1999-11-23   25.4488   26.1225  23.9518    23.9518   7138322\n",
       "       1999-11-24   24.0267   25.1120  23.9518    24.5881   5785607\n",
       "...                     ...       ...      ...        ...       ...\n",
       "ZWS    2025-10-03   46.8700   47.3700  46.6200    46.8400    780500\n",
       "       2025-10-06   47.1600   47.3700  46.6700    47.2900    661000\n",
       "       2025-10-07   47.1800   47.7000  46.7300    47.2200    862300\n",
       "       2025-10-08   47.4600   47.7600  47.0900    47.3300    601600\n",
       "       2025-10-09   47.3900   47.4200  46.6400    46.7600    454200\n",
       "\n",
       "[4423522 rows x 5 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_path = r'c:\\Users\\ping\\Files_win10\\python\\py311\\stocks\\data\\df_OHLCV_stocks_etfs.parquet'\n",
    "df_ohlcv = pd.read_parquet(data_path, engine='pyarrow')\n",
    "print(f'df_ohlcv.info():\\n{df_ohlcv.info()}')\n",
    "df_ohlcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c330ac7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Metric Registry Initialized with: ['Price', 'Sharpe', 'Sharpe (ATR)']\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# GOLDEN COPY - COMPLETE PROJECT CODE (All Fixes Included)\n",
    "# Version: Verified Portfolio and Benchmark Sharpe (ATR)  \n",
    "# Date: 2025-10-15\n",
    "# ==============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import ipywidgets as widgets\n",
    "import time\n",
    "import pprint\n",
    "import os # Make sure os is imported for the export function later\n",
    "import re\n",
    "\n",
    "from datetime import datetime, date\n",
    "from IPython.display import display, Markdown\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "pd.set_option('display.max_rows', 30)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.width', 3000)\n",
    "\n",
    "\n",
    "# --- REFACTORING PHASE 1 CODE: Feature Generation Engine ---\n",
    "\n",
    "def generate_features(df_ohlcv: pd.DataFrame, \n",
    "                      atr_period: int = 14, \n",
    "                      quality_window: int = 252, \n",
    "                      quality_min_periods: int = 126) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generates a comprehensive DataFrame of derived features from raw OHLCV data.\n",
    "\n",
    "    This function performs all heavy, window-based calculations upfront to be used\n",
    "    by downstream analysis functions. It calculates:\n",
    "    1. Technical Indicators: True Range (TR), ATR, and ATRP.\n",
    "    2. Data Quality Metrics: Rolling stale percentage, median dollar volume, etc.\n",
    "\n",
    "    Args:\n",
    "        df_ohlcv: The primary DataFrame with a (Ticker, Date) MultiIndex and\n",
    "                  columns for 'Adj High', 'Adj Low', 'Adj Close', 'Volume'.\n",
    "        atr_period: The lookback period for the ATR's Exponential Moving Average.\n",
    "        quality_window: The rolling window size for data quality metrics.\n",
    "        quality_min_periods: The minimum number of observations required to have\n",
    "                             a valid quality metric.\n",
    "\n",
    "    Returns:\n",
    "        A new DataFrame with the same (Ticker, Date) MultiIndex containing all\n",
    "        calculated feature columns.\n",
    "    \"\"\"\n",
    "    print(\"--- Starting Feature Generation ---\")\n",
    "    \n",
    "    # Ensure the DataFrame is sorted for correct window and shift operations\n",
    "    # FIX: Replaced is_lexsorted() with the current pandas attribute\n",
    "    if not df_ohlcv.index.is_monotonic_increasing:\n",
    "        print(\"Sorting index for calculation accuracy...\")\n",
    "        df_ohlcv = df_ohlcv.sort_index()\n",
    "\n",
    "    # --- 1. Technical Indicator Calculation (TR, ATR, ATRP) ---\n",
    "    print(f\"Calculating technical indicators (ATR Period: {atr_period})...\")\n",
    "    \n",
    "    # Group by ticker to handle each security independently\n",
    "    grouped = df_ohlcv.groupby(level='Ticker')\n",
    "    \n",
    "    # Get the previous day's close required for True Range\n",
    "    prev_close = grouped['Adj Close'].shift(1)\n",
    "    \n",
    "    # Calculate the three components of True Range\n",
    "    high_low = df_ohlcv['Adj High'] - df_ohlcv['Adj Low']\n",
    "    high_prev_close = abs(df_ohlcv['Adj High'] - prev_close)\n",
    "    low_prev_close = abs(df_ohlcv['Adj Low'] - prev_close)\n",
    "    \n",
    "    # Combine the components to get the final TR\n",
    "    tr = pd.concat([high_low, high_prev_close, low_prev_close], axis=1).max(axis=1, skipna=False)\n",
    "    \n",
    "    # # Calculate the ATR using an Exponential Moving Average\n",
    "    # --- FIX IS HERE ---\n",
    "    # Use .transform() to apply the EWM function. \n",
    "    # This guarantees the resulting Series has the exact same index as 'tr',\n",
    "    # preventing the index alignment error during the subsequent division.\n",
    "    atr = tr.groupby(level='Ticker').transform(\n",
    "        lambda x: x.ewm(alpha=1/atr_period, adjust=False).mean()\n",
    "    )\n",
    "\n",
    "    # --- CHANGE 1: Removed .fillna(0) ---\n",
    "    # ATRP will now be NaN on the first day, consistent with TR and ATR.\n",
    "    atrp = (atr / df_ohlcv['Adj Close']).replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    indicator_df = pd.DataFrame({\n",
    "        'TR': tr,\n",
    "        'ATR': atr,\n",
    "        'ATRP': atrp\n",
    "    })\n",
    "    \n",
    "    # --- 2. Data Quality Metric Calculation ---\n",
    "    print(f\"Calculating data quality metrics (Window: {quality_window} days)...\")\n",
    "    \n",
    "    # Create intermediate flags needed for quality calculations\n",
    "    is_stale = np.where((df_ohlcv['Volume'] == 0) | (df_ohlcv['Adj High'] == df_ohlcv['Adj Low']), 1, 0)\n",
    "    dollar_volume = df_ohlcv['Adj Close'] * df_ohlcv['Volume']\n",
    "    has_same_volume = (grouped['Volume'].diff() == 0).astype(int)\n",
    "    \n",
    "    # Combine flags into a temporary DataFrame for rolling calculations\n",
    "    quality_temp_df = pd.DataFrame({\n",
    "        'IsStale': is_stale,\n",
    "        'DollarVolume': dollar_volume,\n",
    "        'HasSameVolume': has_same_volume\n",
    "    }, index=df_ohlcv.index) # Explicitly set index to be safe\n",
    "    \n",
    "    # Perform the rolling calculations on the grouped data\n",
    "    # --- FIX IS HERE ---\n",
    "    # We switch to the older, more compatible dictionary-based aggregation method.\n",
    "    # This syntax is understood by nearly all versions of pandas.\n",
    "    rolling_result = quality_temp_df.groupby(level='Ticker').rolling(\n",
    "        window=quality_window,\n",
    "        min_periods=quality_min_periods\n",
    "    ).agg({\n",
    "        'IsStale': 'mean',\n",
    "        'DollarVolume': 'median',\n",
    "        'HasSameVolume': 'sum'\n",
    "    })\n",
    "    \n",
    "    # The dictionary syntax produces columns with the original names ('IsStale', etc.).\n",
    "    # We now explicitly rename them to our desired final names.\n",
    "    rolling_result = rolling_result.rename(columns={\n",
    "        'IsStale': 'RollingStalePct',\n",
    "        'DollarVolume': 'RollMedDollarVol', # <-- RENAMED HERE\n",
    "        'HasSameVolume': 'RollingSameVolCount'\n",
    "    })\n",
    "\n",
    "    # The index after a grouped rolling operation is hierarchical.\n",
    "    # We remove the outermost 'Ticker' level to restore the original index structure.\n",
    "    rolling_quality = rolling_result.reset_index(level=0, drop=True)\n",
    "\n",
    "    # --- 3. Combine All Features ---\n",
    "    print(\"Combining all feature sets...\")\n",
    "    features_df = pd.concat([indicator_df, rolling_quality], axis=1)\n",
    "    \n",
    "    print(\"✅ Feature generation complete.\")\n",
    "    return features_df\n",
    "\n",
    "def test_features_df(features_df: pd.DataFrame, \n",
    "                     df_ohlcv: pd.DataFrame, \n",
    "                     test_ticker: str = 'AAPL',\n",
    "                     spot_check_date: str = '2020-03-20'):\n",
    "    \"\"\"\n",
    "    Runs a suite of tests to verify the correctness of the generated features_df.\n",
    "\n",
    "    Args:\n",
    "        features_df: The generated DataFrame from the generate_features function.\n",
    "        df_ohlcv: The original source OHLCV DataFrame.\n",
    "        test_ticker: A common, liquid ticker to use for specific value checks.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Running Verification Suite for features_df (Test Ticker: {test_ticker}) ---\")\n",
    "    \n",
    "    # --- Test 1: Structural Integrity ---\n",
    "    print(\"\\n[Test 1: Structural Integrity]\")\n",
    "    assert features_df.index.equals(df_ohlcv.index), \"FAIL: Index does not match original df_ohlcv.\"\n",
    "    print(\"  ✅ PASS: Index matches original df_ohlcv.\")\n",
    "    \n",
    "    expected_cols = ['TR', 'ATR', 'ATRP', 'RollingStalePct', 'RollMedDollarVol', 'RollingSameVolCount']\n",
    "    assert all(col in features_df.columns for col in expected_cols), \"FAIL: Missing one or more expected columns.\"\n",
    "    print(\"  ✅ PASS: All expected feature columns are present.\")\n",
    "    print(f\"  - DataFrame Info:\")\n",
    "    features_df.info(verbose=False, memory_usage='deep')\n",
    "\n",
    "\n",
    "    # --- Test 2: ATR Calculation Logic ---\n",
    "    print(\"\\n[Test 2: ATR Logic Verification]\")\n",
    "    ticker_features = features_df.loc[test_ticker]\n",
    "    \n",
    "    # Test 2a: First TR value should be NaN (since prev_close is NaN)\n",
    "    first_tr = ticker_features['TR'].iloc[0]\n",
    "    assert pd.isna(first_tr), f\"FAIL: First TR value for {test_ticker} should be NaN, but got {first_tr}.\"\n",
    "    print(f\"  ✅ PASS: First TR value for {test_ticker} is NaN as expected.\")\n",
    "\n",
    "    # Test 2b: The first valid ATR should equal the first valid TR (EWM cold start behavior)\n",
    "    first_valid_tr_val = ticker_features['TR'].dropna().iloc[0]\n",
    "    first_valid_atr_val = ticker_features['ATR'].dropna().iloc[0]\n",
    "    assert np.isclose(first_valid_tr_val, first_valid_atr_val), \\\n",
    "        f\"FAIL: First valid ATR ({first_valid_atr_val}) should equal first valid TR ({first_valid_tr_val}).\"\n",
    "    print(\"  ✅ PASS: First valid ATR correctly seeded with first valid TR.\")\n",
    "\n",
    "\n",
    "    # --- Test 3: Rolling Quality Metrics Logic ---\n",
    "    print(\"\\n[Test 3: Rolling Quality Metrics Logic Verification]\")\n",
    "    quality_min_periods = 126 # Should match the parameter used in generation\n",
    "    \n",
    "    # Test 3a: Check for leading NaNs\n",
    "    first_valid_quality_idx = ticker_features['RollingStalePct'].first_valid_index()\n",
    "    if first_valid_quality_idx is None:\n",
    "        print(f\"  - INFO: No valid quality metrics found for {test_ticker} (likely too little data). Skipping test.\")\n",
    "    else:\n",
    "        position_of_first_valid = ticker_features.index.get_loc(first_valid_quality_idx)\n",
    "        assert position_of_first_valid == quality_min_periods - 1, \\\n",
    "            f\"FAIL: First valid quality metric should appear at index {quality_min_periods - 1}, but appeared at {position_of_first_valid}.\"\n",
    "        print(f\"  ✅ PASS: Leading NaNs are present for the first {quality_min_periods - 1} periods as expected.\")\n",
    "\n",
    "\n",
    "    # --- Test 4: Spot Check Against Manual Calculation ---\n",
    "    print(\"\\n[Test 4: Spot Check vs. Manual Calculation]\")\n",
    "    # # Choose a specific date for a manual calculation\n",
    "    # spot_check_date = '2020-03-20' # A volatile day for a good test\n",
    "    \n",
    "    # Manual TR Calculation\n",
    "    today_data = df_ohlcv.loc[(test_ticker, spot_check_date)]\n",
    "    yesterday_data = df_ohlcv.loc[(test_ticker, pd.to_datetime(spot_check_date) - pd.Timedelta(days=1))] # simple lookback for test\n",
    "    \n",
    "    manual_h_l = today_data['Adj High'] - today_data['Adj Low']\n",
    "    manual_h_pc = abs(today_data['Adj High'] - yesterday_data['Adj Close'])\n",
    "    manual_l_pc = abs(today_data['Adj Low'] - yesterday_data['Adj Close'])\n",
    "    manual_tr = max(manual_h_l, manual_h_pc, manual_l_pc)\n",
    "    \n",
    "    code_tr = ticker_features.loc[spot_check_date]['TR']\n",
    "    \n",
    "    assert np.isclose(manual_tr, code_tr), f\"FAIL: Manual TR ({manual_tr:.4f}) does not match code TR ({code_tr:.4f}) on {spot_check_date}.\"\n",
    "    print(f\"  ✅ PASS: Manually calculated TR on {spot_check_date} matches code's TR.\")\n",
    "    \n",
    "    print(\"\\n--- ✅ All Verification Tests Passed ---\")\n",
    "\n",
    "def export_ticker_data(ticker_to_export: str, \n",
    "                         df_ohlcv: pd.DataFrame, \n",
    "                         features_df: pd.DataFrame, \n",
    "                         output_dir: str = 'export_csv'):\n",
    "    \"\"\"\n",
    "    Exports the raw OHLCV data and the corresponding calculated features for a \n",
    "    single ticker to two separate CSV files.\n",
    "\n",
    "    This function is designed for easy manual verification of data and calculations.\n",
    "    It will create the output directory if it does not exist.\n",
    "\n",
    "    Args:\n",
    "        ticker_to_export: The ticker symbol to export (e.g., 'AAPL').\n",
    "        df_ohlcv: The main DataFrame containing the raw OHLCV data with a \n",
    "                  (Ticker, Date) MultiIndex.\n",
    "        features_df: The DataFrame containing the calculated features with a \n",
    "                     (Ticker, Date) MultiIndex.\n",
    "        output_dir: The directory where the CSV files will be saved. \n",
    "                    Defaults to 'export_csv'.\n",
    "    \"\"\"\n",
    "    print(f\"--- Attempting to export data for ticker: {ticker_to_export} ---\")\n",
    "    \n",
    "    # --- 1. Ensure the output directory exists ---\n",
    "    try:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        print(f\"Output directory '{output_dir}' is ready.\")\n",
    "    except OSError as e:\n",
    "        print(f\"Error: Could not create directory '{output_dir}'. Reason: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- 2. Isolate the data for the specified ticker ---\n",
    "    try:\n",
    "        # Use .loc to select all rows for the given ticker from the MultiIndex\n",
    "        ticker_ohlcv = df_ohlcv.loc[ticker_to_export]\n",
    "        ticker_features = features_df.loc[ticker_to_export]\n",
    "        \n",
    "        if ticker_ohlcv.empty:\n",
    "            print(f\"Warning: No OHLCV data found for ticker '{ticker_to_export}'. Cannot export.\")\n",
    "            return\n",
    "            \n",
    "        print(f\"Found {len(ticker_ohlcv)} rows of data for '{ticker_to_export}'.\")\n",
    "        \n",
    "    except KeyError:\n",
    "        print(f\"Error: Ticker '{ticker_to_export}' not found in one or both of the DataFrames. Please check the symbol.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while accessing data: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- 3. Construct file paths and export to CSV ---\n",
    "    try:\n",
    "        # Define the full path for each output file\n",
    "        ohlcv_filename = f\"{ticker_to_export}_ohlcv.csv\"\n",
    "        features_filename = f\"{ticker_to_export}_features.csv\"\n",
    "        \n",
    "        ohlcv_filepath = os.path.join(output_dir, ohlcv_filename)\n",
    "        features_filepath = os.path.join(output_dir, features_filename)\n",
    "        \n",
    "        # Export the DataFrames to CSV. The index (Date) will be included.\n",
    "        ticker_ohlcv.to_csv(ohlcv_filepath)\n",
    "        ticker_features.to_csv(features_filepath)\n",
    "        \n",
    "        print(\"\\n✅ Export successful!\")\n",
    "        print(f\"   - Raw OHLCV data saved to: {ohlcv_filepath}\")\n",
    "        print(f\"   - Calculated features saved to: {features_filepath}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: Failed to write data to CSV files. Reason: {e}\")\n",
    "\n",
    "def create_synthetic_ticker_data(\n",
    "    ticker_name: str = 'SYNTH', \n",
    "    num_days: int = 50,\n",
    "    num_zero_volume_days: int = 5,\n",
    "    num_flat_price_days: int = 3\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Creates a synthetic OHLCV DataFrame with predictable patterns and randomly injected\n",
    "    stale data conditions for robust testing.\n",
    "\n",
    "    Args:\n",
    "        ticker_name: The name for the synthetic ticker.\n",
    "        num_days: The total number of days for the ticker's history.\n",
    "        num_zero_volume_days: The number of random days to set Volume to 0.\n",
    "        num_flat_price_days: The number of random days to set High == Low.\n",
    "\n",
    "    Returns:\n",
    "        A pandas DataFrame with a (Ticker, Date) MultiIndex.\n",
    "    \"\"\"\n",
    "    print(f\"--- Creating synthetic data for '{ticker_name}' with {num_days} days ---\")\n",
    "    \n",
    "    # 1. Create a base DataFrame with \"normal\" data\n",
    "    dates = pd.to_datetime(pd.date_range(start='2023-01-01', periods=num_days, freq='B'))\n",
    "    data = {\n",
    "        'Adj Open': 100.0, 'Adj High': 102.0, 'Adj Low': 98.0,\n",
    "        'Adj Close': 100.0, 'Volume': 1_000_000\n",
    "    }\n",
    "    df = pd.DataFrame(data, index=dates)\n",
    "    df['Adj Close'] = df['Adj Close'] + np.random.randn(num_days) * 0.5 # Add some noise\n",
    "\n",
    "    # 2. Define a \"protected\" window for specific verification tests.\n",
    "    # The `verify_synthetic_ticker_features` function depends on this exact window.\n",
    "    # We will not inject random stale days here.\n",
    "    protected_start_idx, protected_end_idx = 10, 20\n",
    "    \n",
    "    # 3. Inject random \"stale\" days OUTSIDE the protected window\n",
    "    available_indices = df.index.drop(df.index[protected_start_idx:protected_end_idx])\n",
    "    \n",
    "    # Inject zero-volume days\n",
    "    if num_zero_volume_days > 0:\n",
    "        if len(available_indices) < num_zero_volume_days:\n",
    "            raise ValueError(\"Not enough available days to inject zero-volume days.\")\n",
    "        zero_vol_dates = np.random.choice(available_indices, num_zero_volume_days, replace=False)\n",
    "        df.loc[zero_vol_dates, 'Volume'] = 0\n",
    "        print(f\"  - Injected {num_zero_volume_days} random zero-volume 'stale' days.\")\n",
    "        # Update available indices to avoid overlap\n",
    "        available_indices = available_indices.drop(zero_vol_dates)\n",
    "\n",
    "    # Inject flat-price days (High == Low)\n",
    "    if num_flat_price_days > 0:\n",
    "        if len(available_indices) < num_flat_price_days:\n",
    "            raise ValueError(\"Not enough available days to inject flat-price days.\")\n",
    "        flat_price_dates = np.random.choice(available_indices, num_flat_price_days, replace=False)\n",
    "        # Set High and Low to be the same as the Close price for that day\n",
    "        df.loc[flat_price_dates, 'Adj High'] = df.loc[flat_price_dates, 'Adj Close']\n",
    "        df.loc[flat_price_dates, 'Adj Low'] = df.loc[flat_price_dates, 'Adj Close']\n",
    "        print(f\"  - Injected {num_flat_price_days} random flat-price 'stale' days.\")\n",
    "\n",
    "    # 4. Inject the specific, hand-crafted patterns inside the protected window for verification\n",
    "    print(\"  - Injecting specific patterns for programmatic verification...\")\n",
    "    # Pattern for RollingStalePct: 2 stale days in 10 (20%)\n",
    "    df.iloc[10, df.columns.get_loc('Volume')] = 0  # Stale day (zero volume)\n",
    "    df.iloc[11, df.columns.get_loc('Adj High')] = 99.0 # Stale day (High == Low)\n",
    "    df.iloc[11, df.columns.get_loc('Adj Low')] = 99.0\n",
    "    \n",
    "    # Pattern for RollingMedianVolume\n",
    "    for i in range(10):\n",
    "        df.iloc[10 + i, df.columns.get_loc('Adj Close')] = 100.0 # Standardize price for easy median calc\n",
    "        df.iloc[10 + i, df.columns.get_loc('Volume')] = (i + 1) * 10000\n",
    "\n",
    "    # Pattern for RollingSameVolCount\n",
    "    df.iloc[15, df.columns.get_loc('Volume')] = 77777\n",
    "    df.iloc[16, df.columns.get_loc('Volume')] = 77777\n",
    "    df.iloc[17, df.columns.get_loc('Volume')] = 77777\n",
    "    \n",
    "    # 5. Set the MultiIndex\n",
    "    df['Ticker'] = ticker_name\n",
    "    df = df.set_index(['Ticker', df.index])\n",
    "    df.index.names = ['Ticker', 'Date']\n",
    "    \n",
    "    print(\"✅ Synthetic data created successfully.\")\n",
    "    return df\n",
    "\n",
    "def verify_synthetic_ticker_features(features_df: pd.DataFrame, \n",
    "                                       ticker_name: str = 'SYNTH',\n",
    "                                       quality_window: int = 10):\n",
    "    \"\"\"\n",
    "    Verifies the quality metric calculations on the features_df generated from\n",
    "    the synthetic ticker data.\n",
    "\n",
    "    Args:\n",
    "        features_df: The DataFrame of calculated features.\n",
    "        ticker_name: The name of the synthetic ticker.\n",
    "        quality_window: The rolling window used, which must match the window\n",
    "                        of the synthetic data pattern.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Running Verification on Synthetic Ticker '{ticker_name}' ---\")\n",
    "    \n",
    "    # --- Expected values based on our synthetic data design ---\n",
    "    EXPECTED_STALE_PCT = 0.20  # 2 stale days out of 10\n",
    "    EXPECTED_MEDIAN_DOLLAR_VOL = 5_500_000.0 # median of (10k..100k) * price of 100\n",
    "    EXPECTED_SAME_VOL_COUNT = 2.0 # Three consecutive days gives two 'diff() == 0' events\n",
    "\n",
    "    try:\n",
    "        # Isolate the features for our synthetic ticker\n",
    "        ticker_features = features_df.loc[ticker_name]\n",
    "        \n",
    "        # The first valid calculation will be on the last day of our 10-day window.\n",
    "        # The window starts at index 10 and has a length of 10, so it ends at index 19.\n",
    "        verification_date = ticker_features.index[19]\n",
    "        \n",
    "        print(f\"Verifying calculations on date: {verification_date.date()}\")\n",
    "        \n",
    "        # Get the calculated values from the DataFrame\n",
    "        calculated_values = ticker_features.loc[verification_date]\n",
    "        stale_pct = calculated_values['RollingStalePct']\n",
    "        # --- CHANGE 2 (continued): Accessing the renamed column ---\n",
    "        median_vol = calculated_values['RollMedDollarVol'] \n",
    "        same_vol_count = calculated_values['RollingSameVolCount']\n",
    "        \n",
    "        # --- Perform Assertions ---\n",
    "        print(\"\\n[Test 1: RollingStalePct]\")\n",
    "        assert np.isclose(stale_pct, EXPECTED_STALE_PCT), f\"FAIL: Expected {EXPECTED_STALE_PCT}, Got {stale_pct}\"\n",
    "        print(f\"  ✅ PASS: Expected {EXPECTED_STALE_PCT}, Got {stale_pct}\")\n",
    "\n",
    "        print(\"\\n[Test 2: RollMedDollarVol]\")\n",
    "        assert np.isclose(median_vol, EXPECTED_MEDIAN_DOLLAR_VOL), f\"FAIL: Expected {EXPECTED_MEDIAN_DOLLAR_VOL}, Got {median_vol}\"\n",
    "        print(f\"  ✅ PASS: Expected {EXPECTED_MEDIAN_DOLLAR_VOL}, Got {median_vol}\")\n",
    "\n",
    "        print(\"\\n[Test 3: RollingSameVolCount]\")\n",
    "        assert np.isclose(same_vol_count, EXPECTED_SAME_VOL_COUNT), f\"FAIL: Expected {EXPECTED_SAME_VOL_COUNT}, Got {same_vol_count}\"\n",
    "        print(f\"  ✅ PASS: Expected {EXPECTED_SAME_VOL_COUNT}, Got {same_vol_count}\")\n",
    "\n",
    "        print(\"\\n--- ✅ All Synthetic Data Verification Tests Passed ---\")\n",
    "\n",
    "    except KeyError:\n",
    "        print(f\"FAIL: Ticker '{ticker_name}' not found in features_df.\")\n",
    "    except IndexError:\n",
    "        print(\"FAIL: Not enough data in features_df to run verification. Check num_days.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during verification: {e}\")\n",
    "\n",
    "# --- A. HELPER FUNCTIONS ---\n",
    "\n",
    "def calculate_gain(price_series: pd.Series):\n",
    "    \"\"\"Calculates the total gain over a series of prices.\"\"\"\n",
    "    # Ensure there are at least two data points to calculate a gain\n",
    "    if price_series.dropna().shape[0] < 2: return np.nan\n",
    "    # Use forward-fill for the end price and back-fill for the start price\n",
    "    # to handle potential NaNs at the beginning or end of the series.\n",
    "    return (price_series.ffill().iloc[-1] / price_series.bfill().iloc[0]) - 1\n",
    "\n",
    "def calculate_sharpe(return_series: pd.Series):\n",
    "    \"\"\"Calculates the annualized Sharpe ratio from a series of daily returns.\"\"\"\n",
    "    # Ensure there are at least two returns to calculate a standard deviation\n",
    "    if return_series.dropna().shape[0] < 2: return np.nan\n",
    "    std_dev = return_series.std()\n",
    "    # Avoid division by zero if returns are constant\n",
    "    if std_dev > 0 and std_dev != np.inf:\n",
    "        return (return_series.mean() / std_dev) * np.sqrt(252)\n",
    "    return np.nan\n",
    "\n",
    "def calculate_sharpe_atr(return_series: pd.Series, atrp_series: pd.Series):\n",
    "    \"\"\"Calculates a Sharpe-like ratio using mean ATRP as the denominator.\"\"\"\n",
    "    # Ensure there are returns and that ATRP data is valid\n",
    "    if return_series.dropna().shape[0] < 2 or atrp_series.dropna().empty:\n",
    "        return np.nan\n",
    "        \n",
    "    mean_return = return_series.mean()\n",
    "    mean_atrp = atrp_series.mean()\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    if mean_atrp > 0 and mean_atrp != np.inf:\n",
    "        return mean_return / mean_atrp\n",
    "        \n",
    "    return np.nan\n",
    "\n",
    "def print_nested(d, indent=0, width=4):\n",
    "    \"\"\"Pretty-print any nested dict/list/tuple combination.\"\"\"\n",
    "    spacing = ' ' * indent\n",
    "    if isinstance(d, dict):\n",
    "        for k, v in d.items():\n",
    "            print(f'{spacing}{k}:')\n",
    "            print_nested(v, indent + width, width)\n",
    "    elif isinstance(d, (list, tuple)):\n",
    "        for item in d:\n",
    "            print_nested(item, indent, width)\n",
    "    else:\n",
    "        print(f'{spacing}{d}')\n",
    "\n",
    "# --- B. MODULAR METRIC CALCULATION ENGINE ---\n",
    "\n",
    "def calculate_price_metric(metric_data: dict) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Calculates the 'Price' metric (total gain) over the calculation period.\n",
    "\n",
    "    Args:\n",
    "        metric_data: A dictionary containing pre-calculated data Series.\n",
    "                     Requires 'calc_close' (pd.DataFrame): The close prices for the calc period.\n",
    "\n",
    "    Returns:\n",
    "        A pandas Series of the calculated metric values, indexed by Ticker.\n",
    "    \"\"\"\n",
    "    calc_close = metric_data['calc_close']\n",
    "    \n",
    "    # Ensure there are at least two data points to calculate a gain\n",
    "    if len(calc_close) < 2:\n",
    "        return pd.Series(dtype='float64', index=calc_close.columns)\n",
    "\n",
    "    first_prices = calc_close.bfill().iloc[0]\n",
    "    last_prices = calc_close.ffill().iloc[-1]\n",
    "    \n",
    "    # The division of two Series aligns by index (Ticker), which is what we want.\n",
    "    price_metric = last_prices / first_prices\n",
    "    \n",
    "    return price_metric.dropna()\n",
    "\n",
    "def calculate_sharpe_metric(metric_data: dict) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Calculates the annualized 'Sharpe' ratio metric over the calculation period.\n",
    "\n",
    "    Args:\n",
    "        metric_data: A dictionary containing pre-calculated data Series.\n",
    "                     Requires 'daily_returns' (pd.DataFrame): Daily returns for the calc period.\n",
    "\n",
    "    Returns:\n",
    "        A pandas Series of the calculated metric values, indexed by Ticker.\n",
    "    \"\"\"\n",
    "    daily_returns = metric_data['daily_returns']\n",
    "    \n",
    "    # Ensure there's enough data to calculate standard deviation\n",
    "    if len(daily_returns.dropna()) < 2:\n",
    "        return pd.Series(dtype='float64', index=daily_returns.columns)\n",
    "    \n",
    "    mean_returns = daily_returns.mean()\n",
    "    std_returns = daily_returns.std()\n",
    "    \n",
    "    # Standard annualized Sharpe Ratio calculation. Avoid division by zero.\n",
    "    # We replace resulting NaNs/infs with 0 to handle cases of zero volatility.\n",
    "    sharpe_ratio = (mean_returns / std_returns * np.sqrt(252))\n",
    "    \n",
    "    return sharpe_ratio.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "    \n",
    "def calculate_sharpe_atr_metric(metric_data: dict) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Calculates the 'Sharpe (ATR)' metric over the calculation period.\n",
    "\n",
    "    Args:\n",
    "        metric_data: A dictionary containing pre-calculated data Series.\n",
    "                     Requires 'daily_returns' (pd.DataFrame): Daily returns for the calc period.\n",
    "                     Requires 'atrp' (pd.Series): Mean ATRP for each ticker over the period.\n",
    "\n",
    "    Returns:\n",
    "        A pandas Series of the calculated metric values, indexed by Ticker.\n",
    "    \"\"\"\n",
    "    daily_returns = metric_data['daily_returns']\n",
    "    atrp = metric_data['atrp']\n",
    "    \n",
    "    mean_returns = daily_returns.mean()\n",
    "    \n",
    "    # ATRP-based Sharpe. Avoid division by zero.\n",
    "    # We replace resulting NaNs/infs with 0.\n",
    "    sharpe_atr = mean_returns / atrp\n",
    "    \n",
    "    return sharpe_atr.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "\n",
    "def calculate_buy_and_hold_performance(df_close: pd.DataFrame,\n",
    "                                       features_df: pd.DataFrame,\n",
    "                                       tickers: list,\n",
    "                                       start_date: pd.Timestamp,\n",
    "                                       end_date: pd.Timestamp):\n",
    "    \"\"\"\n",
    "    Calculates the performance series for an equal-weighted, buy-and-hold portfolio.\n",
    "\n",
    "    [CORRECTED to calculate a value-weighted ATRP consistent with a buy-and-hold strategy]\n",
    "\n",
    "    Args:\n",
    "        df_close (pd.DataFrame): DataFrame of adjusted close prices with dates as index and tickers as columns.\n",
    "        features_df (pd.DataFrame): DataFrame with a MultiIndex ('Ticker', 'Date') containing features like 'ATRP'.\n",
    "        tickers (list): A list of ticker symbols to include in the portfolio.\n",
    "        start_date (pd.Timestamp): The starting date for the performance calculation.\n",
    "        end_date (pd.Timestamp): The ending date for the performance calculation.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - pd.Series: The normalized portfolio value over time (buy-and-hold).\n",
    "            - pd.Series: The daily portfolio returns (buy-and-hold).\n",
    "            - pd.Series: The daily VALUE-WEIGHTED portfolio ATRP (buy-and-hold).\n",
    "    \"\"\"\n",
    "    if not tickers or tickers is None:\n",
    "        empty_series = pd.Series(dtype='float64')\n",
    "        return empty_series, empty_series, empty_series\n",
    "\n",
    "    # 1. Calculate Portfolio Return Series (This part was already correct)\n",
    "    prices_raw = df_close[tickers].loc[start_date:end_date]\n",
    "    if prices_raw.dropna(how='all').empty:\n",
    "        empty_series = pd.Series(dtype='float64')\n",
    "        return empty_series, empty_series, empty_series\n",
    "\n",
    "    prices_norm = prices_raw.div(prices_raw.bfill().iloc[0])\n",
    "    value_series = prices_norm.mean(axis=1)\n",
    "    return_series = value_series.pct_change()\n",
    "\n",
    "    # 2. Calculate Portfolio ATRP Series (This part is NOW CORRECTED)\n",
    "    # Get the individual ATRP values for each stock in the portfolio\n",
    "    full_period_index = pd.MultiIndex.from_product(\n",
    "        [tickers, return_series.index],\n",
    "        names=['Ticker', 'Date']\n",
    "    )\n",
    "    portfolio_atrp_features = features_df.loc[features_df.index.intersection(full_period_index)]\n",
    "    portfolio_atrp_daily_unstacked = portfolio_atrp_features['ATRP'].unstack(level='Ticker')\n",
    "\n",
    "    # +++ NEW LOGIC: Calculate daily weights based on market value drift +++\n",
    "    # The sum of normalized prices each day represents the total portfolio value\n",
    "    # relative to the start.\n",
    "    daily_portfolio_total_value = prices_norm.sum(axis=1)\n",
    "\n",
    "    # The weight of each asset is its normalized price divided by the daily total.\n",
    "    # .div(..., axis=0) ensures row-wise division.\n",
    "    daily_weights = prices_norm.div(daily_portfolio_total_value, axis=0)\n",
    "    \n",
    "    # Align the ATRP DataFrame columns with the weights DataFrame columns to ensure correct multiplication\n",
    "    # This handles cases where a ticker might have a price but no ATRP value on a given day or vice-versa\n",
    "    aligned_weights, aligned_atrp = daily_weights.align(portfolio_atrp_daily_unstacked, join='inner', axis=1)\n",
    "\n",
    "    # The portfolio ATRP is the weighted sum of the individual ATRPs.\n",
    "    # This is calculated by element-wise multiplication of weights and ATRPs,\n",
    "    # then summing across the tickers for each day.\n",
    "    atrp_series = (aligned_weights * aligned_atrp).sum(axis=1)\n",
    "    # --- END OF CORRECTED LOGIC ---\n",
    "\n",
    "    return value_series, return_series, atrp_series\n",
    "\n",
    "\n",
    "\n",
    "# The single source of truth for all available ranking metrics.\n",
    "# Maps the user-facing name to the calculation function.\n",
    "METRIC_REGISTRY = {\n",
    "    'Price': calculate_price_metric,\n",
    "    'Sharpe': calculate_sharpe_metric,\n",
    "    'Sharpe (ATR)': calculate_sharpe_atr_metric,\n",
    "}\n",
    "\n",
    "print(\"✅ Metric Registry Initialized with:\", list(METRIC_REGISTRY.keys()))\n",
    "\n",
    "# --- B. THE CORE CALCULATION ENGINE ---\n",
    "\n",
    "# def run_walk_forward_step(df_close_full, df_high_full, df_low_full,\n",
    "#                           master_trading_days,\n",
    "#                           start_date, calc_period, fwd_period,\n",
    "#                           metric, rank_start, rank_end, benchmark_ticker,\n",
    "#                           features_df,\n",
    "#                           debug=False):\n",
    "#     \"\"\"\n",
    "#     Runs a single step of the walk-forward analysis with a strict, pre-emptive\n",
    "#     check to ensure the full period is available.\n",
    "#     \"\"\"\n",
    "#     debug_data = {} if debug else None\n",
    "\n",
    "#     # 1. Determine exact date ranges with a NEW pre-emptive check\n",
    "#     try:\n",
    "#         start_idx = master_trading_days.get_loc(start_date)\n",
    "#     except KeyError:\n",
    "#         return ({'error': f\"Start date {start_date.date()} is not a valid trading day.\"}, None)\n",
    "\n",
    "#     # +++ THIS IS THE NEW PRE-EMPTIVE CHECK LOGIC +++\n",
    "#     # Calculate the desired end index without clamping first.\n",
    "#     desired_viz_end_idx = start_idx + calc_period + fwd_period\n",
    "    \n",
    "#     # Check if the desired end index is out of bounds.\n",
    "#     if desired_viz_end_idx >= len(master_trading_days):\n",
    "#         last_available_date = master_trading_days[-1].date()\n",
    "#         required_days = calc_period + fwd_period\n",
    "#         available_days = len(master_trading_days) - start_idx\n",
    "#         error_msg = (f\"Not enough data for the full requested period. \"\n",
    "#                      f\"Required: {required_days} days, Available: {available_days} days until {last_available_date}.\")\n",
    "#         return ({'error': error_msg}, None)\n",
    "#     # --- END OF NEW CHECK ---\n",
    "\n",
    "#     # If the check passes, we know the full period is available.\n",
    "#     # The 'min' calls are now just a redundant safety measure.\n",
    "#     calc_end_idx = min(start_idx + calc_period, len(master_trading_days) - 1)\n",
    "#     viz_end_idx = min(calc_end_idx + fwd_period, len(master_trading_days) - 1)\n",
    "\n",
    "#     safe_start_date = master_trading_days[start_idx]\n",
    "#     safe_calc_end_date = master_trading_days[calc_end_idx]\n",
    "#     safe_viz_end_date = master_trading_days[viz_end_idx]\n",
    "    \n",
    "#     if safe_start_date >= safe_calc_end_date:\n",
    "#         return ({'error': \"Invalid date range (calc period has zero or negative length).\"}, None)\n",
    "\n",
    "#     # (The rest of the function remains completely unchanged...)\n",
    "#     # 2. Slice data for the calculation period\n",
    "#     calc_close_raw = df_close_full.loc[safe_start_date:safe_calc_end_date]\n",
    "#     calc_close = calc_close_raw.dropna(axis=1, how='all')\n",
    "#     if calc_close.shape[1] == 0 or len(calc_close) < 2:\n",
    "#         return ({'error': \"Not enough data in calc period.\"}, None)\n",
    "\n",
    "#     # 3. Calculate INTERMEDIATE data required for the ranking metrics\n",
    "#     daily_returns = calc_close.bfill().ffill().pct_change()\n",
    "#     valid_tickers = calc_close.columns\n",
    "#     calc_period_index = pd.MultiIndex.from_product([valid_tickers, calc_close.index], names=['Ticker', 'Date'])\n",
    "#     features_in_period = features_df.loc[features_df.index.intersection(calc_period_index)]\n",
    "#     atrp = features_in_period.groupby(level='Ticker')['ATRP'].mean()\n",
    "\n",
    "#     # 4. Calculate all ranking metrics by iterating through the METRIC_REGISTRY\n",
    "#     metric_ingredients = { 'calc_close': calc_close, 'daily_returns': daily_returns, 'atrp': atrp, }\n",
    "#     metric_values = {}\n",
    "#     for name, func in METRIC_REGISTRY.items():\n",
    "#         metric_values[name] = func(metric_ingredients)\n",
    "#     if metric not in metric_values or metric_values[metric].empty:\n",
    "#         return ({'error': f\"Metric '{metric}' could not be calculated or resulted in no valid tickers.\"}, None)\n",
    "\n",
    "#     # 5. Rank tickers and select the portfolio\n",
    "#     sorted_tickers = metric_values[metric].sort_values(ascending=False)\n",
    "#     tickers_to_display = sorted_tickers.index[rank_start-1:rank_end].tolist()\n",
    "#     if not tickers_to_display:\n",
    "#         return ({'error': \"No tickers found for the selected rank.\"}, None)\n",
    "\n",
    "#     # 6. Calculate Portfolio & Benchmark Performance\n",
    "#     normalized_plot_data = df_close_full[tickers_to_display].loc[safe_start_date:safe_viz_end_date]\n",
    "#     normalized_plot_data = normalized_plot_data.div(normalized_plot_data.bfill().iloc[0])\n",
    "#     actual_calc_end_ts = calc_close.index.max()\n",
    "#     portfolio_series = normalized_plot_data.mean(axis=1)\n",
    "#     portfolio_return_series = portfolio_series.pct_change()\n",
    "#     benchmark_price_series = df_close_full.get(benchmark_ticker)\n",
    "#     benchmark_return_series = benchmark_price_series.loc[safe_start_date:safe_viz_end_date].bfill().ffill().pct_change() if benchmark_price_series is not None else pd.Series(dtype='float64')\n",
    "#     try:\n",
    "#         boundary_loc = portfolio_return_series.index.get_loc(actual_calc_end_ts)\n",
    "#         calc_portfolio_returns = portfolio_return_series.iloc[:boundary_loc + 1]\n",
    "#         fwd_portfolio_returns = portfolio_return_series.iloc[boundary_loc + 1:]\n",
    "#         if benchmark_price_series is not None:\n",
    "#             bm_boundary_loc = benchmark_return_series.index.get_loc(actual_calc_end_ts)\n",
    "#             calc_benchmark_returns = benchmark_return_series.iloc[:bm_boundary_loc + 1]\n",
    "#             fwd_benchmark_returns = benchmark_return_series.iloc[bm_boundary_loc + 1:]\n",
    "#         else:\n",
    "#             calc_benchmark_returns, fwd_benchmark_returns = pd.Series(dtype='float64'), pd.Series(dtype='float64')\n",
    "#     except (KeyError, IndexError):\n",
    "#         calc_portfolio_returns = portfolio_return_series.loc[:actual_calc_end_ts]\n",
    "#         fwd_portfolio_returns = portfolio_return_series.loc[actual_calc_end_ts:].iloc[1:]\n",
    "#         if benchmark_price_series is not None:\n",
    "#             calc_benchmark_returns = benchmark_return_series.loc[:actual_calc_end_ts]\n",
    "#             fwd_benchmark_returns = benchmark_return_series.loc[actual_calc_end_ts:].iloc[1:]\n",
    "#         else:\n",
    "#             calc_benchmark_returns, fwd_benchmark_returns = pd.Series(dtype='float64'), pd.Series(dtype='float64')\n",
    "#     full_period_index = pd.MultiIndex.from_product([tickers_to_display, portfolio_return_series.index], names=['Ticker', 'Date'])\n",
    "#     portfolio_atrp_features = features_df.loc[features_df.index.intersection(full_period_index)]\n",
    "#     portfolio_atrp_daily_unstacked = portfolio_atrp_features['ATRP'].unstack(level='Ticker')\n",
    "#     portfolio_atrp_series = portfolio_atrp_daily_unstacked.mean(axis=1)\n",
    "#     if benchmark_ticker in df_close_full.columns:\n",
    "#         benchmark_atrp_series = features_df.xs(benchmark_ticker, level='Ticker')['ATRP'].loc[safe_start_date:safe_viz_end_date]\n",
    "#     else:\n",
    "#         benchmark_atrp_series = pd.Series(dtype='float64')\n",
    "#     calc_portfolio_atrp = portfolio_atrp_series.loc[:actual_calc_end_ts]\n",
    "#     fwd_portfolio_atrp = portfolio_atrp_series.loc[actual_calc_end_ts:].iloc[1:]\n",
    "#     calc_benchmark_atrp = benchmark_atrp_series.loc[:actual_calc_end_ts]\n",
    "#     fwd_benchmark_atrp = benchmark_atrp_series.loc[actual_calc_end_ts:].iloc[1:]\n",
    "#     perf_data = {}\n",
    "#     perf_data['calc_p_gain'] = calculate_gain(portfolio_series.loc[:actual_calc_end_ts])\n",
    "#     perf_data['fwd_p_gain'] = calculate_gain(portfolio_series.loc[actual_calc_end_ts:])\n",
    "#     perf_data['full_p_gain'] = calculate_gain(portfolio_series)\n",
    "#     perf_data['calc_p_sharpe'] = calculate_sharpe(calc_portfolio_returns)\n",
    "#     perf_data['fwd_p_sharpe'] = calculate_sharpe(fwd_portfolio_returns)\n",
    "#     perf_data['full_p_sharpe'] = calculate_sharpe(portfolio_return_series)\n",
    "#     perf_data['calc_b_gain'] = calculate_gain(benchmark_price_series.loc[safe_start_date:actual_calc_end_ts]) if benchmark_price_series is not None else np.nan\n",
    "#     perf_data['fwd_b_gain'] = calculate_gain(benchmark_price_series.loc[actual_calc_end_ts:safe_viz_end_date]) if benchmark_price_series is not None else np.nan\n",
    "#     perf_data['full_b_gain'] = calculate_gain(benchmark_price_series.loc[safe_start_date:safe_viz_end_date]) if benchmark_price_series is not None else np.nan\n",
    "#     perf_data['calc_b_sharpe'] = calculate_sharpe(calc_benchmark_returns)\n",
    "#     perf_data['fwd_b_sharpe'] = calculate_sharpe(fwd_benchmark_returns)\n",
    "#     perf_data['full_b_sharpe'] = calculate_sharpe(benchmark_return_series)\n",
    "#     perf_data['calc_p_sharpe_atr'] = calculate_sharpe_atr(calc_portfolio_returns, calc_portfolio_atrp)\n",
    "#     perf_data['fwd_p_sharpe_atr'] = calculate_sharpe_atr(fwd_portfolio_returns, fwd_portfolio_atrp)\n",
    "#     perf_data['full_p_sharpe_atr'] = calculate_sharpe_atr(portfolio_return_series, portfolio_atrp_series)\n",
    "#     perf_data['calc_b_sharpe_atr'] = calculate_sharpe_atr(calc_benchmark_returns, calc_benchmark_atrp)\n",
    "#     perf_data['fwd_b_sharpe_atr'] = calculate_sharpe_atr(fwd_benchmark_returns, fwd_benchmark_atrp)\n",
    "#     perf_data['full_b_sharpe_atr'] = calculate_sharpe_atr(benchmark_return_series, benchmark_atrp_series)\n",
    "#     if debug:\n",
    "#         df_ranking_base = pd.DataFrame({'MeanDailyReturn': daily_returns.mean(),'StdDevDailyReturn': daily_returns.std(),'MeanATRP': atrp})\n",
    "#         df_metrics = pd.DataFrame(metric_values)\n",
    "#         df_metrics.columns = [f'Metric_{col}' for col in df_metrics.columns]\n",
    "#         df_ranking = df_ranking_base.join(df_metrics, how='left')\n",
    "#         df_ranking.index.name = 'Ticker'\n",
    "#         debug_data['ranking_metrics'] = df_ranking.sort_values(f'Metric_{metric}', ascending=False)\n",
    "#     calc_end_prices = calc_close.ffill().iloc[-1]\n",
    "#     fwd_close_slice = df_close_full.loc[actual_calc_end_ts:safe_viz_end_date]\n",
    "#     viz_end_prices = fwd_close_slice.ffill().iloc[-1] if not fwd_close_slice.empty and len(fwd_close_slice) >= 2 else calc_end_prices\n",
    "#     calc_gains = (calc_end_prices / calc_close.bfill().iloc[0]) - 1\n",
    "#     fwd_gains = (viz_end_prices / calc_end_prices) - 1\n",
    "#     results_df = pd.DataFrame({'Rank': range(rank_start, rank_start + len(tickers_to_display)), 'Metric': metric, 'MetricValue': sorted_tickers.loc[tickers_to_display].values, 'CalcPrice': calc_end_prices.loc[tickers_to_display], 'CalcGain': calc_gains.loc[tickers_to_display], 'FwdGain': fwd_gains.loc[tickers_to_display]}, index=pd.Index(tickers_to_display, name='Ticker'))\n",
    "#     if benchmark_price_series is not None and benchmark_ticker in calc_close.columns:\n",
    "#         benchmark_df_row = pd.DataFrame({'Rank': np.nan, 'Metric': metric, 'MetricValue': metric_values[metric].get(benchmark_ticker, np.nan), 'CalcPrice': calc_end_prices[benchmark_ticker], 'CalcGain': calc_gains[benchmark_ticker], 'FwdGain': fwd_gains[benchmark_ticker]}, index=pd.Index([f\"{benchmark_ticker} (BM)\"], name='Ticker'))\n",
    "#         results_df = pd.concat([results_df, benchmark_df_row])\n",
    "#     if debug:\n",
    "#         df_trace = normalized_plot_data.copy()\n",
    "#         df_trace.columns = [f'Norm_Price_{c}' for c in df_trace.columns]\n",
    "#         df_trace['Norm_Price_Portfolio'] = portfolio_series\n",
    "#         if benchmark_price_series is not None and not benchmark_price_series.loc[safe_start_date:safe_viz_end_date].dropna().empty:\n",
    "#             norm_bm = benchmark_price_series.loc[safe_start_date:safe_viz_end_date] / benchmark_price_series.loc[safe_start_date:].bfill().iloc[0]\n",
    "#             df_trace[f'Norm_Price_Benchmark_{benchmark_ticker}'] = norm_bm\n",
    "#         for col in df_trace.columns:\n",
    "#             if 'Norm_Price' in col:\n",
    "#                 df_trace[col.replace('Norm_Price', 'Return')] = df_trace[col].pct_change()\n",
    "#         debug_data['portfolio_trace'] = df_trace\n",
    "#     final_results = {\n",
    "#         'tickers_to_display': tickers_to_display, 'normalized_plot_data': normalized_plot_data,\n",
    "#         'portfolio_series': portfolio_series, 'benchmark_price_series': benchmark_price_series,\n",
    "#         'performance_data': perf_data, 'results_df': results_df, 'actual_calc_end_ts': actual_calc_end_ts,\n",
    "#         'safe_start_date': safe_start_date, 'safe_viz_end_date': safe_viz_end_date,\n",
    "#         'error': None\n",
    "#     }\n",
    "#     return (final_results, debug_data)\n",
    "\n",
    "\n",
    "\n",
    "def run_walk_forward_step(df_close_full, df_high_full, df_low_full,\n",
    "                          master_trading_days,\n",
    "                          start_date, calc_period, fwd_period,\n",
    "                          metric, rank_start, rank_end, benchmark_ticker,\n",
    "                          features_df,\n",
    "                          debug=False):\n",
    "    \"\"\"\n",
    "    Runs a single step of the walk-forward analysis with a strict, pre-emptive\n",
    "    check to ensure the full period is available.\n",
    "    \"\"\"\n",
    "    debug_data = {} if debug else None\n",
    "\n",
    "    # 1. Determine exact date ranges with a NEW pre-emptive check\n",
    "    try:\n",
    "        start_idx = master_trading_days.get_loc(start_date)\n",
    "    except KeyError:\n",
    "        return ({'error': f\"Start date {start_date.date()} is not a valid trading day.\"}, None)\n",
    "\n",
    "    # +++ THIS IS THE NEW PRE-EMPTIVE CHECK LOGIC +++\n",
    "    # Calculate the desired end index without clamping first.\n",
    "    desired_viz_end_idx = start_idx + calc_period + fwd_period\n",
    "    \n",
    "    # Check if the desired end index is out of bounds.\n",
    "    if desired_viz_end_idx >= len(master_trading_days):\n",
    "        last_available_date = master_trading_days[-1].date()\n",
    "        required_days = calc_period + fwd_period\n",
    "        available_days = len(master_trading_days) - start_idx\n",
    "        error_msg = (f\"Not enough data for the full requested period. \"\n",
    "                     f\"Required: {required_days} days, Available: {available_days} days until {last_available_date}.\")\n",
    "        return ({'error': error_msg}, None)\n",
    "    # --- END OF NEW CHECK ---\n",
    "\n",
    "    # If the check passes, we know the full period is available.\n",
    "    # The 'min' calls are now just a redundant safety measure.\n",
    "    calc_end_idx = min(start_idx + calc_period, len(master_trading_days) - 1)\n",
    "    viz_end_idx = min(calc_end_idx + fwd_period, len(master_trading_days) - 1)\n",
    "\n",
    "    safe_start_date = master_trading_days[start_idx]\n",
    "    safe_calc_end_date = master_trading_days[calc_end_idx]\n",
    "    safe_viz_end_date = master_trading_days[viz_end_idx]\n",
    "    \n",
    "    if safe_start_date >= safe_calc_end_date:\n",
    "        return ({'error': \"Invalid date range (calc period has zero or negative length).\"}, None)\n",
    "\n",
    "    # (The rest of the function remains completely unchanged...)\n",
    "    # 2. Slice data for the calculation period\n",
    "    calc_close_raw = df_close_full.loc[safe_start_date:safe_calc_end_date]\n",
    "    calc_close = calc_close_raw.dropna(axis=1, how='all')\n",
    "    if calc_close.shape[1] == 0 or len(calc_close) < 2:\n",
    "        return ({'error': \"Not enough data in calc period.\"}, None)\n",
    "\n",
    "    # 3. Calculate INTERMEDIATE data required for the ranking metrics\n",
    "    daily_returns = calc_close.bfill().ffill().pct_change()\n",
    "    valid_tickers = calc_close.columns\n",
    "    calc_period_index = pd.MultiIndex.from_product([valid_tickers, calc_close.index], names=['Ticker', 'Date'])\n",
    "    features_in_period = features_df.loc[features_df.index.intersection(calc_period_index)]\n",
    "    atrp = features_in_period.groupby(level='Ticker')['ATRP'].mean()\n",
    "\n",
    "    # 4. Calculate all ranking metrics by iterating through the METRIC_REGISTRY\n",
    "    metric_ingredients = { 'calc_close': calc_close, 'daily_returns': daily_returns, 'atrp': atrp, }\n",
    "    metric_values = {}\n",
    "    for name, func in METRIC_REGISTRY.items():\n",
    "        metric_values[name] = func(metric_ingredients)\n",
    "    if metric not in metric_values or metric_values[metric].empty:\n",
    "        return ({'error': f\"Metric '{metric}' could not be calculated or resulted in no valid tickers.\"}, None)\n",
    "\n",
    "    # 5. Rank tickers and select the portfolio\n",
    "    sorted_tickers = metric_values[metric].sort_values(ascending=False)\n",
    "    tickers_to_display = sorted_tickers.index[rank_start-1:rank_end].tolist()\n",
    "    if not tickers_to_display:\n",
    "        return ({'error': \"No tickers found for the selected rank.\"}, None)\n",
    "\n",
    "#################################\n",
    "    # 6. Calculate Portfolio & Benchmark Performance\n",
    "    # +++ THIS ENTIRE SECTION IS REFACTORED +++\n",
    "    actual_calc_end_ts = calc_close.index.max()\n",
    "\n",
    "    # +++ FIX: RE-INTRODUCE THE `normalized_plot_data` CALCULATION HERE +++\n",
    "    # This is needed for the debug trace and the final plot output.\n",
    "    normalized_plot_data = df_close_full[tickers_to_display].loc[safe_start_date:safe_viz_end_date]\n",
    "    normalized_plot_data = normalized_plot_data.div(normalized_plot_data.bfill().iloc[0])\n",
    "    # --- END OF FIX ---\n",
    "\n",
    "    # Use the new central function for the portfolio\n",
    "    portfolio_series, portfolio_return_series, portfolio_atrp_series = \\\n",
    "        calculate_buy_and_hold_performance(df_close_full, features_df, tickers_to_display,\n",
    "                                           safe_start_date, safe_viz_end_date)\n",
    "\n",
    "    # Use the new central function for the benchmark (a portfolio of one)\n",
    "    if benchmark_ticker in df_close_full.columns:\n",
    "        benchmark_price_series, benchmark_return_series, benchmark_atrp_series = \\\n",
    "            calculate_buy_and_hold_performance(df_close_full, features_df, [benchmark_ticker],\n",
    "                                               safe_start_date, safe_viz_end_date)\n",
    "    else:\n",
    "        benchmark_price_series = pd.Series(dtype='float64')\n",
    "        benchmark_return_series = pd.Series(dtype='float64')\n",
    "        benchmark_atrp_series = pd.Series(dtype='float64')\n",
    "\n",
    "    # Now, split the generated series into calc and fwd periods\n",
    "    # This logic remains, but it's much cleaner as it operates on the final series\n",
    "    try:\n",
    "        boundary_loc = portfolio_return_series.index.get_loc(actual_calc_end_ts)\n",
    "        calc_portfolio_returns = portfolio_return_series.iloc[:boundary_loc + 1]\n",
    "        fwd_portfolio_returns = portfolio_return_series.iloc[boundary_loc + 1:]\n",
    "        calc_portfolio_atrp = portfolio_atrp_series.iloc[:boundary_loc + 1]\n",
    "        fwd_portfolio_atrp = portfolio_atrp_series.iloc[boundary_loc + 1:]\n",
    "        \n",
    "        if not benchmark_return_series.empty:\n",
    "            bm_boundary_loc = benchmark_return_series.index.get_loc(actual_calc_end_ts)\n",
    "            calc_benchmark_returns = benchmark_return_series.iloc[:bm_boundary_loc + 1]\n",
    "            fwd_benchmark_returns = benchmark_return_series.iloc[bm_boundary_loc + 1:]\n",
    "            calc_benchmark_atrp = benchmark_atrp_series.iloc[:bm_boundary_loc + 1]\n",
    "            fwd_benchmark_atrp = benchmark_atrp_series.iloc[bm_boundary_loc + 1:]\n",
    "        else:\n",
    "            calc_benchmark_returns, fwd_benchmark_returns = pd.Series(dtype='float64'), pd.Series(dtype='float64')\n",
    "            calc_benchmark_atrp, fwd_benchmark_atrp = pd.Series(dtype='float64'), pd.Series(dtype='float64')\n",
    "\n",
    "    except (KeyError, IndexError): # Fallback for edge cases\n",
    "        calc_portfolio_returns = portfolio_return_series.loc[:actual_calc_end_ts]\n",
    "        fwd_portfolio_returns = portfolio_return_series.loc[actual_calc_end_ts:].iloc[1:]\n",
    "        calc_portfolio_atrp = portfolio_atrp_series.loc[:actual_calc_end_ts]\n",
    "        fwd_portfolio_atrp = portfolio_atrp_series.loc[actual_calc_end_ts:].iloc[1:]\n",
    "        \n",
    "        if not benchmark_return_series.empty:\n",
    "            calc_benchmark_returns = benchmark_return_series.loc[:actual_calc_end_ts]\n",
    "            fwd_benchmark_returns = benchmark_return_series.loc[actual_calc_end_ts:].iloc[1:]\n",
    "            calc_benchmark_atrp = benchmark_atrp_series.loc[:actual_calc_end_ts]\n",
    "            fwd_benchmark_atrp = benchmark_atrp_series.loc[actual_calc_end_ts:].iloc[1:]\n",
    "        else:\n",
    "            calc_benchmark_returns, fwd_benchmark_returns = pd.Series(dtype='float64'), pd.Series(dtype='float64')\n",
    "            calc_benchmark_atrp, fwd_benchmark_atrp = pd.Series(dtype='float64'), pd.Series(dtype='float64')\n",
    "\n",
    "\n",
    "###################\n",
    "\n",
    "    perf_data = {}\n",
    "    perf_data['calc_p_gain'] = calculate_gain(portfolio_series.loc[:actual_calc_end_ts])\n",
    "    perf_data['fwd_p_gain'] = calculate_gain(portfolio_series.loc[actual_calc_end_ts:])\n",
    "    perf_data['full_p_gain'] = calculate_gain(portfolio_series)\n",
    "    perf_data['calc_p_sharpe'] = calculate_sharpe(calc_portfolio_returns)\n",
    "    perf_data['fwd_p_sharpe'] = calculate_sharpe(fwd_portfolio_returns)\n",
    "    perf_data['full_p_sharpe'] = calculate_sharpe(portfolio_return_series)\n",
    "    perf_data['calc_b_gain'] = calculate_gain(benchmark_price_series.loc[safe_start_date:actual_calc_end_ts]) if benchmark_price_series is not None else np.nan\n",
    "    perf_data['fwd_b_gain'] = calculate_gain(benchmark_price_series.loc[actual_calc_end_ts:safe_viz_end_date]) if benchmark_price_series is not None else np.nan\n",
    "    perf_data['full_b_gain'] = calculate_gain(benchmark_price_series.loc[safe_start_date:safe_viz_end_date]) if benchmark_price_series is not None else np.nan\n",
    "    perf_data['calc_b_sharpe'] = calculate_sharpe(calc_benchmark_returns)\n",
    "    perf_data['fwd_b_sharpe'] = calculate_sharpe(fwd_benchmark_returns)\n",
    "    perf_data['full_b_sharpe'] = calculate_sharpe(benchmark_return_series)\n",
    "    perf_data['calc_p_sharpe_atr'] = calculate_sharpe_atr(calc_portfolio_returns, calc_portfolio_atrp)\n",
    "    perf_data['fwd_p_sharpe_atr'] = calculate_sharpe_atr(fwd_portfolio_returns, fwd_portfolio_atrp)\n",
    "    perf_data['full_p_sharpe_atr'] = calculate_sharpe_atr(portfolio_return_series, portfolio_atrp_series)\n",
    "    perf_data['calc_b_sharpe_atr'] = calculate_sharpe_atr(calc_benchmark_returns, calc_benchmark_atrp)\n",
    "    perf_data['fwd_b_sharpe_atr'] = calculate_sharpe_atr(fwd_benchmark_returns, fwd_benchmark_atrp)\n",
    "    perf_data['full_b_sharpe_atr'] = calculate_sharpe_atr(benchmark_return_series, benchmark_atrp_series)\n",
    "    if debug:\n",
    "        df_ranking_base = pd.DataFrame({'MeanDailyReturn': daily_returns.mean(),'StdDevDailyReturn': daily_returns.std(),'MeanATRP': atrp})\n",
    "        df_metrics = pd.DataFrame(metric_values)\n",
    "        df_metrics.columns = [f'Metric_{col}' for col in df_metrics.columns]\n",
    "        df_ranking = df_ranking_base.join(df_metrics, how='left')\n",
    "        df_ranking.index.name = 'Ticker'\n",
    "        debug_data['ranking_metrics'] = df_ranking.sort_values(f'Metric_{metric}', ascending=False)\n",
    "    calc_end_prices = calc_close.ffill().iloc[-1]\n",
    "    fwd_close_slice = df_close_full.loc[actual_calc_end_ts:safe_viz_end_date]\n",
    "    viz_end_prices = fwd_close_slice.ffill().iloc[-1] if not fwd_close_slice.empty and len(fwd_close_slice) >= 2 else calc_end_prices\n",
    "    calc_gains = (calc_end_prices / calc_close.bfill().iloc[0]) - 1\n",
    "    fwd_gains = (viz_end_prices / calc_end_prices) - 1\n",
    "    results_df = pd.DataFrame({'Rank': range(rank_start, rank_start + len(tickers_to_display)), 'Metric': metric, 'MetricValue': sorted_tickers.loc[tickers_to_display].values, 'CalcPrice': calc_end_prices.loc[tickers_to_display], 'CalcGain': calc_gains.loc[tickers_to_display], 'FwdGain': fwd_gains.loc[tickers_to_display]}, index=pd.Index(tickers_to_display, name='Ticker'))\n",
    "    if benchmark_price_series is not None and benchmark_ticker in calc_close.columns:\n",
    "        benchmark_df_row = pd.DataFrame({'Rank': np.nan, 'Metric': metric, 'MetricValue': metric_values[metric].get(benchmark_ticker, np.nan), 'CalcPrice': calc_end_prices[benchmark_ticker], 'CalcGain': calc_gains[benchmark_ticker], 'FwdGain': fwd_gains[benchmark_ticker]}, index=pd.Index([f\"{benchmark_ticker} (BM)\"], name='Ticker'))\n",
    "        results_df = pd.concat([results_df, benchmark_df_row])\n",
    "    if debug:\n",
    "        df_trace = normalized_plot_data.copy()\n",
    "        df_trace.columns = [f'Norm_Price_{c}' for c in df_trace.columns]\n",
    "        df_trace['Norm_Price_Portfolio'] = portfolio_series\n",
    "        if benchmark_price_series is not None and not benchmark_price_series.loc[safe_start_date:safe_viz_end_date].dropna().empty:\n",
    "            norm_bm = benchmark_price_series.loc[safe_start_date:safe_viz_end_date] / benchmark_price_series.loc[safe_start_date:].bfill().iloc[0]\n",
    "            df_trace[f'Norm_Price_Benchmark_{benchmark_ticker}'] = norm_bm\n",
    "        for col in df_trace.columns:\n",
    "            if 'Norm_Price' in col:\n",
    "                df_trace[col.replace('Norm_Price', 'Return')] = df_trace[col].pct_change()\n",
    "        debug_data['portfolio_trace'] = df_trace\n",
    "    final_results = {\n",
    "        'tickers_to_display': tickers_to_display, 'normalized_plot_data': normalized_plot_data,\n",
    "        'portfolio_series': portfolio_series, 'benchmark_price_series': benchmark_price_series,\n",
    "        'performance_data': perf_data, 'results_df': results_df, 'actual_calc_end_ts': actual_calc_end_ts,\n",
    "        'safe_start_date': safe_start_date, 'safe_viz_end_date': safe_viz_end_date,\n",
    "        'error': None\n",
    "    }\n",
    "    return (final_results, debug_data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- C. DYNAMIC DATA QUALITY FILTER FUNCTIONS ---\n",
    "\n",
    "def get_eligible_universe(features_df, filter_date, thresholds):\n",
    "    \"\"\"Filters the universe of tickers based on quality metrics for a given date.\"\"\"\n",
    "    filter_date_ts = pd.to_datetime(filter_date)\n",
    "    # The index is now the comprehensive features_df\n",
    "    date_index = features_df.index.get_level_values('Date').unique().sort_values()\n",
    "    \n",
    "    if filter_date_ts < date_index[0]:\n",
    "        print(f\"Warning: Filter date {filter_date_ts.date()} is before the earliest data point. Returning empty universe.\")\n",
    "        return []\n",
    "        \n",
    "    # Find the most recent date with quality data on or before the filter date\n",
    "    valid_prior_dates = date_index[date_index <= filter_date_ts]\n",
    "    if valid_prior_dates.empty:\n",
    "        print(f\"Warning: No available data found on or before {filter_date_ts.date()}. Returning empty universe.\")\n",
    "        return []\n",
    "        \n",
    "    actual_date_to_use = valid_prior_dates[-1]\n",
    "    if actual_date_to_use.date() != filter_date_ts.date():\n",
    "        print(f\"ℹ️ Info: Filter date {filter_date_ts.date()} not found. Using previous available date {actual_date_to_use.date()}.\")\n",
    "\n",
    "    metrics_on_date = features_df.xs(actual_date_to_use, level='Date')\n",
    "    \n",
    "    # Apply filters using the new column names from features_df\n",
    "    mask = ((metrics_on_date['RollMedDollarVol'] >= thresholds['min_median_dollar_volume']) & # <-- RENAMED\n",
    "            (metrics_on_date['RollingStalePct'] <= thresholds['max_stale_pct']) &\n",
    "            (metrics_on_date['RollingSameVolCount'] <= thresholds['max_same_vol_count']))\n",
    "            \n",
    "    eligible_tickers = metrics_on_date[mask].index.tolist()\n",
    "    all_tickers = metrics_on_date.index.tolist()\n",
    "    print(f\"Dynamic Filter ({filter_date_ts.date()}): Kept {len(eligible_tickers)} of {len(all_tickers)} tickers.\")\n",
    "    return eligible_tickers\n",
    "\n",
    "# --- D. INTERACTIVE ANALYSIS & BACKTESTING TOOLS ---\n",
    "\n",
    "def plot_walk_forward_analyzer(df_ohlcv,\n",
    "                               default_start_date=None, default_calc_period=126, default_fwd_period=63,\n",
    "                               default_metric='Sharpe (ATR)', default_rank_start=1, default_rank_end=10,\n",
    "                               default_benchmark_ticker='VOO', master_calendar_ticker='VOO',\n",
    "                               quality_thresholds={'min_median_dollar_volume': 1_000_000, 'max_stale_pct': 0.05, 'max_same_vol_count': 10},\n",
    "                               debug=False):\n",
    "    # (No changes to the initial setup part of this function...)\n",
    "    print(\"Initializing Walk-Forward Analyzer (using Trading Day Logic)...\")\n",
    "    if not isinstance(df_ohlcv.index, pd.MultiIndex): raise ValueError(\"Input DataFrame must have a (Ticker, Date) MultiIndex.\")\n",
    "    df_ohlcv = df_ohlcv.sort_index()\n",
    "    if master_calendar_ticker not in df_ohlcv.index.get_level_values(0):\n",
    "        raise ValueError(f\"Master calendar ticker '{master_calendar_ticker}' not found in DataFrame.\")\n",
    "    master_trading_days = df_ohlcv.loc[master_calendar_ticker].index.unique().sort_values()\n",
    "    print(f\"Master trading day calendar created from '{master_calendar_ticker}' ({len(master_trading_days)} days).\")\n",
    "    print(\"--- Generating all features upfront... ---\")\n",
    "    features_df = generate_features(df_ohlcv)\n",
    "    print(\"Pre-processing data (unstacking)...\")\n",
    "    df_close_full = df_ohlcv['Adj Close'].unstack(level=0)\n",
    "    df_high_full = df_ohlcv['Adj High'].unstack(level=0)\n",
    "    df_low_full = df_ohlcv['Adj Low'].unstack(level=0)\n",
    "    start_date_picker = widgets.DatePicker(description='Start Date:', value=pd.to_datetime(default_start_date), disabled=False)\n",
    "    calc_period_input = widgets.IntText(value=default_calc_period, description='Calc Period (days):')\n",
    "    fwd_period_input = widgets.IntText(value=default_fwd_period, description='Fwd Period (days):')\n",
    "    if default_metric not in METRIC_REGISTRY:\n",
    "        fallback_metric = list(METRIC_REGISTRY.keys())[0]\n",
    "        print(f\"⚠️ Warning: Default metric '{default_metric}' not in registry. Using '{fallback_metric}'.\")\n",
    "        default_metric = fallback_metric\n",
    "    metric_dropdown = widgets.Dropdown(options=list(METRIC_REGISTRY.keys()), value=default_metric, description='Metric:')\n",
    "    rank_start_input = widgets.IntText(value=default_rank_start, description='Rank Start:')\n",
    "    rank_end_input = widgets.IntText(value=default_rank_end, description='Rank End:')\n",
    "    benchmark_ticker_input = widgets.Text(value=default_benchmark_ticker, description='Benchmark:', placeholder='Enter Ticker')\n",
    "    update_button = widgets.Button(description=\"Update Chart\", button_style='primary')\n",
    "    ticker_list_output = widgets.Output()\n",
    "    results_container, debug_data_container = [None], [None]\n",
    "    fig = go.FigureWidget()\n",
    "    max_traces = 50\n",
    "    for i in range(max_traces): fig.add_trace(go.Scatter(x=[None], y=[None], mode='lines', name=f'placeholder_{i}', visible=False, showlegend=False))\n",
    "    fig.add_trace(go.Scatter(x=[None], y=[None], mode='lines', name='Benchmark', visible=True, showlegend=True, line=dict(color='black', width=3, dash='dash')))\n",
    "    fig.add_trace(go.Scatter(x=[None], y=[None], mode='lines', name='Group Portfolio', visible=True, showlegend=True, line=dict(color='green', width=3)))\n",
    "\n",
    "    def update_plot(button_click):\n",
    "        ticker_list_output.clear_output()\n",
    "        start_date_raw = pd.to_datetime(start_date_picker.value)\n",
    "        start_date_idx = master_trading_days.searchsorted(start_date_raw)\n",
    "        if start_date_idx >= len(master_trading_days):\n",
    "            with ticker_list_output: print(f\"Error: Start date is after the last available trading day.\"); return\n",
    "        actual_start_date = master_trading_days[start_date_idx]\n",
    "        with ticker_list_output:\n",
    "            if start_date_raw.date() != actual_start_date.date():\n",
    "                print(f\"ℹ️ Info: Start date {start_date_raw.date()} is not a trading day. Snapping forward to {actual_start_date.date()}.\")\n",
    "        calc_period = calc_period_input.value; fwd_period = fwd_period_input.value; metric = metric_dropdown.value\n",
    "        rank_start = rank_start_input.value; rank_end = rank_end_input.value; benchmark_ticker = benchmark_ticker_input.value.strip().upper()\n",
    "        if rank_start > rank_end:\n",
    "            with ticker_list_output: print(\"Error: 'Rank Start' must be <= 'Rank End'.\"); return\n",
    "        if rank_start < 1 or calc_period < 2 or fwd_period < 1:\n",
    "            with ticker_list_output: print(\"Error: Ranks must be >= 1, Calc Period >= 2, Fwd Period >= 1.\"); return\n",
    "        required_days = calc_period + fwd_period\n",
    "        if start_date_idx + required_days > len(master_trading_days):\n",
    "            available_days = len(master_trading_days) - start_date_idx; last_available_date = master_trading_days[-1].date()\n",
    "            with ticker_list_output:\n",
    "                print(f\"Error: Not enough data for the requested period.\\n  Start Date: {actual_start_date.date()}\\n  Required Days: {calc_period} (calc) + {fwd_period} (fwd) = {required_days}\\n  Available Days from Start: {available_days} (until {last_available_date})\\n  Please shorten the 'Calc Period' / 'Fwd Period' or choose an earlier 'Start Date'.\")\n",
    "            return\n",
    "        eligible_tickers = get_eligible_universe(features_df, actual_start_date, quality_thresholds)\n",
    "        if not eligible_tickers:\n",
    "            with ticker_list_output: print(f\"Error: No eligible tickers found on {actual_start_date.date()} with the current quality filters.\"); return\n",
    "        df_close_step = df_close_full[eligible_tickers]; df_high_step = df_high_full[eligible_tickers]; df_low_step = df_low_full[eligible_tickers]\n",
    "        results, debug_output = run_walk_forward_step(df_close_full, df_high_full, df_low_full, master_trading_days, actual_start_date, calc_period, fwd_period, metric, rank_start, rank_end, benchmark_ticker, features_df=features_df, debug=debug)\n",
    "        if results.get('error'):\n",
    "            with ticker_list_output: print(f\"Error: {results['error']}\"); return\n",
    "        period_dates = {'calc_period_start': results['safe_start_date'], 'calc_period_end': results['actual_calc_end_ts'], 'forward_period_start': results['actual_calc_end_ts'], 'forward_period_end': results['safe_viz_end_date']}\n",
    "        run_parameters = {'calc_period': calc_period, 'fwd_period': fwd_period, 'rank_metric': metric, 'rank_start': rank_start, 'rank_end': rank_end, 'benchmark_ticker': benchmark_ticker}\n",
    "        results.update(period_dates); results.update(run_parameters)\n",
    "        if debug_output is not None and isinstance(debug_output, dict):\n",
    "            debug_output.update(period_dates); debug_output.update(run_parameters)\n",
    "        with fig.batch_update():\n",
    "            for i in range(max_traces):\n",
    "                trace = fig.data[i]\n",
    "                if i < len(results['tickers_to_display']):\n",
    "                    ticker = results['tickers_to_display'][i]; plot_data_series = results['normalized_plot_data'][ticker]\n",
    "                    trace.x, trace.y, trace.name, trace.visible, trace.showlegend = plot_data_series.index, plot_data_series.values, ticker, True, True\n",
    "                else: trace.visible, trace.showlegend = False, False\n",
    "            benchmark_trace = fig.data[max_traces]\n",
    "            if results['benchmark_price_series'] is not None and not results['benchmark_price_series'].loc[results['safe_start_date']:results['safe_viz_end_date']].dropna().empty:\n",
    "                normalized_benchmark = results['benchmark_price_series'].loc[results['safe_start_date']:results['safe_viz_end_date']] / results['benchmark_price_series'].loc[results['safe_start_date']:].bfill().iloc[0]\n",
    "                benchmark_trace.x, benchmark_trace.y, benchmark_trace.name, benchmark_trace.visible = normalized_benchmark.index, normalized_benchmark.values, f\"Benchmark ({benchmark_ticker})\", True\n",
    "            else: benchmark_trace.visible = False\n",
    "            portfolio_trace = fig.data[max_traces + 1]\n",
    "            portfolio_trace.x, portfolio_trace.y, portfolio_trace.name, portfolio_trace.visible = results['portfolio_series'].index, results['portfolio_series'], 'Group Portfolio', True\n",
    "            fig.layout.shapes = []; fig.add_shape(type=\"line\", x0=results['actual_calc_end_ts'], y0=0, x1=results['actual_calc_end_ts'], y1=1, xref='x', yref='paper', line=dict(color=\"grey\", width=2, dash=\"dash\"))\n",
    "        results_container[0] = results; debug_data_container[0] = debug_output\n",
    "        with ticker_list_output:\n",
    "            print(f\"Analysis Period: {results['safe_start_date'].date()} to {results['safe_viz_end_date'].date()}.\")\n",
    "            pprint.pprint(results['tickers_to_display'])\n",
    "            p = results['performance_data']\n",
    "            \n",
    "            # --- START OF MODIFIED BLOCK ---\n",
    "            rows = []\n",
    "            # Gain Metrics\n",
    "            rows.append({'Metric': 'Group Portfolio Gain', 'Full': p['full_p_gain'], 'Calc': p['calc_p_gain'], 'Fwd': p['fwd_p_gain']})\n",
    "            if not np.isnan(p.get('full_b_gain')):\n",
    "                rows.append({'Metric': f'Benchmark ({benchmark_ticker}) Gain', 'Full': p['full_b_gain'], 'Calc': p['calc_b_gain'], 'Fwd': p['fwd_b_gain']})\n",
    "                rows.append({'Metric': '== Gain Delta (vs Bm)', 'Full': p['full_p_gain'] - p['full_b_gain'], 'Calc': p['calc_p_gain'] - p['calc_b_gain'], 'Fwd': p['fwd_p_gain'] - p['fwd_b_gain']})\n",
    "            \n",
    "            # Standard Sharpe Metrics\n",
    "            rows.append({'Metric': 'Group Portfolio Sharpe', 'Full': p['full_p_sharpe'], 'Calc': p['calc_p_sharpe'], 'Fwd': p['fwd_p_sharpe']})\n",
    "            if not np.isnan(p.get('full_b_sharpe')):\n",
    "                rows.append({'Metric': f'Benchmark ({benchmark_ticker}) Sharpe', 'Full': p['full_b_sharpe'], 'Calc': p['calc_b_sharpe'], 'Fwd': p['fwd_b_sharpe']})\n",
    "                rows.append({'Metric': '== Sharpe Delta (vs Bm)', 'Full': p['full_p_sharpe'] - p['full_b_sharpe'], 'Calc': p['calc_p_sharpe'] - p['calc_b_sharpe'], 'Fwd': p['fwd_p_sharpe'] - p['fwd_b_sharpe']})\n",
    "\n",
    "            # Sharpe (ATR) Metrics\n",
    "            rows.append({'Metric': 'Group Portfolio Sharpe (ATR)', 'Full': p['full_p_sharpe_atr'], 'Calc': p['calc_p_sharpe_atr'], 'Fwd': p['fwd_p_sharpe_atr']})\n",
    "            if not np.isnan(p.get('full_b_sharpe_atr')):\n",
    "                rows.append({'Metric': f'Benchmark ({benchmark_ticker}) Sharpe (ATR)', 'Full': p['full_b_sharpe_atr'], 'Calc': p['calc_b_sharpe_atr'], 'Fwd': p['fwd_b_sharpe_atr']})\n",
    "                rows.append({'Metric': '== Sharpe (ATR) Delta (vs Bm)', 'Full': p['full_p_sharpe_atr'] - p['full_b_sharpe_atr'], 'Calc': p['calc_p_sharpe_atr'] - p['calc_b_sharpe_atr'], 'Fwd': p['fwd_p_sharpe_atr'] - p['fwd_b_sharpe_atr']})\n",
    "\n",
    "            report_df = pd.DataFrame(rows).set_index('Metric')\n",
    "            gain_rows = [row for row in report_df.index if 'Gain' in row]\n",
    "            sharpe_rows = [row for row in report_df.index if 'Sharpe' in row] # This now correctly includes both types of Sharpe\n",
    "            # --- END OF MODIFIED BLOCK ---\n",
    "            \n",
    "            styled_df = report_df.style.format('{:+.2%}', na_rep='N/A', subset=(gain_rows, report_df.columns)).format('{:+.4f}', na_rep='N/A', subset=(sharpe_rows, report_df.columns)).set_properties(**{'text-align': 'right', 'width': '100px'}).set_table_styles([{'selector': 'th.col_heading', 'props': [('text-align', 'right')]}, {'selector': 'th.row_heading', 'props': [('text-align', 'left')]}])\n",
    "            print(\"\\n--- Strategy Performance Summary ---\")\n",
    "            display(styled_df)\n",
    "    fig.update_layout(title_text='Walk-Forward Performance Analysis', xaxis_title='Date', yaxis_title='Normalized Price (Start = 1)', hovermode='x unified', legend_title_text='Tickers (Ranked)', height=600, margin=dict(t=50))\n",
    "    fig.add_hline(y=1, line_width=1, line_dash=\"dash\", line_color=\"grey\")\n",
    "    update_button.on_click(update_plot)\n",
    "    controls_row1 = widgets.HBox([start_date_picker, calc_period_input, fwd_period_input])\n",
    "    controls_row2 = widgets.HBox([metric_dropdown, rank_start_input, rank_end_input, benchmark_ticker_input, update_button])\n",
    "    ui_container = widgets.VBox([controls_row1, controls_row2, ticker_list_output], layout=widgets.Layout(margin='10px 0 20px 0'))\n",
    "    display(ui_container, fig)\n",
    "    update_plot(None)\n",
    "    return (results_container, debug_data_container)\n",
    "\n",
    "def run_full_backtest(df_ohlcv, strategy_params, quality_thresholds):\n",
    "    \"\"\"Runs a full backtest of a strategy over a specified date range.\"\"\"\n",
    "    print(f\"--- Running Full Forensic Backtest for Strategy: {strategy_params['metric']} (Top {strategy_params['rank_start']}-{strategy_params['rank_end']}) ---\")\n",
    "    \n",
    "    # (No changes to the initial setup part...)\n",
    "    start_date, end_date = pd.to_datetime(strategy_params['start_date']), pd.to_datetime(strategy_params['end_date'])\n",
    "    calc_period, fwd_period = strategy_params['calc_period'], strategy_params['fwd_period']\n",
    "    metric, rank_start, rank_end = strategy_params['metric'], strategy_params['rank_start'], strategy_params['rank_end']\n",
    "    benchmark_ticker = strategy_params['benchmark_ticker']\n",
    "    master_calendar_ticker = strategy_params.get('master_calendar_ticker', 'VOO')\n",
    "    if master_calendar_ticker not in df_ohlcv.index.get_level_values(0):\n",
    "        raise ValueError(f\"Master calendar ticker '{master_calendar_ticker}' not found in DataFrame.\")\n",
    "    master_trading_days = df_ohlcv.loc[master_calendar_ticker].index.unique().sort_values()\n",
    "    start_idx = master_trading_days.searchsorted(start_date)\n",
    "    end_idx = master_trading_days.searchsorted(end_date, side='right')\n",
    "    print(\"--- Generating all features upfront... ---\")\n",
    "    features_df = generate_features(df_ohlcv)\n",
    "    df_close_full = df_ohlcv['Adj Close'].unstack(level=0); df_high_full = df_ohlcv['Adj High'].unstack(level=0); df_low_full = df_ohlcv['Adj Low'].unstack(level=0)\n",
    "    \n",
    "    # Loop through all periods in the backtest range\n",
    "    step_indices = range(start_idx, end_idx, fwd_period)\n",
    "    all_fwd_gains, period_by_period_debug = [], {}\n",
    "\n",
    "    print(f\"Simulating {len(step_indices)} periods from {master_trading_days[step_indices[0]].date()} to {master_trading_days[step_indices[-1]].date()}...\")\n",
    "    for current_idx in tqdm(step_indices, desc=\"Backtest Progress\"):\n",
    "        step_date = master_trading_days[current_idx]\n",
    "        eligible_tickers = get_eligible_universe(features_df, step_date, quality_thresholds)\n",
    "        if not eligible_tickers: continue\n",
    "        df_close_step = df_close_full[eligible_tickers]; df_high_step = df_high_full[eligible_tickers]; df_low_step = df_low_full[eligible_tickers]\n",
    "        \n",
    "        # +++ UPDATE THE CALL HERE +++\n",
    "        results, debug_output = run_walk_forward_step(\n",
    "            df_close_step, df_high_step, df_low_step, master_trading_days,\n",
    "            step_date, calc_period, fwd_period,\n",
    "            metric, rank_start, rank_end, benchmark_ticker,\n",
    "            features_df=features_df,  # Pass the features_df\n",
    "            debug=True\n",
    "        )\n",
    "        \n",
    "        # (The rest of the function remains unchanged...)\n",
    "        if results['error'] is None:\n",
    "            fwd_series = results['portfolio_series'].loc[results['actual_calc_end_ts']:]\n",
    "            all_fwd_gains.append(fwd_series.pct_change().dropna())\n",
    "            period_by_period_debug[step_date.date().isoformat()] = debug_output\n",
    "            \n",
    "    if not all_fwd_gains:\n",
    "        print(\"Error: No valid periods were simulated.\"); return None\n",
    "\n",
    "    strategy_returns = pd.concat(all_fwd_gains); strategy_equity_curve = (1 + strategy_returns).cumprod()\n",
    "    benchmark_returns = df_close_full[benchmark_ticker].pct_change().loc[strategy_equity_curve.index]; benchmark_equity_curve = (1 + benchmark_returns).cumprod()\n",
    "    cumulative_equity_df = pd.DataFrame({'Strategy_Equity': strategy_equity_curve, 'Benchmark_Equity': benchmark_equity_curve})\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=cumulative_equity_df.index, y=cumulative_equity_df['Strategy_Equity'], name='Strategy', line=dict(color='green')))\n",
    "    fig.add_trace(go.Scatter(x=cumulative_equity_df.index, y=cumulative_equity_df['Benchmark_Equity'], name=f'Benchmark ({benchmark_ticker})', line=dict(color='black', dash='dash')))\n",
    "    fig.update_layout(title=f\"Cumulative Performance: '{metric}' Strategy (Top {rank_start}-{rank_end})\", xaxis_title=\"Date\", yaxis_title=\"Cumulative Growth\")\n",
    "    fig.show()\n",
    "    final_backtest_results = {'cumulative_performance': cumulative_equity_df, 'period_by_period_debug': period_by_period_debug}\n",
    "    print(\"\\n✅ Full backtest complete. Results object is ready for forensic analysis.\")\n",
    "    return final_backtest_results\n",
    "\n",
    "# --- E. VERIFICATION TOOLS (User Requested) ---\n",
    "\n",
    "def verify_group_tickers_walk_forward_calculation(df_ohlcv, tickers_to_verify, benchmark_ticker,\n",
    "                                                  start_date, calc_period, fwd_period,\n",
    "                                                  master_calendar_ticker='VOO', export_csv=False):\n",
    "    \"\"\"Verifies portfolio and benchmark performance and optionally exports the data.\"\"\"\n",
    "    display(Markdown(f\"## Verification Report for Portfolio vs. Benchmark\"))\n",
    "    display(Markdown(f\"**Portfolio Tickers:** `{tickers_to_verify}`\\n**Benchmark Ticker:** `{benchmark_ticker}`\"))\n",
    "    \n",
    "    # 1. Setup trading day calendar and determine exact period dates\n",
    "    if master_calendar_ticker not in df_ohlcv.index.get_level_values(0):\n",
    "        raise ValueError(f\"Master calendar ticker '{master_calendar_ticker}' not found in DataFrame.\")\n",
    "    master_trading_days = df_ohlcv.loc[master_calendar_ticker].index.unique().sort_values()\n",
    "\n",
    "    start_date_raw = pd.to_datetime(start_date)\n",
    "    start_idx = master_trading_days.searchsorted(start_date_raw)\n",
    "    if start_idx >= len(master_trading_days):\n",
    "        print(f\"Error: Start date {start_date_raw.date()} is after the last available trading day.\"); return\n",
    "    actual_start_date = master_trading_days[start_idx]\n",
    "    \n",
    "    calc_end_idx = min(start_idx + calc_period, len(master_trading_days) - 1)\n",
    "    fwd_end_idx = min(calc_end_idx + fwd_period, len(master_trading_days) - 1)\n",
    "    \n",
    "    actual_calc_end_date = master_trading_days[calc_end_idx]\n",
    "    actual_fwd_end_date = master_trading_days[fwd_end_idx]\n",
    "    \n",
    "    display(Markdown(f\"**Analysis Start:** `{actual_start_date.date()}` (Selected: `{start_date_raw.date()}`)\\n\"\n",
    "                    f\"**Calc End:** `{actual_calc_end_date.date()}` ({calc_period} trading days)\\n\"\n",
    "                    f\"**Fwd End:** `{actual_fwd_end_date.date()}` ({fwd_period} trading days)\"))\n",
    "\n",
    "    # 2. Recreate the portfolio and benchmark series from scratch\n",
    "    df_close_full = df_ohlcv['Adj Close'].unstack(level=0)\n",
    "    portfolio_prices_raw_slice = df_close_full[tickers_to_verify].loc[actual_start_date:actual_fwd_end_date]\n",
    "    portfolio_value_series = portfolio_prices_raw_slice.div(portfolio_prices_raw_slice.bfill().iloc[0]).mean(axis=1)\n",
    "    benchmark_price_series = df_close_full.get(benchmark_ticker)\n",
    "    \n",
    "    # 3. Optionally export the underlying daily data to a CSV for external checking\n",
    "    if export_csv:\n",
    "        export_df = pd.DataFrame({\n",
    "            'Portfolio_Normalized_Price': portfolio_value_series,\n",
    "            'Portfolio_Daily_Return': portfolio_value_series.pct_change()\n",
    "        })\n",
    "        if benchmark_price_series is not None:\n",
    "            norm_bm = benchmark_price_series.loc[actual_start_date:actual_fwd_end_date]\n",
    "            norm_bm = norm_bm / norm_bm.bfill().iloc[0]\n",
    "            export_df['Benchmark_Normalized_Price'] = norm_bm\n",
    "            export_df['Benchmark_Daily_Return'] = norm_bm.pct_change()\n",
    "\n",
    "        output_dir = 'export_csv'\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        tickers_str = '_'.join(tickers_to_verify)\n",
    "        filename = f\"verify_group_{actual_start_date.date()}_{tickers_str}.csv\"\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "        export_df.to_csv(filepath)\n",
    "        print(f\"\\n✅ Data exported to: {filepath}\")\n",
    "\n",
    "    # 4. Define a helper to print detailed calculation steps\n",
    "    def print_verification_steps(title, price_series):\n",
    "        display(Markdown(f\"#### Verification for: `{title}`\"))\n",
    "        if price_series.dropna().shape[0] < 2: print(\"  - Not enough data points.\"); return {'gain': np.nan, 'sharpe': np.nan}\n",
    "        start_price, end_price = price_series.bfill().iloc[0], price_series.ffill().iloc[-1]\n",
    "        gain = (end_price / start_price) - 1\n",
    "        print(f\"Start Value ({price_series.first_valid_index().date()}): {start_price:,.4f}\\nEnd Value   ({price_series.last_valid_index().date()}): {end_price:,.4f}\\nGain = {gain:.2%}\")\n",
    "        returns = price_series.pct_change()\n",
    "        sharpe = calculate_sharpe(returns)\n",
    "        print(f\"Mean Daily Return: {returns.mean():.6f}\\nStd Dev: {returns.std():.6f}\\nSharpe = {sharpe:.2f}\")\n",
    "        return {'gain': gain, 'sharpe': sharpe}\n",
    "\n",
    "    # 5. Run verification for each period\n",
    "    display(Markdown(\"### A. Calculation Period\"))\n",
    "    perf_calc_p = print_verification_steps(\"Group Portfolio\", portfolio_value_series.loc[actual_start_date:actual_calc_end_date])\n",
    "    if benchmark_price_series is not None:\n",
    "        perf_calc_b = print_verification_steps(f\"Benchmark\", benchmark_price_series.loc[actual_start_date:actual_calc_end_date])\n",
    "    \n",
    "    display(Markdown(\"### B. Forward Period\"))\n",
    "    perf_fwd_p = print_verification_steps(\"Group Portfolio\", portfolio_value_series.loc[actual_calc_end_date:actual_fwd_end_date])\n",
    "    if benchmark_price_series is not None:\n",
    "        perf_fwd_b = print_verification_steps(f\"Benchmark\", benchmark_price_series.loc[actual_calc_end_date:actual_fwd_end_date])\n",
    "\n",
    "def verify_ticker_ranking_metrics(df_ohlcv, ticker, start_date, calc_period,\n",
    "                                  master_calendar_ticker='VOO', export_csv=False):\n",
    "    \"\"\"Verifies ranking metrics for a single ticker and optionally exports the data.\"\"\"\n",
    "    display(Markdown(f\"## Verification Report for Ticker Ranking: `{ticker}`\"))\n",
    "    \n",
    "    # 1. Setup trading day calendar and determine exact period dates\n",
    "    if master_calendar_ticker not in df_ohlcv.index.get_level_values(0):\n",
    "        raise ValueError(f\"Master calendar ticker '{master_calendar_ticker}' not found in DataFrame.\")\n",
    "    master_trading_days = df_ohlcv.loc[master_calendar_ticker].index.unique().sort_values()\n",
    "\n",
    "    start_date_raw = pd.to_datetime(start_date)\n",
    "    start_idx = master_trading_days.searchsorted(start_date_raw)\n",
    "    if start_idx >= len(master_trading_days):\n",
    "        print(f\"Error: Start date {start_date_raw.date()} is after the last available trading day.\"); return\n",
    "    actual_start_date = master_trading_days[start_idx]\n",
    "    \n",
    "    calc_end_idx = min(start_idx + calc_period, len(master_trading_days) - 1)\n",
    "    actual_calc_end_date = master_trading_days[calc_end_idx]\n",
    "\n",
    "    # 2. Extract and prepare the raw data for the specific ticker and period\n",
    "    df_ticker = df_ohlcv.loc[ticker].sort_index()\n",
    "    calc_df = df_ticker.loc[actual_start_date:actual_calc_end_date].copy()\n",
    "    if calc_df.empty or len(calc_df) < 2: \n",
    "        print(\"No data or not enough data in calc period.\"); return\n",
    "\n",
    "    display(Markdown(\"### A. Calculation Period (for Ranking Metrics)\"))\n",
    "    display(Markdown(f\"**Period Start:** `{actual_start_date.date()}`\\n\"\n",
    "                    f\"**Period End:** `{actual_calc_end_date.date()}`\\n\"\n",
    "                    f\"**Total Trading Days:** `{len(calc_df)}` (Requested: `{calc_period}`)\"))\n",
    "    \n",
    "    display(Markdown(\"#### Detailed Metric Calculation Data\"))\n",
    "    \n",
    "    # 3. Calculate all intermediate metrics as new columns for full transparency\n",
    "    vdf = calc_df[['Adj High', 'Adj Low', 'Adj Close']].copy()\n",
    "    vdf['Daily_Return'] = vdf['Adj Close'].pct_change()\n",
    "    \n",
    "    # Corrected True Range (TR) calculation for a single ticker (Series)\n",
    "    tr_df = pd.DataFrame({\n",
    "        'h_l': vdf['Adj High'] - vdf['Adj Low'],\n",
    "        'h_cp': abs(vdf['Adj High'] - vdf['Adj Close'].shift(1)),\n",
    "        'l_cp': abs(vdf['Adj Low'] - vdf['Adj Close'].shift(1))\n",
    "    })\n",
    "    vdf['TR'] = tr_df.max(axis=1)\n",
    "    \n",
    "    vdf['ATR_14'] = vdf['TR'].ewm(alpha=1/14, adjust=False).mean()\n",
    "    vdf['ATRP'] = vdf['ATR_14'] / vdf['Adj Close']\n",
    "    \n",
    "    print(\"--- Start of Calculation Period ---\")\n",
    "    display(vdf.head())\n",
    "    print(\"\\n--- End of Calculation Period ---\")\n",
    "    display(vdf.tail())\n",
    "\n",
    "    # 4. Optionally export this detailed breakdown to CSV\n",
    "    if export_csv:\n",
    "        output_dir = 'export_csv'\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        filename = f\"verify_ticker_{actual_start_date.date()}_{ticker}.csv\"\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "        vdf.to_csv(filepath)\n",
    "        print(f\"\\n✅ Data exported to: {filepath}\")\n",
    "    \n",
    "    # 5. Print final metric calculations with formulas\n",
    "    display(Markdown(\"#### `MetricValue` Verification Summary:\"))\n",
    "    \n",
    "    calc_start_price = vdf['Adj Close'].bfill().iloc[0]\n",
    "    calc_end_price = vdf['Adj Close'].ffill().iloc[-1]\n",
    "    price_metric = (calc_end_price / calc_start_price)\n",
    "    print(f\"1. Price Metric: (Last Price / First Price) = ({calc_end_price:.2f} / {calc_start_price:.2f}) = {price_metric:.4f}\")\n",
    "    \n",
    "    daily_returns = vdf['Daily_Return'].dropna()\n",
    "    sharpe_ratio = calculate_sharpe(daily_returns)\n",
    "    print(f\"2. Sharpe Metric: (Mean Daily Return / Std Dev) * sqrt(252) = {sharpe_ratio:.4f}\")\n",
    "\n",
    "    atrp_mean = vdf['ATRP'].mean()\n",
    "    mean_daily_return = vdf['Daily_Return'].mean()\n",
    "    sharpe_atr = (mean_daily_return / atrp_mean) if atrp_mean > 0 else 0\n",
    "    print(f\"3. Sharpe (ATR) Metric: (Mean Daily Return / Mean ATRP) = ({mean_daily_return:.6f} / {atrp_mean:.6f}) = {sharpe_atr:.4f}\")\n",
    "\n",
    "\n",
    "# def verify_sharpe_atr_calculation_checked(df_ohlcv, features_df, tickers_to_verify, benchmark_ticker,\n",
    "#                                     start_date, calc_period, fwd_period,\n",
    "#                                     master_calendar_ticker='VOO', debug=False):\n",
    "#     \"\"\"\n",
    "#     Verifies the Sharpe (ATR) calculations for a portfolio and benchmark.\n",
    "\n",
    "#     This function transparently recalculates the key components for Sharpe (ATR)\n",
    "#     and can optionally export the underlying source data for manual inspection.\n",
    "#     \"\"\"\n",
    "#     display(Markdown(f\"## Verification Report for Sharpe (ATR) Calculation\"))\n",
    "#     display(Markdown(f\"**Portfolio Tickers:** `{tickers_to_verify}`\\n**Benchmark Ticker:** `{benchmark_ticker}`\"))\n",
    "\n",
    "#     # --- 1. Determine Exact Period Dates (No changes here) ---\n",
    "#     master_trading_days = df_ohlcv.loc[master_calendar_ticker].index.unique().sort_values()\n",
    "#     start_date_raw = pd.to_datetime(start_date)\n",
    "#     start_idx = master_trading_days.searchsorted(start_date_raw)\n",
    "#     actual_start_date = master_trading_days[start_idx]\n",
    "#     calc_end_idx = min(start_idx + calc_period, len(master_trading_days) - 1)\n",
    "#     fwd_end_idx = min(calc_end_idx + fwd_period, len(master_trading_days) - 1)\n",
    "#     actual_calc_end_date = master_trading_days[calc_end_idx]\n",
    "#     actual_fwd_end_date = master_trading_days[fwd_end_idx]\n",
    "    \n",
    "#     display(Markdown(f\"**Full Period:** `{actual_start_date.date()}` to `{actual_fwd_end_date.date()}`\\n\"\n",
    "#                     f\"**Calc End Date:** `{actual_calc_end_date.date()}`\"))\n",
    "\n",
    "#     # Original debug export block (can be kept or removed)\n",
    "#     if debug:\n",
    "#         # ... (original export code remains here) ...\n",
    "#         pass # Assuming original block is here\n",
    "\n",
    "#     # --- 2. Recreate Portfolio & Benchmark Series from Scratch ---\n",
    "#     df_close_full = df_ohlcv['Adj Close'].unstack(level=0)\n",
    "#     portfolio_prices_raw = df_close_full[tickers_to_verify].loc[actual_start_date:actual_fwd_end_date]\n",
    "#     portfolio_prices_norm = portfolio_prices_raw.div(portfolio_prices_raw.bfill().iloc[0])\n",
    "#     portfolio_value_series = portfolio_prices_norm.mean(axis=1)\n",
    "#     portfolio_return_series = portfolio_value_series.pct_change()\n",
    "#     p_idx = pd.MultiIndex.from_product([tickers_to_verify, portfolio_return_series.index])\n",
    "#     p_atrp_df = features_df.loc[features_df.index.intersection(p_idx)]['ATRP'].unstack(level=0)\n",
    "#     portfolio_atrp_series = p_atrp_df.mean(axis=1)\n",
    "\n",
    "# ###############################    \n",
    "#     # benchmark_return_series = df_close_full[benchmark_ticker].pct_change().loc[actual_start_date:actual_fwd_end_date]\n",
    "\n",
    "#     # 1. First, slice the raw prices for the desired date range.\n",
    "#     benchmark_prices_raw = df_close_full[benchmark_ticker].loc[actual_start_date:actual_fwd_end_date]\n",
    "#     # 2. Then, calculate the percentage change on the sliced data.\n",
    "#     benchmark_return_series = benchmark_prices_raw.pct_change()\n",
    "\n",
    "# ###############################    \n",
    "#     benchmark_atrp_series = features_df.xs(benchmark_ticker, level='Ticker')['ATRP'].loc[actual_start_date:actual_fwd_end_date]\n",
    "\n",
    "\n",
    "#     # +++ NEW: DETAILED RETURN CALCULATION TRACE (PORTFOLIO) +++\n",
    "#     if debug:\n",
    "#         display(Markdown(\"---\"))\n",
    "#         display(Markdown(\"### 🐛 Detailed Portfolio Return Calculation Trace\"))\n",
    "\n",
    "#         # Step 1: Show the raw prices being used\n",
    "#         print(\"\\n[STEP 1] Raw Adjusted Close prices used for calculation.\")\n",
    "#         print(\"Compare these values with your 'Adj Close' columns.\")\n",
    "#         display(portfolio_prices_raw)\n",
    "\n",
    "#         # Step 2: Show the normalization base and the result\n",
    "#         normalization_base = portfolio_prices_raw.bfill().iloc[0]\n",
    "#         print(\"\\n[STEP 2] Normalization base (first row of prices).\")\n",
    "#         print(\"Each column in Step 1 is divided by this corresponding value.\")\n",
    "#         display(pd.DataFrame(normalization_base).T)\n",
    "\n",
    "#         print(\"\\n[STEP 2a] Normalized prices (Result of division).\")\n",
    "#         print(\"Compare these values with your 'N Close' columns.\")\n",
    "#         display(portfolio_prices_norm)\n",
    "\n",
    "#         # Step 3: Show the averaged portfolio value series\n",
    "#         print(\"\\n[STEP 3] Averaged normalized portfolio value series.\")\n",
    "#         print(\"This is the row-by-row average of the table in Step 2a.\")\n",
    "#         print(\"Compare this series with your 'N_portf' column.\")\n",
    "#         display(pd.DataFrame(portfolio_value_series, columns=['N_portf']))\n",
    "\n",
    "#         # Step 4: Show the final portfolio return series\n",
    "#         print(\"\\n[STEP 4] Final portfolio daily return series (pct_change).\")\n",
    "#         print(\"This is the percentage change of the series in Step 3.\")\n",
    "#         print(\"Compare this series with your 'N_portf_rtn' column.\")\n",
    "#         display(pd.DataFrame(portfolio_return_series, columns=['N_portf_rtn']))\n",
    "#         display(Markdown(\"---\"))\n",
    "#     # +++ END OF NEW DEBUG BLOCK +++\n",
    "\n",
    "\n",
    "#     # --- 3. Define a Helper to Print Detailed Calculation Steps ---\n",
    "#     # MODIFIED to include more debug details inside\n",
    "#     def _calculate_and_print_metrics(period_name, returns, atrps):\n",
    "#         display(Markdown(f\"#### {period_name}\"))\n",
    "#         if returns.dropna().empty or atrps.dropna().empty:\n",
    "#             print(\"  - Not enough data to calculate.\")\n",
    "#             return np.nan\n",
    "\n",
    "#         # Standard calculations\n",
    "#         mean_return = returns.mean()\n",
    "#         mean_atrp = atrps.mean()\n",
    "#         sharpe_atr = mean_return / mean_atrp if mean_atrp > 0 else np.nan\n",
    "\n",
    "#         # +++ ADDED: Detailed Mean Calculation Breakdown +++\n",
    "#         if debug:\n",
    "#             valid_returns = returns.dropna()\n",
    "#             num_returns = valid_returns.count()\n",
    "#             sum_returns = valid_returns.sum()\n",
    "#             manual_mean = sum_returns / num_returns if num_returns > 0 else 0\n",
    "#             print(f\"  - (Debug) Number of valid daily returns: {num_returns}\")\n",
    "#             print(f\"  - (Debug) Sum of all daily returns:      {sum_returns:,.8f}\")\n",
    "#             print(f\"  - (Debug) Manually calculated mean:      {manual_mean:,.8f} (Sum / Count)\")\n",
    "#         # +++ END OF ADDED DETAIL +++\n",
    "\n",
    "#         print(f\"  - Mean Daily Return: {mean_return:,.6f}\")\n",
    "#         print(f\"  - Mean Daily ATRP:  {mean_atrp:,.6f}\")\n",
    "#         print(f\"  - Sharpe (ATR) = (Mean Return / Mean ATRP) = {sharpe_atr:,.4f}\")\n",
    "#         return sharpe_atr\n",
    "\n",
    "#     # --- 4. Run Verification for Portfolio (No changes here) ---\n",
    "#     display(Markdown(\"### A. Group Portfolio Verification\"))\n",
    "#     _calculate_and_print_metrics(\"Full Period\", portfolio_return_series, portfolio_atrp_series)\n",
    "#     _calculate_and_print_metrics(\"Calculation Period\", portfolio_return_series.loc[:actual_calc_end_date], portfolio_atrp_series.loc[:actual_calc_end_date])\n",
    "#     _calculate_and_print_metrics(\"Forward Period\", portfolio_return_series.loc[actual_calc_end_date:].iloc[1:], portfolio_atrp_series.loc[actual_calc_end_date:].iloc[1:])\n",
    "\n",
    "#     # --- 5. Run Verification for Benchmark (No changes here) ---\n",
    "#     display(Markdown(f\"### B. Benchmark ({benchmark_ticker}) Verification\"))\n",
    "#     _calculate_and_print_metrics(\"Full Period\", benchmark_return_series, benchmark_atrp_series)\n",
    "#     _calculate_and_print_metrics(\"Calculation Period\", benchmark_return_series.loc[:actual_calc_end_date], benchmark_atrp_series.loc[:actual_calc_end_date])\n",
    "#     _calculate_and_print_metrics(\"Forward Period\", benchmark_return_series.loc[actual_calc_end_date:].iloc[1:], benchmark_atrp_series.loc[actual_calc_end_date:].iloc[1:])\n",
    "\n",
    "def verify_sharpe_atr_calculation_checked(df_ohlcv, features_df, tickers_to_verify, benchmark_ticker,\n",
    "                                    start_date, calc_period, fwd_period,\n",
    "                                    master_calendar_ticker='VOO', debug=False):\n",
    "    \"\"\"\n",
    "    Verifies the Sharpe (ATR) calculations for a portfolio and benchmark.\n",
    "\n",
    "    [MODIFIED to use the centralized performance function and trace the new value-weighted ATRP logic]\n",
    "    \"\"\"\n",
    "    # Assuming display and Markdown are imported from IPython.display\n",
    "    from IPython.display import display, Markdown\n",
    "    \n",
    "    display(Markdown(f\"## Verification Report for Sharpe (ATR) Calculation\"))\n",
    "    display(Markdown(f\"**Portfolio Tickers:** `{tickers_to_verify}`\\n**Benchmark Ticker:** `{benchmark_ticker}`\"))\n",
    "\n",
    "    # --- 1. Determine Exact Period Dates (No changes) ---\n",
    "    master_trading_days = df_ohlcv.loc[master_calendar_ticker].index.unique().sort_values()\n",
    "    start_date_raw = pd.to_datetime(start_date)\n",
    "    start_idx = master_trading_days.searchsorted(start_date_raw)\n",
    "    actual_start_date = master_trading_days[start_idx]\n",
    "    calc_end_idx = min(start_idx + calc_period, len(master_trading_days) - 1)\n",
    "    fwd_end_idx = min(calc_end_idx + fwd_period, len(master_trading_days) - 1)\n",
    "    actual_calc_end_date = master_trading_days[calc_end_idx]\n",
    "    actual_fwd_end_date = master_trading_days[fwd_end_idx]\n",
    "    \n",
    "    display(Markdown(f\"**Full Period:** `{actual_start_date.date()}` to `{actual_fwd_end_date.date()}`\\n\"\n",
    "                    f\"**Calc End Date:** `{actual_calc_end_date.date()}`\"))\n",
    "\n",
    "    # --- 2. Recreate Portfolio & Benchmark Series using the Centralized Function ---\n",
    "    df_close_full = df_ohlcv['Adj Close'].unstack(level=0)\n",
    "\n",
    "    # Use the central function for both portfolio and benchmark\n",
    "    portfolio_value_series, portfolio_return_series, portfolio_atrp_series = \\\n",
    "        calculate_buy_and_hold_performance(df_close_full, features_df, tickers_to_verify,\n",
    "                                           actual_start_date, actual_fwd_end_date)\n",
    "    \n",
    "    _, benchmark_return_series, benchmark_atrp_series = \\\n",
    "        calculate_buy_and_hold_performance(df_close_full, features_df, [benchmark_ticker],\n",
    "                                           actual_start_date, actual_fwd_end_date)\n",
    "\n",
    "    # --- 3. DETAILED DEBUG TRACE ---\n",
    "    # We re-calculate intermediate steps here ONLY for the purpose of detailed printing.\n",
    "    # The final series used in the analysis below come from the trusted central function.\n",
    "    if debug:\n",
    "        display(Markdown(\"---\"))\n",
    "        # --- RETURN TRACE (largely unchanged) ---\n",
    "        display(Markdown(\"### 🐛 Detailed Portfolio Return Calculation Trace\"))\n",
    "        portfolio_prices_raw = df_close_full[tickers_to_verify].loc[actual_start_date:actual_fwd_end_date]\n",
    "        normalization_base = portfolio_prices_raw.bfill().iloc[0]\n",
    "        portfolio_prices_norm = portfolio_prices_raw.div(normalization_base)\n",
    "        \n",
    "        print(\"\\n[STEP 1] Raw Adjusted Close prices.\")\n",
    "        display(portfolio_prices_raw)\n",
    "        print(\"\\n[STEP 2] Normalization base (first valid row of prices).\")\n",
    "        display(pd.DataFrame(normalization_base).T)\n",
    "        print(\"\\n[STEP 2a] Normalized prices (each stock's value from an initial $1 investment).\")\n",
    "        display(portfolio_prices_norm)\n",
    "        print(\"\\n[STEP 3] Averaged normalized portfolio value series.\")\n",
    "        display(pd.DataFrame(portfolio_value_series, columns=['N_portf_value']))\n",
    "        print(\"\\n[STEP 4] Final portfolio daily return series (pct_change of Step 3).\")\n",
    "        display(pd.DataFrame(portfolio_return_series, columns=['N_portf_rtn']))\n",
    "        \n",
    "        # +++ NEW: DETAILED ATRP CALCULATION TRACE +++\n",
    "        display(Markdown(\"### 🐛 Detailed Portfolio ATRP Calculation Trace (Value-Weighted)\"))\n",
    "        p_idx = pd.MultiIndex.from_product([tickers_to_verify, portfolio_return_series.index])\n",
    "        p_atrp_df = features_df.loc[features_df.index.intersection(p_idx)]['ATRP'].unstack(level=0)\n",
    "        \n",
    "        daily_total_value = portfolio_prices_norm.sum(axis=1)\n",
    "        daily_weights = portfolio_prices_norm.div(daily_total_value, axis=0)\n",
    "\n",
    "        print(\"\\n[STEP 5] Individual component ATRP values for each stock.\")\n",
    "        print(\"This is the raw risk input for each component.\")\n",
    "        display(p_atrp_df)\n",
    "\n",
    "        print(\"\\n[STEP 6] Drifting daily portfolio weights (based on market value).\")\n",
    "        print(\"Note how weights start near equal and drift over time. This is the core of the buy-and-hold logic.\")\n",
    "        display(daily_weights)\n",
    "        \n",
    "        print(\"\\n[STEP 7] Final value-weighted portfolio ATRP series.\")\n",
    "        print(\"Result of (Weights * Component_ATRPs) summed each day.\")\n",
    "        display(pd.DataFrame(portfolio_atrp_series, columns=['value_weighted_atrp']))\n",
    "        display(Markdown(\"---\"))\n",
    "    # +++ END OF DEBUG BLOCK +++\n",
    "\n",
    "    # --- 4. Define a Helper to Print Detailed Calculation Steps (Unchanged) ---\n",
    "    def _calculate_and_print_metrics(period_name, returns, atrps):\n",
    "        display(Markdown(f\"#### {period_name}\"))\n",
    "        if returns.dropna().empty or atrps.dropna().empty:\n",
    "            print(\"  - Not enough data to calculate.\")\n",
    "            return np.nan\n",
    "        mean_return = returns.mean()\n",
    "        mean_atrp = atrps.mean()\n",
    "        sharpe_atr = mean_return / mean_atrp if mean_atrp > 0 else np.nan\n",
    "        if debug:\n",
    "            # ... (debug printouts for mean calculation) ...\n",
    "            pass\n",
    "        print(f\"  - Mean Daily Return: {mean_return:,.6f}\")\n",
    "        print(f\"  - Mean Daily ATRP:  {mean_atrp:,.6f}\")\n",
    "        print(f\"  - Sharpe (ATR) = (Mean Return / Mean ATRP) = {sharpe_atr:,.4f}\")\n",
    "        return sharpe_atr\n",
    "\n",
    "    # --- 5. Run Verification for Portfolio (Unchanged) ---\n",
    "    display(Markdown(\"### A. Group Portfolio Verification\"))\n",
    "    _calculate_and_print_metrics(\"Full Period\", portfolio_return_series, portfolio_atrp_series)\n",
    "    _calculate_and_print_metrics(\"Calculation Period\", portfolio_return_series.loc[:actual_calc_end_date], portfolio_atrp_series.loc[:actual_calc_end_date])\n",
    "    _calculate_and_print_metrics(\"Forward Period\", portfolio_return_series.loc[actual_calc_end_date:].iloc[1:], portfolio_atrp_series.loc[actual_calc_end_date:].iloc[1:])\n",
    "\n",
    "    # --- 6. Run Verification for Benchmark (Unchanged) ---\n",
    "    display(Markdown(f\"### B. Benchmark ({benchmark_ticker}) Verification\"))\n",
    "    _calculate_and_print_metrics(\"Full Period\", benchmark_return_series, benchmark_atrp_series)\n",
    "    _calculate_and_print_metrics(\"Calculation Period\", benchmark_return_series.loc[:actual_calc_end_date], benchmark_atrp_series.loc[:actual_calc_end_date])\n",
    "    _calculate_and_print_metrics(\"Forward Period\", benchmark_return_series.loc[actual_calc_end_date:].iloc[1:], benchmark_atrp_series.loc[actual_calc_end_date:].iloc[1:])\n",
    "\n",
    "\n",
    "\n",
    "# --- F. AUTOMATION SCRIPT - STRATEGY SEARCH ---\n",
    "\n",
    "def run_strategy_search(df_ohlcv, config):\n",
    "    \"\"\"\n",
    "    Runs the main backtesting loop with checkpointing to be resumable.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # --- 1. SETUP & LOAD PROGRESS ---\n",
    "    print(\"--- Phase 1: Pre-processing and Loading Progress ---\")\n",
    "\n",
    "    # +++ ADD THIS BLOCK +++\n",
    "    # Pre-calculate all features for the entire dataset ONCE.\n",
    "    print(\"--- Generating all features upfront... ---\")\n",
    "    features_df = generate_features(df_ohlcv)\n",
    "\n",
    "    # --- DELETE THIS LINE ---\n",
    "    # quality_metrics_df = calculate_rolling_quality_metrics(df_ohlcv, window=252)\n",
    "    \n",
    "    print(\"Unstacking data for performance...\")\n",
    "    df_close_full = df_ohlcv['Adj Close'].unstack(level=0)\n",
    "    df_high_full = df_ohlcv['Adj High'].unstack(level=0)\n",
    "    df_low_full = df_ohlcv['Adj Low'].unstack(level=0)\n",
    "    \n",
    "    master_calendar_ticker = config['master_calendar_ticker']\n",
    "    master_trading_days = df_ohlcv.loc[master_calendar_ticker].index.unique().sort_values()\n",
    "    print(f\"Master trading day calendar created from '{master_calendar_ticker}' ({len(master_trading_days)} days).\")\n",
    "\n",
    "    results_path = config['results_output_path']\n",
    "    completed_params = set()\n",
    "    \n",
    "    if os.path.exists(results_path):\n",
    "        print(f\"Found existing results file. Loading progress from: {results_path}\")\n",
    "        df_progress = pd.read_csv(results_path)\n",
    "        for _, row in df_progress.iterrows():\n",
    "            param_key = (\n",
    "                row['calc_period'], row['fwd_period'], row['metric'],\n",
    "                (row['rank_start'], row['rank_end'])\n",
    "            )\n",
    "            completed_params.add(param_key)\n",
    "        print(f\"Found {len(completed_params)} completed parameter sets to skip.\")\n",
    "    else:\n",
    "        print(\"No existing results file found. Starting a new run.\")\n",
    "\n",
    "    print(\"✅ Pre-processing complete.\\n\")\n",
    "\n",
    "    # --- 2. SETUP THE MAIN LOOP (No changes here) ---\n",
    "    print(\"--- Phase 2: Setting up Simulation Loops ---\")\n",
    "    param_combinations = list(product(\n",
    "        config['calc_periods'], config['fwd_periods'],\n",
    "        config['metrics'], config['rank_slices']\n",
    "    ))\n",
    "    search_start_date = pd.to_datetime(config['search_start_date'])\n",
    "    search_end_date = pd.to_datetime(config['search_end_date'])\n",
    "    start_idx = master_trading_days.searchsorted(search_start_date, side='left')\n",
    "    end_idx = master_trading_days.searchsorted(search_end_date, side='right')\n",
    "    step_dates_map = {}\n",
    "    print(\"Pre-calculating rebalancing schedules for each holding period...\")\n",
    "    for fwd_period in sorted(config['fwd_periods']):\n",
    "        step_indices = range(start_idx, end_idx, fwd_period)\n",
    "        step_dates_map[fwd_period] = master_trading_days[step_indices]\n",
    "        print(f\"  - Holding Period {fwd_period} days: {len(step_dates_map[fwd_period])} rebalances\")\n",
    "    print(f\"Found {len(param_combinations)} total parameter sets to simulate.\")\n",
    "    print(\"✅ Setup complete. Starting main loop...\\n\")\n",
    "\n",
    "    # --- 3. RUN THE MAIN LOOP ---\n",
    "    print(\"--- Phase 3: Running Simulations ---\")\n",
    "    pbar = tqdm(param_combinations, desc=\"Parameter Sets\")\n",
    "    \n",
    "    for params in pbar:\n",
    "        calc_period, fwd_period, metric, rank_slice = params\n",
    "        rank_start, rank_end = rank_slice\n",
    "        \n",
    "        param_key = (calc_period, fwd_period, metric, rank_slice)\n",
    "        if param_key in completed_params:\n",
    "            pbar.set_description(f\"Skipping {param_key}\")\n",
    "            continue\n",
    "\n",
    "        pbar.set_description(f\"Running {param_key}\")\n",
    "        \n",
    "        current_params_results = []\n",
    "        \n",
    "        current_step_dates = step_dates_map[fwd_period]\n",
    "        for step_date in current_step_dates:\n",
    "            # +++ UPDATE THIS CALL +++\n",
    "            eligible_tickers = get_eligible_universe(\n",
    "                features_df, filter_date=step_date, thresholds=config['quality_thresholds']\n",
    "            )\n",
    "            if not eligible_tickers: continue\n",
    "            \n",
    "            df_close_step = df_close_full[eligible_tickers]\n",
    "            df_high_step = df_high_full[eligible_tickers]\n",
    "            df_low_step = df_low_full[eligible_tickers]\n",
    "\n",
    "            # +++ UPDATE THIS CALL +++\n",
    "            step_result, _ = run_walk_forward_step(\n",
    "                df_close_full=df_close_step, df_high_full=df_high_step, df_low_full=df_low_step,\n",
    "                master_trading_days=master_trading_days, start_date=step_date,\n",
    "                calc_period=calc_period, fwd_period=fwd_period,\n",
    "                metric=metric, rank_start=rank_start, rank_end=rank_end,\n",
    "                benchmark_ticker=config['benchmark_ticker'],\n",
    "                features_df=features_df, # Pass the features_df\n",
    "                debug=False\n",
    "            )\n",
    "            \n",
    "            if step_result['error'] is None:\n",
    "                p = step_result['performance_data']\n",
    "                log_entry = {\n",
    "                    'step_date': step_date.date(), 'calc_period': calc_period,\n",
    "                    'fwd_period': fwd_period, 'metric': metric,\n",
    "                    'rank_start': rank_start, 'rank_end': rank_end,\n",
    "                    'num_universe': len(eligible_tickers),\n",
    "                    'num_portfolio': len(step_result['tickers_to_display']),\n",
    "                    'fwd_p_gain': p['fwd_p_gain'], 'fwd_b_gain': p['fwd_b_gain'],\n",
    "                    'fwd_gain_delta': p['fwd_p_gain'] - p['fwd_b_gain'] if not np.isnan(p['fwd_b_gain']) else np.nan,\n",
    "                    'fwd_p_sharpe': p['fwd_p_sharpe'],\n",
    "                }\n",
    "                current_params_results.append(log_entry)\n",
    "        \n",
    "        # --- CHECKPOINTING (No changes here) ---\n",
    "        if current_params_results:\n",
    "            df_to_append = pd.DataFrame(current_params_results)\n",
    "            df_to_append.to_csv(\n",
    "                results_path,\n",
    "                mode='a',\n",
    "                header=not os.path.exists(results_path),\n",
    "                index=False\n",
    "            )\n",
    "            completed_params.add(param_key)\n",
    "\n",
    "    print(\"✅ Main loop finished.\\n\")\n",
    "    \n",
    "    # --- 4. RETURN FINAL DATAFRAME (No changes here) ---\n",
    "    print(\"--- Phase 4: Loading Final Results ---\")\n",
    "    if os.path.exists(results_path):\n",
    "        final_df = pd.read_csv(results_path)\n",
    "        end_time = time.time()\n",
    "        print(f\"✅ Process complete. Total execution time: {time.time() - start_time:.2f} seconds.\")\n",
    "        return final_df\n",
    "    else:\n",
    "        print(\"Warning: No results were generated.\")\n",
    "        return None    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91c1701e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Walk-Forward Analyzer (using Trading Day Logic)...\n",
      "Master trading day calendar created from 'VOO' (3795 days).\n",
      "--- Generating all features upfront... ---\n",
      "--- Starting Feature Generation ---\n",
      "Calculating technical indicators (ATR Period: 14)...\n",
      "Calculating data quality metrics (Window: 252 days)...\n",
      "Combining all feature sets...\n",
      "✅ Feature generation complete.\n",
      "Pre-processing data (unstacking)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a82f0f081a4549058dd3e2ecbae84605",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(DatePicker(value=Timestamp('2018-10-03 00:00:00'), description='Start Date:', st…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8461a87290624e15b116d2ddd2b3d620",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'mode': 'lines',\n",
       "              'name': 'placeholder_0',\n",
       "              'showlegend': False,\n",
       "              'type': 'scatter',\n",
       "              'uid': 'a9b49555-5eb4-47e4-b535-bc43ebafff5f',\n",
       "              'visible': False,\n",
       "              'x': [None],\n",
       "              'y': [None]},\n",
       "             {'mode': 'lines',\n",
       "              'name': 'placeholder_1',\n",
       "              'showlegend': False,\n",
       "              'type': 'scatter',\n",
       "              'uid': '1e87a0c5-e381-4cb0-8325-bff7d0363f42',\n",
       "              'visible': False,\n",
       "              'x': [None],\n",
       "              'y': [None]},\n",
       "             {'mode': 'lines',\n",
       "              'name': 'placeholder_2',\n",
       "              'showlegend': False,\n",
       "              'type': 'scatter',\n",
       "              'uid': '2d66d579-ad4b-4d83-bb53-2678d6b24140',\n",
       "              'visible': False,\n",
       "              'x': [None],\n",
       "              'y': [None]},\n",
       "             {'mode': 'lines',\n",
       "              'name': 'placeholder_3',\n",
       "              'showlegend': False,\n",
       "              'type': 'scatter',\n",
       "              'uid': 'afecc52a-1743-46eb-90af-bb1aa6e687b2',\n",
       "              'visible': False,\n",
       "              'x': [None],\n",
       "              'y': [None]},\n",
       "             {'mode': 'lines',\n",
       "              'name': 'placeholder_4',\n",
       "              'showlegend': False,\n",
       "              'type': 'scatter',\n",
       "              'uid': 'bb0d4e66-6765-4fde-bfe8-df6b61d47d15',\n",
       "              'visible': False,\n",
       "              'x': [None],\n",
       "              'y': [None]},\n",
       "             {'mode': 'lines',\n",
       "              'name': 'placeholder_5',\n",
       "              'showlegend': False,\n",
       "              'type': 'scatter',\n",
       "              'uid': '894916c7-fb88-4866-95be-f1d4b004607a',\n",
       "              'visible': False,\n",
       "              'x': [None],\n",
       "              'y': [None]},\n",
       "             {'mode': 'lines',\n",
       "              'name': 'placeholder_6',\n",
       "              'showlegend': False,\n",
       "              'type': 'scatter',\n",
       "              'uid': 'c914f478-f552-45a2-9ab1-4840ab74d407',\n",
       "              'visible': False,\n",
       "              'x': [None],\n",
       "              'y': [None]},\n",
       "             {'mode': 'lines',\n",
       "              'name': 'placeholder_7',\n",
       "              'showlegend': False,\n",
       "              'type': 'scatter',\n",
       "              'uid': '93d79b72-5575-4faa-a978-e1e00fb00053',\n",
       "              'visible': False,\n",
       "              'x': [None],\n",
       "              'y': [None]},\n",
       "             {'mode': 'lines',\n",
       "              'name': 'placeholder_8',\n",
       "              'showlegend': False,\n",
       "              'type': 'scatter',\n",
       "              'uid': '9f49fd5a-0f0e-44fa-9a6e-862afab558d4',\n",
       "              'visible': False,\n",
       "              'x': [None],\n",
       "              'y': [None]},\n",
       "             {'mode': 'lines',\n",
       "              'name': 'placeholder_9',\n",
       "              'showlegend': False,\n",
       "              'type': 'scatter',\n",
       "              'uid': '93b6b21f-d8c2-4f9c-be31-f7b230956856',\n",
       "              'visible': False,\n",
       "              'x': [None],\n",
       "              'y': [None]},\n",
       "             {'mode': 'lines',\n",
       "              'name': 'placeholder_10',\n",
       "              'showlegend': False,\n",
       "              'type': 'scatter',\n",
       "              'uid': 'aa38dff0-e2b8-4429-a29a-dafcf1945818',\n",
       "              'visible': False,\n",
       "              'x': [None],\n",
       "              'y': [None]},\n",
       "             {'mode': 'lines',\n",
       "              'name': 'placeholder_11',\n",
       "              'showlegend': False,\n",
       "              'type': 'scatter',\n",
       "              'uid': '9687942c-b298-4c48-aa09-01b8de91b067',\n",
       "              'visible': False,\n",
       "              'x': [None],\n",
       "              'y': [None]},\n",
       "             {'mode': 'lines',\n",
       "              'name': 'placeholder_12',\n",
       "              'showlegend': False,\n",
       "              'type': 'scatter',\n",
       "              'uid': '3383345c-ea24-461c-bb6f-53e740922d96',\n",
       "              'visible': False,\n",
       "              'x': [None],\n",
       "              'y': [None]},\n",
       "             {'mode': 'lines',\n",
       "              'name': 'placeholder_13',\n",
       "              'showlegend': False,\n",
       "              'type': 'scatter',\n",
       "              'uid': '07cf8a25-ecaf-4058-b1dc-fd37d9f2bfec',\n",
       "              'visible': False,\n",
       "              'x': [None],\n",
       "              'y': [None]},\n",
       "             {'mode': 'lines',\n",
       "              'name': 'placeholder_14',\n",
       "              'showlegend': False,\n",
       "              'type': 'scatter',\n",
       "              'uid': '4122ea12-b2df-4fb7-a7ab-9c5c76a49214',\n",
       "              'visible': False,\n",
       "              'x': [None],\n",
       "              'y': [None]},\n",
       "             {'mode': 'lines',\n",
       "              'name': 'placeholder_15',\n",
       "              'showlegend': False,\n",
       "              'type': 'scatter',\n",
       "              'uid': '9193f2aa-a695-4f26-a4a2-c893a049a8ea',\n",
       "              'visible': False,\n",
       "              'x': [None],\n",
       "              'y': [None]},\n",
       "             {'mode': 'lines',\n",
       "              'name': 'placeholder_16',\n",
       "              'showlegend': False,\n",
       "              'type': 'scatter',\n",
       "              'uid': 'c3e6229c-0f4d-4dd5-b547-e72d822bdfa5',\n",
       "              'visible': False,\n",
       "              'x': [None],\n",
       "              'y': [None]},\n",
       "             {'mode': 'lines',\n",
       "              'name': 'placeholder_17',\n",
       "              'showlegend': False,\n",
       "              'type': 'scatter',\n",
       "              'uid': '98663e4a-8272-464b-8728-81fa3d9f279b',\n",
       "              'visible': False,\n",
       "              'x': [None],\n",
       "              'y': [None]},\n",
       "             {'mode': 'lines',\n",
       "              'name': 'placeholder_18',\n",
       "              'showlegend': False,\n",
       "              'type': 'scatter',\n",
       "              'uid': '3f271231-6317-467c-bd7b-646ceab70e2a',\n",
       "              'visible': False,\n",
       "              'x': [None],\n",
       "              'y': [None]},\n",
       "             {'mode': 'lines',\n",
       "              'name': 'placeholder_19',\n",
       "              'showlegend': False,\n",
       "              'type': 'scatter',\n",
       "              'uid': '29c432a0-1eb0-48e2-8763-919e7a945206',\n",
       "              'visible': False,\n",
       "              'x': [None],\n",
       "              'y': [None]},\n",
       "             {'mode': 'lines',\n",
       "              'name': 'placeholder_20',\n",
       "              'showlegend': False,\n",
       "              'type': 'scatter',\n",
       "              'uid': '285b9cfd-cfb8-4c98-b559-b17d46c7f1d3',\n",
       "              'visible': False,\n",
       "              'x': [None],\n",
       "              'y': [None]},\n",
       "             {'mode': 'lines',\n",
       "              'name': 'placeholder_21',\n",
       "              'showlegend': False,\n",
       "              'type': 'scatter',\n",
       "              'uid': '0c77e9f3-7eaa-4a27-945d-cd3617dae0e0',\n",
       "              'visible': False,\n",
       "              'x': [None],\n",
       "              'y': [None]},\n",
       "             {'mode': 'lines',\n",
       "              'name': 'placeholder_22',\n",
       "              'showlegend': False,\n",
       "              'type': 'scatter',\n",
       "              'uid': '8ccb77da-892d-4dba-a71d-2235dc04e20e',\n",
       "              'visible': False,\n",
       "              'x': [None],\n",
       "              'y': [None]},\n",
       "             {'mode': 'lines',\n",
       "              'name': 'placeholder_23',\n",
       "              'showlegend': False,\n",
       "              'type': 'scatter',\n",
       "              'uid': 'b4139fe3-9ae1-47a4-bc5a-7549e2b2e747',\n",
       "              'visible': False,\n",
       "              'x': [None],\n",
       "              'y': [None]},\n",
       "             {'mode': 'lines',\n",
       "              'name': 'placeholder_24',\n",
       "              'showlegend': False,\n",
       "              'type': 'scatter',\n",
       "              'uid': '7aaebb6e-7d82-49e5-aa84-e96d16a805d7',\n",
       "              'visible': False,\n",
       "              'x': [None],\n",
       "              'y': [None]},\n",
       "             {'mode': 'lines',\n",
       "              'name': 'placeholder_25',\n",
       "              'showlegend': False,\n",
       "              'type': 'scatter',\n",
       "              'uid': 'b0ba0503-158a-4121-9e32-4abd363be86f',\n",
       "              'visible': False,\n",
       "              'x': [None],\n",
       "              'y': [None]},\n",
       "             {'mode': 'lines',\n",
       "              'name': 'placeholder_26',\n",
       "              'showlegend': False,\n",
       "              'type': 'scatter',\n",
       "              'uid': 'c6f684c2-18b2-419f-8202-9a46b4b6ad34',\n",
       "              'visible': False,\n",
       "              'x': [None],\n",
       "              'y': [None]},\n",
       "             {'mode': 'lines',\n",
       "              'name': 'placeholder_27',\n",
       "              'showlegend': False,\n",
       "              'type': 'scatter',\n",
       "              'uid': '4a0196ed-8fd1-4643-9d48-2762f2aea6f5',\n",
       "              'visible': False,\n",
       "              'x': [None],\n",
       "              'y': [None]},\n",
       "             {'mode': 'lines',\n",
       "              'name': 'placeholder_28',\n",
       "              'showlegend': False,\n",
       "              'type': 'scatter',\n",
       "              'uid': '5c3c12d8-0699-412c-8858-8d74f69ab09b',\n",
       "              'visible': False,\n",
       "              'x': [None],\n",
       "              'y': [None]},\n",
       "             {'mode': 'lines',\n",
       "              'name': 'placeholder_29',\n",
       "              'showlegend': False,\n",
       "              'type': 'scatter',\n",
       "              'uid': '85d633fa-0d1f-42c9-801c-7f5af6024204',\n",
       "              'visible': False,\n",
       "              'x': [None],\n",
       "              'y': [None]},\n",
       "             {'mode': 'lines',\n",
       "              'name': 'placeholder_30',\n",
       "              'showlegend': False,\n",
       "              'type': 'scatter',\n",
       "              'uid': '11dda62b-c012-44c3-b966-4fb8d7e3cee6',\n",
       "              'visible': False,\n",
       "              'x': [None],\n",
       "              'y': [None]},\n",
       "             {'mode': 'lines',\n",
       "              'name': 'placeholder_31',\n",
       "              'showlegend': False,\n",
       "              'type': 'scatter',\n",
       "              'uid': '7aaa1f86-4583-412d-83d9-243982157968',\n",
       "              'visible': False,\n",
       "              'x': [None],\n",
       "              'y': [None]},\n",
       "             {'mode': 'lines',\n",
       "              'name': 'placeholder_32',\n",
       "              'showlegend': False,\n",
       "              'type': 'scatter',\n",
       "              'uid': '6b588f91-85da-4117-b01e-9f5cf3ac6bb0',\n",
       "              'visible': False,\n",
       "              'x': [None],\n",
       "              'y': [None]},\n",
       "             {'mode': 'lines',\n",
       "              'name': 'placeholder_33',\n",
       "              'showlegend': False,\n",
       "              'type': 'scatter',\n",
       "              'uid': '15d6a08c-62ae-4ed4-b6d8-ed0abde58515',\n",
       "              'visible': False,\n",
       "              'x': [None],\n",
       "              'y': [None]},\n",
       "             {'mode': 'lines',\n",
       "              'name': 'placeholder_34',\n",
       "              'showlegend': False,\n",
       "              'type': 'scatter',\n",
       "              'uid': 'bd5a1159-b1ed-45a9-86f1-288fae09ef06',\n",
       "              'visible': False,\n",
       "              'x': [None],\n",
       "              'y': [None]},\n",
       "             {'mode': 'lines',\n",
       "              'name': 'placeholder_35',\n",
       "              'showlegend': False,\n",
       "              'type': 'scatter',\n",
       "              'uid': '9301bb13-a88d-46ba-8b1a-96d37406347c',\n",
       "              'visible': False,\n",
       "              'x': [None],\n",
       "              'y': [None]},\n",
       "             {'mode': 'lines',\n",
       "              'name': 'placeholder_36',\n",
       "              'showlegend': False,\n",
       "              'type': 'scatter',\n",
       "              'uid': 'fd9f28e3-1254-4963-9056-7a025349ff12',\n",
       "              'visible': False,\n",
       "              'x': [None],\n",
       "              'y': [None]},\n",
       "             {'mode': 'lines',\n",
       "              'name': 'placeholder_37',\n",
       "              'showlegend': False,\n",
       "              'type': 'scatter',\n",
       "              'uid': 'c84bc4d7-8347-4370-af25-084a0b880b2b',\n",
       "              'visible': False,\n",
       "              'x': [None],\n",
       "              'y': [None]},\n",
       "             {'mode': 'lines',\n",
       "              'name': 'placeholder_38',\n",
       "              'showlegend': False,\n",
       "              'type': 'scatter',\n",
       "              'uid': '270d3caa-4314-42d0-85f8-11b18086aabb',\n",
       "              'visible': False,\n",
       "              'x': [None],\n",
       "              'y': [None]},\n",
       "             {'mode': 'lines',\n",
       "              'name': 'placeholder_39',\n",
       "              'showlegend': False,\n",
       "              'type': 'scatter',\n",
       "              'uid': 'b1f5947f-22df-4a43-8d6b-a722f179d192',\n",
       "              'visible': False,\n",
       "              'x': [None],\n",
       "              'y': [None]},\n",
       "             {'mode': 'lines',\n",
       "              'name': 'placeholder_40',\n",
       "              'showlegend': False,\n",
       "              'type': 'scatter',\n",
       "              'uid': '92f04329-b70d-4f42-9f10-c9ee58e881a1',\n",
       "              'visible': False,\n",
       "              'x': [None],\n",
       "              'y': [None]},\n",
       "             {'mode': 'lines',\n",
       "              'name': 'placeholder_41',\n",
       "              'showlegend': False,\n",
       "              'type': 'scatter',\n",
       "              'uid': 'd5e46045-3aad-49a5-a713-dc8886def217',\n",
       "              'visible': False,\n",
       "              'x': [None],\n",
       "              'y': [None]},\n",
       "             {'mode': 'lines',\n",
       "              'name': 'placeholder_42',\n",
       "              'showlegend': False,\n",
       "              'type': 'scatter',\n",
       "              'uid': 'aa5bd0eb-6352-4ebc-bb2d-f6314ab9b5dc',\n",
       "              'visible': False,\n",
       "              'x': [None],\n",
       "              'y': [None]},\n",
       "             {'mode': 'lines',\n",
       "              'name': 'placeholder_43',\n",
       "              'showlegend': False,\n",
       "              'type': 'scatter',\n",
       "              'uid': 'ba29b1a1-6e5f-4f33-8fbb-ae9b424c4a35',\n",
       "              'visible': False,\n",
       "              'x': [None],\n",
       "              'y': [None]},\n",
       "             {'mode': 'lines',\n",
       "              'name': 'placeholder_44',\n",
       "              'showlegend': False,\n",
       "              'type': 'scatter',\n",
       "              'uid': 'ff217476-9d36-4fad-bd88-c993e1f3f064',\n",
       "              'visible': False,\n",
       "              'x': [None],\n",
       "              'y': [None]},\n",
       "             {'mode': 'lines',\n",
       "              'name': 'placeholder_45',\n",
       "              'showlegend': False,\n",
       "              'type': 'scatter',\n",
       "              'uid': 'c811bc9d-b856-412f-8275-aecf372b1c1b',\n",
       "              'visible': False,\n",
       "              'x': [None],\n",
       "              'y': [None]},\n",
       "             {'mode': 'lines',\n",
       "              'name': 'placeholder_46',\n",
       "              'showlegend': False,\n",
       "              'type': 'scatter',\n",
       "              'uid': '4144e266-99fa-4935-9a88-7feda7b6ff4a',\n",
       "              'visible': False,\n",
       "              'x': [None],\n",
       "              'y': [None]},\n",
       "             {'mode': 'lines',\n",
       "              'name': 'placeholder_47',\n",
       "              'showlegend': False,\n",
       "              'type': 'scatter',\n",
       "              'uid': '16f664ec-0b3e-4a2d-91a0-a47341460371',\n",
       "              'visible': False,\n",
       "              'x': [None],\n",
       "              'y': [None]},\n",
       "             {'mode': 'lines',\n",
       "              'name': 'placeholder_48',\n",
       "              'showlegend': False,\n",
       "              'type': 'scatter',\n",
       "              'uid': '10e4855e-dd09-49c1-a30d-08ffdb740a4d',\n",
       "              'visible': False,\n",
       "              'x': [None],\n",
       "              'y': [None]},\n",
       "             {'mode': 'lines',\n",
       "              'name': 'placeholder_49',\n",
       "              'showlegend': False,\n",
       "              'type': 'scatter',\n",
       "              'uid': 'e662e9ee-4492-4444-ab00-c67683a9aab1',\n",
       "              'visible': False,\n",
       "              'x': [None],\n",
       "              'y': [None]},\n",
       "             {'line': {'color': 'black', 'dash': 'dash', 'width': 3},\n",
       "              'mode': 'lines',\n",
       "              'name': 'Benchmark',\n",
       "              'showlegend': True,\n",
       "              'type': 'scatter',\n",
       "              'uid': '9d252622-1f4e-46fc-92c0-9e5e5eb961da',\n",
       "              'visible': True,\n",
       "              'x': [None],\n",
       "              'y': [None]},\n",
       "             {'line': {'color': 'green', 'width': 3},\n",
       "              'mode': 'lines',\n",
       "              'name': 'Group Portfolio',\n",
       "              'showlegend': True,\n",
       "              'type': 'scatter',\n",
       "              'uid': '0cab7d46-9ae2-4fab-b6ff-1c09bce789f2',\n",
       "              'visible': True,\n",
       "              'x': [None],\n",
       "              'y': [None]}],\n",
       "    'layout': {'height': 600,\n",
       "               'hovermode': 'x unified',\n",
       "               'legend': {'title': {'text': 'Tickers (Ranked)'}},\n",
       "               'margin': {'t': 50},\n",
       "               'shapes': [{'line': {'color': 'grey', 'dash': 'dash', 'width': 1},\n",
       "                           'type': 'line',\n",
       "                           'x0': 0,\n",
       "                           'x1': 1,\n",
       "                           'xref': 'x domain',\n",
       "                           'y0': 1,\n",
       "                           'y1': 1,\n",
       "                           'yref': 'y'}],\n",
       "               'template': '...',\n",
       "               'title': {'text': 'Walk-Forward Performance Analysis'},\n",
       "               'xaxis': {'title': {'text': 'Date'}},\n",
       "               'yaxis': {'title': {'text': 'Normalized Price (Start = 1)'}}}\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dynamic Filter (2018-10-03): Kept 618 of 722 tickers.\n"
     ]
    }
   ],
   "source": [
    "results_container, debug_container = plot_walk_forward_analyzer(\n",
    "    df_ohlcv=df_ohlcv,\n",
    "    default_start_date='2018-10-03',\n",
    "    default_calc_period=252,\n",
    "    default_fwd_period=63,\n",
    "    default_metric='Sharpe (ATR)',\n",
    "    default_rank_start=1,\n",
    "    default_rank_end=5,\n",
    "    default_benchmark_ticker='VOO',\n",
    "    quality_thresholds={ 'min_median_dollar_volume': 10_000_000, \n",
    "                         'max_stale_pct': 0.05, \n",
    "                         'max_same_vol_count': 1 },\n",
    "    debug=True  # <-- Activate the new mode!\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0fea8d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Regenerating features for verification ---\n",
      "--- Starting Feature Generation ---\n",
      "Calculating technical indicators (ATR Period: 14)...\n",
      "Calculating data quality metrics (Window: 252 days)...\n",
      "Combining all feature sets...\n",
      "✅ Feature generation complete.\n"
     ]
    }
   ],
   "source": [
    "# Assume 'df_ohlcv' is your loaded dataset\n",
    "# You might need to regenerate features_df if it's not in your notebook's memory\n",
    "print(\"--- Regenerating features for verification ---\")\n",
    "features_df = generate_features(df_ohlcv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca56efc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers_to_verify = ['STIP', 'RCL']\n",
    "start_date = '2025-08-13'\n",
    "end_date = '2025-09-04'\n",
    "benchmark_ticker = 'VOO'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Verification Report for Sharpe (ATR) Calculation"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Portfolio Tickers:** `['STIP', 'RCL']`\n",
       "**Benchmark Ticker:** `VOO`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Full Period:** `2025-08-13` to `2025-09-04`\n",
       "**Calc End Date:** `2025-08-27`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### 🐛 Detailed Portfolio Return Calculation Trace"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 1] Raw Adjusted Close prices.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Ticker</th>\n",
       "      <th>STIP</th>\n",
       "      <th>RCL</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2025-08-13</th>\n",
       "      <td>102.515</td>\n",
       "      <td>312.462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-14</th>\n",
       "      <td>102.446</td>\n",
       "      <td>311.345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-15</th>\n",
       "      <td>102.376</td>\n",
       "      <td>312.990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-18</th>\n",
       "      <td>102.346</td>\n",
       "      <td>325.801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-19</th>\n",
       "      <td>102.406</td>\n",
       "      <td>329.061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-20</th>\n",
       "      <td>102.476</td>\n",
       "      <td>328.114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-21</th>\n",
       "      <td>102.486</td>\n",
       "      <td>324.036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-22</th>\n",
       "      <td>102.883</td>\n",
       "      <td>343.616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-25</th>\n",
       "      <td>102.813</td>\n",
       "      <td>343.915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-26</th>\n",
       "      <td>102.982</td>\n",
       "      <td>352.658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-27</th>\n",
       "      <td>103.161</td>\n",
       "      <td>357.972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-28</th>\n",
       "      <td>103.161</td>\n",
       "      <td>364.721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-29</th>\n",
       "      <td>103.201</td>\n",
       "      <td>362.109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-09-02</th>\n",
       "      <td>103.101</td>\n",
       "      <td>352.997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-09-03</th>\n",
       "      <td>103.141</td>\n",
       "      <td>353.914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-09-04</th>\n",
       "      <td>103.171</td>\n",
       "      <td>358.959</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Ticker         STIP      RCL\n",
       "Date                        \n",
       "2025-08-13  102.515  312.462\n",
       "2025-08-14  102.446  311.345\n",
       "2025-08-15  102.376  312.990\n",
       "2025-08-18  102.346  325.801\n",
       "2025-08-19  102.406  329.061\n",
       "2025-08-20  102.476  328.114\n",
       "2025-08-21  102.486  324.036\n",
       "2025-08-22  102.883  343.616\n",
       "2025-08-25  102.813  343.915\n",
       "2025-08-26  102.982  352.658\n",
       "2025-08-27  103.161  357.972\n",
       "2025-08-28  103.161  364.721\n",
       "2025-08-29  103.201  362.109\n",
       "2025-09-02  103.101  352.997\n",
       "2025-09-03  103.141  353.914\n",
       "2025-09-04  103.171  358.959"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 2] Normalization base (first valid row of prices).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Ticker</th>\n",
       "      <th>STIP</th>\n",
       "      <th>RCL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2025-08-13</th>\n",
       "      <td>102.515</td>\n",
       "      <td>312.462</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Ticker         STIP      RCL\n",
       "2025-08-13  102.515  312.462"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 2a] Normalized prices (each stock's value from an initial $1 investment).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Ticker</th>\n",
       "      <th>STIP</th>\n",
       "      <th>RCL</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2025-08-13</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-14</th>\n",
       "      <td>0.999327</td>\n",
       "      <td>0.996425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-15</th>\n",
       "      <td>0.998644</td>\n",
       "      <td>1.001690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-18</th>\n",
       "      <td>0.998351</td>\n",
       "      <td>1.042690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-19</th>\n",
       "      <td>0.998937</td>\n",
       "      <td>1.053123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-20</th>\n",
       "      <td>0.999620</td>\n",
       "      <td>1.050092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-21</th>\n",
       "      <td>0.999717</td>\n",
       "      <td>1.037041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-22</th>\n",
       "      <td>1.003590</td>\n",
       "      <td>1.099705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-25</th>\n",
       "      <td>1.002907</td>\n",
       "      <td>1.100662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-26</th>\n",
       "      <td>1.004555</td>\n",
       "      <td>1.128643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-27</th>\n",
       "      <td>1.006302</td>\n",
       "      <td>1.145650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-28</th>\n",
       "      <td>1.006302</td>\n",
       "      <td>1.167249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-29</th>\n",
       "      <td>1.006692</td>\n",
       "      <td>1.158890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-09-02</th>\n",
       "      <td>1.005716</td>\n",
       "      <td>1.129728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-09-03</th>\n",
       "      <td>1.006106</td>\n",
       "      <td>1.132663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-09-04</th>\n",
       "      <td>1.006399</td>\n",
       "      <td>1.148808</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Ticker          STIP       RCL\n",
       "Date                          \n",
       "2025-08-13  1.000000  1.000000\n",
       "2025-08-14  0.999327  0.996425\n",
       "2025-08-15  0.998644  1.001690\n",
       "2025-08-18  0.998351  1.042690\n",
       "2025-08-19  0.998937  1.053123\n",
       "2025-08-20  0.999620  1.050092\n",
       "2025-08-21  0.999717  1.037041\n",
       "2025-08-22  1.003590  1.099705\n",
       "2025-08-25  1.002907  1.100662\n",
       "2025-08-26  1.004555  1.128643\n",
       "2025-08-27  1.006302  1.145650\n",
       "2025-08-28  1.006302  1.167249\n",
       "2025-08-29  1.006692  1.158890\n",
       "2025-09-02  1.005716  1.129728\n",
       "2025-09-03  1.006106  1.132663\n",
       "2025-09-04  1.006399  1.148808"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 3] Averaged normalized portfolio value series.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N_portf_value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2025-08-13</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-14</th>\n",
       "      <td>0.997876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-15</th>\n",
       "      <td>1.000167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-18</th>\n",
       "      <td>1.020521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-19</th>\n",
       "      <td>1.026030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-20</th>\n",
       "      <td>1.024856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-21</th>\n",
       "      <td>1.018379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-22</th>\n",
       "      <td>1.051647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-25</th>\n",
       "      <td>1.051784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-26</th>\n",
       "      <td>1.066599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-27</th>\n",
       "      <td>1.075976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-28</th>\n",
       "      <td>1.086775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-29</th>\n",
       "      <td>1.082791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-09-02</th>\n",
       "      <td>1.067722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-09-03</th>\n",
       "      <td>1.069384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-09-04</th>\n",
       "      <td>1.077604</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            N_portf_value\n",
       "Date                     \n",
       "2025-08-13       1.000000\n",
       "2025-08-14       0.997876\n",
       "2025-08-15       1.000167\n",
       "2025-08-18       1.020521\n",
       "2025-08-19       1.026030\n",
       "2025-08-20       1.024856\n",
       "2025-08-21       1.018379\n",
       "2025-08-22       1.051647\n",
       "2025-08-25       1.051784\n",
       "2025-08-26       1.066599\n",
       "2025-08-27       1.075976\n",
       "2025-08-28       1.086775\n",
       "2025-08-29       1.082791\n",
       "2025-09-02       1.067722\n",
       "2025-09-03       1.069384\n",
       "2025-09-04       1.077604"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 4] Final portfolio daily return series (pct_change of Step 3).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N_portf_rtn</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2025-08-13</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-14</th>\n",
       "      <td>-0.002124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-15</th>\n",
       "      <td>0.002296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-18</th>\n",
       "      <td>0.020350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-19</th>\n",
       "      <td>0.005398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-20</th>\n",
       "      <td>-0.001144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-21</th>\n",
       "      <td>-0.006320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-22</th>\n",
       "      <td>0.032668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-25</th>\n",
       "      <td>0.000130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-26</th>\n",
       "      <td>0.014085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-27</th>\n",
       "      <td>0.008791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-28</th>\n",
       "      <td>0.010037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-29</th>\n",
       "      <td>-0.003666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-09-02</th>\n",
       "      <td>-0.013917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-09-03</th>\n",
       "      <td>0.001557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-09-04</th>\n",
       "      <td>0.007686</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            N_portf_rtn\n",
       "Date                   \n",
       "2025-08-13          NaN\n",
       "2025-08-14    -0.002124\n",
       "2025-08-15     0.002296\n",
       "2025-08-18     0.020350\n",
       "2025-08-19     0.005398\n",
       "2025-08-20    -0.001144\n",
       "2025-08-21    -0.006320\n",
       "2025-08-22     0.032668\n",
       "2025-08-25     0.000130\n",
       "2025-08-26     0.014085\n",
       "2025-08-27     0.008791\n",
       "2025-08-28     0.010037\n",
       "2025-08-29    -0.003666\n",
       "2025-09-02    -0.013917\n",
       "2025-09-03     0.001557\n",
       "2025-09-04     0.007686"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### 🐛 Detailed Portfolio ATRP Calculation Trace (Value-Weighted)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 5] Individual component ATRP values for each stock.\n",
      "This is the raw risk input for each component.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Ticker</th>\n",
       "      <th>RCL</th>\n",
       "      <th>STIP</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2025-08-13</th>\n",
       "      <td>0.030911</td>\n",
       "      <td>0.001345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-14</th>\n",
       "      <td>0.029561</td>\n",
       "      <td>0.001312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-15</th>\n",
       "      <td>0.028527</td>\n",
       "      <td>0.001316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-18</th>\n",
       "      <td>0.029111</td>\n",
       "      <td>0.001292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-19</th>\n",
       "      <td>0.029510</td>\n",
       "      <td>0.001255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-20</th>\n",
       "      <td>0.030209</td>\n",
       "      <td>0.001247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-21</th>\n",
       "      <td>0.029868</td>\n",
       "      <td>0.001220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-22</th>\n",
       "      <td>0.030322</td>\n",
       "      <td>0.001404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-25</th>\n",
       "      <td>0.029449</td>\n",
       "      <td>0.001380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-26</th>\n",
       "      <td>0.028503</td>\n",
       "      <td>0.001411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-27</th>\n",
       "      <td>0.027809</td>\n",
       "      <td>0.001459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-28</th>\n",
       "      <td>0.026756</td>\n",
       "      <td>0.001397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-29</th>\n",
       "      <td>0.026863</td>\n",
       "      <td>0.001358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-09-02</th>\n",
       "      <td>0.028604</td>\n",
       "      <td>0.001352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-09-03</th>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.001345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-09-04</th>\n",
       "      <td>0.027238</td>\n",
       "      <td>0.001290</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Ticker           RCL      STIP\n",
       "Date                          \n",
       "2025-08-13  0.030911  0.001345\n",
       "2025-08-14  0.029561  0.001312\n",
       "2025-08-15  0.028527  0.001316\n",
       "2025-08-18  0.029111  0.001292\n",
       "2025-08-19  0.029510  0.001255\n",
       "2025-08-20  0.030209  0.001247\n",
       "2025-08-21  0.029868  0.001220\n",
       "2025-08-22  0.030322  0.001404\n",
       "2025-08-25  0.029449  0.001380\n",
       "2025-08-26  0.028503  0.001411\n",
       "2025-08-27  0.027809  0.001459\n",
       "2025-08-28  0.026756  0.001397\n",
       "2025-08-29  0.026863  0.001358\n",
       "2025-09-02  0.028604  0.001352\n",
       "2025-09-03  0.027778  0.001345\n",
       "2025-09-04  0.027238  0.001290"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 6] Drifting daily portfolio weights (based on market value).\n",
      "Note how weights start near equal and drift over time. This is the core of the buy-and-hold logic.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Ticker</th>\n",
       "      <th>STIP</th>\n",
       "      <th>RCL</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2025-08-13</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-14</th>\n",
       "      <td>0.500727</td>\n",
       "      <td>0.499273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-15</th>\n",
       "      <td>0.499239</td>\n",
       "      <td>0.500761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-18</th>\n",
       "      <td>0.489138</td>\n",
       "      <td>0.510862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-19</th>\n",
       "      <td>0.486797</td>\n",
       "      <td>0.513203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-20</th>\n",
       "      <td>0.487688</td>\n",
       "      <td>0.512312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-21</th>\n",
       "      <td>0.490837</td>\n",
       "      <td>0.509163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-22</th>\n",
       "      <td>0.477151</td>\n",
       "      <td>0.522849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-25</th>\n",
       "      <td>0.476764</td>\n",
       "      <td>0.523236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-26</th>\n",
       "      <td>0.470915</td>\n",
       "      <td>0.529085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-27</th>\n",
       "      <td>0.467623</td>\n",
       "      <td>0.532377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-28</th>\n",
       "      <td>0.462976</td>\n",
       "      <td>0.537024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-29</th>\n",
       "      <td>0.464860</td>\n",
       "      <td>0.535140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-09-02</th>\n",
       "      <td>0.470964</td>\n",
       "      <td>0.529036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-09-03</th>\n",
       "      <td>0.470414</td>\n",
       "      <td>0.529586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-09-04</th>\n",
       "      <td>0.466962</td>\n",
       "      <td>0.533038</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Ticker          STIP       RCL\n",
       "Date                          \n",
       "2025-08-13  0.500000  0.500000\n",
       "2025-08-14  0.500727  0.499273\n",
       "2025-08-15  0.499239  0.500761\n",
       "2025-08-18  0.489138  0.510862\n",
       "2025-08-19  0.486797  0.513203\n",
       "2025-08-20  0.487688  0.512312\n",
       "2025-08-21  0.490837  0.509163\n",
       "2025-08-22  0.477151  0.522849\n",
       "2025-08-25  0.476764  0.523236\n",
       "2025-08-26  0.470915  0.529085\n",
       "2025-08-27  0.467623  0.532377\n",
       "2025-08-28  0.462976  0.537024\n",
       "2025-08-29  0.464860  0.535140\n",
       "2025-09-02  0.470964  0.529036\n",
       "2025-09-03  0.470414  0.529586\n",
       "2025-09-04  0.466962  0.533038"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 7] Final value-weighted portfolio ATRP series.\n",
      "Result of (Weights * Component_ATRPs) summed each day.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value_weighted_atrp</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2025-08-13</th>\n",
       "      <td>0.016128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-14</th>\n",
       "      <td>0.015416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-15</th>\n",
       "      <td>0.014943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-18</th>\n",
       "      <td>0.015504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-19</th>\n",
       "      <td>0.015755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-20</th>\n",
       "      <td>0.016085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-21</th>\n",
       "      <td>0.015807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-22</th>\n",
       "      <td>0.016524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-25</th>\n",
       "      <td>0.016067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-26</th>\n",
       "      <td>0.015745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-27</th>\n",
       "      <td>0.015487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-28</th>\n",
       "      <td>0.015015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-29</th>\n",
       "      <td>0.015007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-09-02</th>\n",
       "      <td>0.015769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-09-03</th>\n",
       "      <td>0.015343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-09-04</th>\n",
       "      <td>0.015122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            value_weighted_atrp\n",
       "Date                           \n",
       "2025-08-13             0.016128\n",
       "2025-08-14             0.015416\n",
       "2025-08-15             0.014943\n",
       "2025-08-18             0.015504\n",
       "2025-08-19             0.015755\n",
       "2025-08-20             0.016085\n",
       "2025-08-21             0.015807\n",
       "2025-08-22             0.016524\n",
       "2025-08-25             0.016067\n",
       "2025-08-26             0.015745\n",
       "2025-08-27             0.015487\n",
       "2025-08-28             0.015015\n",
       "2025-08-29             0.015007\n",
       "2025-09-02             0.015769\n",
       "2025-09-03             0.015343\n",
       "2025-09-04             0.015122"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### A. Group Portfolio Verification"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Full Period"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Mean Daily Return: 0.005055\n",
      "  - Mean Daily ATRP:  0.015607\n",
      "  - Sharpe (ATR) = (Mean Return / Mean ATRP) = 0.3239\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Calculation Period"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Mean Daily Return: 0.007413\n",
      "  - Mean Daily ATRP:  0.015769\n",
      "  - Sharpe (ATR) = (Mean Return / Mean ATRP) = 0.4701\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Forward Period"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Mean Daily Return: 0.000339\n",
      "  - Mean Daily ATRP:  0.015251\n",
      "  - Sharpe (ATR) = (Mean Return / Mean ATRP) = 0.0223\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### B. Benchmark (VOO) Verification"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Full Period"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Mean Daily Return: 0.000438\n",
      "  - Mean Daily ATRP:  0.008277\n",
      "  - Sharpe (ATR) = (Mean Return / Mean ATRP) = 0.0529\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Calculation Period"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Mean Daily Return: 0.000283\n",
      "  - Mean Daily ATRP:  0.008317\n",
      "  - Sharpe (ATR) = (Mean Return / Mean ATRP) = 0.0340\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Forward Period"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Mean Daily Return: 0.000748\n",
      "  - Mean Daily ATRP:  0.008191\n",
      "  - Sharpe (ATR) = (Mean Return / Mean ATRP) = 0.0913\n"
     ]
    }
   ],
   "source": [
    "# Now call the verification function with the exact parameters from the UI\n",
    "verify_sharpe_atr_calculation_checked(\n",
    "    df_ohlcv=df_ohlcv,\n",
    "    features_df=features_df,\n",
    "    tickers_to_verify=tickers_to_verify, # <-- From the UI output\n",
    "    benchmark_ticker=benchmark_ticker,\n",
    "    start_date=start_date,\n",
    "    calc_period=10,\n",
    "    fwd_period=5,\n",
    "    debug=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89540b88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13650d09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cf510b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ==============================================================================\n",
    "# # GOLDEN COPY - COMPLETE PROJECT CODE (All Fixes Included)\n",
    "# # Version: Verified Portfolio and Benchmark Sharpe (ATR)  \n",
    "# # Date: 2025-10-15\n",
    "# # ==============================================================================\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import plotly.graph_objects as go\n",
    "# import ipywidgets as widgets\n",
    "# import time\n",
    "# import pprint\n",
    "# import os # Make sure os is imported for the export function later\n",
    "# import re\n",
    "\n",
    "# from datetime import datetime, date\n",
    "# from IPython.display import display, Markdown\n",
    "# from tqdm.auto import tqdm\n",
    "# from pathlib import Path\n",
    "# from itertools import product\n",
    "\n",
    "\n",
    "# pd.set_option('display.max_rows', 30)\n",
    "# pd.set_option('display.max_columns', 50)\n",
    "# pd.set_option('display.width', 3000)\n",
    "\n",
    "\n",
    "# # --- REFACTORING PHASE 1 CODE: Feature Generation Engine ---\n",
    "\n",
    "# def generate_features(df_ohlcv: pd.DataFrame, \n",
    "#                       atr_period: int = 14, \n",
    "#                       quality_window: int = 252, \n",
    "#                       quality_min_periods: int = 126) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     Generates a comprehensive DataFrame of derived features from raw OHLCV data.\n",
    "\n",
    "#     This function performs all heavy, window-based calculations upfront to be used\n",
    "#     by downstream analysis functions. It calculates:\n",
    "#     1. Technical Indicators: True Range (TR), ATR, and ATRP.\n",
    "#     2. Data Quality Metrics: Rolling stale percentage, median dollar volume, etc.\n",
    "\n",
    "#     Args:\n",
    "#         df_ohlcv: The primary DataFrame with a (Ticker, Date) MultiIndex and\n",
    "#                   columns for 'Adj High', 'Adj Low', 'Adj Close', 'Volume'.\n",
    "#         atr_period: The lookback period for the ATR's Exponential Moving Average.\n",
    "#         quality_window: The rolling window size for data quality metrics.\n",
    "#         quality_min_periods: The minimum number of observations required to have\n",
    "#                              a valid quality metric.\n",
    "\n",
    "#     Returns:\n",
    "#         A new DataFrame with the same (Ticker, Date) MultiIndex containing all\n",
    "#         calculated feature columns.\n",
    "#     \"\"\"\n",
    "#     print(\"--- Starting Feature Generation ---\")\n",
    "    \n",
    "#     # Ensure the DataFrame is sorted for correct window and shift operations\n",
    "#     # FIX: Replaced is_lexsorted() with the current pandas attribute\n",
    "#     if not df_ohlcv.index.is_monotonic_increasing:\n",
    "#         print(\"Sorting index for calculation accuracy...\")\n",
    "#         df_ohlcv = df_ohlcv.sort_index()\n",
    "\n",
    "#     # --- 1. Technical Indicator Calculation (TR, ATR, ATRP) ---\n",
    "#     print(f\"Calculating technical indicators (ATR Period: {atr_period})...\")\n",
    "    \n",
    "#     # Group by ticker to handle each security independently\n",
    "#     grouped = df_ohlcv.groupby(level='Ticker')\n",
    "    \n",
    "#     # Get the previous day's close required for True Range\n",
    "#     prev_close = grouped['Adj Close'].shift(1)\n",
    "    \n",
    "#     # Calculate the three components of True Range\n",
    "#     high_low = df_ohlcv['Adj High'] - df_ohlcv['Adj Low']\n",
    "#     high_prev_close = abs(df_ohlcv['Adj High'] - prev_close)\n",
    "#     low_prev_close = abs(df_ohlcv['Adj Low'] - prev_close)\n",
    "    \n",
    "#     # Combine the components to get the final TR\n",
    "#     tr = pd.concat([high_low, high_prev_close, low_prev_close], axis=1).max(axis=1, skipna=False)\n",
    "    \n",
    "#     # # Calculate the ATR using an Exponential Moving Average\n",
    "#     # --- FIX IS HERE ---\n",
    "#     # Use .transform() to apply the EWM function. \n",
    "#     # This guarantees the resulting Series has the exact same index as 'tr',\n",
    "#     # preventing the index alignment error during the subsequent division.\n",
    "#     atr = tr.groupby(level='Ticker').transform(\n",
    "#         lambda x: x.ewm(alpha=1/atr_period, adjust=False).mean()\n",
    "#     )\n",
    "\n",
    "#     # --- CHANGE 1: Removed .fillna(0) ---\n",
    "#     # ATRP will now be NaN on the first day, consistent with TR and ATR.\n",
    "#     atrp = (atr / df_ohlcv['Adj Close']).replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "#     indicator_df = pd.DataFrame({\n",
    "#         'TR': tr,\n",
    "#         'ATR': atr,\n",
    "#         'ATRP': atrp\n",
    "#     })\n",
    "    \n",
    "#     # --- 2. Data Quality Metric Calculation ---\n",
    "#     print(f\"Calculating data quality metrics (Window: {quality_window} days)...\")\n",
    "    \n",
    "#     # Create intermediate flags needed for quality calculations\n",
    "#     is_stale = np.where((df_ohlcv['Volume'] == 0) | (df_ohlcv['Adj High'] == df_ohlcv['Adj Low']), 1, 0)\n",
    "#     dollar_volume = df_ohlcv['Adj Close'] * df_ohlcv['Volume']\n",
    "#     has_same_volume = (grouped['Volume'].diff() == 0).astype(int)\n",
    "    \n",
    "#     # Combine flags into a temporary DataFrame for rolling calculations\n",
    "#     quality_temp_df = pd.DataFrame({\n",
    "#         'IsStale': is_stale,\n",
    "#         'DollarVolume': dollar_volume,\n",
    "#         'HasSameVolume': has_same_volume\n",
    "#     }, index=df_ohlcv.index) # Explicitly set index to be safe\n",
    "    \n",
    "#     # Perform the rolling calculations on the grouped data\n",
    "#     # --- FIX IS HERE ---\n",
    "#     # We switch to the older, more compatible dictionary-based aggregation method.\n",
    "#     # This syntax is understood by nearly all versions of pandas.\n",
    "#     rolling_result = quality_temp_df.groupby(level='Ticker').rolling(\n",
    "#         window=quality_window,\n",
    "#         min_periods=quality_min_periods\n",
    "#     ).agg({\n",
    "#         'IsStale': 'mean',\n",
    "#         'DollarVolume': 'median',\n",
    "#         'HasSameVolume': 'sum'\n",
    "#     })\n",
    "    \n",
    "#     # The dictionary syntax produces columns with the original names ('IsStale', etc.).\n",
    "#     # We now explicitly rename them to our desired final names.\n",
    "#     rolling_result = rolling_result.rename(columns={\n",
    "#         'IsStale': 'RollingStalePct',\n",
    "#         'DollarVolume': 'RollMedDollarVol', # <-- RENAMED HERE\n",
    "#         'HasSameVolume': 'RollingSameVolCount'\n",
    "#     })\n",
    "\n",
    "#     # The index after a grouped rolling operation is hierarchical.\n",
    "#     # We remove the outermost 'Ticker' level to restore the original index structure.\n",
    "#     rolling_quality = rolling_result.reset_index(level=0, drop=True)\n",
    "\n",
    "#     # --- 3. Combine All Features ---\n",
    "#     print(\"Combining all feature sets...\")\n",
    "#     features_df = pd.concat([indicator_df, rolling_quality], axis=1)\n",
    "    \n",
    "#     print(\"✅ Feature generation complete.\")\n",
    "#     return features_df\n",
    "\n",
    "# def test_features_df(features_df: pd.DataFrame, \n",
    "#                      df_ohlcv: pd.DataFrame, \n",
    "#                      test_ticker: str = 'AAPL',\n",
    "#                      spot_check_date: str = '2020-03-20'):\n",
    "#     \"\"\"\n",
    "#     Runs a suite of tests to verify the correctness of the generated features_df.\n",
    "\n",
    "#     Args:\n",
    "#         features_df: The generated DataFrame from the generate_features function.\n",
    "#         df_ohlcv: The original source OHLCV DataFrame.\n",
    "#         test_ticker: A common, liquid ticker to use for specific value checks.\n",
    "#     \"\"\"\n",
    "#     print(f\"\\n--- Running Verification Suite for features_df (Test Ticker: {test_ticker}) ---\")\n",
    "    \n",
    "#     # --- Test 1: Structural Integrity ---\n",
    "#     print(\"\\n[Test 1: Structural Integrity]\")\n",
    "#     assert features_df.index.equals(df_ohlcv.index), \"FAIL: Index does not match original df_ohlcv.\"\n",
    "#     print(\"  ✅ PASS: Index matches original df_ohlcv.\")\n",
    "    \n",
    "#     expected_cols = ['TR', 'ATR', 'ATRP', 'RollingStalePct', 'RollMedDollarVol', 'RollingSameVolCount']\n",
    "#     assert all(col in features_df.columns for col in expected_cols), \"FAIL: Missing one or more expected columns.\"\n",
    "#     print(\"  ✅ PASS: All expected feature columns are present.\")\n",
    "#     print(f\"  - DataFrame Info:\")\n",
    "#     features_df.info(verbose=False, memory_usage='deep')\n",
    "\n",
    "\n",
    "#     # --- Test 2: ATR Calculation Logic ---\n",
    "#     print(\"\\n[Test 2: ATR Logic Verification]\")\n",
    "#     ticker_features = features_df.loc[test_ticker]\n",
    "    \n",
    "#     # Test 2a: First TR value should be NaN (since prev_close is NaN)\n",
    "#     first_tr = ticker_features['TR'].iloc[0]\n",
    "#     assert pd.isna(first_tr), f\"FAIL: First TR value for {test_ticker} should be NaN, but got {first_tr}.\"\n",
    "#     print(f\"  ✅ PASS: First TR value for {test_ticker} is NaN as expected.\")\n",
    "\n",
    "#     # Test 2b: The first valid ATR should equal the first valid TR (EWM cold start behavior)\n",
    "#     first_valid_tr_val = ticker_features['TR'].dropna().iloc[0]\n",
    "#     first_valid_atr_val = ticker_features['ATR'].dropna().iloc[0]\n",
    "#     assert np.isclose(first_valid_tr_val, first_valid_atr_val), \\\n",
    "#         f\"FAIL: First valid ATR ({first_valid_atr_val}) should equal first valid TR ({first_valid_tr_val}).\"\n",
    "#     print(\"  ✅ PASS: First valid ATR correctly seeded with first valid TR.\")\n",
    "\n",
    "\n",
    "#     # --- Test 3: Rolling Quality Metrics Logic ---\n",
    "#     print(\"\\n[Test 3: Rolling Quality Metrics Logic Verification]\")\n",
    "#     quality_min_periods = 126 # Should match the parameter used in generation\n",
    "    \n",
    "#     # Test 3a: Check for leading NaNs\n",
    "#     first_valid_quality_idx = ticker_features['RollingStalePct'].first_valid_index()\n",
    "#     if first_valid_quality_idx is None:\n",
    "#         print(f\"  - INFO: No valid quality metrics found for {test_ticker} (likely too little data). Skipping test.\")\n",
    "#     else:\n",
    "#         position_of_first_valid = ticker_features.index.get_loc(first_valid_quality_idx)\n",
    "#         assert position_of_first_valid == quality_min_periods - 1, \\\n",
    "#             f\"FAIL: First valid quality metric should appear at index {quality_min_periods - 1}, but appeared at {position_of_first_valid}.\"\n",
    "#         print(f\"  ✅ PASS: Leading NaNs are present for the first {quality_min_periods - 1} periods as expected.\")\n",
    "\n",
    "\n",
    "#     # --- Test 4: Spot Check Against Manual Calculation ---\n",
    "#     print(\"\\n[Test 4: Spot Check vs. Manual Calculation]\")\n",
    "#     # # Choose a specific date for a manual calculation\n",
    "#     # spot_check_date = '2020-03-20' # A volatile day for a good test\n",
    "    \n",
    "#     # Manual TR Calculation\n",
    "#     today_data = df_ohlcv.loc[(test_ticker, spot_check_date)]\n",
    "#     yesterday_data = df_ohlcv.loc[(test_ticker, pd.to_datetime(spot_check_date) - pd.Timedelta(days=1))] # simple lookback for test\n",
    "    \n",
    "#     manual_h_l = today_data['Adj High'] - today_data['Adj Low']\n",
    "#     manual_h_pc = abs(today_data['Adj High'] - yesterday_data['Adj Close'])\n",
    "#     manual_l_pc = abs(today_data['Adj Low'] - yesterday_data['Adj Close'])\n",
    "#     manual_tr = max(manual_h_l, manual_h_pc, manual_l_pc)\n",
    "    \n",
    "#     code_tr = ticker_features.loc[spot_check_date]['TR']\n",
    "    \n",
    "#     assert np.isclose(manual_tr, code_tr), f\"FAIL: Manual TR ({manual_tr:.4f}) does not match code TR ({code_tr:.4f}) on {spot_check_date}.\"\n",
    "#     print(f\"  ✅ PASS: Manually calculated TR on {spot_check_date} matches code's TR.\")\n",
    "    \n",
    "#     print(\"\\n--- ✅ All Verification Tests Passed ---\")\n",
    "\n",
    "# def export_ticker_data(ticker_to_export: str, \n",
    "#                          df_ohlcv: pd.DataFrame, \n",
    "#                          features_df: pd.DataFrame, \n",
    "#                          output_dir: str = 'export_csv'):\n",
    "#     \"\"\"\n",
    "#     Exports the raw OHLCV data and the corresponding calculated features for a \n",
    "#     single ticker to two separate CSV files.\n",
    "\n",
    "#     This function is designed for easy manual verification of data and calculations.\n",
    "#     It will create the output directory if it does not exist.\n",
    "\n",
    "#     Args:\n",
    "#         ticker_to_export: The ticker symbol to export (e.g., 'AAPL').\n",
    "#         df_ohlcv: The main DataFrame containing the raw OHLCV data with a \n",
    "#                   (Ticker, Date) MultiIndex.\n",
    "#         features_df: The DataFrame containing the calculated features with a \n",
    "#                      (Ticker, Date) MultiIndex.\n",
    "#         output_dir: The directory where the CSV files will be saved. \n",
    "#                     Defaults to 'export_csv'.\n",
    "#     \"\"\"\n",
    "#     print(f\"--- Attempting to export data for ticker: {ticker_to_export} ---\")\n",
    "    \n",
    "#     # --- 1. Ensure the output directory exists ---\n",
    "#     try:\n",
    "#         os.makedirs(output_dir, exist_ok=True)\n",
    "#         print(f\"Output directory '{output_dir}' is ready.\")\n",
    "#     except OSError as e:\n",
    "#         print(f\"Error: Could not create directory '{output_dir}'. Reason: {e}\")\n",
    "#         return\n",
    "\n",
    "#     # --- 2. Isolate the data for the specified ticker ---\n",
    "#     try:\n",
    "#         # Use .loc to select all rows for the given ticker from the MultiIndex\n",
    "#         ticker_ohlcv = df_ohlcv.loc[ticker_to_export]\n",
    "#         ticker_features = features_df.loc[ticker_to_export]\n",
    "        \n",
    "#         if ticker_ohlcv.empty:\n",
    "#             print(f\"Warning: No OHLCV data found for ticker '{ticker_to_export}'. Cannot export.\")\n",
    "#             return\n",
    "            \n",
    "#         print(f\"Found {len(ticker_ohlcv)} rows of data for '{ticker_to_export}'.\")\n",
    "        \n",
    "#     except KeyError:\n",
    "#         print(f\"Error: Ticker '{ticker_to_export}' not found in one or both of the DataFrames. Please check the symbol.\")\n",
    "#         return\n",
    "#     except Exception as e:\n",
    "#         print(f\"An unexpected error occurred while accessing data: {e}\")\n",
    "#         return\n",
    "\n",
    "#     # --- 3. Construct file paths and export to CSV ---\n",
    "#     try:\n",
    "#         # Define the full path for each output file\n",
    "#         ohlcv_filename = f\"{ticker_to_export}_ohlcv.csv\"\n",
    "#         features_filename = f\"{ticker_to_export}_features.csv\"\n",
    "        \n",
    "#         ohlcv_filepath = os.path.join(output_dir, ohlcv_filename)\n",
    "#         features_filepath = os.path.join(output_dir, features_filename)\n",
    "        \n",
    "#         # Export the DataFrames to CSV. The index (Date) will be included.\n",
    "#         ticker_ohlcv.to_csv(ohlcv_filepath)\n",
    "#         ticker_features.to_csv(features_filepath)\n",
    "        \n",
    "#         print(\"\\n✅ Export successful!\")\n",
    "#         print(f\"   - Raw OHLCV data saved to: {ohlcv_filepath}\")\n",
    "#         print(f\"   - Calculated features saved to: {features_filepath}\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error: Failed to write data to CSV files. Reason: {e}\")\n",
    "\n",
    "# def create_synthetic_ticker_data(\n",
    "#     ticker_name: str = 'SYNTH', \n",
    "#     num_days: int = 50,\n",
    "#     num_zero_volume_days: int = 5,\n",
    "#     num_flat_price_days: int = 3\n",
    "# ) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     Creates a synthetic OHLCV DataFrame with predictable patterns and randomly injected\n",
    "#     stale data conditions for robust testing.\n",
    "\n",
    "#     Args:\n",
    "#         ticker_name: The name for the synthetic ticker.\n",
    "#         num_days: The total number of days for the ticker's history.\n",
    "#         num_zero_volume_days: The number of random days to set Volume to 0.\n",
    "#         num_flat_price_days: The number of random days to set High == Low.\n",
    "\n",
    "#     Returns:\n",
    "#         A pandas DataFrame with a (Ticker, Date) MultiIndex.\n",
    "#     \"\"\"\n",
    "#     print(f\"--- Creating synthetic data for '{ticker_name}' with {num_days} days ---\")\n",
    "    \n",
    "#     # 1. Create a base DataFrame with \"normal\" data\n",
    "#     dates = pd.to_datetime(pd.date_range(start='2023-01-01', periods=num_days, freq='B'))\n",
    "#     data = {\n",
    "#         'Adj Open': 100.0, 'Adj High': 102.0, 'Adj Low': 98.0,\n",
    "#         'Adj Close': 100.0, 'Volume': 1_000_000\n",
    "#     }\n",
    "#     df = pd.DataFrame(data, index=dates)\n",
    "#     df['Adj Close'] = df['Adj Close'] + np.random.randn(num_days) * 0.5 # Add some noise\n",
    "\n",
    "#     # 2. Define a \"protected\" window for specific verification tests.\n",
    "#     # The `verify_synthetic_ticker_features` function depends on this exact window.\n",
    "#     # We will not inject random stale days here.\n",
    "#     protected_start_idx, protected_end_idx = 10, 20\n",
    "    \n",
    "#     # 3. Inject random \"stale\" days OUTSIDE the protected window\n",
    "#     available_indices = df.index.drop(df.index[protected_start_idx:protected_end_idx])\n",
    "    \n",
    "#     # Inject zero-volume days\n",
    "#     if num_zero_volume_days > 0:\n",
    "#         if len(available_indices) < num_zero_volume_days:\n",
    "#             raise ValueError(\"Not enough available days to inject zero-volume days.\")\n",
    "#         zero_vol_dates = np.random.choice(available_indices, num_zero_volume_days, replace=False)\n",
    "#         df.loc[zero_vol_dates, 'Volume'] = 0\n",
    "#         print(f\"  - Injected {num_zero_volume_days} random zero-volume 'stale' days.\")\n",
    "#         # Update available indices to avoid overlap\n",
    "#         available_indices = available_indices.drop(zero_vol_dates)\n",
    "\n",
    "#     # Inject flat-price days (High == Low)\n",
    "#     if num_flat_price_days > 0:\n",
    "#         if len(available_indices) < num_flat_price_days:\n",
    "#             raise ValueError(\"Not enough available days to inject flat-price days.\")\n",
    "#         flat_price_dates = np.random.choice(available_indices, num_flat_price_days, replace=False)\n",
    "#         # Set High and Low to be the same as the Close price for that day\n",
    "#         df.loc[flat_price_dates, 'Adj High'] = df.loc[flat_price_dates, 'Adj Close']\n",
    "#         df.loc[flat_price_dates, 'Adj Low'] = df.loc[flat_price_dates, 'Adj Close']\n",
    "#         print(f\"  - Injected {num_flat_price_days} random flat-price 'stale' days.\")\n",
    "\n",
    "#     # 4. Inject the specific, hand-crafted patterns inside the protected window for verification\n",
    "#     print(\"  - Injecting specific patterns for programmatic verification...\")\n",
    "#     # Pattern for RollingStalePct: 2 stale days in 10 (20%)\n",
    "#     df.iloc[10, df.columns.get_loc('Volume')] = 0  # Stale day (zero volume)\n",
    "#     df.iloc[11, df.columns.get_loc('Adj High')] = 99.0 # Stale day (High == Low)\n",
    "#     df.iloc[11, df.columns.get_loc('Adj Low')] = 99.0\n",
    "    \n",
    "#     # Pattern for RollingMedianVolume\n",
    "#     for i in range(10):\n",
    "#         df.iloc[10 + i, df.columns.get_loc('Adj Close')] = 100.0 # Standardize price for easy median calc\n",
    "#         df.iloc[10 + i, df.columns.get_loc('Volume')] = (i + 1) * 10000\n",
    "\n",
    "#     # Pattern for RollingSameVolCount\n",
    "#     df.iloc[15, df.columns.get_loc('Volume')] = 77777\n",
    "#     df.iloc[16, df.columns.get_loc('Volume')] = 77777\n",
    "#     df.iloc[17, df.columns.get_loc('Volume')] = 77777\n",
    "    \n",
    "#     # 5. Set the MultiIndex\n",
    "#     df['Ticker'] = ticker_name\n",
    "#     df = df.set_index(['Ticker', df.index])\n",
    "#     df.index.names = ['Ticker', 'Date']\n",
    "    \n",
    "#     print(\"✅ Synthetic data created successfully.\")\n",
    "#     return df\n",
    "\n",
    "# def verify_synthetic_ticker_features(features_df: pd.DataFrame, \n",
    "#                                        ticker_name: str = 'SYNTH',\n",
    "#                                        quality_window: int = 10):\n",
    "#     \"\"\"\n",
    "#     Verifies the quality metric calculations on the features_df generated from\n",
    "#     the synthetic ticker data.\n",
    "\n",
    "#     Args:\n",
    "#         features_df: The DataFrame of calculated features.\n",
    "#         ticker_name: The name of the synthetic ticker.\n",
    "#         quality_window: The rolling window used, which must match the window\n",
    "#                         of the synthetic data pattern.\n",
    "#     \"\"\"\n",
    "#     print(f\"\\n--- Running Verification on Synthetic Ticker '{ticker_name}' ---\")\n",
    "    \n",
    "#     # --- Expected values based on our synthetic data design ---\n",
    "#     EXPECTED_STALE_PCT = 0.20  # 2 stale days out of 10\n",
    "#     EXPECTED_MEDIAN_DOLLAR_VOL = 5_500_000.0 # median of (10k..100k) * price of 100\n",
    "#     EXPECTED_SAME_VOL_COUNT = 2.0 # Three consecutive days gives two 'diff() == 0' events\n",
    "\n",
    "#     try:\n",
    "#         # Isolate the features for our synthetic ticker\n",
    "#         ticker_features = features_df.loc[ticker_name]\n",
    "        \n",
    "#         # The first valid calculation will be on the last day of our 10-day window.\n",
    "#         # The window starts at index 10 and has a length of 10, so it ends at index 19.\n",
    "#         verification_date = ticker_features.index[19]\n",
    "        \n",
    "#         print(f\"Verifying calculations on date: {verification_date.date()}\")\n",
    "        \n",
    "#         # Get the calculated values from the DataFrame\n",
    "#         calculated_values = ticker_features.loc[verification_date]\n",
    "#         stale_pct = calculated_values['RollingStalePct']\n",
    "#         # --- CHANGE 2 (continued): Accessing the renamed column ---\n",
    "#         median_vol = calculated_values['RollMedDollarVol'] \n",
    "#         same_vol_count = calculated_values['RollingSameVolCount']\n",
    "        \n",
    "#         # --- Perform Assertions ---\n",
    "#         print(\"\\n[Test 1: RollingStalePct]\")\n",
    "#         assert np.isclose(stale_pct, EXPECTED_STALE_PCT), f\"FAIL: Expected {EXPECTED_STALE_PCT}, Got {stale_pct}\"\n",
    "#         print(f\"  ✅ PASS: Expected {EXPECTED_STALE_PCT}, Got {stale_pct}\")\n",
    "\n",
    "#         print(\"\\n[Test 2: RollMedDollarVol]\")\n",
    "#         assert np.isclose(median_vol, EXPECTED_MEDIAN_DOLLAR_VOL), f\"FAIL: Expected {EXPECTED_MEDIAN_DOLLAR_VOL}, Got {median_vol}\"\n",
    "#         print(f\"  ✅ PASS: Expected {EXPECTED_MEDIAN_DOLLAR_VOL}, Got {median_vol}\")\n",
    "\n",
    "#         print(\"\\n[Test 3: RollingSameVolCount]\")\n",
    "#         assert np.isclose(same_vol_count, EXPECTED_SAME_VOL_COUNT), f\"FAIL: Expected {EXPECTED_SAME_VOL_COUNT}, Got {same_vol_count}\"\n",
    "#         print(f\"  ✅ PASS: Expected {EXPECTED_SAME_VOL_COUNT}, Got {same_vol_count}\")\n",
    "\n",
    "#         print(\"\\n--- ✅ All Synthetic Data Verification Tests Passed ---\")\n",
    "\n",
    "#     except KeyError:\n",
    "#         print(f\"FAIL: Ticker '{ticker_name}' not found in features_df.\")\n",
    "#     except IndexError:\n",
    "#         print(\"FAIL: Not enough data in features_df to run verification. Check num_days.\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"An unexpected error occurred during verification: {e}\")\n",
    "\n",
    "# # --- A. HELPER FUNCTIONS ---\n",
    "\n",
    "# def calculate_gain(price_series: pd.Series):\n",
    "#     \"\"\"Calculates the total gain over a series of prices.\"\"\"\n",
    "#     # Ensure there are at least two data points to calculate a gain\n",
    "#     if price_series.dropna().shape[0] < 2: return np.nan\n",
    "#     # Use forward-fill for the end price and back-fill for the start price\n",
    "#     # to handle potential NaNs at the beginning or end of the series.\n",
    "#     return (price_series.ffill().iloc[-1] / price_series.bfill().iloc[0]) - 1\n",
    "\n",
    "# def calculate_sharpe(return_series: pd.Series):\n",
    "#     \"\"\"Calculates the annualized Sharpe ratio from a series of daily returns.\"\"\"\n",
    "#     # Ensure there are at least two returns to calculate a standard deviation\n",
    "#     if return_series.dropna().shape[0] < 2: return np.nan\n",
    "#     std_dev = return_series.std()\n",
    "#     # Avoid division by zero if returns are constant\n",
    "#     if std_dev > 0 and std_dev != np.inf:\n",
    "#         return (return_series.mean() / std_dev) * np.sqrt(252)\n",
    "#     return np.nan\n",
    "\n",
    "# def calculate_sharpe_atr(return_series: pd.Series, atrp_series: pd.Series):\n",
    "#     \"\"\"Calculates a Sharpe-like ratio using mean ATRP as the denominator.\"\"\"\n",
    "#     # Ensure there are returns and that ATRP data is valid\n",
    "#     if return_series.dropna().shape[0] < 2 or atrp_series.dropna().empty:\n",
    "#         return np.nan\n",
    "        \n",
    "#     mean_return = return_series.mean()\n",
    "#     mean_atrp = atrp_series.mean()\n",
    "    \n",
    "#     # Avoid division by zero\n",
    "#     if mean_atrp > 0 and mean_atrp != np.inf:\n",
    "#         return mean_return / mean_atrp\n",
    "        \n",
    "#     return np.nan\n",
    "\n",
    "# def print_nested(d, indent=0, width=4):\n",
    "#     \"\"\"Pretty-print any nested dict/list/tuple combination.\"\"\"\n",
    "#     spacing = ' ' * indent\n",
    "#     if isinstance(d, dict):\n",
    "#         for k, v in d.items():\n",
    "#             print(f'{spacing}{k}:')\n",
    "#             print_nested(v, indent + width, width)\n",
    "#     elif isinstance(d, (list, tuple)):\n",
    "#         for item in d:\n",
    "#             print_nested(item, indent, width)\n",
    "#     else:\n",
    "#         print(f'{spacing}{d}')\n",
    "\n",
    "# # --- B. MODULAR METRIC CALCULATION ENGINE ---\n",
    "\n",
    "# def calculate_price_metric(metric_data: dict) -> pd.Series:\n",
    "#     \"\"\"\n",
    "#     Calculates the 'Price' metric (total gain) over the calculation period.\n",
    "\n",
    "#     Args:\n",
    "#         metric_data: A dictionary containing pre-calculated data Series.\n",
    "#                      Requires 'calc_close' (pd.DataFrame): The close prices for the calc period.\n",
    "\n",
    "#     Returns:\n",
    "#         A pandas Series of the calculated metric values, indexed by Ticker.\n",
    "#     \"\"\"\n",
    "#     calc_close = metric_data['calc_close']\n",
    "    \n",
    "#     # Ensure there are at least two data points to calculate a gain\n",
    "#     if len(calc_close) < 2:\n",
    "#         return pd.Series(dtype='float64', index=calc_close.columns)\n",
    "\n",
    "#     first_prices = calc_close.bfill().iloc[0]\n",
    "#     last_prices = calc_close.ffill().iloc[-1]\n",
    "    \n",
    "#     # The division of two Series aligns by index (Ticker), which is what we want.\n",
    "#     price_metric = last_prices / first_prices\n",
    "    \n",
    "#     return price_metric.dropna()\n",
    "\n",
    "# def calculate_sharpe_metric(metric_data: dict) -> pd.Series:\n",
    "#     \"\"\"\n",
    "#     Calculates the annualized 'Sharpe' ratio metric over the calculation period.\n",
    "\n",
    "#     Args:\n",
    "#         metric_data: A dictionary containing pre-calculated data Series.\n",
    "#                      Requires 'daily_returns' (pd.DataFrame): Daily returns for the calc period.\n",
    "\n",
    "#     Returns:\n",
    "#         A pandas Series of the calculated metric values, indexed by Ticker.\n",
    "#     \"\"\"\n",
    "#     daily_returns = metric_data['daily_returns']\n",
    "    \n",
    "#     # Ensure there's enough data to calculate standard deviation\n",
    "#     if len(daily_returns.dropna()) < 2:\n",
    "#         return pd.Series(dtype='float64', index=daily_returns.columns)\n",
    "    \n",
    "#     mean_returns = daily_returns.mean()\n",
    "#     std_returns = daily_returns.std()\n",
    "    \n",
    "#     # Standard annualized Sharpe Ratio calculation. Avoid division by zero.\n",
    "#     # We replace resulting NaNs/infs with 0 to handle cases of zero volatility.\n",
    "#     sharpe_ratio = (mean_returns / std_returns * np.sqrt(252))\n",
    "    \n",
    "#     return sharpe_ratio.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "    \n",
    "# def calculate_sharpe_atr_metric(metric_data: dict) -> pd.Series:\n",
    "#     \"\"\"\n",
    "#     Calculates the 'Sharpe (ATR)' metric over the calculation period.\n",
    "\n",
    "#     Args:\n",
    "#         metric_data: A dictionary containing pre-calculated data Series.\n",
    "#                      Requires 'daily_returns' (pd.DataFrame): Daily returns for the calc period.\n",
    "#                      Requires 'atrp' (pd.Series): Mean ATRP for each ticker over the period.\n",
    "\n",
    "#     Returns:\n",
    "#         A pandas Series of the calculated metric values, indexed by Ticker.\n",
    "#     \"\"\"\n",
    "#     daily_returns = metric_data['daily_returns']\n",
    "#     atrp = metric_data['atrp']\n",
    "    \n",
    "#     mean_returns = daily_returns.mean()\n",
    "    \n",
    "#     # ATRP-based Sharpe. Avoid division by zero.\n",
    "#     # We replace resulting NaNs/infs with 0.\n",
    "#     sharpe_atr = mean_returns / atrp\n",
    "    \n",
    "#     return sharpe_atr.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "# # The single source of truth for all available ranking metrics.\n",
    "# # Maps the user-facing name to the calculation function.\n",
    "# METRIC_REGISTRY = {\n",
    "#     'Price': calculate_price_metric,\n",
    "#     'Sharpe': calculate_sharpe_metric,\n",
    "#     'Sharpe (ATR)': calculate_sharpe_atr_metric,\n",
    "# }\n",
    "\n",
    "# print(\"✅ Metric Registry Initialized with:\", list(METRIC_REGISTRY.keys()))\n",
    "\n",
    "# # --- B. THE CORE CALCULATION ENGINE ---\n",
    "\n",
    "# def run_walk_forward_step(df_close_full, df_high_full, df_low_full,\n",
    "#                           master_trading_days,\n",
    "#                           start_date, calc_period, fwd_period,\n",
    "#                           metric, rank_start, rank_end, benchmark_ticker,\n",
    "#                           features_df,\n",
    "#                           debug=False):\n",
    "#     \"\"\"\n",
    "#     Runs a single step of the walk-forward analysis with a strict, pre-emptive\n",
    "#     check to ensure the full period is available.\n",
    "#     \"\"\"\n",
    "#     debug_data = {} if debug else None\n",
    "\n",
    "#     # 1. Determine exact date ranges with a NEW pre-emptive check\n",
    "#     try:\n",
    "#         start_idx = master_trading_days.get_loc(start_date)\n",
    "#     except KeyError:\n",
    "#         return ({'error': f\"Start date {start_date.date()} is not a valid trading day.\"}, None)\n",
    "\n",
    "#     # +++ THIS IS THE NEW PRE-EMPTIVE CHECK LOGIC +++\n",
    "#     # Calculate the desired end index without clamping first.\n",
    "#     desired_viz_end_idx = start_idx + calc_period + fwd_period\n",
    "    \n",
    "#     # Check if the desired end index is out of bounds.\n",
    "#     if desired_viz_end_idx >= len(master_trading_days):\n",
    "#         last_available_date = master_trading_days[-1].date()\n",
    "#         required_days = calc_period + fwd_period\n",
    "#         available_days = len(master_trading_days) - start_idx\n",
    "#         error_msg = (f\"Not enough data for the full requested period. \"\n",
    "#                      f\"Required: {required_days} days, Available: {available_days} days until {last_available_date}.\")\n",
    "#         return ({'error': error_msg}, None)\n",
    "#     # --- END OF NEW CHECK ---\n",
    "\n",
    "#     # If the check passes, we know the full period is available.\n",
    "#     # The 'min' calls are now just a redundant safety measure.\n",
    "#     calc_end_idx = min(start_idx + calc_period, len(master_trading_days) - 1)\n",
    "#     viz_end_idx = min(calc_end_idx + fwd_period, len(master_trading_days) - 1)\n",
    "\n",
    "#     safe_start_date = master_trading_days[start_idx]\n",
    "#     safe_calc_end_date = master_trading_days[calc_end_idx]\n",
    "#     safe_viz_end_date = master_trading_days[viz_end_idx]\n",
    "    \n",
    "#     if safe_start_date >= safe_calc_end_date:\n",
    "#         return ({'error': \"Invalid date range (calc period has zero or negative length).\"}, None)\n",
    "\n",
    "#     # (The rest of the function remains completely unchanged...)\n",
    "#     # 2. Slice data for the calculation period\n",
    "#     calc_close_raw = df_close_full.loc[safe_start_date:safe_calc_end_date]\n",
    "#     calc_close = calc_close_raw.dropna(axis=1, how='all')\n",
    "#     if calc_close.shape[1] == 0 or len(calc_close) < 2:\n",
    "#         return ({'error': \"Not enough data in calc period.\"}, None)\n",
    "\n",
    "#     # 3. Calculate INTERMEDIATE data required for the ranking metrics\n",
    "#     daily_returns = calc_close.bfill().ffill().pct_change()\n",
    "#     valid_tickers = calc_close.columns\n",
    "#     calc_period_index = pd.MultiIndex.from_product([valid_tickers, calc_close.index], names=['Ticker', 'Date'])\n",
    "#     features_in_period = features_df.loc[features_df.index.intersection(calc_period_index)]\n",
    "#     atrp = features_in_period.groupby(level='Ticker')['ATRP'].mean()\n",
    "\n",
    "#     # 4. Calculate all ranking metrics by iterating through the METRIC_REGISTRY\n",
    "#     metric_ingredients = { 'calc_close': calc_close, 'daily_returns': daily_returns, 'atrp': atrp, }\n",
    "#     metric_values = {}\n",
    "#     for name, func in METRIC_REGISTRY.items():\n",
    "#         metric_values[name] = func(metric_ingredients)\n",
    "#     if metric not in metric_values or metric_values[metric].empty:\n",
    "#         return ({'error': f\"Metric '{metric}' could not be calculated or resulted in no valid tickers.\"}, None)\n",
    "\n",
    "#     # 5. Rank tickers and select the portfolio\n",
    "#     sorted_tickers = metric_values[metric].sort_values(ascending=False)\n",
    "#     tickers_to_display = sorted_tickers.index[rank_start-1:rank_end].tolist()\n",
    "#     if not tickers_to_display:\n",
    "#         return ({'error': \"No tickers found for the selected rank.\"}, None)\n",
    "\n",
    "#     # 6. Calculate Portfolio & Benchmark Performance\n",
    "#     normalized_plot_data = df_close_full[tickers_to_display].loc[safe_start_date:safe_viz_end_date]\n",
    "#     normalized_plot_data = normalized_plot_data.div(normalized_plot_data.bfill().iloc[0])\n",
    "#     actual_calc_end_ts = calc_close.index.max()\n",
    "#     portfolio_series = normalized_plot_data.mean(axis=1)\n",
    "#     portfolio_return_series = portfolio_series.pct_change()\n",
    "#     benchmark_price_series = df_close_full.get(benchmark_ticker)\n",
    "#     benchmark_return_series = benchmark_price_series.loc[safe_start_date:safe_viz_end_date].bfill().ffill().pct_change() if benchmark_price_series is not None else pd.Series(dtype='float64')\n",
    "#     try:\n",
    "#         boundary_loc = portfolio_return_series.index.get_loc(actual_calc_end_ts)\n",
    "#         calc_portfolio_returns = portfolio_return_series.iloc[:boundary_loc + 1]\n",
    "#         fwd_portfolio_returns = portfolio_return_series.iloc[boundary_loc + 1:]\n",
    "#         if benchmark_price_series is not None:\n",
    "#             bm_boundary_loc = benchmark_return_series.index.get_loc(actual_calc_end_ts)\n",
    "#             calc_benchmark_returns = benchmark_return_series.iloc[:bm_boundary_loc + 1]\n",
    "#             fwd_benchmark_returns = benchmark_return_series.iloc[bm_boundary_loc + 1:]\n",
    "#         else:\n",
    "#             calc_benchmark_returns, fwd_benchmark_returns = pd.Series(dtype='float64'), pd.Series(dtype='float64')\n",
    "#     except (KeyError, IndexError):\n",
    "#         calc_portfolio_returns = portfolio_return_series.loc[:actual_calc_end_ts]\n",
    "#         fwd_portfolio_returns = portfolio_return_series.loc[actual_calc_end_ts:].iloc[1:]\n",
    "#         if benchmark_price_series is not None:\n",
    "#             calc_benchmark_returns = benchmark_return_series.loc[:actual_calc_end_ts]\n",
    "#             fwd_benchmark_returns = benchmark_return_series.loc[actual_calc_end_ts:].iloc[1:]\n",
    "#         else:\n",
    "#             calc_benchmark_returns, fwd_benchmark_returns = pd.Series(dtype='float64'), pd.Series(dtype='float64')\n",
    "#     full_period_index = pd.MultiIndex.from_product([tickers_to_display, portfolio_return_series.index], names=['Ticker', 'Date'])\n",
    "#     portfolio_atrp_features = features_df.loc[features_df.index.intersection(full_period_index)]\n",
    "#     portfolio_atrp_daily_unstacked = portfolio_atrp_features['ATRP'].unstack(level='Ticker')\n",
    "#     portfolio_atrp_series = portfolio_atrp_daily_unstacked.mean(axis=1)\n",
    "#     if benchmark_ticker in df_close_full.columns:\n",
    "#         benchmark_atrp_series = features_df.xs(benchmark_ticker, level='Ticker')['ATRP'].loc[safe_start_date:safe_viz_end_date]\n",
    "#     else:\n",
    "#         benchmark_atrp_series = pd.Series(dtype='float64')\n",
    "#     calc_portfolio_atrp = portfolio_atrp_series.loc[:actual_calc_end_ts]\n",
    "#     fwd_portfolio_atrp = portfolio_atrp_series.loc[actual_calc_end_ts:].iloc[1:]\n",
    "#     calc_benchmark_atrp = benchmark_atrp_series.loc[:actual_calc_end_ts]\n",
    "#     fwd_benchmark_atrp = benchmark_atrp_series.loc[actual_calc_end_ts:].iloc[1:]\n",
    "#     perf_data = {}\n",
    "#     perf_data['calc_p_gain'] = calculate_gain(portfolio_series.loc[:actual_calc_end_ts])\n",
    "#     perf_data['fwd_p_gain'] = calculate_gain(portfolio_series.loc[actual_calc_end_ts:])\n",
    "#     perf_data['full_p_gain'] = calculate_gain(portfolio_series)\n",
    "#     perf_data['calc_p_sharpe'] = calculate_sharpe(calc_portfolio_returns)\n",
    "#     perf_data['fwd_p_sharpe'] = calculate_sharpe(fwd_portfolio_returns)\n",
    "#     perf_data['full_p_sharpe'] = calculate_sharpe(portfolio_return_series)\n",
    "#     perf_data['calc_b_gain'] = calculate_gain(benchmark_price_series.loc[safe_start_date:actual_calc_end_ts]) if benchmark_price_series is not None else np.nan\n",
    "#     perf_data['fwd_b_gain'] = calculate_gain(benchmark_price_series.loc[actual_calc_end_ts:safe_viz_end_date]) if benchmark_price_series is not None else np.nan\n",
    "#     perf_data['full_b_gain'] = calculate_gain(benchmark_price_series.loc[safe_start_date:safe_viz_end_date]) if benchmark_price_series is not None else np.nan\n",
    "#     perf_data['calc_b_sharpe'] = calculate_sharpe(calc_benchmark_returns)\n",
    "#     perf_data['fwd_b_sharpe'] = calculate_sharpe(fwd_benchmark_returns)\n",
    "#     perf_data['full_b_sharpe'] = calculate_sharpe(benchmark_return_series)\n",
    "#     perf_data['calc_p_sharpe_atr'] = calculate_sharpe_atr(calc_portfolio_returns, calc_portfolio_atrp)\n",
    "#     perf_data['fwd_p_sharpe_atr'] = calculate_sharpe_atr(fwd_portfolio_returns, fwd_portfolio_atrp)\n",
    "#     perf_data['full_p_sharpe_atr'] = calculate_sharpe_atr(portfolio_return_series, portfolio_atrp_series)\n",
    "#     perf_data['calc_b_sharpe_atr'] = calculate_sharpe_atr(calc_benchmark_returns, calc_benchmark_atrp)\n",
    "#     perf_data['fwd_b_sharpe_atr'] = calculate_sharpe_atr(fwd_benchmark_returns, fwd_benchmark_atrp)\n",
    "#     perf_data['full_b_sharpe_atr'] = calculate_sharpe_atr(benchmark_return_series, benchmark_atrp_series)\n",
    "#     if debug:\n",
    "#         df_ranking_base = pd.DataFrame({'MeanDailyReturn': daily_returns.mean(),'StdDevDailyReturn': daily_returns.std(),'MeanATRP': atrp})\n",
    "#         df_metrics = pd.DataFrame(metric_values)\n",
    "#         df_metrics.columns = [f'Metric_{col}' for col in df_metrics.columns]\n",
    "#         df_ranking = df_ranking_base.join(df_metrics, how='left')\n",
    "#         df_ranking.index.name = 'Ticker'\n",
    "#         debug_data['ranking_metrics'] = df_ranking.sort_values(f'Metric_{metric}', ascending=False)\n",
    "#     calc_end_prices = calc_close.ffill().iloc[-1]\n",
    "#     fwd_close_slice = df_close_full.loc[actual_calc_end_ts:safe_viz_end_date]\n",
    "#     viz_end_prices = fwd_close_slice.ffill().iloc[-1] if not fwd_close_slice.empty and len(fwd_close_slice) >= 2 else calc_end_prices\n",
    "#     calc_gains = (calc_end_prices / calc_close.bfill().iloc[0]) - 1\n",
    "#     fwd_gains = (viz_end_prices / calc_end_prices) - 1\n",
    "#     results_df = pd.DataFrame({'Rank': range(rank_start, rank_start + len(tickers_to_display)), 'Metric': metric, 'MetricValue': sorted_tickers.loc[tickers_to_display].values, 'CalcPrice': calc_end_prices.loc[tickers_to_display], 'CalcGain': calc_gains.loc[tickers_to_display], 'FwdGain': fwd_gains.loc[tickers_to_display]}, index=pd.Index(tickers_to_display, name='Ticker'))\n",
    "#     if benchmark_price_series is not None and benchmark_ticker in calc_close.columns:\n",
    "#         benchmark_df_row = pd.DataFrame({'Rank': np.nan, 'Metric': metric, 'MetricValue': metric_values[metric].get(benchmark_ticker, np.nan), 'CalcPrice': calc_end_prices[benchmark_ticker], 'CalcGain': calc_gains[benchmark_ticker], 'FwdGain': fwd_gains[benchmark_ticker]}, index=pd.Index([f\"{benchmark_ticker} (BM)\"], name='Ticker'))\n",
    "#         results_df = pd.concat([results_df, benchmark_df_row])\n",
    "#     if debug:\n",
    "#         df_trace = normalized_plot_data.copy()\n",
    "#         df_trace.columns = [f'Norm_Price_{c}' for c in df_trace.columns]\n",
    "#         df_trace['Norm_Price_Portfolio'] = portfolio_series\n",
    "#         if benchmark_price_series is not None and not benchmark_price_series.loc[safe_start_date:safe_viz_end_date].dropna().empty:\n",
    "#             norm_bm = benchmark_price_series.loc[safe_start_date:safe_viz_end_date] / benchmark_price_series.loc[safe_start_date:].bfill().iloc[0]\n",
    "#             df_trace[f'Norm_Price_Benchmark_{benchmark_ticker}'] = norm_bm\n",
    "#         for col in df_trace.columns:\n",
    "#             if 'Norm_Price' in col:\n",
    "#                 df_trace[col.replace('Norm_Price', 'Return')] = df_trace[col].pct_change()\n",
    "#         debug_data['portfolio_trace'] = df_trace\n",
    "#     final_results = {\n",
    "#         'tickers_to_display': tickers_to_display, 'normalized_plot_data': normalized_plot_data,\n",
    "#         'portfolio_series': portfolio_series, 'benchmark_price_series': benchmark_price_series,\n",
    "#         'performance_data': perf_data, 'results_df': results_df, 'actual_calc_end_ts': actual_calc_end_ts,\n",
    "#         'safe_start_date': safe_start_date, 'safe_viz_end_date': safe_viz_end_date,\n",
    "#         'error': None\n",
    "#     }\n",
    "#     return (final_results, debug_data)\n",
    "\n",
    "# # --- C. DYNAMIC DATA QUALITY FILTER FUNCTIONS ---\n",
    "\n",
    "# def get_eligible_universe(features_df, filter_date, thresholds):\n",
    "#     \"\"\"Filters the universe of tickers based on quality metrics for a given date.\"\"\"\n",
    "#     filter_date_ts = pd.to_datetime(filter_date)\n",
    "#     # The index is now the comprehensive features_df\n",
    "#     date_index = features_df.index.get_level_values('Date').unique().sort_values()\n",
    "    \n",
    "#     if filter_date_ts < date_index[0]:\n",
    "#         print(f\"Warning: Filter date {filter_date_ts.date()} is before the earliest data point. Returning empty universe.\")\n",
    "#         return []\n",
    "        \n",
    "#     # Find the most recent date with quality data on or before the filter date\n",
    "#     valid_prior_dates = date_index[date_index <= filter_date_ts]\n",
    "#     if valid_prior_dates.empty:\n",
    "#         print(f\"Warning: No available data found on or before {filter_date_ts.date()}. Returning empty universe.\")\n",
    "#         return []\n",
    "        \n",
    "#     actual_date_to_use = valid_prior_dates[-1]\n",
    "#     if actual_date_to_use.date() != filter_date_ts.date():\n",
    "#         print(f\"ℹ️ Info: Filter date {filter_date_ts.date()} not found. Using previous available date {actual_date_to_use.date()}.\")\n",
    "\n",
    "#     metrics_on_date = features_df.xs(actual_date_to_use, level='Date')\n",
    "    \n",
    "#     # Apply filters using the new column names from features_df\n",
    "#     mask = ((metrics_on_date['RollMedDollarVol'] >= thresholds['min_median_dollar_volume']) & # <-- RENAMED\n",
    "#             (metrics_on_date['RollingStalePct'] <= thresholds['max_stale_pct']) &\n",
    "#             (metrics_on_date['RollingSameVolCount'] <= thresholds['max_same_vol_count']))\n",
    "            \n",
    "#     eligible_tickers = metrics_on_date[mask].index.tolist()\n",
    "#     all_tickers = metrics_on_date.index.tolist()\n",
    "#     print(f\"Dynamic Filter ({filter_date_ts.date()}): Kept {len(eligible_tickers)} of {len(all_tickers)} tickers.\")\n",
    "#     return eligible_tickers\n",
    "\n",
    "# # --- D. INTERACTIVE ANALYSIS & BACKTESTING TOOLS ---\n",
    "\n",
    "# def plot_walk_forward_analyzer(df_ohlcv,\n",
    "#                                default_start_date=None, default_calc_period=126, default_fwd_period=63,\n",
    "#                                default_metric='Sharpe (ATR)', default_rank_start=1, default_rank_end=10,\n",
    "#                                default_benchmark_ticker='VOO', master_calendar_ticker='VOO',\n",
    "#                                quality_thresholds={'min_median_dollar_volume': 1_000_000, 'max_stale_pct': 0.05, 'max_same_vol_count': 10},\n",
    "#                                debug=False):\n",
    "#     # (No changes to the initial setup part of this function...)\n",
    "#     print(\"Initializing Walk-Forward Analyzer (using Trading Day Logic)...\")\n",
    "#     if not isinstance(df_ohlcv.index, pd.MultiIndex): raise ValueError(\"Input DataFrame must have a (Ticker, Date) MultiIndex.\")\n",
    "#     df_ohlcv = df_ohlcv.sort_index()\n",
    "#     if master_calendar_ticker not in df_ohlcv.index.get_level_values(0):\n",
    "#         raise ValueError(f\"Master calendar ticker '{master_calendar_ticker}' not found in DataFrame.\")\n",
    "#     master_trading_days = df_ohlcv.loc[master_calendar_ticker].index.unique().sort_values()\n",
    "#     print(f\"Master trading day calendar created from '{master_calendar_ticker}' ({len(master_trading_days)} days).\")\n",
    "#     print(\"--- Generating all features upfront... ---\")\n",
    "#     features_df = generate_features(df_ohlcv)\n",
    "#     print(\"Pre-processing data (unstacking)...\")\n",
    "#     df_close_full = df_ohlcv['Adj Close'].unstack(level=0)\n",
    "#     df_high_full = df_ohlcv['Adj High'].unstack(level=0)\n",
    "#     df_low_full = df_ohlcv['Adj Low'].unstack(level=0)\n",
    "#     start_date_picker = widgets.DatePicker(description='Start Date:', value=pd.to_datetime(default_start_date), disabled=False)\n",
    "#     calc_period_input = widgets.IntText(value=default_calc_period, description='Calc Period (days):')\n",
    "#     fwd_period_input = widgets.IntText(value=default_fwd_period, description='Fwd Period (days):')\n",
    "#     if default_metric not in METRIC_REGISTRY:\n",
    "#         fallback_metric = list(METRIC_REGISTRY.keys())[0]\n",
    "#         print(f\"⚠️ Warning: Default metric '{default_metric}' not in registry. Using '{fallback_metric}'.\")\n",
    "#         default_metric = fallback_metric\n",
    "#     metric_dropdown = widgets.Dropdown(options=list(METRIC_REGISTRY.keys()), value=default_metric, description='Metric:')\n",
    "#     rank_start_input = widgets.IntText(value=default_rank_start, description='Rank Start:')\n",
    "#     rank_end_input = widgets.IntText(value=default_rank_end, description='Rank End:')\n",
    "#     benchmark_ticker_input = widgets.Text(value=default_benchmark_ticker, description='Benchmark:', placeholder='Enter Ticker')\n",
    "#     update_button = widgets.Button(description=\"Update Chart\", button_style='primary')\n",
    "#     ticker_list_output = widgets.Output()\n",
    "#     results_container, debug_data_container = [None], [None]\n",
    "#     fig = go.FigureWidget()\n",
    "#     max_traces = 50\n",
    "#     for i in range(max_traces): fig.add_trace(go.Scatter(x=[None], y=[None], mode='lines', name=f'placeholder_{i}', visible=False, showlegend=False))\n",
    "#     fig.add_trace(go.Scatter(x=[None], y=[None], mode='lines', name='Benchmark', visible=True, showlegend=True, line=dict(color='black', width=3, dash='dash')))\n",
    "#     fig.add_trace(go.Scatter(x=[None], y=[None], mode='lines', name='Group Portfolio', visible=True, showlegend=True, line=dict(color='green', width=3)))\n",
    "\n",
    "#     def update_plot(button_click):\n",
    "#         ticker_list_output.clear_output()\n",
    "#         start_date_raw = pd.to_datetime(start_date_picker.value)\n",
    "#         start_date_idx = master_trading_days.searchsorted(start_date_raw)\n",
    "#         if start_date_idx >= len(master_trading_days):\n",
    "#             with ticker_list_output: print(f\"Error: Start date is after the last available trading day.\"); return\n",
    "#         actual_start_date = master_trading_days[start_date_idx]\n",
    "#         with ticker_list_output:\n",
    "#             if start_date_raw.date() != actual_start_date.date():\n",
    "#                 print(f\"ℹ️ Info: Start date {start_date_raw.date()} is not a trading day. Snapping forward to {actual_start_date.date()}.\")\n",
    "#         calc_period = calc_period_input.value; fwd_period = fwd_period_input.value; metric = metric_dropdown.value\n",
    "#         rank_start = rank_start_input.value; rank_end = rank_end_input.value; benchmark_ticker = benchmark_ticker_input.value.strip().upper()\n",
    "#         if rank_start > rank_end:\n",
    "#             with ticker_list_output: print(\"Error: 'Rank Start' must be <= 'Rank End'.\"); return\n",
    "#         if rank_start < 1 or calc_period < 2 or fwd_period < 1:\n",
    "#             with ticker_list_output: print(\"Error: Ranks must be >= 1, Calc Period >= 2, Fwd Period >= 1.\"); return\n",
    "#         required_days = calc_period + fwd_period\n",
    "#         if start_date_idx + required_days > len(master_trading_days):\n",
    "#             available_days = len(master_trading_days) - start_date_idx; last_available_date = master_trading_days[-1].date()\n",
    "#             with ticker_list_output:\n",
    "#                 print(f\"Error: Not enough data for the requested period.\\n  Start Date: {actual_start_date.date()}\\n  Required Days: {calc_period} (calc) + {fwd_period} (fwd) = {required_days}\\n  Available Days from Start: {available_days} (until {last_available_date})\\n  Please shorten the 'Calc Period' / 'Fwd Period' or choose an earlier 'Start Date'.\")\n",
    "#             return\n",
    "#         eligible_tickers = get_eligible_universe(features_df, actual_start_date, quality_thresholds)\n",
    "#         if not eligible_tickers:\n",
    "#             with ticker_list_output: print(f\"Error: No eligible tickers found on {actual_start_date.date()} with the current quality filters.\"); return\n",
    "#         df_close_step = df_close_full[eligible_tickers]; df_high_step = df_high_full[eligible_tickers]; df_low_step = df_low_full[eligible_tickers]\n",
    "#         results, debug_output = run_walk_forward_step(df_close_full, df_high_full, df_low_full, master_trading_days, actual_start_date, calc_period, fwd_period, metric, rank_start, rank_end, benchmark_ticker, features_df=features_df, debug=debug)\n",
    "#         if results.get('error'):\n",
    "#             with ticker_list_output: print(f\"Error: {results['error']}\"); return\n",
    "#         period_dates = {'calc_period_start': results['safe_start_date'], 'calc_period_end': results['actual_calc_end_ts'], 'forward_period_start': results['actual_calc_end_ts'], 'forward_period_end': results['safe_viz_end_date']}\n",
    "#         run_parameters = {'calc_period': calc_period, 'fwd_period': fwd_period, 'rank_metric': metric, 'rank_start': rank_start, 'rank_end': rank_end, 'benchmark_ticker': benchmark_ticker}\n",
    "#         results.update(period_dates); results.update(run_parameters)\n",
    "#         if debug_output is not None and isinstance(debug_output, dict):\n",
    "#             debug_output.update(period_dates); debug_output.update(run_parameters)\n",
    "#         with fig.batch_update():\n",
    "#             for i in range(max_traces):\n",
    "#                 trace = fig.data[i]\n",
    "#                 if i < len(results['tickers_to_display']):\n",
    "#                     ticker = results['tickers_to_display'][i]; plot_data_series = results['normalized_plot_data'][ticker]\n",
    "#                     trace.x, trace.y, trace.name, trace.visible, trace.showlegend = plot_data_series.index, plot_data_series.values, ticker, True, True\n",
    "#                 else: trace.visible, trace.showlegend = False, False\n",
    "#             benchmark_trace = fig.data[max_traces]\n",
    "#             if results['benchmark_price_series'] is not None and not results['benchmark_price_series'].loc[results['safe_start_date']:results['safe_viz_end_date']].dropna().empty:\n",
    "#                 normalized_benchmark = results['benchmark_price_series'].loc[results['safe_start_date']:results['safe_viz_end_date']] / results['benchmark_price_series'].loc[results['safe_start_date']:].bfill().iloc[0]\n",
    "#                 benchmark_trace.x, benchmark_trace.y, benchmark_trace.name, benchmark_trace.visible = normalized_benchmark.index, normalized_benchmark.values, f\"Benchmark ({benchmark_ticker})\", True\n",
    "#             else: benchmark_trace.visible = False\n",
    "#             portfolio_trace = fig.data[max_traces + 1]\n",
    "#             portfolio_trace.x, portfolio_trace.y, portfolio_trace.name, portfolio_trace.visible = results['portfolio_series'].index, results['portfolio_series'], 'Group Portfolio', True\n",
    "#             fig.layout.shapes = []; fig.add_shape(type=\"line\", x0=results['actual_calc_end_ts'], y0=0, x1=results['actual_calc_end_ts'], y1=1, xref='x', yref='paper', line=dict(color=\"grey\", width=2, dash=\"dash\"))\n",
    "#         results_container[0] = results; debug_data_container[0] = debug_output\n",
    "#         with ticker_list_output:\n",
    "#             print(f\"Analysis Period: {results['safe_start_date'].date()} to {results['safe_viz_end_date'].date()}.\")\n",
    "#             pprint.pprint(results['tickers_to_display'])\n",
    "#             p = results['performance_data']\n",
    "            \n",
    "#             # --- START OF MODIFIED BLOCK ---\n",
    "#             rows = []\n",
    "#             # Gain Metrics\n",
    "#             rows.append({'Metric': 'Group Portfolio Gain', 'Full': p['full_p_gain'], 'Calc': p['calc_p_gain'], 'Fwd': p['fwd_p_gain']})\n",
    "#             if not np.isnan(p.get('full_b_gain')):\n",
    "#                 rows.append({'Metric': f'Benchmark ({benchmark_ticker}) Gain', 'Full': p['full_b_gain'], 'Calc': p['calc_b_gain'], 'Fwd': p['fwd_b_gain']})\n",
    "#                 rows.append({'Metric': '== Gain Delta (vs Bm)', 'Full': p['full_p_gain'] - p['full_b_gain'], 'Calc': p['calc_p_gain'] - p['calc_b_gain'], 'Fwd': p['fwd_p_gain'] - p['fwd_b_gain']})\n",
    "            \n",
    "#             # Standard Sharpe Metrics\n",
    "#             rows.append({'Metric': 'Group Portfolio Sharpe', 'Full': p['full_p_sharpe'], 'Calc': p['calc_p_sharpe'], 'Fwd': p['fwd_p_sharpe']})\n",
    "#             if not np.isnan(p.get('full_b_sharpe')):\n",
    "#                 rows.append({'Metric': f'Benchmark ({benchmark_ticker}) Sharpe', 'Full': p['full_b_sharpe'], 'Calc': p['calc_b_sharpe'], 'Fwd': p['fwd_b_sharpe']})\n",
    "#                 rows.append({'Metric': '== Sharpe Delta (vs Bm)', 'Full': p['full_p_sharpe'] - p['full_b_sharpe'], 'Calc': p['calc_p_sharpe'] - p['calc_b_sharpe'], 'Fwd': p['fwd_p_sharpe'] - p['fwd_b_sharpe']})\n",
    "\n",
    "#             # Sharpe (ATR) Metrics\n",
    "#             rows.append({'Metric': 'Group Portfolio Sharpe (ATR)', 'Full': p['full_p_sharpe_atr'], 'Calc': p['calc_p_sharpe_atr'], 'Fwd': p['fwd_p_sharpe_atr']})\n",
    "#             if not np.isnan(p.get('full_b_sharpe_atr')):\n",
    "#                 rows.append({'Metric': f'Benchmark ({benchmark_ticker}) Sharpe (ATR)', 'Full': p['full_b_sharpe_atr'], 'Calc': p['calc_b_sharpe_atr'], 'Fwd': p['fwd_b_sharpe_atr']})\n",
    "#                 rows.append({'Metric': '== Sharpe (ATR) Delta (vs Bm)', 'Full': p['full_p_sharpe_atr'] - p['full_b_sharpe_atr'], 'Calc': p['calc_p_sharpe_atr'] - p['calc_b_sharpe_atr'], 'Fwd': p['fwd_p_sharpe_atr'] - p['fwd_b_sharpe_atr']})\n",
    "\n",
    "#             report_df = pd.DataFrame(rows).set_index('Metric')\n",
    "#             gain_rows = [row for row in report_df.index if 'Gain' in row]\n",
    "#             sharpe_rows = [row for row in report_df.index if 'Sharpe' in row] # This now correctly includes both types of Sharpe\n",
    "#             # --- END OF MODIFIED BLOCK ---\n",
    "            \n",
    "#             styled_df = report_df.style.format('{:+.2%}', na_rep='N/A', subset=(gain_rows, report_df.columns)).format('{:+.4f}', na_rep='N/A', subset=(sharpe_rows, report_df.columns)).set_properties(**{'text-align': 'right', 'width': '100px'}).set_table_styles([{'selector': 'th.col_heading', 'props': [('text-align', 'right')]}, {'selector': 'th.row_heading', 'props': [('text-align', 'left')]}])\n",
    "#             print(\"\\n--- Strategy Performance Summary ---\")\n",
    "#             display(styled_df)\n",
    "#     fig.update_layout(title_text='Walk-Forward Performance Analysis', xaxis_title='Date', yaxis_title='Normalized Price (Start = 1)', hovermode='x unified', legend_title_text='Tickers (Ranked)', height=600, margin=dict(t=50))\n",
    "#     fig.add_hline(y=1, line_width=1, line_dash=\"dash\", line_color=\"grey\")\n",
    "#     update_button.on_click(update_plot)\n",
    "#     controls_row1 = widgets.HBox([start_date_picker, calc_period_input, fwd_period_input])\n",
    "#     controls_row2 = widgets.HBox([metric_dropdown, rank_start_input, rank_end_input, benchmark_ticker_input, update_button])\n",
    "#     ui_container = widgets.VBox([controls_row1, controls_row2, ticker_list_output], layout=widgets.Layout(margin='10px 0 20px 0'))\n",
    "#     display(ui_container, fig)\n",
    "#     update_plot(None)\n",
    "#     return (results_container, debug_data_container)\n",
    "\n",
    "# def run_full_backtest(df_ohlcv, strategy_params, quality_thresholds):\n",
    "#     \"\"\"Runs a full backtest of a strategy over a specified date range.\"\"\"\n",
    "#     print(f\"--- Running Full Forensic Backtest for Strategy: {strategy_params['metric']} (Top {strategy_params['rank_start']}-{strategy_params['rank_end']}) ---\")\n",
    "    \n",
    "#     # (No changes to the initial setup part...)\n",
    "#     start_date, end_date = pd.to_datetime(strategy_params['start_date']), pd.to_datetime(strategy_params['end_date'])\n",
    "#     calc_period, fwd_period = strategy_params['calc_period'], strategy_params['fwd_period']\n",
    "#     metric, rank_start, rank_end = strategy_params['metric'], strategy_params['rank_start'], strategy_params['rank_end']\n",
    "#     benchmark_ticker = strategy_params['benchmark_ticker']\n",
    "#     master_calendar_ticker = strategy_params.get('master_calendar_ticker', 'VOO')\n",
    "#     if master_calendar_ticker not in df_ohlcv.index.get_level_values(0):\n",
    "#         raise ValueError(f\"Master calendar ticker '{master_calendar_ticker}' not found in DataFrame.\")\n",
    "#     master_trading_days = df_ohlcv.loc[master_calendar_ticker].index.unique().sort_values()\n",
    "#     start_idx = master_trading_days.searchsorted(start_date)\n",
    "#     end_idx = master_trading_days.searchsorted(end_date, side='right')\n",
    "#     print(\"--- Generating all features upfront... ---\")\n",
    "#     features_df = generate_features(df_ohlcv)\n",
    "#     df_close_full = df_ohlcv['Adj Close'].unstack(level=0); df_high_full = df_ohlcv['Adj High'].unstack(level=0); df_low_full = df_ohlcv['Adj Low'].unstack(level=0)\n",
    "    \n",
    "#     # Loop through all periods in the backtest range\n",
    "#     step_indices = range(start_idx, end_idx, fwd_period)\n",
    "#     all_fwd_gains, period_by_period_debug = [], {}\n",
    "\n",
    "#     print(f\"Simulating {len(step_indices)} periods from {master_trading_days[step_indices[0]].date()} to {master_trading_days[step_indices[-1]].date()}...\")\n",
    "#     for current_idx in tqdm(step_indices, desc=\"Backtest Progress\"):\n",
    "#         step_date = master_trading_days[current_idx]\n",
    "#         eligible_tickers = get_eligible_universe(features_df, step_date, quality_thresholds)\n",
    "#         if not eligible_tickers: continue\n",
    "#         df_close_step = df_close_full[eligible_tickers]; df_high_step = df_high_full[eligible_tickers]; df_low_step = df_low_full[eligible_tickers]\n",
    "        \n",
    "#         # +++ UPDATE THE CALL HERE +++\n",
    "#         results, debug_output = run_walk_forward_step(\n",
    "#             df_close_step, df_high_step, df_low_step, master_trading_days,\n",
    "#             step_date, calc_period, fwd_period,\n",
    "#             metric, rank_start, rank_end, benchmark_ticker,\n",
    "#             features_df=features_df,  # Pass the features_df\n",
    "#             debug=True\n",
    "#         )\n",
    "        \n",
    "#         # (The rest of the function remains unchanged...)\n",
    "#         if results['error'] is None:\n",
    "#             fwd_series = results['portfolio_series'].loc[results['actual_calc_end_ts']:]\n",
    "#             all_fwd_gains.append(fwd_series.pct_change().dropna())\n",
    "#             period_by_period_debug[step_date.date().isoformat()] = debug_output\n",
    "            \n",
    "#     if not all_fwd_gains:\n",
    "#         print(\"Error: No valid periods were simulated.\"); return None\n",
    "\n",
    "#     strategy_returns = pd.concat(all_fwd_gains); strategy_equity_curve = (1 + strategy_returns).cumprod()\n",
    "#     benchmark_returns = df_close_full[benchmark_ticker].pct_change().loc[strategy_equity_curve.index]; benchmark_equity_curve = (1 + benchmark_returns).cumprod()\n",
    "#     cumulative_equity_df = pd.DataFrame({'Strategy_Equity': strategy_equity_curve, 'Benchmark_Equity': benchmark_equity_curve})\n",
    "#     fig = go.Figure()\n",
    "#     fig.add_trace(go.Scatter(x=cumulative_equity_df.index, y=cumulative_equity_df['Strategy_Equity'], name='Strategy', line=dict(color='green')))\n",
    "#     fig.add_trace(go.Scatter(x=cumulative_equity_df.index, y=cumulative_equity_df['Benchmark_Equity'], name=f'Benchmark ({benchmark_ticker})', line=dict(color='black', dash='dash')))\n",
    "#     fig.update_layout(title=f\"Cumulative Performance: '{metric}' Strategy (Top {rank_start}-{rank_end})\", xaxis_title=\"Date\", yaxis_title=\"Cumulative Growth\")\n",
    "#     fig.show()\n",
    "#     final_backtest_results = {'cumulative_performance': cumulative_equity_df, 'period_by_period_debug': period_by_period_debug}\n",
    "#     print(\"\\n✅ Full backtest complete. Results object is ready for forensic analysis.\")\n",
    "#     return final_backtest_results\n",
    "\n",
    "# # --- E. VERIFICATION TOOLS (User Requested) ---\n",
    "\n",
    "# def verify_group_tickers_walk_forward_calculation(df_ohlcv, tickers_to_verify, benchmark_ticker,\n",
    "#                                                   start_date, calc_period, fwd_period,\n",
    "#                                                   master_calendar_ticker='VOO', export_csv=False):\n",
    "#     \"\"\"Verifies portfolio and benchmark performance and optionally exports the data.\"\"\"\n",
    "#     display(Markdown(f\"## Verification Report for Portfolio vs. Benchmark\"))\n",
    "#     display(Markdown(f\"**Portfolio Tickers:** `{tickers_to_verify}`\\n**Benchmark Ticker:** `{benchmark_ticker}`\"))\n",
    "    \n",
    "#     # 1. Setup trading day calendar and determine exact period dates\n",
    "#     if master_calendar_ticker not in df_ohlcv.index.get_level_values(0):\n",
    "#         raise ValueError(f\"Master calendar ticker '{master_calendar_ticker}' not found in DataFrame.\")\n",
    "#     master_trading_days = df_ohlcv.loc[master_calendar_ticker].index.unique().sort_values()\n",
    "\n",
    "#     start_date_raw = pd.to_datetime(start_date)\n",
    "#     start_idx = master_trading_days.searchsorted(start_date_raw)\n",
    "#     if start_idx >= len(master_trading_days):\n",
    "#         print(f\"Error: Start date {start_date_raw.date()} is after the last available trading day.\"); return\n",
    "#     actual_start_date = master_trading_days[start_idx]\n",
    "    \n",
    "#     calc_end_idx = min(start_idx + calc_period, len(master_trading_days) - 1)\n",
    "#     fwd_end_idx = min(calc_end_idx + fwd_period, len(master_trading_days) - 1)\n",
    "    \n",
    "#     actual_calc_end_date = master_trading_days[calc_end_idx]\n",
    "#     actual_fwd_end_date = master_trading_days[fwd_end_idx]\n",
    "    \n",
    "#     display(Markdown(f\"**Analysis Start:** `{actual_start_date.date()}` (Selected: `{start_date_raw.date()}`)\\n\"\n",
    "#                     f\"**Calc End:** `{actual_calc_end_date.date()}` ({calc_period} trading days)\\n\"\n",
    "#                     f\"**Fwd End:** `{actual_fwd_end_date.date()}` ({fwd_period} trading days)\"))\n",
    "\n",
    "#     # 2. Recreate the portfolio and benchmark series from scratch\n",
    "#     df_close_full = df_ohlcv['Adj Close'].unstack(level=0)\n",
    "#     portfolio_prices_raw_slice = df_close_full[tickers_to_verify].loc[actual_start_date:actual_fwd_end_date]\n",
    "#     portfolio_value_series = portfolio_prices_raw_slice.div(portfolio_prices_raw_slice.bfill().iloc[0]).mean(axis=1)\n",
    "#     benchmark_price_series = df_close_full.get(benchmark_ticker)\n",
    "    \n",
    "#     # 3. Optionally export the underlying daily data to a CSV for external checking\n",
    "#     if export_csv:\n",
    "#         export_df = pd.DataFrame({\n",
    "#             'Portfolio_Normalized_Price': portfolio_value_series,\n",
    "#             'Portfolio_Daily_Return': portfolio_value_series.pct_change()\n",
    "#         })\n",
    "#         if benchmark_price_series is not None:\n",
    "#             norm_bm = benchmark_price_series.loc[actual_start_date:actual_fwd_end_date]\n",
    "#             norm_bm = norm_bm / norm_bm.bfill().iloc[0]\n",
    "#             export_df['Benchmark_Normalized_Price'] = norm_bm\n",
    "#             export_df['Benchmark_Daily_Return'] = norm_bm.pct_change()\n",
    "\n",
    "#         output_dir = 'export_csv'\n",
    "#         os.makedirs(output_dir, exist_ok=True)\n",
    "#         tickers_str = '_'.join(tickers_to_verify)\n",
    "#         filename = f\"verify_group_{actual_start_date.date()}_{tickers_str}.csv\"\n",
    "#         filepath = os.path.join(output_dir, filename)\n",
    "#         export_df.to_csv(filepath)\n",
    "#         print(f\"\\n✅ Data exported to: {filepath}\")\n",
    "\n",
    "#     # 4. Define a helper to print detailed calculation steps\n",
    "#     def print_verification_steps(title, price_series):\n",
    "#         display(Markdown(f\"#### Verification for: `{title}`\"))\n",
    "#         if price_series.dropna().shape[0] < 2: print(\"  - Not enough data points.\"); return {'gain': np.nan, 'sharpe': np.nan}\n",
    "#         start_price, end_price = price_series.bfill().iloc[0], price_series.ffill().iloc[-1]\n",
    "#         gain = (end_price / start_price) - 1\n",
    "#         print(f\"Start Value ({price_series.first_valid_index().date()}): {start_price:,.4f}\\nEnd Value   ({price_series.last_valid_index().date()}): {end_price:,.4f}\\nGain = {gain:.2%}\")\n",
    "#         returns = price_series.pct_change()\n",
    "#         sharpe = calculate_sharpe(returns)\n",
    "#         print(f\"Mean Daily Return: {returns.mean():.6f}\\nStd Dev: {returns.std():.6f}\\nSharpe = {sharpe:.2f}\")\n",
    "#         return {'gain': gain, 'sharpe': sharpe}\n",
    "\n",
    "#     # 5. Run verification for each period\n",
    "#     display(Markdown(\"### A. Calculation Period\"))\n",
    "#     perf_calc_p = print_verification_steps(\"Group Portfolio\", portfolio_value_series.loc[actual_start_date:actual_calc_end_date])\n",
    "#     if benchmark_price_series is not None:\n",
    "#         perf_calc_b = print_verification_steps(f\"Benchmark\", benchmark_price_series.loc[actual_start_date:actual_calc_end_date])\n",
    "    \n",
    "#     display(Markdown(\"### B. Forward Period\"))\n",
    "#     perf_fwd_p = print_verification_steps(\"Group Portfolio\", portfolio_value_series.loc[actual_calc_end_date:actual_fwd_end_date])\n",
    "#     if benchmark_price_series is not None:\n",
    "#         perf_fwd_b = print_verification_steps(f\"Benchmark\", benchmark_price_series.loc[actual_calc_end_date:actual_fwd_end_date])\n",
    "\n",
    "# def verify_ticker_ranking_metrics(df_ohlcv, ticker, start_date, calc_period,\n",
    "#                                   master_calendar_ticker='VOO', export_csv=False):\n",
    "#     \"\"\"Verifies ranking metrics for a single ticker and optionally exports the data.\"\"\"\n",
    "#     display(Markdown(f\"## Verification Report for Ticker Ranking: `{ticker}`\"))\n",
    "    \n",
    "#     # 1. Setup trading day calendar and determine exact period dates\n",
    "#     if master_calendar_ticker not in df_ohlcv.index.get_level_values(0):\n",
    "#         raise ValueError(f\"Master calendar ticker '{master_calendar_ticker}' not found in DataFrame.\")\n",
    "#     master_trading_days = df_ohlcv.loc[master_calendar_ticker].index.unique().sort_values()\n",
    "\n",
    "#     start_date_raw = pd.to_datetime(start_date)\n",
    "#     start_idx = master_trading_days.searchsorted(start_date_raw)\n",
    "#     if start_idx >= len(master_trading_days):\n",
    "#         print(f\"Error: Start date {start_date_raw.date()} is after the last available trading day.\"); return\n",
    "#     actual_start_date = master_trading_days[start_idx]\n",
    "    \n",
    "#     calc_end_idx = min(start_idx + calc_period, len(master_trading_days) - 1)\n",
    "#     actual_calc_end_date = master_trading_days[calc_end_idx]\n",
    "\n",
    "#     # 2. Extract and prepare the raw data for the specific ticker and period\n",
    "#     df_ticker = df_ohlcv.loc[ticker].sort_index()\n",
    "#     calc_df = df_ticker.loc[actual_start_date:actual_calc_end_date].copy()\n",
    "#     if calc_df.empty or len(calc_df) < 2: \n",
    "#         print(\"No data or not enough data in calc period.\"); return\n",
    "\n",
    "#     display(Markdown(\"### A. Calculation Period (for Ranking Metrics)\"))\n",
    "#     display(Markdown(f\"**Period Start:** `{actual_start_date.date()}`\\n\"\n",
    "#                     f\"**Period End:** `{actual_calc_end_date.date()}`\\n\"\n",
    "#                     f\"**Total Trading Days:** `{len(calc_df)}` (Requested: `{calc_period}`)\"))\n",
    "    \n",
    "#     display(Markdown(\"#### Detailed Metric Calculation Data\"))\n",
    "    \n",
    "#     # 3. Calculate all intermediate metrics as new columns for full transparency\n",
    "#     vdf = calc_df[['Adj High', 'Adj Low', 'Adj Close']].copy()\n",
    "#     vdf['Daily_Return'] = vdf['Adj Close'].pct_change()\n",
    "    \n",
    "#     # Corrected True Range (TR) calculation for a single ticker (Series)\n",
    "#     tr_df = pd.DataFrame({\n",
    "#         'h_l': vdf['Adj High'] - vdf['Adj Low'],\n",
    "#         'h_cp': abs(vdf['Adj High'] - vdf['Adj Close'].shift(1)),\n",
    "#         'l_cp': abs(vdf['Adj Low'] - vdf['Adj Close'].shift(1))\n",
    "#     })\n",
    "#     vdf['TR'] = tr_df.max(axis=1)\n",
    "    \n",
    "#     vdf['ATR_14'] = vdf['TR'].ewm(alpha=1/14, adjust=False).mean()\n",
    "#     vdf['ATRP'] = vdf['ATR_14'] / vdf['Adj Close']\n",
    "    \n",
    "#     print(\"--- Start of Calculation Period ---\")\n",
    "#     display(vdf.head())\n",
    "#     print(\"\\n--- End of Calculation Period ---\")\n",
    "#     display(vdf.tail())\n",
    "\n",
    "#     # 4. Optionally export this detailed breakdown to CSV\n",
    "#     if export_csv:\n",
    "#         output_dir = 'export_csv'\n",
    "#         os.makedirs(output_dir, exist_ok=True)\n",
    "#         filename = f\"verify_ticker_{actual_start_date.date()}_{ticker}.csv\"\n",
    "#         filepath = os.path.join(output_dir, filename)\n",
    "#         vdf.to_csv(filepath)\n",
    "#         print(f\"\\n✅ Data exported to: {filepath}\")\n",
    "    \n",
    "#     # 5. Print final metric calculations with formulas\n",
    "#     display(Markdown(\"#### `MetricValue` Verification Summary:\"))\n",
    "    \n",
    "#     calc_start_price = vdf['Adj Close'].bfill().iloc[0]\n",
    "#     calc_end_price = vdf['Adj Close'].ffill().iloc[-1]\n",
    "#     price_metric = (calc_end_price / calc_start_price)\n",
    "#     print(f\"1. Price Metric: (Last Price / First Price) = ({calc_end_price:.2f} / {calc_start_price:.2f}) = {price_metric:.4f}\")\n",
    "    \n",
    "#     daily_returns = vdf['Daily_Return'].dropna()\n",
    "#     sharpe_ratio = calculate_sharpe(daily_returns)\n",
    "#     print(f\"2. Sharpe Metric: (Mean Daily Return / Std Dev) * sqrt(252) = {sharpe_ratio:.4f}\")\n",
    "\n",
    "#     atrp_mean = vdf['ATRP'].mean()\n",
    "#     mean_daily_return = vdf['Daily_Return'].mean()\n",
    "#     sharpe_atr = (mean_daily_return / atrp_mean) if atrp_mean > 0 else 0\n",
    "#     print(f\"3. Sharpe (ATR) Metric: (Mean Daily Return / Mean ATRP) = ({mean_daily_return:.6f} / {atrp_mean:.6f}) = {sharpe_atr:.4f}\")\n",
    "\n",
    "\n",
    "# def verify_sharpe_atr_calculation_checked(df_ohlcv, features_df, tickers_to_verify, benchmark_ticker,\n",
    "#                                     start_date, calc_period, fwd_period,\n",
    "#                                     master_calendar_ticker='VOO', debug=False):\n",
    "#     \"\"\"\n",
    "#     Verifies the Sharpe (ATR) calculations for a portfolio and benchmark.\n",
    "\n",
    "#     This function transparently recalculates the key components for Sharpe (ATR)\n",
    "#     and can optionally export the underlying source data for manual inspection.\n",
    "#     \"\"\"\n",
    "#     display(Markdown(f\"## Verification Report for Sharpe (ATR) Calculation\"))\n",
    "#     display(Markdown(f\"**Portfolio Tickers:** `{tickers_to_verify}`\\n**Benchmark Ticker:** `{benchmark_ticker}`\"))\n",
    "\n",
    "#     # --- 1. Determine Exact Period Dates (No changes here) ---\n",
    "#     master_trading_days = df_ohlcv.loc[master_calendar_ticker].index.unique().sort_values()\n",
    "#     start_date_raw = pd.to_datetime(start_date)\n",
    "#     start_idx = master_trading_days.searchsorted(start_date_raw)\n",
    "#     actual_start_date = master_trading_days[start_idx]\n",
    "#     calc_end_idx = min(start_idx + calc_period, len(master_trading_days) - 1)\n",
    "#     fwd_end_idx = min(calc_end_idx + fwd_period, len(master_trading_days) - 1)\n",
    "#     actual_calc_end_date = master_trading_days[calc_end_idx]\n",
    "#     actual_fwd_end_date = master_trading_days[fwd_end_idx]\n",
    "    \n",
    "#     display(Markdown(f\"**Full Period:** `{actual_start_date.date()}` to `{actual_fwd_end_date.date()}`\\n\"\n",
    "#                     f\"**Calc End Date:** `{actual_calc_end_date.date()}`\"))\n",
    "\n",
    "#     # Original debug export block (can be kept or removed)\n",
    "#     if debug:\n",
    "#         # ... (original export code remains here) ...\n",
    "#         pass # Assuming original block is here\n",
    "\n",
    "#     # --- 2. Recreate Portfolio & Benchmark Series from Scratch ---\n",
    "#     df_close_full = df_ohlcv['Adj Close'].unstack(level=0)\n",
    "#     portfolio_prices_raw = df_close_full[tickers_to_verify].loc[actual_start_date:actual_fwd_end_date]\n",
    "#     portfolio_prices_norm = portfolio_prices_raw.div(portfolio_prices_raw.bfill().iloc[0])\n",
    "#     portfolio_value_series = portfolio_prices_norm.mean(axis=1)\n",
    "#     portfolio_return_series = portfolio_value_series.pct_change()\n",
    "#     p_idx = pd.MultiIndex.from_product([tickers_to_verify, portfolio_return_series.index])\n",
    "#     p_atrp_df = features_df.loc[features_df.index.intersection(p_idx)]['ATRP'].unstack(level=0)\n",
    "#     portfolio_atrp_series = p_atrp_df.mean(axis=1)\n",
    "\n",
    "# ###############################    \n",
    "#     # benchmark_return_series = df_close_full[benchmark_ticker].pct_change().loc[actual_start_date:actual_fwd_end_date]\n",
    "\n",
    "#     # 1. First, slice the raw prices for the desired date range.\n",
    "#     benchmark_prices_raw = df_close_full[benchmark_ticker].loc[actual_start_date:actual_fwd_end_date]\n",
    "#     # 2. Then, calculate the percentage change on the sliced data.\n",
    "#     benchmark_return_series = benchmark_prices_raw.pct_change()\n",
    "\n",
    "# ###############################    \n",
    "#     benchmark_atrp_series = features_df.xs(benchmark_ticker, level='Ticker')['ATRP'].loc[actual_start_date:actual_fwd_end_date]\n",
    "\n",
    "\n",
    "#     # +++ NEW: DETAILED RETURN CALCULATION TRACE (PORTFOLIO) +++\n",
    "#     if debug:\n",
    "#         display(Markdown(\"---\"))\n",
    "#         display(Markdown(\"### 🐛 Detailed Portfolio Return Calculation Trace\"))\n",
    "\n",
    "#         # Step 1: Show the raw prices being used\n",
    "#         print(\"\\n[STEP 1] Raw Adjusted Close prices used for calculation.\")\n",
    "#         print(\"Compare these values with your 'Adj Close' columns.\")\n",
    "#         display(portfolio_prices_raw)\n",
    "\n",
    "#         # Step 2: Show the normalization base and the result\n",
    "#         normalization_base = portfolio_prices_raw.bfill().iloc[0]\n",
    "#         print(\"\\n[STEP 2] Normalization base (first row of prices).\")\n",
    "#         print(\"Each column in Step 1 is divided by this corresponding value.\")\n",
    "#         display(pd.DataFrame(normalization_base).T)\n",
    "\n",
    "#         print(\"\\n[STEP 2a] Normalized prices (Result of division).\")\n",
    "#         print(\"Compare these values with your 'N Close' columns.\")\n",
    "#         display(portfolio_prices_norm)\n",
    "\n",
    "#         # Step 3: Show the averaged portfolio value series\n",
    "#         print(\"\\n[STEP 3] Averaged normalized portfolio value series.\")\n",
    "#         print(\"This is the row-by-row average of the table in Step 2a.\")\n",
    "#         print(\"Compare this series with your 'N_portf' column.\")\n",
    "#         display(pd.DataFrame(portfolio_value_series, columns=['N_portf']))\n",
    "\n",
    "#         # Step 4: Show the final portfolio return series\n",
    "#         print(\"\\n[STEP 4] Final portfolio daily return series (pct_change).\")\n",
    "#         print(\"This is the percentage change of the series in Step 3.\")\n",
    "#         print(\"Compare this series with your 'N_portf_rtn' column.\")\n",
    "#         display(pd.DataFrame(portfolio_return_series, columns=['N_portf_rtn']))\n",
    "#         display(Markdown(\"---\"))\n",
    "#     # +++ END OF NEW DEBUG BLOCK +++\n",
    "\n",
    "\n",
    "#     # --- 3. Define a Helper to Print Detailed Calculation Steps ---\n",
    "#     # MODIFIED to include more debug details inside\n",
    "#     def _calculate_and_print_metrics(period_name, returns, atrps):\n",
    "#         display(Markdown(f\"#### {period_name}\"))\n",
    "#         if returns.dropna().empty or atrps.dropna().empty:\n",
    "#             print(\"  - Not enough data to calculate.\")\n",
    "#             return np.nan\n",
    "\n",
    "#         # Standard calculations\n",
    "#         mean_return = returns.mean()\n",
    "#         mean_atrp = atrps.mean()\n",
    "#         sharpe_atr = mean_return / mean_atrp if mean_atrp > 0 else np.nan\n",
    "\n",
    "#         # +++ ADDED: Detailed Mean Calculation Breakdown +++\n",
    "#         if debug:\n",
    "#             valid_returns = returns.dropna()\n",
    "#             num_returns = valid_returns.count()\n",
    "#             sum_returns = valid_returns.sum()\n",
    "#             manual_mean = sum_returns / num_returns if num_returns > 0 else 0\n",
    "#             print(f\"  - (Debug) Number of valid daily returns: {num_returns}\")\n",
    "#             print(f\"  - (Debug) Sum of all daily returns:      {sum_returns:,.8f}\")\n",
    "#             print(f\"  - (Debug) Manually calculated mean:      {manual_mean:,.8f} (Sum / Count)\")\n",
    "#         # +++ END OF ADDED DETAIL +++\n",
    "\n",
    "#         print(f\"  - Mean Daily Return: {mean_return:,.6f}\")\n",
    "#         print(f\"  - Mean Daily ATRP:  {mean_atrp:,.6f}\")\n",
    "#         print(f\"  - Sharpe (ATR) = (Mean Return / Mean ATRP) = {sharpe_atr:,.4f}\")\n",
    "#         return sharpe_atr\n",
    "\n",
    "#     # --- 4. Run Verification for Portfolio (No changes here) ---\n",
    "#     display(Markdown(\"### A. Group Portfolio Verification\"))\n",
    "#     _calculate_and_print_metrics(\"Full Period\", portfolio_return_series, portfolio_atrp_series)\n",
    "#     _calculate_and_print_metrics(\"Calculation Period\", portfolio_return_series.loc[:actual_calc_end_date], portfolio_atrp_series.loc[:actual_calc_end_date])\n",
    "#     _calculate_and_print_metrics(\"Forward Period\", portfolio_return_series.loc[actual_calc_end_date:].iloc[1:], portfolio_atrp_series.loc[actual_calc_end_date:].iloc[1:])\n",
    "\n",
    "#     # --- 5. Run Verification for Benchmark (No changes here) ---\n",
    "#     display(Markdown(f\"### B. Benchmark ({benchmark_ticker}) Verification\"))\n",
    "#     _calculate_and_print_metrics(\"Full Period\", benchmark_return_series, benchmark_atrp_series)\n",
    "#     _calculate_and_print_metrics(\"Calculation Period\", benchmark_return_series.loc[:actual_calc_end_date], benchmark_atrp_series.loc[:actual_calc_end_date])\n",
    "#     _calculate_and_print_metrics(\"Forward Period\", benchmark_return_series.loc[actual_calc_end_date:].iloc[1:], benchmark_atrp_series.loc[actual_calc_end_date:].iloc[1:])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # --- F. AUTOMATION SCRIPT - STRATEGY SEARCH ---\n",
    "\n",
    "# def run_strategy_search(df_ohlcv, config):\n",
    "#     \"\"\"\n",
    "#     Runs the main backtesting loop with checkpointing to be resumable.\n",
    "#     \"\"\"\n",
    "#     start_time = time.time()\n",
    "    \n",
    "#     # --- 1. SETUP & LOAD PROGRESS ---\n",
    "#     print(\"--- Phase 1: Pre-processing and Loading Progress ---\")\n",
    "\n",
    "#     # +++ ADD THIS BLOCK +++\n",
    "#     # Pre-calculate all features for the entire dataset ONCE.\n",
    "#     print(\"--- Generating all features upfront... ---\")\n",
    "#     features_df = generate_features(df_ohlcv)\n",
    "\n",
    "#     # --- DELETE THIS LINE ---\n",
    "#     # quality_metrics_df = calculate_rolling_quality_metrics(df_ohlcv, window=252)\n",
    "    \n",
    "#     print(\"Unstacking data for performance...\")\n",
    "#     df_close_full = df_ohlcv['Adj Close'].unstack(level=0)\n",
    "#     df_high_full = df_ohlcv['Adj High'].unstack(level=0)\n",
    "#     df_low_full = df_ohlcv['Adj Low'].unstack(level=0)\n",
    "    \n",
    "#     master_calendar_ticker = config['master_calendar_ticker']\n",
    "#     master_trading_days = df_ohlcv.loc[master_calendar_ticker].index.unique().sort_values()\n",
    "#     print(f\"Master trading day calendar created from '{master_calendar_ticker}' ({len(master_trading_days)} days).\")\n",
    "\n",
    "#     results_path = config['results_output_path']\n",
    "#     completed_params = set()\n",
    "    \n",
    "#     if os.path.exists(results_path):\n",
    "#         print(f\"Found existing results file. Loading progress from: {results_path}\")\n",
    "#         df_progress = pd.read_csv(results_path)\n",
    "#         for _, row in df_progress.iterrows():\n",
    "#             param_key = (\n",
    "#                 row['calc_period'], row['fwd_period'], row['metric'],\n",
    "#                 (row['rank_start'], row['rank_end'])\n",
    "#             )\n",
    "#             completed_params.add(param_key)\n",
    "#         print(f\"Found {len(completed_params)} completed parameter sets to skip.\")\n",
    "#     else:\n",
    "#         print(\"No existing results file found. Starting a new run.\")\n",
    "\n",
    "#     print(\"✅ Pre-processing complete.\\n\")\n",
    "\n",
    "#     # --- 2. SETUP THE MAIN LOOP (No changes here) ---\n",
    "#     print(\"--- Phase 2: Setting up Simulation Loops ---\")\n",
    "#     param_combinations = list(product(\n",
    "#         config['calc_periods'], config['fwd_periods'],\n",
    "#         config['metrics'], config['rank_slices']\n",
    "#     ))\n",
    "#     search_start_date = pd.to_datetime(config['search_start_date'])\n",
    "#     search_end_date = pd.to_datetime(config['search_end_date'])\n",
    "#     start_idx = master_trading_days.searchsorted(search_start_date, side='left')\n",
    "#     end_idx = master_trading_days.searchsorted(search_end_date, side='right')\n",
    "#     step_dates_map = {}\n",
    "#     print(\"Pre-calculating rebalancing schedules for each holding period...\")\n",
    "#     for fwd_period in sorted(config['fwd_periods']):\n",
    "#         step_indices = range(start_idx, end_idx, fwd_period)\n",
    "#         step_dates_map[fwd_period] = master_trading_days[step_indices]\n",
    "#         print(f\"  - Holding Period {fwd_period} days: {len(step_dates_map[fwd_period])} rebalances\")\n",
    "#     print(f\"Found {len(param_combinations)} total parameter sets to simulate.\")\n",
    "#     print(\"✅ Setup complete. Starting main loop...\\n\")\n",
    "\n",
    "#     # --- 3. RUN THE MAIN LOOP ---\n",
    "#     print(\"--- Phase 3: Running Simulations ---\")\n",
    "#     pbar = tqdm(param_combinations, desc=\"Parameter Sets\")\n",
    "    \n",
    "#     for params in pbar:\n",
    "#         calc_period, fwd_period, metric, rank_slice = params\n",
    "#         rank_start, rank_end = rank_slice\n",
    "        \n",
    "#         param_key = (calc_period, fwd_period, metric, rank_slice)\n",
    "#         if param_key in completed_params:\n",
    "#             pbar.set_description(f\"Skipping {param_key}\")\n",
    "#             continue\n",
    "\n",
    "#         pbar.set_description(f\"Running {param_key}\")\n",
    "        \n",
    "#         current_params_results = []\n",
    "        \n",
    "#         current_step_dates = step_dates_map[fwd_period]\n",
    "#         for step_date in current_step_dates:\n",
    "#             # +++ UPDATE THIS CALL +++\n",
    "#             eligible_tickers = get_eligible_universe(\n",
    "#                 features_df, filter_date=step_date, thresholds=config['quality_thresholds']\n",
    "#             )\n",
    "#             if not eligible_tickers: continue\n",
    "            \n",
    "#             df_close_step = df_close_full[eligible_tickers]\n",
    "#             df_high_step = df_high_full[eligible_tickers]\n",
    "#             df_low_step = df_low_full[eligible_tickers]\n",
    "\n",
    "#             # +++ UPDATE THIS CALL +++\n",
    "#             step_result, _ = run_walk_forward_step(\n",
    "#                 df_close_full=df_close_step, df_high_full=df_high_step, df_low_full=df_low_step,\n",
    "#                 master_trading_days=master_trading_days, start_date=step_date,\n",
    "#                 calc_period=calc_period, fwd_period=fwd_period,\n",
    "#                 metric=metric, rank_start=rank_start, rank_end=rank_end,\n",
    "#                 benchmark_ticker=config['benchmark_ticker'],\n",
    "#                 features_df=features_df, # Pass the features_df\n",
    "#                 debug=False\n",
    "#             )\n",
    "            \n",
    "#             if step_result['error'] is None:\n",
    "#                 p = step_result['performance_data']\n",
    "#                 log_entry = {\n",
    "#                     'step_date': step_date.date(), 'calc_period': calc_period,\n",
    "#                     'fwd_period': fwd_period, 'metric': metric,\n",
    "#                     'rank_start': rank_start, 'rank_end': rank_end,\n",
    "#                     'num_universe': len(eligible_tickers),\n",
    "#                     'num_portfolio': len(step_result['tickers_to_display']),\n",
    "#                     'fwd_p_gain': p['fwd_p_gain'], 'fwd_b_gain': p['fwd_b_gain'],\n",
    "#                     'fwd_gain_delta': p['fwd_p_gain'] - p['fwd_b_gain'] if not np.isnan(p['fwd_b_gain']) else np.nan,\n",
    "#                     'fwd_p_sharpe': p['fwd_p_sharpe'],\n",
    "#                 }\n",
    "#                 current_params_results.append(log_entry)\n",
    "        \n",
    "#         # --- CHECKPOINTING (No changes here) ---\n",
    "#         if current_params_results:\n",
    "#             df_to_append = pd.DataFrame(current_params_results)\n",
    "#             df_to_append.to_csv(\n",
    "#                 results_path,\n",
    "#                 mode='a',\n",
    "#                 header=not os.path.exists(results_path),\n",
    "#                 index=False\n",
    "#             )\n",
    "#             completed_params.add(param_key)\n",
    "\n",
    "#     print(\"✅ Main loop finished.\\n\")\n",
    "    \n",
    "#     # --- 4. RETURN FINAL DATAFRAME (No changes here) ---\n",
    "#     print(\"--- Phase 4: Loading Final Results ---\")\n",
    "#     if os.path.exists(results_path):\n",
    "#         final_df = pd.read_csv(results_path)\n",
    "#         end_time = time.time()\n",
    "#         print(f\"✅ Process complete. Total execution time: {time.time() - start_time:.2f} seconds.\")\n",
    "#         return final_df\n",
    "#     else:\n",
    "#         print(\"Warning: No results were generated.\")\n",
    "#         return None    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072b3d79",
   "metadata": {},
   "source": [
    "# Below are only functions that required to run plot_walk_forward_analyzer  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64cc5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import ipywidgets as widgets\n",
    "import pprint\n",
    "from IPython.display import display, Markdown\n",
    "import os # Make sure os is imported for the export function later\n",
    "\n",
    "\n",
    "def calculate_gain(price_series: pd.Series):\n",
    "    \"\"\"Calculates the total gain over a series of prices.\"\"\"\n",
    "    # Ensure there are at least two data points to calculate a gain\n",
    "    if price_series.dropna().shape[0] < 2: return np.nan\n",
    "    # Use forward-fill for the end price and back-fill for the start price\n",
    "    # to handle potential NaNs at the beginning or end of the series.\n",
    "    return (price_series.ffill().iloc[-1] / price_series.bfill().iloc[0]) - 1\n",
    "\n",
    "def calculate_sharpe(return_series: pd.Series):\n",
    "    \"\"\"Calculates the annualized Sharpe ratio from a series of daily returns.\"\"\"\n",
    "    # Ensure there are at least two returns to calculate a standard deviation\n",
    "    if return_series.dropna().shape[0] < 2: return np.nan\n",
    "    std_dev = return_series.std()\n",
    "    # Avoid division by zero if returns are constant\n",
    "    if std_dev > 0 and std_dev != np.inf:\n",
    "        return (return_series.mean() / std_dev) * np.sqrt(252)\n",
    "    return np.nan\n",
    "\n",
    "def calculate_sharpe_atr(return_series: pd.Series, atrp_series: pd.Series):\n",
    "    \"\"\"Calculates a Sharpe-like ratio using mean ATRP as the denominator.\"\"\"\n",
    "    # Ensure there are returns and that ATRP data is valid\n",
    "    if return_series.dropna().shape[0] < 2 or atrp_series.dropna().empty:\n",
    "        return np.nan\n",
    "        \n",
    "    mean_return = return_series.mean()\n",
    "    mean_atrp = atrp_series.mean()\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    if mean_atrp > 0 and mean_atrp != np.inf:\n",
    "        return mean_return / mean_atrp\n",
    "        \n",
    "    return np.nan\n",
    "\n",
    "\n",
    "# --- B. MODULAR METRIC CALCULATION ENGINE ---\n",
    "\n",
    "def calculate_price_metric(metric_data: dict) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Calculates the 'Price' metric (total gain) over the calculation period.\n",
    "\n",
    "    Args:\n",
    "        metric_data: A dictionary containing pre-calculated data Series.\n",
    "                     Requires 'calc_close' (pd.DataFrame): The close prices for the calc period.\n",
    "\n",
    "    Returns:\n",
    "        A pandas Series of the calculated metric values, indexed by Ticker.\n",
    "    \"\"\"\n",
    "    calc_close = metric_data['calc_close']\n",
    "    \n",
    "    # Ensure there are at least two data points to calculate a gain\n",
    "    if len(calc_close) < 2:\n",
    "        return pd.Series(dtype='float64', index=calc_close.columns)\n",
    "\n",
    "    first_prices = calc_close.bfill().iloc[0]\n",
    "    last_prices = calc_close.ffill().iloc[-1]\n",
    "    \n",
    "    # The division of two Series aligns by index (Ticker), which is what we want.\n",
    "    price_metric = last_prices / first_prices\n",
    "    \n",
    "    return price_metric.dropna()\n",
    "\n",
    "def calculate_sharpe_metric(metric_data: dict) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Calculates the annualized 'Sharpe' ratio metric over the calculation period.\n",
    "\n",
    "    Args:\n",
    "        metric_data: A dictionary containing pre-calculated data Series.\n",
    "                     Requires 'daily_returns' (pd.DataFrame): Daily returns for the calc period.\n",
    "\n",
    "    Returns:\n",
    "        A pandas Series of the calculated metric values, indexed by Ticker.\n",
    "    \"\"\"\n",
    "    daily_returns = metric_data['daily_returns']\n",
    "    \n",
    "    # Ensure there's enough data to calculate standard deviation\n",
    "    if len(daily_returns.dropna()) < 2:\n",
    "        return pd.Series(dtype='float64', index=daily_returns.columns)\n",
    "    \n",
    "    mean_returns = daily_returns.mean()\n",
    "    std_returns = daily_returns.std()\n",
    "    \n",
    "    # Standard annualized Sharpe Ratio calculation. Avoid division by zero.\n",
    "    # We replace resulting NaNs/infs with 0 to handle cases of zero volatility.\n",
    "    sharpe_ratio = (mean_returns / std_returns * np.sqrt(252))\n",
    "    \n",
    "    return sharpe_ratio.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "    \n",
    "def calculate_sharpe_atr_metric(metric_data: dict) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Calculates the 'Sharpe (ATR)' metric over the calculation period.\n",
    "\n",
    "    Args:\n",
    "        metric_data: A dictionary containing pre-calculated data Series.\n",
    "                     Requires 'daily_returns' (pd.DataFrame): Daily returns for the calc period.\n",
    "                     Requires 'atrp' (pd.Series): Mean ATRP for each ticker over the period.\n",
    "\n",
    "    Returns:\n",
    "        A pandas Series of the calculated metric values, indexed by Ticker.\n",
    "    \"\"\"\n",
    "    daily_returns = metric_data['daily_returns']\n",
    "    atrp = metric_data['atrp']\n",
    "    \n",
    "    mean_returns = daily_returns.mean()\n",
    "    \n",
    "    # ATRP-based Sharpe. Avoid division by zero.\n",
    "    # We replace resulting NaNs/infs with 0.\n",
    "    sharpe_atr = mean_returns / atrp\n",
    "    \n",
    "    return sharpe_atr.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "\n",
    "METRIC_REGISTRY = {\n",
    "    'Price': calculate_price_metric,\n",
    "    'Sharpe': calculate_sharpe_metric,\n",
    "    'Sharpe (ATR)': calculate_sharpe_atr_metric,\n",
    "}\n",
    "\n",
    "def plot_walk_forward_analyzer(df_ohlcv,\n",
    "                               default_start_date=None, default_calc_period=126, default_fwd_period=63,\n",
    "                               default_metric='Sharpe (ATR)', default_rank_start=1, default_rank_end=10,\n",
    "                               default_benchmark_ticker='VOO', master_calendar_ticker='VOO',\n",
    "                               quality_thresholds={'min_median_dollar_volume': 1_000_000, 'max_stale_pct': 0.05, 'max_same_vol_count': 10},\n",
    "                               debug=False):\n",
    "    # (No changes to the initial setup part of this function...)\n",
    "    print(\"Initializing Walk-Forward Analyzer (using Trading Day Logic)...\")\n",
    "    if not isinstance(df_ohlcv.index, pd.MultiIndex): raise ValueError(\"Input DataFrame must have a (Ticker, Date) MultiIndex.\")\n",
    "    df_ohlcv = df_ohlcv.sort_index()\n",
    "    if master_calendar_ticker not in df_ohlcv.index.get_level_values(0):\n",
    "        raise ValueError(f\"Master calendar ticker '{master_calendar_ticker}' not found in DataFrame.\")\n",
    "    master_trading_days = df_ohlcv.loc[master_calendar_ticker].index.unique().sort_values()\n",
    "    print(f\"Master trading day calendar created from '{master_calendar_ticker}' ({len(master_trading_days)} days).\")\n",
    "    print(\"--- Generating all features upfront... ---\")\n",
    "    features_df = generate_features(df_ohlcv)\n",
    "    print(\"Pre-processing data (unstacking)...\")\n",
    "    df_close_full = df_ohlcv['Adj Close'].unstack(level=0)\n",
    "    df_high_full = df_ohlcv['Adj High'].unstack(level=0)\n",
    "    df_low_full = df_ohlcv['Adj Low'].unstack(level=0)\n",
    "    start_date_picker = widgets.DatePicker(description='Start Date:', value=pd.to_datetime(default_start_date), disabled=False)\n",
    "    calc_period_input = widgets.IntText(value=default_calc_period, description='Calc Period (days):')\n",
    "    fwd_period_input = widgets.IntText(value=default_fwd_period, description='Fwd Period (days):')\n",
    "    if default_metric not in METRIC_REGISTRY:\n",
    "        fallback_metric = list(METRIC_REGISTRY.keys())[0]\n",
    "        print(f\"⚠️ Warning: Default metric '{default_metric}' not in registry. Using '{fallback_metric}'.\")\n",
    "        default_metric = fallback_metric\n",
    "    metric_dropdown = widgets.Dropdown(options=list(METRIC_REGISTRY.keys()), value=default_metric, description='Metric:')\n",
    "    rank_start_input = widgets.IntText(value=default_rank_start, description='Rank Start:')\n",
    "    rank_end_input = widgets.IntText(value=default_rank_end, description='Rank End:')\n",
    "    benchmark_ticker_input = widgets.Text(value=default_benchmark_ticker, description='Benchmark:', placeholder='Enter Ticker')\n",
    "    update_button = widgets.Button(description=\"Update Chart\", button_style='primary')\n",
    "    ticker_list_output = widgets.Output()\n",
    "    results_container, debug_data_container = [None], [None]\n",
    "    fig = go.FigureWidget()\n",
    "    max_traces = 50\n",
    "    for i in range(max_traces): fig.add_trace(go.Scatter(x=[None], y=[None], mode='lines', name=f'placeholder_{i}', visible=False, showlegend=False))\n",
    "    fig.add_trace(go.Scatter(x=[None], y=[None], mode='lines', name='Benchmark', visible=True, showlegend=True, line=dict(color='black', width=3, dash='dash')))\n",
    "    fig.add_trace(go.Scatter(x=[None], y=[None], mode='lines', name='Group Portfolio', visible=True, showlegend=True, line=dict(color='green', width=3)))\n",
    "\n",
    "    def update_plot(button_click):\n",
    "        ticker_list_output.clear_output()\n",
    "        start_date_raw = pd.to_datetime(start_date_picker.value)\n",
    "        start_date_idx = master_trading_days.searchsorted(start_date_raw)\n",
    "        if start_date_idx >= len(master_trading_days):\n",
    "            with ticker_list_output: print(f\"Error: Start date is after the last available trading day.\"); return\n",
    "        actual_start_date = master_trading_days[start_date_idx]\n",
    "        with ticker_list_output:\n",
    "            if start_date_raw.date() != actual_start_date.date():\n",
    "                print(f\"ℹ️ Info: Start date {start_date_raw.date()} is not a trading day. Snapping forward to {actual_start_date.date()}.\")\n",
    "        calc_period = calc_period_input.value; fwd_period = fwd_period_input.value; metric = metric_dropdown.value\n",
    "        rank_start = rank_start_input.value; rank_end = rank_end_input.value; benchmark_ticker = benchmark_ticker_input.value.strip().upper()\n",
    "        if rank_start > rank_end:\n",
    "            with ticker_list_output: print(\"Error: 'Rank Start' must be <= 'Rank End'.\"); return\n",
    "        if rank_start < 1 or calc_period < 2 or fwd_period < 1:\n",
    "            with ticker_list_output: print(\"Error: Ranks must be >= 1, Calc Period >= 2, Fwd Period >= 1.\"); return\n",
    "        required_days = calc_period + fwd_period\n",
    "        if start_date_idx + required_days > len(master_trading_days):\n",
    "            available_days = len(master_trading_days) - start_date_idx; last_available_date = master_trading_days[-1].date()\n",
    "            with ticker_list_output:\n",
    "                print(f\"Error: Not enough data for the requested period.\\n  Start Date: {actual_start_date.date()}\\n  Required Days: {calc_period} (calc) + {fwd_period} (fwd) = {required_days}\\n  Available Days from Start: {available_days} (until {last_available_date})\\n  Please shorten the 'Calc Period' / 'Fwd Period' or choose an earlier 'Start Date'.\")\n",
    "            return\n",
    "        eligible_tickers = get_eligible_universe(features_df, actual_start_date, quality_thresholds)\n",
    "        if not eligible_tickers:\n",
    "            with ticker_list_output: print(f\"Error: No eligible tickers found on {actual_start_date.date()} with the current quality filters.\"); return\n",
    "        df_close_step = df_close_full[eligible_tickers]; df_high_step = df_high_full[eligible_tickers]; df_low_step = df_low_full[eligible_tickers]\n",
    "        results, debug_output = run_walk_forward_step(df_close_full, df_high_full, df_low_full, master_trading_days, actual_start_date, calc_period, fwd_period, metric, rank_start, rank_end, benchmark_ticker, features_df=features_df, debug=debug)\n",
    "        if results.get('error'):\n",
    "            with ticker_list_output: print(f\"Error: {results['error']}\"); return\n",
    "        period_dates = {'calc_period_start': results['safe_start_date'], 'calc_period_end': results['actual_calc_end_ts'], 'forward_period_start': results['actual_calc_end_ts'], 'forward_period_end': results['safe_viz_end_date']}\n",
    "        run_parameters = {'calc_period': calc_period, 'fwd_period': fwd_period, 'rank_metric': metric, 'rank_start': rank_start, 'rank_end': rank_end, 'benchmark_ticker': benchmark_ticker}\n",
    "        results.update(period_dates); results.update(run_parameters)\n",
    "        if debug_output is not None and isinstance(debug_output, dict):\n",
    "            debug_output.update(period_dates); debug_output.update(run_parameters)\n",
    "        with fig.batch_update():\n",
    "            for i in range(max_traces):\n",
    "                trace = fig.data[i]\n",
    "                if i < len(results['tickers_to_display']):\n",
    "                    ticker = results['tickers_to_display'][i]; plot_data_series = results['normalized_plot_data'][ticker]\n",
    "                    trace.x, trace.y, trace.name, trace.visible, trace.showlegend = plot_data_series.index, plot_data_series.values, ticker, True, True\n",
    "                else: trace.visible, trace.showlegend = False, False\n",
    "            benchmark_trace = fig.data[max_traces]\n",
    "            if results['benchmark_price_series'] is not None and not results['benchmark_price_series'].loc[results['safe_start_date']:results['safe_viz_end_date']].dropna().empty:\n",
    "                normalized_benchmark = results['benchmark_price_series'].loc[results['safe_start_date']:results['safe_viz_end_date']] / results['benchmark_price_series'].loc[results['safe_start_date']:].bfill().iloc[0]\n",
    "                benchmark_trace.x, benchmark_trace.y, benchmark_trace.name, benchmark_trace.visible = normalized_benchmark.index, normalized_benchmark.values, f\"Benchmark ({benchmark_ticker})\", True\n",
    "            else: benchmark_trace.visible = False\n",
    "            portfolio_trace = fig.data[max_traces + 1]\n",
    "            portfolio_trace.x, portfolio_trace.y, portfolio_trace.name, portfolio_trace.visible = results['portfolio_series'].index, results['portfolio_series'], 'Group Portfolio', True\n",
    "            fig.layout.shapes = []; fig.add_shape(type=\"line\", x0=results['actual_calc_end_ts'], y0=0, x1=results['actual_calc_end_ts'], y1=1, xref='x', yref='paper', line=dict(color=\"grey\", width=2, dash=\"dash\"))\n",
    "        results_container[0] = results; debug_data_container[0] = debug_output\n",
    "        with ticker_list_output:\n",
    "            print(f\"Analysis Period: {results['safe_start_date'].date()} to {results['safe_viz_end_date'].date()}.\")\n",
    "            pprint.pprint(results['tickers_to_display'])\n",
    "            p = results['performance_data']\n",
    "            \n",
    "            # --- START OF MODIFIED BLOCK ---\n",
    "            rows = []\n",
    "            # Gain Metrics\n",
    "            rows.append({'Metric': 'Group Portfolio Gain', 'Full': p['full_p_gain'], 'Calc': p['calc_p_gain'], 'Fwd': p['fwd_p_gain']})\n",
    "            if not np.isnan(p.get('full_b_gain')):\n",
    "                rows.append({'Metric': f'Benchmark ({benchmark_ticker}) Gain', 'Full': p['full_b_gain'], 'Calc': p['calc_b_gain'], 'Fwd': p['fwd_b_gain']})\n",
    "                rows.append({'Metric': '== Gain Delta (vs Bm)', 'Full': p['full_p_gain'] - p['full_b_gain'], 'Calc': p['calc_p_gain'] - p['calc_b_gain'], 'Fwd': p['fwd_p_gain'] - p['fwd_b_gain']})\n",
    "            \n",
    "            # Standard Sharpe Metrics\n",
    "            rows.append({'Metric': 'Group Portfolio Sharpe', 'Full': p['full_p_sharpe'], 'Calc': p['calc_p_sharpe'], 'Fwd': p['fwd_p_sharpe']})\n",
    "            if not np.isnan(p.get('full_b_sharpe')):\n",
    "                rows.append({'Metric': f'Benchmark ({benchmark_ticker}) Sharpe', 'Full': p['full_b_sharpe'], 'Calc': p['calc_b_sharpe'], 'Fwd': p['fwd_b_sharpe']})\n",
    "                rows.append({'Metric': '== Sharpe Delta (vs Bm)', 'Full': p['full_p_sharpe'] - p['full_b_sharpe'], 'Calc': p['calc_p_sharpe'] - p['calc_b_sharpe'], 'Fwd': p['fwd_p_sharpe'] - p['fwd_b_sharpe']})\n",
    "\n",
    "            # Sharpe (ATR) Metrics\n",
    "            rows.append({'Metric': 'Group Portfolio Sharpe (ATR)', 'Full': p['full_p_sharpe_atr'], 'Calc': p['calc_p_sharpe_atr'], 'Fwd': p['fwd_p_sharpe_atr']})\n",
    "            if not np.isnan(p.get('full_b_sharpe_atr')):\n",
    "                rows.append({'Metric': f'Benchmark ({benchmark_ticker}) Sharpe (ATR)', 'Full': p['full_b_sharpe_atr'], 'Calc': p['calc_b_sharpe_atr'], 'Fwd': p['fwd_b_sharpe_atr']})\n",
    "                rows.append({'Metric': '== Sharpe (ATR) Delta (vs Bm)', 'Full': p['full_p_sharpe_atr'] - p['full_b_sharpe_atr'], 'Calc': p['calc_p_sharpe_atr'] - p['calc_b_sharpe_atr'], 'Fwd': p['fwd_p_sharpe_atr'] - p['fwd_b_sharpe_atr']})\n",
    "\n",
    "            report_df = pd.DataFrame(rows).set_index('Metric')\n",
    "            gain_rows = [row for row in report_df.index if 'Gain' in row]\n",
    "            sharpe_rows = [row for row in report_df.index if 'Sharpe' in row] # This now correctly includes both types of Sharpe\n",
    "            # --- END OF MODIFIED BLOCK ---\n",
    "            \n",
    "            styled_df = report_df.style.format('{:+.2%}', na_rep='N/A', subset=(gain_rows, report_df.columns)).format('{:+.4f}', na_rep='N/A', subset=(sharpe_rows, report_df.columns)).set_properties(**{'text-align': 'right', 'width': '100px'}).set_table_styles([{'selector': 'th.col_heading', 'props': [('text-align', 'right')]}, {'selector': 'th.row_heading', 'props': [('text-align', 'left')]}])\n",
    "            print(\"\\n--- Strategy Performance Summary ---\")\n",
    "            display(styled_df)\n",
    "    fig.update_layout(title_text='Walk-Forward Performance Analysis', xaxis_title='Date', yaxis_title='Normalized Price (Start = 1)', hovermode='x unified', legend_title_text='Tickers (Ranked)', height=600, margin=dict(t=50))\n",
    "    fig.add_hline(y=1, line_width=1, line_dash=\"dash\", line_color=\"grey\")\n",
    "    update_button.on_click(update_plot)\n",
    "    controls_row1 = widgets.HBox([start_date_picker, calc_period_input, fwd_period_input])\n",
    "    controls_row2 = widgets.HBox([metric_dropdown, rank_start_input, rank_end_input, benchmark_ticker_input, update_button])\n",
    "    ui_container = widgets.VBox([controls_row1, controls_row2, ticker_list_output], layout=widgets.Layout(margin='10px 0 20px 0'))\n",
    "    display(ui_container, fig)\n",
    "    update_plot(None)\n",
    "    return (results_container, debug_data_container)\n",
    "\n",
    "\n",
    "def generate_features(df_ohlcv: pd.DataFrame, \n",
    "                      atr_period: int = 14, \n",
    "                      quality_window: int = 252, \n",
    "                      quality_min_periods: int = 126) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generates a comprehensive DataFrame of derived features from raw OHLCV data.\n",
    "\n",
    "    This function performs all heavy, window-based calculations upfront to be used\n",
    "    by downstream analysis functions. It calculates:\n",
    "    1. Technical Indicators: True Range (TR), ATR, and ATRP.\n",
    "    2. Data Quality Metrics: Rolling stale percentage, median dollar volume, etc.\n",
    "\n",
    "    Args:\n",
    "        df_ohlcv: The primary DataFrame with a (Ticker, Date) MultiIndex and\n",
    "                  columns for 'Adj High', 'Adj Low', 'Adj Close', 'Volume'.\n",
    "        atr_period: The lookback period for the ATR's Exponential Moving Average.\n",
    "        quality_window: The rolling window size for data quality metrics.\n",
    "        quality_min_periods: The minimum number of observations required to have\n",
    "                             a valid quality metric.\n",
    "\n",
    "    Returns:\n",
    "        A new DataFrame with the same (Ticker, Date) MultiIndex containing all\n",
    "        calculated feature columns.\n",
    "    \"\"\"\n",
    "    print(\"--- Starting Feature Generation ---\")\n",
    "    \n",
    "    # Ensure the DataFrame is sorted for correct window and shift operations\n",
    "    # FIX: Replaced is_lexsorted() with the current pandas attribute\n",
    "    if not df_ohlcv.index.is_monotonic_increasing:\n",
    "        print(\"Sorting index for calculation accuracy...\")\n",
    "        df_ohlcv = df_ohlcv.sort_index()\n",
    "\n",
    "    # --- 1. Technical Indicator Calculation (TR, ATR, ATRP) ---\n",
    "    print(f\"Calculating technical indicators (ATR Period: {atr_period})...\")\n",
    "    \n",
    "    # Group by ticker to handle each security independently\n",
    "    grouped = df_ohlcv.groupby(level='Ticker')\n",
    "    \n",
    "    # Get the previous day's close required for True Range\n",
    "    prev_close = grouped['Adj Close'].shift(1)\n",
    "    \n",
    "    # Calculate the three components of True Range\n",
    "    high_low = df_ohlcv['Adj High'] - df_ohlcv['Adj Low']\n",
    "    high_prev_close = abs(df_ohlcv['Adj High'] - prev_close)\n",
    "    low_prev_close = abs(df_ohlcv['Adj Low'] - prev_close)\n",
    "    \n",
    "    # Combine the components to get the final TR\n",
    "    tr = pd.concat([high_low, high_prev_close, low_prev_close], axis=1).max(axis=1, skipna=False)\n",
    "    \n",
    "    # # Calculate the ATR using an Exponential Moving Average\n",
    "    # --- FIX IS HERE ---\n",
    "    # Use .transform() to apply the EWM function. \n",
    "    # This guarantees the resulting Series has the exact same index as 'tr',\n",
    "    # preventing the index alignment error during the subsequent division.\n",
    "    atr = tr.groupby(level='Ticker').transform(\n",
    "        lambda x: x.ewm(alpha=1/atr_period, adjust=False).mean()\n",
    "    )\n",
    "\n",
    "    # --- CHANGE 1: Removed .fillna(0) ---\n",
    "    # ATRP will now be NaN on the first day, consistent with TR and ATR.\n",
    "    atrp = (atr / df_ohlcv['Adj Close']).replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    indicator_df = pd.DataFrame({\n",
    "        'TR': tr,\n",
    "        'ATR': atr,\n",
    "        'ATRP': atrp\n",
    "    })\n",
    "    \n",
    "    # --- 2. Data Quality Metric Calculation ---\n",
    "    print(f\"Calculating data quality metrics (Window: {quality_window} days)...\")\n",
    "    \n",
    "    # Create intermediate flags needed for quality calculations\n",
    "    is_stale = np.where((df_ohlcv['Volume'] == 0) | (df_ohlcv['Adj High'] == df_ohlcv['Adj Low']), 1, 0)\n",
    "    dollar_volume = df_ohlcv['Adj Close'] * df_ohlcv['Volume']\n",
    "    has_same_volume = (grouped['Volume'].diff() == 0).astype(int)\n",
    "    \n",
    "    # Combine flags into a temporary DataFrame for rolling calculations\n",
    "    quality_temp_df = pd.DataFrame({\n",
    "        'IsStale': is_stale,\n",
    "        'DollarVolume': dollar_volume,\n",
    "        'HasSameVolume': has_same_volume\n",
    "    }, index=df_ohlcv.index) # Explicitly set index to be safe\n",
    "    \n",
    "    # Perform the rolling calculations on the grouped data\n",
    "    # --- FIX IS HERE ---\n",
    "    # We switch to the older, more compatible dictionary-based aggregation method.\n",
    "    # This syntax is understood by nearly all versions of pandas.\n",
    "    rolling_result = quality_temp_df.groupby(level='Ticker').rolling(\n",
    "        window=quality_window,\n",
    "        min_periods=quality_min_periods\n",
    "    ).agg({\n",
    "        'IsStale': 'mean',\n",
    "        'DollarVolume': 'median',\n",
    "        'HasSameVolume': 'sum'\n",
    "    })\n",
    "    \n",
    "    # The dictionary syntax produces columns with the original names ('IsStale', etc.).\n",
    "    # We now explicitly rename them to our desired final names.\n",
    "    rolling_result = rolling_result.rename(columns={\n",
    "        'IsStale': 'RollingStalePct',\n",
    "        'DollarVolume': 'RollMedDollarVol', # <-- RENAMED HERE\n",
    "        'HasSameVolume': 'RollingSameVolCount'\n",
    "    })\n",
    "\n",
    "    # The index after a grouped rolling operation is hierarchical.\n",
    "    # We remove the outermost 'Ticker' level to restore the original index structure.\n",
    "    rolling_quality = rolling_result.reset_index(level=0, drop=True)\n",
    "\n",
    "    # --- 3. Combine All Features ---\n",
    "    print(\"Combining all feature sets...\")\n",
    "    features_df = pd.concat([indicator_df, rolling_quality], axis=1)\n",
    "    \n",
    "    print(\"✅ Feature generation complete.\")\n",
    "    return features_df\n",
    "\n",
    "\n",
    "def get_eligible_universe(features_df, filter_date, thresholds):\n",
    "    \"\"\"Filters the universe of tickers based on quality metrics for a given date.\"\"\"\n",
    "    filter_date_ts = pd.to_datetime(filter_date)\n",
    "    # The index is now the comprehensive features_df\n",
    "    date_index = features_df.index.get_level_values('Date').unique().sort_values()\n",
    "    \n",
    "    if filter_date_ts < date_index[0]:\n",
    "        print(f\"Warning: Filter date {filter_date_ts.date()} is before the earliest data point. Returning empty universe.\")\n",
    "        return []\n",
    "        \n",
    "    # Find the most recent date with quality data on or before the filter date\n",
    "    valid_prior_dates = date_index[date_index <= filter_date_ts]\n",
    "    if valid_prior_dates.empty:\n",
    "        print(f\"Warning: No available data found on or before {filter_date_ts.date()}. Returning empty universe.\")\n",
    "        return []\n",
    "        \n",
    "    actual_date_to_use = valid_prior_dates[-1]\n",
    "    if actual_date_to_use.date() != filter_date_ts.date():\n",
    "        print(f\"ℹ️ Info: Filter date {filter_date_ts.date()} not found. Using previous available date {actual_date_to_use.date()}.\")\n",
    "\n",
    "    metrics_on_date = features_df.xs(actual_date_to_use, level='Date')\n",
    "    \n",
    "    # Apply filters using the new column names from features_df\n",
    "    mask = ((metrics_on_date['RollMedDollarVol'] >= thresholds['min_median_dollar_volume']) & # <-- RENAMED\n",
    "            (metrics_on_date['RollingStalePct'] <= thresholds['max_stale_pct']) &\n",
    "            (metrics_on_date['RollingSameVolCount'] <= thresholds['max_same_vol_count']))\n",
    "            \n",
    "    eligible_tickers = metrics_on_date[mask].index.tolist()\n",
    "    all_tickers = metrics_on_date.index.tolist()\n",
    "    print(f\"Dynamic Filter ({filter_date_ts.date()}): Kept {len(eligible_tickers)} of {len(all_tickers)} tickers.\")\n",
    "    return eligible_tickers\n",
    "\n",
    "\n",
    "def run_walk_forward_step(df_close_full, df_high_full, df_low_full,\n",
    "                          master_trading_days,\n",
    "                          start_date, calc_period, fwd_period,\n",
    "                          metric, rank_start, rank_end, benchmark_ticker,\n",
    "                          features_df,\n",
    "                          debug=False):\n",
    "    \"\"\"\n",
    "    Runs a single step of the walk-forward analysis with a strict, pre-emptive\n",
    "    check to ensure the full period is available.\n",
    "    \"\"\"\n",
    "    debug_data = {} if debug else None\n",
    "\n",
    "    # 1. Determine exact date ranges with a NEW pre-emptive check\n",
    "    try:\n",
    "        start_idx = master_trading_days.get_loc(start_date)\n",
    "    except KeyError:\n",
    "        return ({'error': f\"Start date {start_date.date()} is not a valid trading day.\"}, None)\n",
    "\n",
    "    # +++ THIS IS THE NEW PRE-EMPTIVE CHECK LOGIC +++\n",
    "    # Calculate the desired end index without clamping first.\n",
    "    desired_viz_end_idx = start_idx + calc_period + fwd_period\n",
    "    \n",
    "    # Check if the desired end index is out of bounds.\n",
    "    if desired_viz_end_idx >= len(master_trading_days):\n",
    "        last_available_date = master_trading_days[-1].date()\n",
    "        required_days = calc_period + fwd_period\n",
    "        available_days = len(master_trading_days) - start_idx\n",
    "        error_msg = (f\"Not enough data for the full requested period. \"\n",
    "                     f\"Required: {required_days} days, Available: {available_days} days until {last_available_date}.\")\n",
    "        return ({'error': error_msg}, None)\n",
    "    # --- END OF NEW CHECK ---\n",
    "\n",
    "    # If the check passes, we know the full period is available.\n",
    "    # The 'min' calls are now just a redundant safety measure.\n",
    "    calc_end_idx = min(start_idx + calc_period, len(master_trading_days) - 1)\n",
    "    viz_end_idx = min(calc_end_idx + fwd_period, len(master_trading_days) - 1)\n",
    "\n",
    "    safe_start_date = master_trading_days[start_idx]\n",
    "    safe_calc_end_date = master_trading_days[calc_end_idx]\n",
    "    safe_viz_end_date = master_trading_days[viz_end_idx]\n",
    "    \n",
    "    if safe_start_date >= safe_calc_end_date:\n",
    "        return ({'error': \"Invalid date range (calc period has zero or negative length).\"}, None)\n",
    "\n",
    "    # (The rest of the function remains completely unchanged...)\n",
    "    # 2. Slice data for the calculation period\n",
    "    calc_close_raw = df_close_full.loc[safe_start_date:safe_calc_end_date]\n",
    "    calc_close = calc_close_raw.dropna(axis=1, how='all')\n",
    "    if calc_close.shape[1] == 0 or len(calc_close) < 2:\n",
    "        return ({'error': \"Not enough data in calc period.\"}, None)\n",
    "\n",
    "    # 3. Calculate INTERMEDIATE data required for the ranking metrics\n",
    "    daily_returns = calc_close.bfill().ffill().pct_change()\n",
    "    valid_tickers = calc_close.columns\n",
    "    calc_period_index = pd.MultiIndex.from_product([valid_tickers, calc_close.index], names=['Ticker', 'Date'])\n",
    "    features_in_period = features_df.loc[features_df.index.intersection(calc_period_index)]\n",
    "    atrp = features_in_period.groupby(level='Ticker')['ATRP'].mean()\n",
    "\n",
    "    # 4. Calculate all ranking metrics by iterating through the METRIC_REGISTRY\n",
    "    metric_ingredients = { 'calc_close': calc_close, 'daily_returns': daily_returns, 'atrp': atrp, }\n",
    "    metric_values = {}\n",
    "    for name, func in METRIC_REGISTRY.items():\n",
    "        metric_values[name] = func(metric_ingredients)\n",
    "    if metric not in metric_values or metric_values[metric].empty:\n",
    "        return ({'error': f\"Metric '{metric}' could not be calculated or resulted in no valid tickers.\"}, None)\n",
    "\n",
    "    # 5. Rank tickers and select the portfolio\n",
    "    sorted_tickers = metric_values[metric].sort_values(ascending=False)\n",
    "    tickers_to_display = sorted_tickers.index[rank_start-1:rank_end].tolist()\n",
    "    if not tickers_to_display:\n",
    "        return ({'error': \"No tickers found for the selected rank.\"}, None)\n",
    "\n",
    "    # 6. Calculate Portfolio & Benchmark Performance\n",
    "    normalized_plot_data = df_close_full[tickers_to_display].loc[safe_start_date:safe_viz_end_date]\n",
    "    normalized_plot_data = normalized_plot_data.div(normalized_plot_data.bfill().iloc[0])\n",
    "    actual_calc_end_ts = calc_close.index.max()\n",
    "    portfolio_series = normalized_plot_data.mean(axis=1)\n",
    "    portfolio_return_series = portfolio_series.pct_change()\n",
    "    benchmark_price_series = df_close_full.get(benchmark_ticker)\n",
    "    benchmark_return_series = benchmark_price_series.loc[safe_start_date:safe_viz_end_date].bfill().ffill().pct_change() if benchmark_price_series is not None else pd.Series(dtype='float64')\n",
    "    try:\n",
    "        boundary_loc = portfolio_return_series.index.get_loc(actual_calc_end_ts)\n",
    "        calc_portfolio_returns = portfolio_return_series.iloc[:boundary_loc + 1]\n",
    "        fwd_portfolio_returns = portfolio_return_series.iloc[boundary_loc + 1:]\n",
    "        if benchmark_price_series is not None:\n",
    "            bm_boundary_loc = benchmark_return_series.index.get_loc(actual_calc_end_ts)\n",
    "            calc_benchmark_returns = benchmark_return_series.iloc[:bm_boundary_loc + 1]\n",
    "            fwd_benchmark_returns = benchmark_return_series.iloc[bm_boundary_loc + 1:]\n",
    "        else:\n",
    "            calc_benchmark_returns, fwd_benchmark_returns = pd.Series(dtype='float64'), pd.Series(dtype='float64')\n",
    "    except (KeyError, IndexError):\n",
    "        calc_portfolio_returns = portfolio_return_series.loc[:actual_calc_end_ts]\n",
    "        fwd_portfolio_returns = portfolio_return_series.loc[actual_calc_end_ts:].iloc[1:]\n",
    "        if benchmark_price_series is not None:\n",
    "            calc_benchmark_returns = benchmark_return_series.loc[:actual_calc_end_ts]\n",
    "            fwd_benchmark_returns = benchmark_return_series.loc[actual_calc_end_ts:].iloc[1:]\n",
    "        else:\n",
    "            calc_benchmark_returns, fwd_benchmark_returns = pd.Series(dtype='float64'), pd.Series(dtype='float64')\n",
    "    full_period_index = pd.MultiIndex.from_product([tickers_to_display, portfolio_return_series.index], names=['Ticker', 'Date'])\n",
    "    portfolio_atrp_features = features_df.loc[features_df.index.intersection(full_period_index)]\n",
    "    portfolio_atrp_daily_unstacked = portfolio_atrp_features['ATRP'].unstack(level='Ticker')\n",
    "    portfolio_atrp_series = portfolio_atrp_daily_unstacked.mean(axis=1)\n",
    "    if benchmark_ticker in df_close_full.columns:\n",
    "        benchmark_atrp_series = features_df.xs(benchmark_ticker, level='Ticker')['ATRP'].loc[safe_start_date:safe_viz_end_date]\n",
    "    else:\n",
    "        benchmark_atrp_series = pd.Series(dtype='float64')\n",
    "    calc_portfolio_atrp = portfolio_atrp_series.loc[:actual_calc_end_ts]\n",
    "    fwd_portfolio_atrp = portfolio_atrp_series.loc[actual_calc_end_ts:].iloc[1:]\n",
    "    calc_benchmark_atrp = benchmark_atrp_series.loc[:actual_calc_end_ts]\n",
    "    fwd_benchmark_atrp = benchmark_atrp_series.loc[actual_calc_end_ts:].iloc[1:]\n",
    "    perf_data = {}\n",
    "    perf_data['calc_p_gain'] = calculate_gain(portfolio_series.loc[:actual_calc_end_ts])\n",
    "    perf_data['fwd_p_gain'] = calculate_gain(portfolio_series.loc[actual_calc_end_ts:])\n",
    "    perf_data['full_p_gain'] = calculate_gain(portfolio_series)\n",
    "    perf_data['calc_p_sharpe'] = calculate_sharpe(calc_portfolio_returns)\n",
    "    perf_data['fwd_p_sharpe'] = calculate_sharpe(fwd_portfolio_returns)\n",
    "    perf_data['full_p_sharpe'] = calculate_sharpe(portfolio_return_series)\n",
    "    perf_data['calc_b_gain'] = calculate_gain(benchmark_price_series.loc[safe_start_date:actual_calc_end_ts]) if benchmark_price_series is not None else np.nan\n",
    "    perf_data['fwd_b_gain'] = calculate_gain(benchmark_price_series.loc[actual_calc_end_ts:safe_viz_end_date]) if benchmark_price_series is not None else np.nan\n",
    "    perf_data['full_b_gain'] = calculate_gain(benchmark_price_series.loc[safe_start_date:safe_viz_end_date]) if benchmark_price_series is not None else np.nan\n",
    "    perf_data['calc_b_sharpe'] = calculate_sharpe(calc_benchmark_returns)\n",
    "    perf_data['fwd_b_sharpe'] = calculate_sharpe(fwd_benchmark_returns)\n",
    "    perf_data['full_b_sharpe'] = calculate_sharpe(benchmark_return_series)\n",
    "    perf_data['calc_p_sharpe_atr'] = calculate_sharpe_atr(calc_portfolio_returns, calc_portfolio_atrp)\n",
    "    perf_data['fwd_p_sharpe_atr'] = calculate_sharpe_atr(fwd_portfolio_returns, fwd_portfolio_atrp)\n",
    "    perf_data['full_p_sharpe_atr'] = calculate_sharpe_atr(portfolio_return_series, portfolio_atrp_series)\n",
    "    perf_data['calc_b_sharpe_atr'] = calculate_sharpe_atr(calc_benchmark_returns, calc_benchmark_atrp)\n",
    "    perf_data['fwd_b_sharpe_atr'] = calculate_sharpe_atr(fwd_benchmark_returns, fwd_benchmark_atrp)\n",
    "    perf_data['full_b_sharpe_atr'] = calculate_sharpe_atr(benchmark_return_series, benchmark_atrp_series)\n",
    "    if debug:\n",
    "        df_ranking_base = pd.DataFrame({'MeanDailyReturn': daily_returns.mean(),'StdDevDailyReturn': daily_returns.std(),'MeanATRP': atrp})\n",
    "        df_metrics = pd.DataFrame(metric_values)\n",
    "        df_metrics.columns = [f'Metric_{col}' for col in df_metrics.columns]\n",
    "        df_ranking = df_ranking_base.join(df_metrics, how='left')\n",
    "        df_ranking.index.name = 'Ticker'\n",
    "        debug_data['ranking_metrics'] = df_ranking.sort_values(f'Metric_{metric}', ascending=False)\n",
    "    calc_end_prices = calc_close.ffill().iloc[-1]\n",
    "    fwd_close_slice = df_close_full.loc[actual_calc_end_ts:safe_viz_end_date]\n",
    "    viz_end_prices = fwd_close_slice.ffill().iloc[-1] if not fwd_close_slice.empty and len(fwd_close_slice) >= 2 else calc_end_prices\n",
    "    calc_gains = (calc_end_prices / calc_close.bfill().iloc[0]) - 1\n",
    "    fwd_gains = (viz_end_prices / calc_end_prices) - 1\n",
    "    results_df = pd.DataFrame({'Rank': range(rank_start, rank_start + len(tickers_to_display)), 'Metric': metric, 'MetricValue': sorted_tickers.loc[tickers_to_display].values, 'CalcPrice': calc_end_prices.loc[tickers_to_display], 'CalcGain': calc_gains.loc[tickers_to_display], 'FwdGain': fwd_gains.loc[tickers_to_display]}, index=pd.Index(tickers_to_display, name='Ticker'))\n",
    "    if benchmark_price_series is not None and benchmark_ticker in calc_close.columns:\n",
    "        benchmark_df_row = pd.DataFrame({'Rank': np.nan, 'Metric': metric, 'MetricValue': metric_values[metric].get(benchmark_ticker, np.nan), 'CalcPrice': calc_end_prices[benchmark_ticker], 'CalcGain': calc_gains[benchmark_ticker], 'FwdGain': fwd_gains[benchmark_ticker]}, index=pd.Index([f\"{benchmark_ticker} (BM)\"], name='Ticker'))\n",
    "        results_df = pd.concat([results_df, benchmark_df_row])\n",
    "    if debug:\n",
    "        df_trace = normalized_plot_data.copy()\n",
    "        df_trace.columns = [f'Norm_Price_{c}' for c in df_trace.columns]\n",
    "        df_trace['Norm_Price_Portfolio'] = portfolio_series\n",
    "        if benchmark_price_series is not None and not benchmark_price_series.loc[safe_start_date:safe_viz_end_date].dropna().empty:\n",
    "            norm_bm = benchmark_price_series.loc[safe_start_date:safe_viz_end_date] / benchmark_price_series.loc[safe_start_date:].bfill().iloc[0]\n",
    "            df_trace[f'Norm_Price_Benchmark_{benchmark_ticker}'] = norm_bm\n",
    "        for col in df_trace.columns:\n",
    "            if 'Norm_Price' in col:\n",
    "                df_trace[col.replace('Norm_Price', 'Return')] = df_trace[col].pct_change()\n",
    "        debug_data['portfolio_trace'] = df_trace\n",
    "    final_results = {\n",
    "        'tickers_to_display': tickers_to_display, 'normalized_plot_data': normalized_plot_data,\n",
    "        'portfolio_series': portfolio_series, 'benchmark_price_series': benchmark_price_series,\n",
    "        'performance_data': perf_data, 'results_df': results_df, 'actual_calc_end_ts': actual_calc_end_ts,\n",
    "        'safe_start_date': safe_start_date, 'safe_viz_end_date': safe_viz_end_date,\n",
    "        'error': None\n",
    "    }\n",
    "    return (final_results, debug_data)\n",
    "\n",
    "def verify_sharpe_atr_calculation_checked(df_ohlcv, features_df, tickers_to_verify, benchmark_ticker,\n",
    "                                    start_date, calc_period, fwd_period,\n",
    "                                    master_calendar_ticker='VOO', debug=False):\n",
    "    \"\"\"\n",
    "    Verifies the Sharpe (ATR) calculations for a portfolio and benchmark.\n",
    "\n",
    "    This function transparently recalculates the key components for Sharpe (ATR)\n",
    "    and can optionally export the underlying source data for manual inspection.\n",
    "    \"\"\"\n",
    "    display(Markdown(f\"## Verification Report for Sharpe (ATR) Calculation\"))\n",
    "    display(Markdown(f\"**Portfolio Tickers:** `{tickers_to_verify}`\\n**Benchmark Ticker:** `{benchmark_ticker}`\"))\n",
    "\n",
    "    # --- 1. Determine Exact Period Dates (No changes here) ---\n",
    "    master_trading_days = df_ohlcv.loc[master_calendar_ticker].index.unique().sort_values()\n",
    "    start_date_raw = pd.to_datetime(start_date)\n",
    "    start_idx = master_trading_days.searchsorted(start_date_raw)\n",
    "    actual_start_date = master_trading_days[start_idx]\n",
    "    calc_end_idx = min(start_idx + calc_period, len(master_trading_days) - 1)\n",
    "    fwd_end_idx = min(calc_end_idx + fwd_period, len(master_trading_days) - 1)\n",
    "    actual_calc_end_date = master_trading_days[calc_end_idx]\n",
    "    actual_fwd_end_date = master_trading_days[fwd_end_idx]\n",
    "    \n",
    "    display(Markdown(f\"**Full Period:** `{actual_start_date.date()}` to `{actual_fwd_end_date.date()}`\\n\"\n",
    "                    f\"**Calc End Date:** `{actual_calc_end_date.date()}`\"))\n",
    "\n",
    "    # Original debug export block (can be kept or removed)\n",
    "    if debug:\n",
    "        # ... (original export code remains here) ...\n",
    "        pass # Assuming original block is here\n",
    "\n",
    "    # --- 2. Recreate Portfolio & Benchmark Series from Scratch ---\n",
    "    df_close_full = df_ohlcv['Adj Close'].unstack(level=0)\n",
    "    portfolio_prices_raw = df_close_full[tickers_to_verify].loc[actual_start_date:actual_fwd_end_date]\n",
    "    portfolio_prices_norm = portfolio_prices_raw.div(portfolio_prices_raw.bfill().iloc[0])\n",
    "    portfolio_value_series = portfolio_prices_norm.mean(axis=1)\n",
    "    portfolio_return_series = portfolio_value_series.pct_change()\n",
    "    p_idx = pd.MultiIndex.from_product([tickers_to_verify, portfolio_return_series.index])\n",
    "    p_atrp_df = features_df.loc[features_df.index.intersection(p_idx)]['ATRP'].unstack(level=0)\n",
    "    portfolio_atrp_series = p_atrp_df.mean(axis=1)\n",
    "\n",
    "###############################    \n",
    "    # benchmark_return_series = df_close_full[benchmark_ticker].pct_change().loc[actual_start_date:actual_fwd_end_date]\n",
    "\n",
    "    # 1. First, slice the raw prices for the desired date range.\n",
    "    benchmark_prices_raw = df_close_full[benchmark_ticker].loc[actual_start_date:actual_fwd_end_date]\n",
    "    # 2. Then, calculate the percentage change on the sliced data.\n",
    "    benchmark_return_series = benchmark_prices_raw.pct_change()\n",
    "\n",
    "###############################    \n",
    "    benchmark_atrp_series = features_df.xs(benchmark_ticker, level='Ticker')['ATRP'].loc[actual_start_date:actual_fwd_end_date]\n",
    "\n",
    "\n",
    "    # +++ NEW: DETAILED RETURN CALCULATION TRACE (PORTFOLIO) +++\n",
    "    if debug:\n",
    "        display(Markdown(\"---\"))\n",
    "        display(Markdown(\"### 🐛 Detailed Portfolio Return Calculation Trace\"))\n",
    "\n",
    "        # Step 1: Show the raw prices being used\n",
    "        print(\"\\n[STEP 1] Raw Adjusted Close prices used for calculation.\")\n",
    "        print(\"Compare these values with your 'Adj Close' columns.\")\n",
    "        display(portfolio_prices_raw)\n",
    "\n",
    "        # Step 2: Show the normalization base and the result\n",
    "        normalization_base = portfolio_prices_raw.bfill().iloc[0]\n",
    "        print(\"\\n[STEP 2] Normalization base (first row of prices).\")\n",
    "        print(\"Each column in Step 1 is divided by this corresponding value.\")\n",
    "        display(pd.DataFrame(normalization_base).T)\n",
    "\n",
    "        print(\"\\n[STEP 2a] Normalized prices (Result of division).\")\n",
    "        print(\"Compare these values with your 'N Close' columns.\")\n",
    "        display(portfolio_prices_norm)\n",
    "\n",
    "        # Step 3: Show the averaged portfolio value series\n",
    "        print(\"\\n[STEP 3] Averaged normalized portfolio value series.\")\n",
    "        print(\"This is the row-by-row average of the table in Step 2a.\")\n",
    "        print(\"Compare this series with your 'N_portf' column.\")\n",
    "        display(pd.DataFrame(portfolio_value_series, columns=['N_portf']))\n",
    "\n",
    "        # Step 4: Show the final portfolio return series\n",
    "        print(\"\\n[STEP 4] Final portfolio daily return series (pct_change).\")\n",
    "        print(\"This is the percentage change of the series in Step 3.\")\n",
    "        print(\"Compare this series with your 'N_portf_rtn' column.\")\n",
    "        display(pd.DataFrame(portfolio_return_series, columns=['N_portf_rtn']))\n",
    "        display(Markdown(\"---\"))\n",
    "    # +++ END OF NEW DEBUG BLOCK +++\n",
    "\n",
    "\n",
    "    # --- 3. Define a Helper to Print Detailed Calculation Steps ---\n",
    "    # MODIFIED to include more debug details inside\n",
    "    def _calculate_and_print_metrics(period_name, returns, atrps):\n",
    "        display(Markdown(f\"#### {period_name}\"))\n",
    "        if returns.dropna().empty or atrps.dropna().empty:\n",
    "            print(\"  - Not enough data to calculate.\")\n",
    "            return np.nan\n",
    "\n",
    "        # Standard calculations\n",
    "        mean_return = returns.mean()\n",
    "        mean_atrp = atrps.mean()\n",
    "        sharpe_atr = mean_return / mean_atrp if mean_atrp > 0 else np.nan\n",
    "\n",
    "        # +++ ADDED: Detailed Mean Calculation Breakdown +++\n",
    "        if debug:\n",
    "            valid_returns = returns.dropna()\n",
    "            num_returns = valid_returns.count()\n",
    "            sum_returns = valid_returns.sum()\n",
    "            manual_mean = sum_returns / num_returns if num_returns > 0 else 0\n",
    "            print(f\"  - (Debug) Number of valid daily returns: {num_returns}\")\n",
    "            print(f\"  - (Debug) Sum of all daily returns:      {sum_returns:,.8f}\")\n",
    "            print(f\"  - (Debug) Manually calculated mean:      {manual_mean:,.8f} (Sum / Count)\")\n",
    "        # +++ END OF ADDED DETAIL +++\n",
    "\n",
    "        print(f\"  - Mean Daily Return: {mean_return:,.6f}\")\n",
    "        print(f\"  - Mean Daily ATRP:  {mean_atrp:,.6f}\")\n",
    "        print(f\"  - Sharpe (ATR) = (Mean Return / Mean ATRP) = {sharpe_atr:,.4f}\")\n",
    "        return sharpe_atr\n",
    "\n",
    "    # --- 4. Run Verification for Portfolio (No changes here) ---\n",
    "    display(Markdown(\"### A. Group Portfolio Verification\"))\n",
    "    _calculate_and_print_metrics(\"Full Period\", portfolio_return_series, portfolio_atrp_series)\n",
    "    _calculate_and_print_metrics(\"Calculation Period\", portfolio_return_series.loc[:actual_calc_end_date], portfolio_atrp_series.loc[:actual_calc_end_date])\n",
    "    _calculate_and_print_metrics(\"Forward Period\", portfolio_return_series.loc[actual_calc_end_date:].iloc[1:], portfolio_atrp_series.loc[actual_calc_end_date:].iloc[1:])\n",
    "\n",
    "    # --- 5. Run Verification for Benchmark (No changes here) ---\n",
    "    display(Markdown(f\"### B. Benchmark ({benchmark_ticker}) Verification\"))\n",
    "    _calculate_and_print_metrics(\"Full Period\", benchmark_return_series, benchmark_atrp_series)\n",
    "    _calculate_and_print_metrics(\"Calculation Period\", benchmark_return_series.loc[:actual_calc_end_date], benchmark_atrp_series.loc[:actual_calc_end_date])\n",
    "    _calculate_and_print_metrics(\"Forward Period\", benchmark_return_series.loc[actual_calc_end_date:].iloc[1:], benchmark_atrp_series.loc[actual_calc_end_date:].iloc[1:])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53c79216",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Verification Report for Sharpe (ATR) Calculation"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Portfolio Tickers:** `['STIP', 'RCL']`\n",
       "**Benchmark Ticker:** `VOO`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Full Period:** `2025-08-13` to `2025-09-04`\n",
       "**Calc End Date:** `2025-08-27`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### 🐛 Detailed Portfolio Return Calculation Trace"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 1] Raw Adjusted Close prices.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Ticker</th>\n",
       "      <th>STIP</th>\n",
       "      <th>RCL</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2025-08-13</th>\n",
       "      <td>102.515</td>\n",
       "      <td>312.462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-14</th>\n",
       "      <td>102.446</td>\n",
       "      <td>311.345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-15</th>\n",
       "      <td>102.376</td>\n",
       "      <td>312.990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-18</th>\n",
       "      <td>102.346</td>\n",
       "      <td>325.801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-19</th>\n",
       "      <td>102.406</td>\n",
       "      <td>329.061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-20</th>\n",
       "      <td>102.476</td>\n",
       "      <td>328.114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-21</th>\n",
       "      <td>102.486</td>\n",
       "      <td>324.036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-22</th>\n",
       "      <td>102.883</td>\n",
       "      <td>343.616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-25</th>\n",
       "      <td>102.813</td>\n",
       "      <td>343.915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-26</th>\n",
       "      <td>102.982</td>\n",
       "      <td>352.658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-27</th>\n",
       "      <td>103.161</td>\n",
       "      <td>357.972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-28</th>\n",
       "      <td>103.161</td>\n",
       "      <td>364.721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-29</th>\n",
       "      <td>103.201</td>\n",
       "      <td>362.109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-09-02</th>\n",
       "      <td>103.101</td>\n",
       "      <td>352.997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-09-03</th>\n",
       "      <td>103.141</td>\n",
       "      <td>353.914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-09-04</th>\n",
       "      <td>103.171</td>\n",
       "      <td>358.959</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Ticker         STIP      RCL\n",
       "Date                        \n",
       "2025-08-13  102.515  312.462\n",
       "2025-08-14  102.446  311.345\n",
       "2025-08-15  102.376  312.990\n",
       "2025-08-18  102.346  325.801\n",
       "2025-08-19  102.406  329.061\n",
       "2025-08-20  102.476  328.114\n",
       "2025-08-21  102.486  324.036\n",
       "2025-08-22  102.883  343.616\n",
       "2025-08-25  102.813  343.915\n",
       "2025-08-26  102.982  352.658\n",
       "2025-08-27  103.161  357.972\n",
       "2025-08-28  103.161  364.721\n",
       "2025-08-29  103.201  362.109\n",
       "2025-09-02  103.101  352.997\n",
       "2025-09-03  103.141  353.914\n",
       "2025-09-04  103.171  358.959"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 2] Normalization base (first valid row of prices).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Ticker</th>\n",
       "      <th>STIP</th>\n",
       "      <th>RCL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2025-08-13</th>\n",
       "      <td>102.515</td>\n",
       "      <td>312.462</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Ticker         STIP      RCL\n",
       "2025-08-13  102.515  312.462"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 2a] Normalized prices (each stock's value from an initial $1 investment).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Ticker</th>\n",
       "      <th>STIP</th>\n",
       "      <th>RCL</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2025-08-13</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-14</th>\n",
       "      <td>0.999327</td>\n",
       "      <td>0.996425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-15</th>\n",
       "      <td>0.998644</td>\n",
       "      <td>1.001690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-18</th>\n",
       "      <td>0.998351</td>\n",
       "      <td>1.042690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-19</th>\n",
       "      <td>0.998937</td>\n",
       "      <td>1.053123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-20</th>\n",
       "      <td>0.999620</td>\n",
       "      <td>1.050092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-21</th>\n",
       "      <td>0.999717</td>\n",
       "      <td>1.037041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-22</th>\n",
       "      <td>1.003590</td>\n",
       "      <td>1.099705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-25</th>\n",
       "      <td>1.002907</td>\n",
       "      <td>1.100662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-26</th>\n",
       "      <td>1.004555</td>\n",
       "      <td>1.128643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-27</th>\n",
       "      <td>1.006302</td>\n",
       "      <td>1.145650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-28</th>\n",
       "      <td>1.006302</td>\n",
       "      <td>1.167249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-29</th>\n",
       "      <td>1.006692</td>\n",
       "      <td>1.158890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-09-02</th>\n",
       "      <td>1.005716</td>\n",
       "      <td>1.129728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-09-03</th>\n",
       "      <td>1.006106</td>\n",
       "      <td>1.132663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-09-04</th>\n",
       "      <td>1.006399</td>\n",
       "      <td>1.148808</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Ticker          STIP       RCL\n",
       "Date                          \n",
       "2025-08-13  1.000000  1.000000\n",
       "2025-08-14  0.999327  0.996425\n",
       "2025-08-15  0.998644  1.001690\n",
       "2025-08-18  0.998351  1.042690\n",
       "2025-08-19  0.998937  1.053123\n",
       "2025-08-20  0.999620  1.050092\n",
       "2025-08-21  0.999717  1.037041\n",
       "2025-08-22  1.003590  1.099705\n",
       "2025-08-25  1.002907  1.100662\n",
       "2025-08-26  1.004555  1.128643\n",
       "2025-08-27  1.006302  1.145650\n",
       "2025-08-28  1.006302  1.167249\n",
       "2025-08-29  1.006692  1.158890\n",
       "2025-09-02  1.005716  1.129728\n",
       "2025-09-03  1.006106  1.132663\n",
       "2025-09-04  1.006399  1.148808"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 3] Averaged normalized portfolio value series.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N_portf_value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2025-08-13</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-14</th>\n",
       "      <td>0.997876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-15</th>\n",
       "      <td>1.000167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-18</th>\n",
       "      <td>1.020521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-19</th>\n",
       "      <td>1.026030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-20</th>\n",
       "      <td>1.024856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-21</th>\n",
       "      <td>1.018379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-22</th>\n",
       "      <td>1.051647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-25</th>\n",
       "      <td>1.051784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-26</th>\n",
       "      <td>1.066599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-27</th>\n",
       "      <td>1.075976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-28</th>\n",
       "      <td>1.086775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-29</th>\n",
       "      <td>1.082791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-09-02</th>\n",
       "      <td>1.067722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-09-03</th>\n",
       "      <td>1.069384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-09-04</th>\n",
       "      <td>1.077604</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            N_portf_value\n",
       "Date                     \n",
       "2025-08-13       1.000000\n",
       "2025-08-14       0.997876\n",
       "2025-08-15       1.000167\n",
       "2025-08-18       1.020521\n",
       "2025-08-19       1.026030\n",
       "2025-08-20       1.024856\n",
       "2025-08-21       1.018379\n",
       "2025-08-22       1.051647\n",
       "2025-08-25       1.051784\n",
       "2025-08-26       1.066599\n",
       "2025-08-27       1.075976\n",
       "2025-08-28       1.086775\n",
       "2025-08-29       1.082791\n",
       "2025-09-02       1.067722\n",
       "2025-09-03       1.069384\n",
       "2025-09-04       1.077604"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 4] Final portfolio daily return series (pct_change of Step 3).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N_portf_rtn</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2025-08-13</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-14</th>\n",
       "      <td>-0.002124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-15</th>\n",
       "      <td>0.002296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-18</th>\n",
       "      <td>0.020350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-19</th>\n",
       "      <td>0.005398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-20</th>\n",
       "      <td>-0.001144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-21</th>\n",
       "      <td>-0.006320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-22</th>\n",
       "      <td>0.032668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-25</th>\n",
       "      <td>0.000130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-26</th>\n",
       "      <td>0.014085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-27</th>\n",
       "      <td>0.008791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-28</th>\n",
       "      <td>0.010037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-29</th>\n",
       "      <td>-0.003666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-09-02</th>\n",
       "      <td>-0.013917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-09-03</th>\n",
       "      <td>0.001557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-09-04</th>\n",
       "      <td>0.007686</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            N_portf_rtn\n",
       "Date                   \n",
       "2025-08-13          NaN\n",
       "2025-08-14    -0.002124\n",
       "2025-08-15     0.002296\n",
       "2025-08-18     0.020350\n",
       "2025-08-19     0.005398\n",
       "2025-08-20    -0.001144\n",
       "2025-08-21    -0.006320\n",
       "2025-08-22     0.032668\n",
       "2025-08-25     0.000130\n",
       "2025-08-26     0.014085\n",
       "2025-08-27     0.008791\n",
       "2025-08-28     0.010037\n",
       "2025-08-29    -0.003666\n",
       "2025-09-02    -0.013917\n",
       "2025-09-03     0.001557\n",
       "2025-09-04     0.007686"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### 🐛 Detailed Portfolio ATRP Calculation Trace (Value-Weighted)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 5] Individual component ATRP values for each stock.\n",
      "This is the raw risk input for each component.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Ticker</th>\n",
       "      <th>RCL</th>\n",
       "      <th>STIP</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2025-08-13</th>\n",
       "      <td>0.030911</td>\n",
       "      <td>0.001345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-14</th>\n",
       "      <td>0.029561</td>\n",
       "      <td>0.001312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-15</th>\n",
       "      <td>0.028527</td>\n",
       "      <td>0.001316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-18</th>\n",
       "      <td>0.029111</td>\n",
       "      <td>0.001292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-19</th>\n",
       "      <td>0.029510</td>\n",
       "      <td>0.001255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-20</th>\n",
       "      <td>0.030209</td>\n",
       "      <td>0.001247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-21</th>\n",
       "      <td>0.029868</td>\n",
       "      <td>0.001220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-22</th>\n",
       "      <td>0.030322</td>\n",
       "      <td>0.001404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-25</th>\n",
       "      <td>0.029449</td>\n",
       "      <td>0.001380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-26</th>\n",
       "      <td>0.028503</td>\n",
       "      <td>0.001411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-27</th>\n",
       "      <td>0.027809</td>\n",
       "      <td>0.001459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-28</th>\n",
       "      <td>0.026756</td>\n",
       "      <td>0.001397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-29</th>\n",
       "      <td>0.026863</td>\n",
       "      <td>0.001358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-09-02</th>\n",
       "      <td>0.028604</td>\n",
       "      <td>0.001352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-09-03</th>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.001345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-09-04</th>\n",
       "      <td>0.027238</td>\n",
       "      <td>0.001290</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Ticker           RCL      STIP\n",
       "Date                          \n",
       "2025-08-13  0.030911  0.001345\n",
       "2025-08-14  0.029561  0.001312\n",
       "2025-08-15  0.028527  0.001316\n",
       "2025-08-18  0.029111  0.001292\n",
       "2025-08-19  0.029510  0.001255\n",
       "2025-08-20  0.030209  0.001247\n",
       "2025-08-21  0.029868  0.001220\n",
       "2025-08-22  0.030322  0.001404\n",
       "2025-08-25  0.029449  0.001380\n",
       "2025-08-26  0.028503  0.001411\n",
       "2025-08-27  0.027809  0.001459\n",
       "2025-08-28  0.026756  0.001397\n",
       "2025-08-29  0.026863  0.001358\n",
       "2025-09-02  0.028604  0.001352\n",
       "2025-09-03  0.027778  0.001345\n",
       "2025-09-04  0.027238  0.001290"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 6] Drifting daily portfolio weights (based on market value).\n",
      "Note how weights start near equal and drift over time. This is the core of the buy-and-hold logic.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Ticker</th>\n",
       "      <th>STIP</th>\n",
       "      <th>RCL</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2025-08-13</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-14</th>\n",
       "      <td>0.500727</td>\n",
       "      <td>0.499273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-15</th>\n",
       "      <td>0.499239</td>\n",
       "      <td>0.500761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-18</th>\n",
       "      <td>0.489138</td>\n",
       "      <td>0.510862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-19</th>\n",
       "      <td>0.486797</td>\n",
       "      <td>0.513203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-20</th>\n",
       "      <td>0.487688</td>\n",
       "      <td>0.512312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-21</th>\n",
       "      <td>0.490837</td>\n",
       "      <td>0.509163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-22</th>\n",
       "      <td>0.477151</td>\n",
       "      <td>0.522849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-25</th>\n",
       "      <td>0.476764</td>\n",
       "      <td>0.523236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-26</th>\n",
       "      <td>0.470915</td>\n",
       "      <td>0.529085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-27</th>\n",
       "      <td>0.467623</td>\n",
       "      <td>0.532377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-28</th>\n",
       "      <td>0.462976</td>\n",
       "      <td>0.537024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-29</th>\n",
       "      <td>0.464860</td>\n",
       "      <td>0.535140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-09-02</th>\n",
       "      <td>0.470964</td>\n",
       "      <td>0.529036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-09-03</th>\n",
       "      <td>0.470414</td>\n",
       "      <td>0.529586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-09-04</th>\n",
       "      <td>0.466962</td>\n",
       "      <td>0.533038</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Ticker          STIP       RCL\n",
       "Date                          \n",
       "2025-08-13  0.500000  0.500000\n",
       "2025-08-14  0.500727  0.499273\n",
       "2025-08-15  0.499239  0.500761\n",
       "2025-08-18  0.489138  0.510862\n",
       "2025-08-19  0.486797  0.513203\n",
       "2025-08-20  0.487688  0.512312\n",
       "2025-08-21  0.490837  0.509163\n",
       "2025-08-22  0.477151  0.522849\n",
       "2025-08-25  0.476764  0.523236\n",
       "2025-08-26  0.470915  0.529085\n",
       "2025-08-27  0.467623  0.532377\n",
       "2025-08-28  0.462976  0.537024\n",
       "2025-08-29  0.464860  0.535140\n",
       "2025-09-02  0.470964  0.529036\n",
       "2025-09-03  0.470414  0.529586\n",
       "2025-09-04  0.466962  0.533038"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 7] Final value-weighted portfolio ATRP series.\n",
      "Result of (Weights * Component_ATRPs) summed each day.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value_weighted_atrp</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2025-08-13</th>\n",
       "      <td>0.016128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-14</th>\n",
       "      <td>0.015416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-15</th>\n",
       "      <td>0.014943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-18</th>\n",
       "      <td>0.015504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-19</th>\n",
       "      <td>0.015755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-20</th>\n",
       "      <td>0.016085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-21</th>\n",
       "      <td>0.015807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-22</th>\n",
       "      <td>0.016524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-25</th>\n",
       "      <td>0.016067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-26</th>\n",
       "      <td>0.015745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-27</th>\n",
       "      <td>0.015487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-28</th>\n",
       "      <td>0.015015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-29</th>\n",
       "      <td>0.015007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-09-02</th>\n",
       "      <td>0.015769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-09-03</th>\n",
       "      <td>0.015343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-09-04</th>\n",
       "      <td>0.015122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            value_weighted_atrp\n",
       "Date                           \n",
       "2025-08-13             0.016128\n",
       "2025-08-14             0.015416\n",
       "2025-08-15             0.014943\n",
       "2025-08-18             0.015504\n",
       "2025-08-19             0.015755\n",
       "2025-08-20             0.016085\n",
       "2025-08-21             0.015807\n",
       "2025-08-22             0.016524\n",
       "2025-08-25             0.016067\n",
       "2025-08-26             0.015745\n",
       "2025-08-27             0.015487\n",
       "2025-08-28             0.015015\n",
       "2025-08-29             0.015007\n",
       "2025-09-02             0.015769\n",
       "2025-09-03             0.015343\n",
       "2025-09-04             0.015122"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### A. Group Portfolio Verification"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Full Period"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Mean Daily Return: 0.005055\n",
      "  - Mean Daily ATRP:  0.015607\n",
      "  - Sharpe (ATR) = (Mean Return / Mean ATRP) = 0.3239\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Calculation Period"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Mean Daily Return: 0.007413\n",
      "  - Mean Daily ATRP:  0.015769\n",
      "  - Sharpe (ATR) = (Mean Return / Mean ATRP) = 0.4701\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Forward Period"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Mean Daily Return: 0.000339\n",
      "  - Mean Daily ATRP:  0.015251\n",
      "  - Sharpe (ATR) = (Mean Return / Mean ATRP) = 0.0223\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### B. Benchmark (VOO) Verification"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Full Period"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Mean Daily Return: 0.000438\n",
      "  - Mean Daily ATRP:  0.008277\n",
      "  - Sharpe (ATR) = (Mean Return / Mean ATRP) = 0.0529\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Calculation Period"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Mean Daily Return: 0.000283\n",
      "  - Mean Daily ATRP:  0.008317\n",
      "  - Sharpe (ATR) = (Mean Return / Mean ATRP) = 0.0340\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Forward Period"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Mean Daily Return: 0.000748\n",
      "  - Mean Daily ATRP:  0.008191\n",
      "  - Sharpe (ATR) = (Mean Return / Mean ATRP) = 0.0913\n"
     ]
    }
   ],
   "source": [
    "# Now call the verification function with the exact parameters from the UI\n",
    "verify_sharpe_atr_calculation_checked(\n",
    "    df_ohlcv=df_ohlcv,\n",
    "    features_df=features_df,\n",
    "    tickers_to_verify=tickers_to_verify, # <-- From the UI output\n",
    "    benchmark_ticker=benchmark_ticker,\n",
    "    start_date=start_date,\n",
    "    calc_period=10,\n",
    "    fwd_period=5,\n",
    "    debug=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da532d8",
   "metadata": {},
   "source": [
    "# PORTFOLIO & BENCHMARK SHARPE (ATR) Calculation have been verified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0586ab73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f55cab9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf901ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63e95d07",
   "metadata": {},
   "source": [
    "### My Verification of Portfolio Sharpe (ATR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2206d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import ipywidgets as widgets\n",
    "import pprint\n",
    "from IPython.display import display, Markdown\n",
    "import os # Make sure os is imported for the export function later\n",
    "\n",
    "\n",
    "# def calculate_gain(price_series: pd.Series):\n",
    "#     \"\"\"Calculates the total gain over a series of prices.\"\"\"\n",
    "#     # Ensure there are at least two data points to calculate a gain\n",
    "#     if price_series.dropna().shape[0] < 2: return np.nan\n",
    "#     # Use forward-fill for the end price and back-fill for the start price\n",
    "#     # to handle potential NaNs at the beginning or end of the series.\n",
    "#     return (price_series.ffill().iloc[-1] / price_series.bfill().iloc[0]) - 1\n",
    "\n",
    "# def calculate_sharpe(return_series: pd.Series):\n",
    "#     \"\"\"Calculates the annualized Sharpe ratio from a series of daily returns.\"\"\"\n",
    "#     # Ensure there are at least two returns to calculate a standard deviation\n",
    "#     if return_series.dropna().shape[0] < 2: return np.nan\n",
    "#     std_dev = return_series.std()\n",
    "#     # Avoid division by zero if returns are constant\n",
    "#     if std_dev > 0 and std_dev != np.inf:\n",
    "#         return (return_series.mean() / std_dev) * np.sqrt(252)\n",
    "#     return np.nan\n",
    "\n",
    "# def calculate_sharpe_atr(return_series: pd.Series, atrp_series: pd.Series):\n",
    "#     \"\"\"Calculates a Sharpe-like ratio using mean ATRP as the denominator.\"\"\"\n",
    "#     # Ensure there are returns and that ATRP data is valid\n",
    "#     if return_series.dropna().shape[0] < 2 or atrp_series.dropna().empty:\n",
    "#         return np.nan\n",
    "        \n",
    "#     mean_return = return_series.mean()\n",
    "#     mean_atrp = atrp_series.mean()\n",
    "    \n",
    "#     # Avoid division by zero\n",
    "#     if mean_atrp > 0 and mean_atrp != np.inf:\n",
    "#         return mean_return / mean_atrp\n",
    "        \n",
    "#     return np.nan\n",
    "\n",
    "\n",
    "# # --- B. MODULAR METRIC CALCULATION ENGINE ---\n",
    "\n",
    "# def calculate_price_metric(metric_data: dict) -> pd.Series:\n",
    "#     \"\"\"\n",
    "#     Calculates the 'Price' metric (total gain) over the calculation period.\n",
    "\n",
    "#     Args:\n",
    "#         metric_data: A dictionary containing pre-calculated data Series.\n",
    "#                      Requires 'calc_close' (pd.DataFrame): The close prices for the calc period.\n",
    "\n",
    "#     Returns:\n",
    "#         A pandas Series of the calculated metric values, indexed by Ticker.\n",
    "#     \"\"\"\n",
    "#     calc_close = metric_data['calc_close']\n",
    "    \n",
    "#     # Ensure there are at least two data points to calculate a gain\n",
    "#     if len(calc_close) < 2:\n",
    "#         return pd.Series(dtype='float64', index=calc_close.columns)\n",
    "\n",
    "#     first_prices = calc_close.bfill().iloc[0]\n",
    "#     last_prices = calc_close.ffill().iloc[-1]\n",
    "    \n",
    "#     # The division of two Series aligns by index (Ticker), which is what we want.\n",
    "#     price_metric = last_prices / first_prices\n",
    "    \n",
    "#     return price_metric.dropna()\n",
    "\n",
    "# def calculate_sharpe_metric(metric_data: dict) -> pd.Series:\n",
    "#     \"\"\"\n",
    "#     Calculates the annualized 'Sharpe' ratio metric over the calculation period.\n",
    "\n",
    "#     Args:\n",
    "#         metric_data: A dictionary containing pre-calculated data Series.\n",
    "#                      Requires 'daily_returns' (pd.DataFrame): Daily returns for the calc period.\n",
    "\n",
    "#     Returns:\n",
    "#         A pandas Series of the calculated metric values, indexed by Ticker.\n",
    "#     \"\"\"\n",
    "#     daily_returns = metric_data['daily_returns']\n",
    "    \n",
    "#     # Ensure there's enough data to calculate standard deviation\n",
    "#     if len(daily_returns.dropna()) < 2:\n",
    "#         return pd.Series(dtype='float64', index=daily_returns.columns)\n",
    "    \n",
    "#     mean_returns = daily_returns.mean()\n",
    "#     std_returns = daily_returns.std()\n",
    "    \n",
    "#     # Standard annualized Sharpe Ratio calculation. Avoid division by zero.\n",
    "#     # We replace resulting NaNs/infs with 0 to handle cases of zero volatility.\n",
    "#     sharpe_ratio = (mean_returns / std_returns * np.sqrt(252))\n",
    "    \n",
    "#     return sharpe_ratio.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "    \n",
    "# def calculate_sharpe_atr_metric(metric_data: dict) -> pd.Series:\n",
    "#     \"\"\"\n",
    "#     Calculates the 'Sharpe (ATR)' metric over the calculation period.\n",
    "\n",
    "#     Args:\n",
    "#         metric_data: A dictionary containing pre-calculated data Series.\n",
    "#                      Requires 'daily_returns' (pd.DataFrame): Daily returns for the calc period.\n",
    "#                      Requires 'atrp' (pd.Series): Mean ATRP for each ticker over the period.\n",
    "\n",
    "#     Returns:\n",
    "#         A pandas Series of the calculated metric values, indexed by Ticker.\n",
    "#     \"\"\"\n",
    "#     daily_returns = metric_data['daily_returns']\n",
    "#     atrp = metric_data['atrp']\n",
    "    \n",
    "#     mean_returns = daily_returns.mean()\n",
    "    \n",
    "#     # ATRP-based Sharpe. Avoid division by zero.\n",
    "#     # We replace resulting NaNs/infs with 0.\n",
    "#     sharpe_atr = mean_returns / atrp\n",
    "    \n",
    "#     return sharpe_atr.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "\n",
    "# METRIC_REGISTRY = {\n",
    "#     'Price': calculate_price_metric,\n",
    "#     'Sharpe': calculate_sharpe_metric,\n",
    "#     'Sharpe (ATR)': calculate_sharpe_atr_metric,\n",
    "# }\n",
    "\n",
    "# def plot_walk_forward_analyzer(df_ohlcv,\n",
    "#                                default_start_date=None, default_calc_period=126, default_fwd_period=63,\n",
    "#                                default_metric='Sharpe (ATR)', default_rank_start=1, default_rank_end=10,\n",
    "#                                default_benchmark_ticker='VOO', master_calendar_ticker='VOO',\n",
    "#                                quality_thresholds={'min_median_dollar_volume': 1_000_000, 'max_stale_pct': 0.05, 'max_same_vol_count': 10},\n",
    "#                                debug=False):\n",
    "#     # (No changes to the initial setup part of this function...)\n",
    "#     print(\"Initializing Walk-Forward Analyzer (using Trading Day Logic)...\")\n",
    "#     if not isinstance(df_ohlcv.index, pd.MultiIndex): raise ValueError(\"Input DataFrame must have a (Ticker, Date) MultiIndex.\")\n",
    "#     df_ohlcv = df_ohlcv.sort_index()\n",
    "#     if master_calendar_ticker not in df_ohlcv.index.get_level_values(0):\n",
    "#         raise ValueError(f\"Master calendar ticker '{master_calendar_ticker}' not found in DataFrame.\")\n",
    "#     master_trading_days = df_ohlcv.loc[master_calendar_ticker].index.unique().sort_values()\n",
    "#     print(f\"Master trading day calendar created from '{master_calendar_ticker}' ({len(master_trading_days)} days).\")\n",
    "#     print(\"--- Generating all features upfront... ---\")\n",
    "#     features_df = generate_features(df_ohlcv)\n",
    "#     print(\"Pre-processing data (unstacking)...\")\n",
    "#     df_close_full = df_ohlcv['Adj Close'].unstack(level=0)\n",
    "#     df_high_full = df_ohlcv['Adj High'].unstack(level=0)\n",
    "#     df_low_full = df_ohlcv['Adj Low'].unstack(level=0)\n",
    "#     start_date_picker = widgets.DatePicker(description='Start Date:', value=pd.to_datetime(default_start_date), disabled=False)\n",
    "#     calc_period_input = widgets.IntText(value=default_calc_period, description='Calc Period (days):')\n",
    "#     fwd_period_input = widgets.IntText(value=default_fwd_period, description='Fwd Period (days):')\n",
    "#     if default_metric not in METRIC_REGISTRY:\n",
    "#         fallback_metric = list(METRIC_REGISTRY.keys())[0]\n",
    "#         print(f\"⚠️ Warning: Default metric '{default_metric}' not in registry. Using '{fallback_metric}'.\")\n",
    "#         default_metric = fallback_metric\n",
    "#     metric_dropdown = widgets.Dropdown(options=list(METRIC_REGISTRY.keys()), value=default_metric, description='Metric:')\n",
    "#     rank_start_input = widgets.IntText(value=default_rank_start, description='Rank Start:')\n",
    "#     rank_end_input = widgets.IntText(value=default_rank_end, description='Rank End:')\n",
    "#     benchmark_ticker_input = widgets.Text(value=default_benchmark_ticker, description='Benchmark:', placeholder='Enter Ticker')\n",
    "#     update_button = widgets.Button(description=\"Update Chart\", button_style='primary')\n",
    "#     ticker_list_output = widgets.Output()\n",
    "#     results_container, debug_data_container = [None], [None]\n",
    "#     fig = go.FigureWidget()\n",
    "#     max_traces = 50\n",
    "#     for i in range(max_traces): fig.add_trace(go.Scatter(x=[None], y=[None], mode='lines', name=f'placeholder_{i}', visible=False, showlegend=False))\n",
    "#     fig.add_trace(go.Scatter(x=[None], y=[None], mode='lines', name='Benchmark', visible=True, showlegend=True, line=dict(color='black', width=3, dash='dash')))\n",
    "#     fig.add_trace(go.Scatter(x=[None], y=[None], mode='lines', name='Group Portfolio', visible=True, showlegend=True, line=dict(color='green', width=3)))\n",
    "\n",
    "#     def update_plot(button_click):\n",
    "#         ticker_list_output.clear_output()\n",
    "#         start_date_raw = pd.to_datetime(start_date_picker.value)\n",
    "#         start_date_idx = master_trading_days.searchsorted(start_date_raw)\n",
    "#         if start_date_idx >= len(master_trading_days):\n",
    "#             with ticker_list_output: print(f\"Error: Start date is after the last available trading day.\"); return\n",
    "#         actual_start_date = master_trading_days[start_date_idx]\n",
    "#         with ticker_list_output:\n",
    "#             if start_date_raw.date() != actual_start_date.date():\n",
    "#                 print(f\"ℹ️ Info: Start date {start_date_raw.date()} is not a trading day. Snapping forward to {actual_start_date.date()}.\")\n",
    "#         calc_period = calc_period_input.value; fwd_period = fwd_period_input.value; metric = metric_dropdown.value\n",
    "#         rank_start = rank_start_input.value; rank_end = rank_end_input.value; benchmark_ticker = benchmark_ticker_input.value.strip().upper()\n",
    "#         if rank_start > rank_end:\n",
    "#             with ticker_list_output: print(\"Error: 'Rank Start' must be <= 'Rank End'.\"); return\n",
    "#         if rank_start < 1 or calc_period < 2 or fwd_period < 1:\n",
    "#             with ticker_list_output: print(\"Error: Ranks must be >= 1, Calc Period >= 2, Fwd Period >= 1.\"); return\n",
    "#         required_days = calc_period + fwd_period\n",
    "#         if start_date_idx + required_days > len(master_trading_days):\n",
    "#             available_days = len(master_trading_days) - start_date_idx; last_available_date = master_trading_days[-1].date()\n",
    "#             with ticker_list_output:\n",
    "#                 print(f\"Error: Not enough data for the requested period.\\n  Start Date: {actual_start_date.date()}\\n  Required Days: {calc_period} (calc) + {fwd_period} (fwd) = {required_days}\\n  Available Days from Start: {available_days} (until {last_available_date})\\n  Please shorten the 'Calc Period' / 'Fwd Period' or choose an earlier 'Start Date'.\")\n",
    "#             return\n",
    "#         eligible_tickers = get_eligible_universe(features_df, actual_start_date, quality_thresholds)\n",
    "#         if not eligible_tickers:\n",
    "#             with ticker_list_output: print(f\"Error: No eligible tickers found on {actual_start_date.date()} with the current quality filters.\"); return\n",
    "#         df_close_step = df_close_full[eligible_tickers]; df_high_step = df_high_full[eligible_tickers]; df_low_step = df_low_full[eligible_tickers]\n",
    "#         results, debug_output = run_walk_forward_step(df_close_full, df_high_full, df_low_full, master_trading_days, actual_start_date, calc_period, fwd_period, metric, rank_start, rank_end, benchmark_ticker, features_df=features_df, debug=debug)\n",
    "#         if results.get('error'):\n",
    "#             with ticker_list_output: print(f\"Error: {results['error']}\"); return\n",
    "#         period_dates = {'calc_period_start': results['safe_start_date'], 'calc_period_end': results['actual_calc_end_ts'], 'forward_period_start': results['actual_calc_end_ts'], 'forward_period_end': results['safe_viz_end_date']}\n",
    "#         run_parameters = {'calc_period': calc_period, 'fwd_period': fwd_period, 'rank_metric': metric, 'rank_start': rank_start, 'rank_end': rank_end, 'benchmark_ticker': benchmark_ticker}\n",
    "#         results.update(period_dates); results.update(run_parameters)\n",
    "#         if debug_output is not None and isinstance(debug_output, dict):\n",
    "#             debug_output.update(period_dates); debug_output.update(run_parameters)\n",
    "#         with fig.batch_update():\n",
    "#             for i in range(max_traces):\n",
    "#                 trace = fig.data[i]\n",
    "#                 if i < len(results['tickers_to_display']):\n",
    "#                     ticker = results['tickers_to_display'][i]; plot_data_series = results['normalized_plot_data'][ticker]\n",
    "#                     trace.x, trace.y, trace.name, trace.visible, trace.showlegend = plot_data_series.index, plot_data_series.values, ticker, True, True\n",
    "#                 else: trace.visible, trace.showlegend = False, False\n",
    "#             benchmark_trace = fig.data[max_traces]\n",
    "#             if results['benchmark_price_series'] is not None and not results['benchmark_price_series'].loc[results['safe_start_date']:results['safe_viz_end_date']].dropna().empty:\n",
    "#                 normalized_benchmark = results['benchmark_price_series'].loc[results['safe_start_date']:results['safe_viz_end_date']] / results['benchmark_price_series'].loc[results['safe_start_date']:].bfill().iloc[0]\n",
    "#                 benchmark_trace.x, benchmark_trace.y, benchmark_trace.name, benchmark_trace.visible = normalized_benchmark.index, normalized_benchmark.values, f\"Benchmark ({benchmark_ticker})\", True\n",
    "#             else: benchmark_trace.visible = False\n",
    "#             portfolio_trace = fig.data[max_traces + 1]\n",
    "#             portfolio_trace.x, portfolio_trace.y, portfolio_trace.name, portfolio_trace.visible = results['portfolio_series'].index, results['portfolio_series'], 'Group Portfolio', True\n",
    "#             fig.layout.shapes = []; fig.add_shape(type=\"line\", x0=results['actual_calc_end_ts'], y0=0, x1=results['actual_calc_end_ts'], y1=1, xref='x', yref='paper', line=dict(color=\"grey\", width=2, dash=\"dash\"))\n",
    "#         results_container[0] = results; debug_data_container[0] = debug_output\n",
    "#         with ticker_list_output:\n",
    "#             print(f\"Analysis Period: {results['safe_start_date'].date()} to {results['safe_viz_end_date'].date()}.\")\n",
    "#             pprint.pprint(results['tickers_to_display'])\n",
    "#             p = results['performance_data']\n",
    "            \n",
    "#             # --- START OF MODIFIED BLOCK ---\n",
    "#             rows = []\n",
    "#             # Gain Metrics\n",
    "#             rows.append({'Metric': 'Group Portfolio Gain', 'Full': p['full_p_gain'], 'Calc': p['calc_p_gain'], 'Fwd': p['fwd_p_gain']})\n",
    "#             if not np.isnan(p.get('full_b_gain')):\n",
    "#                 rows.append({'Metric': f'Benchmark ({benchmark_ticker}) Gain', 'Full': p['full_b_gain'], 'Calc': p['calc_b_gain'], 'Fwd': p['fwd_b_gain']})\n",
    "#                 rows.append({'Metric': '== Gain Delta (vs Bm)', 'Full': p['full_p_gain'] - p['full_b_gain'], 'Calc': p['calc_p_gain'] - p['calc_b_gain'], 'Fwd': p['fwd_p_gain'] - p['fwd_b_gain']})\n",
    "            \n",
    "#             # Standard Sharpe Metrics\n",
    "#             rows.append({'Metric': 'Group Portfolio Sharpe', 'Full': p['full_p_sharpe'], 'Calc': p['calc_p_sharpe'], 'Fwd': p['fwd_p_sharpe']})\n",
    "#             if not np.isnan(p.get('full_b_sharpe')):\n",
    "#                 rows.append({'Metric': f'Benchmark ({benchmark_ticker}) Sharpe', 'Full': p['full_b_sharpe'], 'Calc': p['calc_b_sharpe'], 'Fwd': p['fwd_b_sharpe']})\n",
    "#                 rows.append({'Metric': '== Sharpe Delta (vs Bm)', 'Full': p['full_p_sharpe'] - p['full_b_sharpe'], 'Calc': p['calc_p_sharpe'] - p['calc_b_sharpe'], 'Fwd': p['fwd_p_sharpe'] - p['fwd_b_sharpe']})\n",
    "\n",
    "#             # Sharpe (ATR) Metrics\n",
    "#             rows.append({'Metric': 'Group Portfolio Sharpe (ATR)', 'Full': p['full_p_sharpe_atr'], 'Calc': p['calc_p_sharpe_atr'], 'Fwd': p['fwd_p_sharpe_atr']})\n",
    "#             if not np.isnan(p.get('full_b_sharpe_atr')):\n",
    "#                 rows.append({'Metric': f'Benchmark ({benchmark_ticker}) Sharpe (ATR)', 'Full': p['full_b_sharpe_atr'], 'Calc': p['calc_b_sharpe_atr'], 'Fwd': p['fwd_b_sharpe_atr']})\n",
    "#                 rows.append({'Metric': '== Sharpe (ATR) Delta (vs Bm)', 'Full': p['full_p_sharpe_atr'] - p['full_b_sharpe_atr'], 'Calc': p['calc_p_sharpe_atr'] - p['calc_b_sharpe_atr'], 'Fwd': p['fwd_p_sharpe_atr'] - p['fwd_b_sharpe_atr']})\n",
    "\n",
    "#             report_df = pd.DataFrame(rows).set_index('Metric')\n",
    "#             gain_rows = [row for row in report_df.index if 'Gain' in row]\n",
    "#             sharpe_rows = [row for row in report_df.index if 'Sharpe' in row] # This now correctly includes both types of Sharpe\n",
    "#             # --- END OF MODIFIED BLOCK ---\n",
    "            \n",
    "#             styled_df = report_df.style.format('{:+.2%}', na_rep='N/A', subset=(gain_rows, report_df.columns)).format('{:+.2f}', na_rep='N/A', subset=(sharpe_rows, report_df.columns)).set_properties(**{'text-align': 'right', 'width': '100px'}).set_table_styles([{'selector': 'th.col_heading', 'props': [('text-align', 'right')]}, {'selector': 'th.row_heading', 'props': [('text-align', 'left')]}])\n",
    "#             print(\"\\n--- Strategy Performance Summary ---\")\n",
    "#             display(styled_df)\n",
    "#     fig.update_layout(title_text='Walk-Forward Performance Analysis', xaxis_title='Date', yaxis_title='Normalized Price (Start = 1)', hovermode='x unified', legend_title_text='Tickers (Ranked)', height=600, margin=dict(t=50))\n",
    "#     fig.add_hline(y=1, line_width=1, line_dash=\"dash\", line_color=\"grey\")\n",
    "#     update_button.on_click(update_plot)\n",
    "#     controls_row1 = widgets.HBox([start_date_picker, calc_period_input, fwd_period_input])\n",
    "#     controls_row2 = widgets.HBox([metric_dropdown, rank_start_input, rank_end_input, benchmark_ticker_input, update_button])\n",
    "#     ui_container = widgets.VBox([controls_row1, controls_row2, ticker_list_output], layout=widgets.Layout(margin='10px 0 20px 0'))\n",
    "#     display(ui_container, fig)\n",
    "#     update_plot(None)\n",
    "#     return (results_container, debug_data_container)\n",
    "\n",
    "\n",
    "def generate_features(df_ohlcv: pd.DataFrame, \n",
    "                      atr_period: int = 14, \n",
    "                      quality_window: int = 252, \n",
    "                      quality_min_periods: int = 126) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generates a comprehensive DataFrame of derived features from raw OHLCV data.\n",
    "\n",
    "    This function performs all heavy, window-based calculations upfront to be used\n",
    "    by downstream analysis functions. It calculates:\n",
    "    1. Technical Indicators: True Range (TR), ATR, and ATRP.\n",
    "    2. Data Quality Metrics: Rolling stale percentage, median dollar volume, etc.\n",
    "\n",
    "    Args:\n",
    "        df_ohlcv: The primary DataFrame with a (Ticker, Date) MultiIndex and\n",
    "                  columns for 'Adj High', 'Adj Low', 'Adj Close', 'Volume'.\n",
    "        atr_period: The lookback period for the ATR's Exponential Moving Average.\n",
    "        quality_window: The rolling window size for data quality metrics.\n",
    "        quality_min_periods: The minimum number of observations required to have\n",
    "                             a valid quality metric.\n",
    "\n",
    "    Returns:\n",
    "        A new DataFrame with the same (Ticker, Date) MultiIndex containing all\n",
    "        calculated feature columns.\n",
    "    \"\"\"\n",
    "    print(\"--- Starting Feature Generation ---\")\n",
    "    \n",
    "    # Ensure the DataFrame is sorted for correct window and shift operations\n",
    "    # FIX: Replaced is_lexsorted() with the current pandas attribute\n",
    "    if not df_ohlcv.index.is_monotonic_increasing:\n",
    "        print(\"Sorting index for calculation accuracy...\")\n",
    "        df_ohlcv = df_ohlcv.sort_index()\n",
    "\n",
    "    # --- 1. Technical Indicator Calculation (TR, ATR, ATRP) ---\n",
    "    print(f\"Calculating technical indicators (ATR Period: {atr_period})...\")\n",
    "    \n",
    "    # Group by ticker to handle each security independently\n",
    "    grouped = df_ohlcv.groupby(level='Ticker')\n",
    "    \n",
    "    # Get the previous day's close required for True Range\n",
    "    prev_close = grouped['Adj Close'].shift(1)\n",
    "    \n",
    "    # Calculate the three components of True Range\n",
    "    high_low = df_ohlcv['Adj High'] - df_ohlcv['Adj Low']\n",
    "    high_prev_close = abs(df_ohlcv['Adj High'] - prev_close)\n",
    "    low_prev_close = abs(df_ohlcv['Adj Low'] - prev_close)\n",
    "    \n",
    "    # Combine the components to get the final TR\n",
    "    tr = pd.concat([high_low, high_prev_close, low_prev_close], axis=1).max(axis=1, skipna=False)\n",
    "    \n",
    "    # # Calculate the ATR using an Exponential Moving Average\n",
    "    # --- FIX IS HERE ---\n",
    "    # Use .transform() to apply the EWM function. \n",
    "    # This guarantees the resulting Series has the exact same index as 'tr',\n",
    "    # preventing the index alignment error during the subsequent division.\n",
    "    atr = tr.groupby(level='Ticker').transform(\n",
    "        lambda x: x.ewm(alpha=1/atr_period, adjust=False).mean()\n",
    "    )\n",
    "\n",
    "    # --- CHANGE 1: Removed .fillna(0) ---\n",
    "    # ATRP will now be NaN on the first day, consistent with TR and ATR.\n",
    "    atrp = (atr / df_ohlcv['Adj Close']).replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    indicator_df = pd.DataFrame({\n",
    "        'TR': tr,\n",
    "        'ATR': atr,\n",
    "        'ATRP': atrp\n",
    "    })\n",
    "    \n",
    "    # --- 2. Data Quality Metric Calculation ---\n",
    "    print(f\"Calculating data quality metrics (Window: {quality_window} days)...\")\n",
    "    \n",
    "    # Create intermediate flags needed for quality calculations\n",
    "    is_stale = np.where((df_ohlcv['Volume'] == 0) | (df_ohlcv['Adj High'] == df_ohlcv['Adj Low']), 1, 0)\n",
    "    dollar_volume = df_ohlcv['Adj Close'] * df_ohlcv['Volume']\n",
    "    has_same_volume = (grouped['Volume'].diff() == 0).astype(int)\n",
    "    \n",
    "    # Combine flags into a temporary DataFrame for rolling calculations\n",
    "    quality_temp_df = pd.DataFrame({\n",
    "        'IsStale': is_stale,\n",
    "        'DollarVolume': dollar_volume,\n",
    "        'HasSameVolume': has_same_volume\n",
    "    }, index=df_ohlcv.index) # Explicitly set index to be safe\n",
    "    \n",
    "    # Perform the rolling calculations on the grouped data\n",
    "    # --- FIX IS HERE ---\n",
    "    # We switch to the older, more compatible dictionary-based aggregation method.\n",
    "    # This syntax is understood by nearly all versions of pandas.\n",
    "    rolling_result = quality_temp_df.groupby(level='Ticker').rolling(\n",
    "        window=quality_window,\n",
    "        min_periods=quality_min_periods\n",
    "    ).agg({\n",
    "        'IsStale': 'mean',\n",
    "        'DollarVolume': 'median',\n",
    "        'HasSameVolume': 'sum'\n",
    "    })\n",
    "    \n",
    "    # The dictionary syntax produces columns with the original names ('IsStale', etc.).\n",
    "    # We now explicitly rename them to our desired final names.\n",
    "    rolling_result = rolling_result.rename(columns={\n",
    "        'IsStale': 'RollingStalePct',\n",
    "        'DollarVolume': 'RollMedDollarVol', # <-- RENAMED HERE\n",
    "        'HasSameVolume': 'RollingSameVolCount'\n",
    "    })\n",
    "\n",
    "    # The index after a grouped rolling operation is hierarchical.\n",
    "    # We remove the outermost 'Ticker' level to restore the original index structure.\n",
    "    rolling_quality = rolling_result.reset_index(level=0, drop=True)\n",
    "\n",
    "    # --- 3. Combine All Features ---\n",
    "    print(\"Combining all feature sets...\")\n",
    "    features_df = pd.concat([indicator_df, rolling_quality], axis=1)\n",
    "    \n",
    "    print(\"✅ Feature generation complete.\")\n",
    "    return features_df\n",
    "\n",
    "\n",
    "# def get_eligible_universe(features_df, filter_date, thresholds):\n",
    "#     \"\"\"Filters the universe of tickers based on quality metrics for a given date.\"\"\"\n",
    "#     filter_date_ts = pd.to_datetime(filter_date)\n",
    "#     # The index is now the comprehensive features_df\n",
    "#     date_index = features_df.index.get_level_values('Date').unique().sort_values()\n",
    "    \n",
    "#     if filter_date_ts < date_index[0]:\n",
    "#         print(f\"Warning: Filter date {filter_date_ts.date()} is before the earliest data point. Returning empty universe.\")\n",
    "#         return []\n",
    "        \n",
    "#     # Find the most recent date with quality data on or before the filter date\n",
    "#     valid_prior_dates = date_index[date_index <= filter_date_ts]\n",
    "#     if valid_prior_dates.empty:\n",
    "#         print(f\"Warning: No available data found on or before {filter_date_ts.date()}. Returning empty universe.\")\n",
    "#         return []\n",
    "        \n",
    "#     actual_date_to_use = valid_prior_dates[-1]\n",
    "#     if actual_date_to_use.date() != filter_date_ts.date():\n",
    "#         print(f\"ℹ️ Info: Filter date {filter_date_ts.date()} not found. Using previous available date {actual_date_to_use.date()}.\")\n",
    "\n",
    "#     metrics_on_date = features_df.xs(actual_date_to_use, level='Date')\n",
    "    \n",
    "#     # Apply filters using the new column names from features_df\n",
    "#     mask = ((metrics_on_date['RollMedDollarVol'] >= thresholds['min_median_dollar_volume']) & # <-- RENAMED\n",
    "#             (metrics_on_date['RollingStalePct'] <= thresholds['max_stale_pct']) &\n",
    "#             (metrics_on_date['RollingSameVolCount'] <= thresholds['max_same_vol_count']))\n",
    "            \n",
    "#     eligible_tickers = metrics_on_date[mask].index.tolist()\n",
    "#     all_tickers = metrics_on_date.index.tolist()\n",
    "#     print(f\"Dynamic Filter ({filter_date_ts.date()}): Kept {len(eligible_tickers)} of {len(all_tickers)} tickers.\")\n",
    "#     return eligible_tickers\n",
    "\n",
    "\n",
    "# def run_walk_forward_step(df_close_full, df_high_full, df_low_full,\n",
    "#                           master_trading_days,\n",
    "#                           start_date, calc_period, fwd_period,\n",
    "#                           metric, rank_start, rank_end, benchmark_ticker,\n",
    "#                           features_df,\n",
    "#                           debug=False):\n",
    "#     \"\"\"\n",
    "#     Runs a single step of the walk-forward analysis with a strict, pre-emptive\n",
    "#     check to ensure the full period is available.\n",
    "#     \"\"\"\n",
    "#     debug_data = {} if debug else None\n",
    "\n",
    "#     # 1. Determine exact date ranges with a NEW pre-emptive check\n",
    "#     try:\n",
    "#         start_idx = master_trading_days.get_loc(start_date)\n",
    "#     except KeyError:\n",
    "#         return ({'error': f\"Start date {start_date.date()} is not a valid trading day.\"}, None)\n",
    "\n",
    "#     # +++ THIS IS THE NEW PRE-EMPTIVE CHECK LOGIC +++\n",
    "#     # Calculate the desired end index without clamping first.\n",
    "#     desired_viz_end_idx = start_idx + calc_period + fwd_period\n",
    "    \n",
    "#     # Check if the desired end index is out of bounds.\n",
    "#     if desired_viz_end_idx >= len(master_trading_days):\n",
    "#         last_available_date = master_trading_days[-1].date()\n",
    "#         required_days = calc_period + fwd_period\n",
    "#         available_days = len(master_trading_days) - start_idx\n",
    "#         error_msg = (f\"Not enough data for the full requested period. \"\n",
    "#                      f\"Required: {required_days} days, Available: {available_days} days until {last_available_date}.\")\n",
    "#         return ({'error': error_msg}, None)\n",
    "#     # --- END OF NEW CHECK ---\n",
    "\n",
    "#     # If the check passes, we know the full period is available.\n",
    "#     # The 'min' calls are now just a redundant safety measure.\n",
    "#     calc_end_idx = min(start_idx + calc_period, len(master_trading_days) - 1)\n",
    "#     viz_end_idx = min(calc_end_idx + fwd_period, len(master_trading_days) - 1)\n",
    "\n",
    "#     safe_start_date = master_trading_days[start_idx]\n",
    "#     safe_calc_end_date = master_trading_days[calc_end_idx]\n",
    "#     safe_viz_end_date = master_trading_days[viz_end_idx]\n",
    "    \n",
    "#     if safe_start_date >= safe_calc_end_date:\n",
    "#         return ({'error': \"Invalid date range (calc period has zero or negative length).\"}, None)\n",
    "\n",
    "#     # (The rest of the function remains completely unchanged...)\n",
    "#     # 2. Slice data for the calculation period\n",
    "#     calc_close_raw = df_close_full.loc[safe_start_date:safe_calc_end_date]\n",
    "#     calc_close = calc_close_raw.dropna(axis=1, how='all')\n",
    "#     if calc_close.shape[1] == 0 or len(calc_close) < 2:\n",
    "#         return ({'error': \"Not enough data in calc period.\"}, None)\n",
    "\n",
    "#     # 3. Calculate INTERMEDIATE data required for the ranking metrics\n",
    "#     daily_returns = calc_close.bfill().ffill().pct_change()\n",
    "#     valid_tickers = calc_close.columns\n",
    "#     calc_period_index = pd.MultiIndex.from_product([valid_tickers, calc_close.index], names=['Ticker', 'Date'])\n",
    "#     features_in_period = features_df.loc[features_df.index.intersection(calc_period_index)]\n",
    "#     atrp = features_in_period.groupby(level='Ticker')['ATRP'].mean()\n",
    "\n",
    "#     # 4. Calculate all ranking metrics by iterating through the METRIC_REGISTRY\n",
    "#     metric_ingredients = { 'calc_close': calc_close, 'daily_returns': daily_returns, 'atrp': atrp, }\n",
    "#     metric_values = {}\n",
    "#     for name, func in METRIC_REGISTRY.items():\n",
    "#         metric_values[name] = func(metric_ingredients)\n",
    "#     if metric not in metric_values or metric_values[metric].empty:\n",
    "#         return ({'error': f\"Metric '{metric}' could not be calculated or resulted in no valid tickers.\"}, None)\n",
    "\n",
    "#     # 5. Rank tickers and select the portfolio\n",
    "#     sorted_tickers = metric_values[metric].sort_values(ascending=False)\n",
    "#     tickers_to_display = sorted_tickers.index[rank_start-1:rank_end].tolist()\n",
    "#     if not tickers_to_display:\n",
    "#         return ({'error': \"No tickers found for the selected rank.\"}, None)\n",
    "\n",
    "#     # 6. Calculate Portfolio & Benchmark Performance\n",
    "#     normalized_plot_data = df_close_full[tickers_to_display].loc[safe_start_date:safe_viz_end_date]\n",
    "#     normalized_plot_data = normalized_plot_data.div(normalized_plot_data.bfill().iloc[0])\n",
    "#     actual_calc_end_ts = calc_close.index.max()\n",
    "#     portfolio_series = normalized_plot_data.mean(axis=1)\n",
    "#     portfolio_return_series = portfolio_series.pct_change()\n",
    "#     benchmark_price_series = df_close_full.get(benchmark_ticker)\n",
    "#     benchmark_return_series = benchmark_price_series.loc[safe_start_date:safe_viz_end_date].bfill().ffill().pct_change() if benchmark_price_series is not None else pd.Series(dtype='float64')\n",
    "#     try:\n",
    "#         boundary_loc = portfolio_return_series.index.get_loc(actual_calc_end_ts)\n",
    "#         calc_portfolio_returns = portfolio_return_series.iloc[:boundary_loc + 1]\n",
    "#         fwd_portfolio_returns = portfolio_return_series.iloc[boundary_loc + 1:]\n",
    "#         if benchmark_price_series is not None:\n",
    "#             bm_boundary_loc = benchmark_return_series.index.get_loc(actual_calc_end_ts)\n",
    "#             calc_benchmark_returns = benchmark_return_series.iloc[:bm_boundary_loc + 1]\n",
    "#             fwd_benchmark_returns = benchmark_return_series.iloc[bm_boundary_loc + 1:]\n",
    "#         else:\n",
    "#             calc_benchmark_returns, fwd_benchmark_returns = pd.Series(dtype='float64'), pd.Series(dtype='float64')\n",
    "#     except (KeyError, IndexError):\n",
    "#         calc_portfolio_returns = portfolio_return_series.loc[:actual_calc_end_ts]\n",
    "#         fwd_portfolio_returns = portfolio_return_series.loc[actual_calc_end_ts:].iloc[1:]\n",
    "#         if benchmark_price_series is not None:\n",
    "#             calc_benchmark_returns = benchmark_return_series.loc[:actual_calc_end_ts]\n",
    "#             fwd_benchmark_returns = benchmark_return_series.loc[actual_calc_end_ts:].iloc[1:]\n",
    "#         else:\n",
    "#             calc_benchmark_returns, fwd_benchmark_returns = pd.Series(dtype='float64'), pd.Series(dtype='float64')\n",
    "#     full_period_index = pd.MultiIndex.from_product([tickers_to_display, portfolio_return_series.index], names=['Ticker', 'Date'])\n",
    "#     portfolio_atrp_features = features_df.loc[features_df.index.intersection(full_period_index)]\n",
    "#     portfolio_atrp_daily_unstacked = portfolio_atrp_features['ATRP'].unstack(level='Ticker')\n",
    "#     portfolio_atrp_series = portfolio_atrp_daily_unstacked.mean(axis=1)\n",
    "#     if benchmark_ticker in df_close_full.columns:\n",
    "#         benchmark_atrp_series = features_df.xs(benchmark_ticker, level='Ticker')['ATRP'].loc[safe_start_date:safe_viz_end_date]\n",
    "#     else:\n",
    "#         benchmark_atrp_series = pd.Series(dtype='float64')\n",
    "#     calc_portfolio_atrp = portfolio_atrp_series.loc[:actual_calc_end_ts]\n",
    "#     fwd_portfolio_atrp = portfolio_atrp_series.loc[actual_calc_end_ts:].iloc[1:]\n",
    "#     calc_benchmark_atrp = benchmark_atrp_series.loc[:actual_calc_end_ts]\n",
    "#     fwd_benchmark_atrp = benchmark_atrp_series.loc[actual_calc_end_ts:].iloc[1:]\n",
    "#     perf_data = {}\n",
    "#     perf_data['calc_p_gain'] = calculate_gain(portfolio_series.loc[:actual_calc_end_ts])\n",
    "#     perf_data['fwd_p_gain'] = calculate_gain(portfolio_series.loc[actual_calc_end_ts:])\n",
    "#     perf_data['full_p_gain'] = calculate_gain(portfolio_series)\n",
    "#     perf_data['calc_p_sharpe'] = calculate_sharpe(calc_portfolio_returns)\n",
    "#     perf_data['fwd_p_sharpe'] = calculate_sharpe(fwd_portfolio_returns)\n",
    "#     perf_data['full_p_sharpe'] = calculate_sharpe(portfolio_return_series)\n",
    "#     perf_data['calc_b_gain'] = calculate_gain(benchmark_price_series.loc[safe_start_date:actual_calc_end_ts]) if benchmark_price_series is not None else np.nan\n",
    "#     perf_data['fwd_b_gain'] = calculate_gain(benchmark_price_series.loc[actual_calc_end_ts:safe_viz_end_date]) if benchmark_price_series is not None else np.nan\n",
    "#     perf_data['full_b_gain'] = calculate_gain(benchmark_price_series.loc[safe_start_date:safe_viz_end_date]) if benchmark_price_series is not None else np.nan\n",
    "#     perf_data['calc_b_sharpe'] = calculate_sharpe(calc_benchmark_returns)\n",
    "#     perf_data['fwd_b_sharpe'] = calculate_sharpe(fwd_benchmark_returns)\n",
    "#     perf_data['full_b_sharpe'] = calculate_sharpe(benchmark_return_series)\n",
    "#     perf_data['calc_p_sharpe_atr'] = calculate_sharpe_atr(calc_portfolio_returns, calc_portfolio_atrp)\n",
    "#     perf_data['fwd_p_sharpe_atr'] = calculate_sharpe_atr(fwd_portfolio_returns, fwd_portfolio_atrp)\n",
    "#     perf_data['full_p_sharpe_atr'] = calculate_sharpe_atr(portfolio_return_series, portfolio_atrp_series)\n",
    "#     perf_data['calc_b_sharpe_atr'] = calculate_sharpe_atr(calc_benchmark_returns, calc_benchmark_atrp)\n",
    "#     perf_data['fwd_b_sharpe_atr'] = calculate_sharpe_atr(fwd_benchmark_returns, fwd_benchmark_atrp)\n",
    "#     perf_data['full_b_sharpe_atr'] = calculate_sharpe_atr(benchmark_return_series, benchmark_atrp_series)\n",
    "#     if debug:\n",
    "#         df_ranking_base = pd.DataFrame({'MeanDailyReturn': daily_returns.mean(),'StdDevDailyReturn': daily_returns.std(),'MeanATRP': atrp})\n",
    "#         df_metrics = pd.DataFrame(metric_values)\n",
    "#         df_metrics.columns = [f'Metric_{col}' for col in df_metrics.columns]\n",
    "#         df_ranking = df_ranking_base.join(df_metrics, how='left')\n",
    "#         df_ranking.index.name = 'Ticker'\n",
    "#         debug_data['ranking_metrics'] = df_ranking.sort_values(f'Metric_{metric}', ascending=False)\n",
    "#     calc_end_prices = calc_close.ffill().iloc[-1]\n",
    "#     fwd_close_slice = df_close_full.loc[actual_calc_end_ts:safe_viz_end_date]\n",
    "#     viz_end_prices = fwd_close_slice.ffill().iloc[-1] if not fwd_close_slice.empty and len(fwd_close_slice) >= 2 else calc_end_prices\n",
    "#     calc_gains = (calc_end_prices / calc_close.bfill().iloc[0]) - 1\n",
    "#     fwd_gains = (viz_end_prices / calc_end_prices) - 1\n",
    "#     results_df = pd.DataFrame({'Rank': range(rank_start, rank_start + len(tickers_to_display)), 'Metric': metric, 'MetricValue': sorted_tickers.loc[tickers_to_display].values, 'CalcPrice': calc_end_prices.loc[tickers_to_display], 'CalcGain': calc_gains.loc[tickers_to_display], 'FwdGain': fwd_gains.loc[tickers_to_display]}, index=pd.Index(tickers_to_display, name='Ticker'))\n",
    "#     if benchmark_price_series is not None and benchmark_ticker in calc_close.columns:\n",
    "#         benchmark_df_row = pd.DataFrame({'Rank': np.nan, 'Metric': metric, 'MetricValue': metric_values[metric].get(benchmark_ticker, np.nan), 'CalcPrice': calc_end_prices[benchmark_ticker], 'CalcGain': calc_gains[benchmark_ticker], 'FwdGain': fwd_gains[benchmark_ticker]}, index=pd.Index([f\"{benchmark_ticker} (BM)\"], name='Ticker'))\n",
    "#         results_df = pd.concat([results_df, benchmark_df_row])\n",
    "#     if debug:\n",
    "#         df_trace = normalized_plot_data.copy()\n",
    "#         df_trace.columns = [f'Norm_Price_{c}' for c in df_trace.columns]\n",
    "#         df_trace['Norm_Price_Portfolio'] = portfolio_series\n",
    "#         if benchmark_price_series is not None and not benchmark_price_series.loc[safe_start_date:safe_viz_end_date].dropna().empty:\n",
    "#             norm_bm = benchmark_price_series.loc[safe_start_date:safe_viz_end_date] / benchmark_price_series.loc[safe_start_date:].bfill().iloc[0]\n",
    "#             df_trace[f'Norm_Price_Benchmark_{benchmark_ticker}'] = norm_bm\n",
    "#         for col in df_trace.columns:\n",
    "#             if 'Norm_Price' in col:\n",
    "#                 df_trace[col.replace('Norm_Price', 'Return')] = df_trace[col].pct_change()\n",
    "#         debug_data['portfolio_trace'] = df_trace\n",
    "#     final_results = {\n",
    "#         'tickers_to_display': tickers_to_display, 'normalized_plot_data': normalized_plot_data,\n",
    "#         'portfolio_series': portfolio_series, 'benchmark_price_series': benchmark_price_series,\n",
    "#         'performance_data': perf_data, 'results_df': results_df, 'actual_calc_end_ts': actual_calc_end_ts,\n",
    "#         'safe_start_date': safe_start_date, 'safe_viz_end_date': safe_viz_end_date,\n",
    "#         'error': None\n",
    "#     }\n",
    "#     return (final_results, debug_data)\n",
    "\n",
    "def verify_sharpe_atr_calculation_checked(df_ohlcv, features_df, tickers_to_verify, benchmark_ticker,\n",
    "                                    start_date, calc_period, fwd_period,\n",
    "                                    master_calendar_ticker='VOO', debug=False):\n",
    "    \"\"\"\n",
    "    Verifies the Sharpe (ATR) calculations for a portfolio and benchmark.\n",
    "\n",
    "    This function transparently recalculates the key components for Sharpe (ATR)\n",
    "    and can optionally export the underlying source data for manual inspection.\n",
    "    \"\"\"\n",
    "    display(Markdown(f\"## Verification Report for Sharpe (ATR) Calculation\"))\n",
    "    display(Markdown(f\"**Portfolio Tickers:** `{tickers_to_verify}`\\n**Benchmark Ticker:** `{benchmark_ticker}`\"))\n",
    "\n",
    "    # --- 1. Determine Exact Period Dates (No changes here) ---\n",
    "    master_trading_days = df_ohlcv.loc[master_calendar_ticker].index.unique().sort_values()\n",
    "    start_date_raw = pd.to_datetime(start_date)\n",
    "    start_idx = master_trading_days.searchsorted(start_date_raw)\n",
    "    actual_start_date = master_trading_days[start_idx]\n",
    "    calc_end_idx = min(start_idx + calc_period, len(master_trading_days) - 1)\n",
    "    fwd_end_idx = min(calc_end_idx + fwd_period, len(master_trading_days) - 1)\n",
    "    actual_calc_end_date = master_trading_days[calc_end_idx]\n",
    "    actual_fwd_end_date = master_trading_days[fwd_end_idx]\n",
    "    \n",
    "    display(Markdown(f\"**Full Period:** `{actual_start_date.date()}` to `{actual_fwd_end_date.date()}`\\n\"\n",
    "                    f\"**Calc End Date:** `{actual_calc_end_date.date()}`\"))\n",
    "\n",
    "    # Original debug export block (can be kept or removed)\n",
    "    if debug:\n",
    "        # ... (original export code remains here) ...\n",
    "        pass # Assuming original block is here\n",
    "\n",
    "    # --- 2. Recreate Portfolio & Benchmark Series from Scratch ---\n",
    "    df_close_full = df_ohlcv['Adj Close'].unstack(level=0)\n",
    "    portfolio_prices_raw = df_close_full[tickers_to_verify].loc[actual_start_date:actual_fwd_end_date]\n",
    "    portfolio_prices_norm = portfolio_prices_raw.div(portfolio_prices_raw.bfill().iloc[0])\n",
    "    portfolio_value_series = portfolio_prices_norm.mean(axis=1)\n",
    "    portfolio_return_series = portfolio_value_series.pct_change()\n",
    "    p_idx = pd.MultiIndex.from_product([tickers_to_verify, portfolio_return_series.index])\n",
    "    p_atrp_df = features_df.loc[features_df.index.intersection(p_idx)]['ATRP'].unstack(level=0)\n",
    "    portfolio_atrp_series = p_atrp_df.mean(axis=1)\n",
    "\n",
    "###############################    \n",
    "    # benchmark_return_series = df_close_full[benchmark_ticker].pct_change().loc[actual_start_date:actual_fwd_end_date]\n",
    "\n",
    "    # 1. First, slice the raw prices for the desired date range.\n",
    "    benchmark_prices_raw = df_close_full[benchmark_ticker].loc[actual_start_date:actual_fwd_end_date]\n",
    "    # 2. Then, calculate the percentage change on the sliced data.\n",
    "    benchmark_return_series = benchmark_prices_raw.pct_change()\n",
    "\n",
    "###############################    \n",
    "    benchmark_atrp_series = features_df.xs(benchmark_ticker, level='Ticker')['ATRP'].loc[actual_start_date:actual_fwd_end_date]\n",
    "\n",
    "\n",
    "    # +++ NEW: DETAILED RETURN CALCULATION TRACE (PORTFOLIO) +++\n",
    "    if debug:\n",
    "        display(Markdown(\"---\"))\n",
    "        display(Markdown(\"### 🐛 Detailed Portfolio Return Calculation Trace\"))\n",
    "\n",
    "        # Step 1: Show the raw prices being used\n",
    "        print(\"\\n[STEP 1] Raw Adjusted Close prices used for calculation.\")\n",
    "        print(\"Compare these values with your 'Adj Close' columns.\")\n",
    "        display(portfolio_prices_raw)\n",
    "\n",
    "        # Step 2: Show the normalization base and the result\n",
    "        normalization_base = portfolio_prices_raw.bfill().iloc[0]\n",
    "        print(\"\\n[STEP 2] Normalization base (first row of prices).\")\n",
    "        print(\"Each column in Step 1 is divided by this corresponding value.\")\n",
    "        display(pd.DataFrame(normalization_base).T)\n",
    "\n",
    "        print(\"\\n[STEP 2a] Normalized prices (Result of division).\")\n",
    "        print(\"Compare these values with your 'N Close' columns.\")\n",
    "        display(portfolio_prices_norm)\n",
    "\n",
    "        # Step 3: Show the averaged portfolio value series\n",
    "        print(\"\\n[STEP 3] Averaged normalized portfolio value series.\")\n",
    "        print(\"This is the row-by-row average of the table in Step 2a.\")\n",
    "        print(\"Compare this series with your 'N_portf' column.\")\n",
    "        display(pd.DataFrame(portfolio_value_series, columns=['N_portf']))\n",
    "\n",
    "        # Step 4: Show the final portfolio return series\n",
    "        print(\"\\n[STEP 4] Final portfolio daily return series (pct_change).\")\n",
    "        print(\"This is the percentage change of the series in Step 3.\")\n",
    "        print(\"Compare this series with your 'N_portf_rtn' column.\")\n",
    "        display(pd.DataFrame(portfolio_return_series, columns=['N_portf_rtn']))\n",
    "        display(Markdown(\"---\"))\n",
    "    # +++ END OF NEW DEBUG BLOCK +++\n",
    "\n",
    "\n",
    "    # --- 3. Define a Helper to Print Detailed Calculation Steps ---\n",
    "    # MODIFIED to include more debug details inside\n",
    "    def _calculate_and_print_metrics(period_name, returns, atrps):\n",
    "        display(Markdown(f\"#### {period_name}\"))\n",
    "        if returns.dropna().empty or atrps.dropna().empty:\n",
    "            print(\"  - Not enough data to calculate.\")\n",
    "            return np.nan\n",
    "\n",
    "        # Standard calculations\n",
    "        mean_return = returns.mean()\n",
    "        mean_atrp = atrps.mean()\n",
    "        sharpe_atr = mean_return / mean_atrp if mean_atrp > 0 else np.nan\n",
    "\n",
    "        # +++ ADDED: Detailed Mean Calculation Breakdown +++\n",
    "        if debug:\n",
    "            valid_returns = returns.dropna()\n",
    "            num_returns = valid_returns.count()\n",
    "            sum_returns = valid_returns.sum()\n",
    "            manual_mean = sum_returns / num_returns if num_returns > 0 else 0\n",
    "            print(f\"  - (Debug) Number of valid daily returns: {num_returns}\")\n",
    "            print(f\"  - (Debug) Sum of all daily returns:      {sum_returns:,.8f}\")\n",
    "            print(f\"  - (Debug) Manually calculated mean:      {manual_mean:,.8f} (Sum / Count)\")\n",
    "        # +++ END OF ADDED DETAIL +++\n",
    "\n",
    "        print(f\"  - Mean Daily Return: {mean_return:,.6f}\")\n",
    "        print(f\"  - Mean Daily ATRP:  {mean_atrp:,.6f}\")\n",
    "        print(f\"  - Sharpe (ATR) = (Mean Return / Mean ATRP) = {sharpe_atr:,.4f}\")\n",
    "        return sharpe_atr\n",
    "\n",
    "    # --- 4. Run Verification for Portfolio (No changes here) ---\n",
    "    display(Markdown(\"### A. Group Portfolio Verification\"))\n",
    "    _calculate_and_print_metrics(\"Full Period\", portfolio_return_series, portfolio_atrp_series)\n",
    "    _calculate_and_print_metrics(\"Calculation Period\", portfolio_return_series.loc[:actual_calc_end_date], portfolio_atrp_series.loc[:actual_calc_end_date])\n",
    "    _calculate_and_print_metrics(\"Forward Period\", portfolio_return_series.loc[actual_calc_end_date:].iloc[1:], portfolio_atrp_series.loc[actual_calc_end_date:].iloc[1:])\n",
    "\n",
    "    # --- 5. Run Verification for Benchmark (No changes here) ---\n",
    "    display(Markdown(f\"### B. Benchmark ({benchmark_ticker}) Verification\"))\n",
    "    _calculate_and_print_metrics(\"Full Period\", benchmark_return_series, benchmark_atrp_series)\n",
    "    _calculate_and_print_metrics(\"Calculation Period\", benchmark_return_series.loc[:actual_calc_end_date], benchmark_atrp_series.loc[:actual_calc_end_date])\n",
    "    _calculate_and_print_metrics(\"Forward Period\", benchmark_return_series.loc[actual_calc_end_date:].iloc[1:], benchmark_atrp_series.loc[actual_calc_end_date:].iloc[1:])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5c908a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume 'df_ohlcv' is your loaded dataset\n",
    "# You might need to regenerate features_df if it's not in your notebook's memory\n",
    "print(\"--- Regenerating features for verification ---\")\n",
    "features_df = generate_features(df_ohlcv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af59dfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers_to_verify = ['STIP', 'RCL']\n",
    "start_date = '2025-08-13'\n",
    "end_date = '2025-09-04'\n",
    "benchmark_ticker = 'VOO'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a7a71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_sharpe_atr_calculation_checked(df_ohlcv, features_df, tickers_to_verify, benchmark_ticker,\n",
    "                                    start_date, calc_period, fwd_period,\n",
    "                                    master_calendar_ticker='VOO', debug=False):\n",
    "    \"\"\"\n",
    "    Verifies the Sharpe (ATR) calculations for a portfolio and benchmark.\n",
    "\n",
    "    This function transparently recalculates the key components for Sharpe (ATR)\n",
    "    and can optionally export the underlying source data for manual inspection.\n",
    "    \"\"\"\n",
    "    display(Markdown(f\"## Verification Report for Sharpe (ATR) Calculation\"))\n",
    "    display(Markdown(f\"**Portfolio Tickers:** `{tickers_to_verify}`\\n**Benchmark Ticker:** `{benchmark_ticker}`\"))\n",
    "\n",
    "    # --- 1. Determine Exact Period Dates (No changes here) ---\n",
    "    master_trading_days = df_ohlcv.loc[master_calendar_ticker].index.unique().sort_values()\n",
    "    start_date_raw = pd.to_datetime(start_date)\n",
    "    start_idx = master_trading_days.searchsorted(start_date_raw)\n",
    "    actual_start_date = master_trading_days[start_idx]\n",
    "    calc_end_idx = min(start_idx + calc_period, len(master_trading_days) - 1)\n",
    "    fwd_end_idx = min(calc_end_idx + fwd_period, len(master_trading_days) - 1)\n",
    "    actual_calc_end_date = master_trading_days[calc_end_idx]\n",
    "    actual_fwd_end_date = master_trading_days[fwd_end_idx]\n",
    "    \n",
    "    display(Markdown(f\"**Full Period:** `{actual_start_date.date()}` to `{actual_fwd_end_date.date()}`\\n\"\n",
    "                    f\"**Calc End Date:** `{actual_calc_end_date.date()}`\"))\n",
    "\n",
    "    # Original debug export block (can be kept or removed)\n",
    "    if debug:\n",
    "        # ... (original export code remains here) ...\n",
    "        pass # Assuming original block is here\n",
    "\n",
    "    # --- 2. Recreate Portfolio & Benchmark Series from Scratch ---\n",
    "    df_close_full = df_ohlcv['Adj Close'].unstack(level=0)\n",
    "    portfolio_prices_raw = df_close_full[tickers_to_verify].loc[actual_start_date:actual_fwd_end_date]\n",
    "    portfolio_prices_norm = portfolio_prices_raw.div(portfolio_prices_raw.bfill().iloc[0])\n",
    "    portfolio_value_series = portfolio_prices_norm.mean(axis=1)\n",
    "    portfolio_return_series = portfolio_value_series.pct_change()\n",
    "    p_idx = pd.MultiIndex.from_product([tickers_to_verify, portfolio_return_series.index])\n",
    "    p_atrp_df = features_df.loc[features_df.index.intersection(p_idx)]['ATRP'].unstack(level=0)\n",
    "    portfolio_atrp_series = p_atrp_df.mean(axis=1)\n",
    "\n",
    "###############################    \n",
    "    # benchmark_return_series = df_close_full[benchmark_ticker].pct_change().loc[actual_start_date:actual_fwd_end_date]\n",
    "\n",
    "    # 1. First, slice the raw prices for the desired date range.\n",
    "    benchmark_prices_raw = df_close_full[benchmark_ticker].loc[actual_start_date:actual_fwd_end_date]\n",
    "    # 2. Then, calculate the percentage change on the sliced data.\n",
    "    benchmark_return_series = benchmark_prices_raw.pct_change()\n",
    "\n",
    "###############################    \n",
    "    benchmark_atrp_series = features_df.xs(benchmark_ticker, level='Ticker')['ATRP'].loc[actual_start_date:actual_fwd_end_date]\n",
    "\n",
    "\n",
    "    # +++ NEW: DETAILED RETURN CALCULATION TRACE (PORTFOLIO) +++\n",
    "    if debug:\n",
    "        display(Markdown(\"---\"))\n",
    "        display(Markdown(\"### 🐛 Detailed Portfolio Return Calculation Trace\"))\n",
    "\n",
    "        # Step 1: Show the raw prices being used\n",
    "        print(\"\\n[STEP 1] Raw Adjusted Close prices used for calculation.\")\n",
    "        print(\"Compare these values with your 'Adj Close' columns.\")\n",
    "        display(portfolio_prices_raw)\n",
    "\n",
    "        # Step 2: Show the normalization base and the result\n",
    "        normalization_base = portfolio_prices_raw.bfill().iloc[0]\n",
    "        print(\"\\n[STEP 2] Normalization base (first row of prices).\")\n",
    "        print(\"Each column in Step 1 is divided by this corresponding value.\")\n",
    "        display(pd.DataFrame(normalization_base).T)\n",
    "\n",
    "        print(\"\\n[STEP 2a] Normalized prices (Result of division).\")\n",
    "        print(\"Compare these values with your 'N Close' columns.\")\n",
    "        display(portfolio_prices_norm)\n",
    "\n",
    "        # Step 3: Show the averaged portfolio value series\n",
    "        print(\"\\n[STEP 3] Averaged normalized portfolio value series.\")\n",
    "        print(\"This is the row-by-row average of the table in Step 2a.\")\n",
    "        print(\"Compare this series with your 'N_portf' column.\")\n",
    "        display(pd.DataFrame(portfolio_value_series, columns=['N_portf']))\n",
    "\n",
    "        # Step 4: Show the final portfolio return series\n",
    "        print(\"\\n[STEP 4] Final portfolio daily return series (pct_change).\")\n",
    "        print(\"This is the percentage change of the series in Step 3.\")\n",
    "        print(\"Compare this series with your 'N_portf_rtn' column.\")\n",
    "        display(pd.DataFrame(portfolio_return_series, columns=['N_portf_rtn']))\n",
    "        display(Markdown(\"---\"))\n",
    "    # +++ END OF NEW DEBUG BLOCK +++\n",
    "\n",
    "\n",
    "    # --- 3. Define a Helper to Print Detailed Calculation Steps ---\n",
    "    # MODIFIED to include more debug details inside\n",
    "    def _calculate_and_print_metrics(period_name, returns, atrps):\n",
    "        display(Markdown(f\"#### {period_name}\"))\n",
    "        if returns.dropna().empty or atrps.dropna().empty:\n",
    "            print(\"  - Not enough data to calculate.\")\n",
    "            return np.nan\n",
    "\n",
    "        # Standard calculations\n",
    "        mean_return = returns.mean()\n",
    "        mean_atrp = atrps.mean()\n",
    "        sharpe_atr = mean_return / mean_atrp if mean_atrp > 0 else np.nan\n",
    "\n",
    "        # +++ ADDED: Detailed Mean Calculation Breakdown +++\n",
    "        if debug:\n",
    "            valid_returns = returns.dropna()\n",
    "            num_returns = valid_returns.count()\n",
    "            sum_returns = valid_returns.sum()\n",
    "            manual_mean = sum_returns / num_returns if num_returns > 0 else 0\n",
    "            print(f\"  - (Debug) Number of valid daily returns: {num_returns}\")\n",
    "            print(f\"  - (Debug) Sum of all daily returns:      {sum_returns:,.8f}\")\n",
    "            print(f\"  - (Debug) Manually calculated mean:      {manual_mean:,.8f} (Sum / Count)\")\n",
    "        # +++ END OF ADDED DETAIL +++\n",
    "\n",
    "        print(f\"  - Mean Daily Return: {mean_return:,.6f}\")\n",
    "        print(f\"  - Mean Daily ATRP:  {mean_atrp:,.6f}\")\n",
    "        print(f\"  - Sharpe (ATR) = (Mean Return / Mean ATRP) = {sharpe_atr:,.4f}\")\n",
    "        return sharpe_atr\n",
    "\n",
    "    # --- 4. Run Verification for Portfolio (No changes here) ---\n",
    "    display(Markdown(\"### A. Group Portfolio Verification\"))\n",
    "    _calculate_and_print_metrics(\"Full Period\", portfolio_return_series, portfolio_atrp_series)\n",
    "    _calculate_and_print_metrics(\"Calculation Period\", portfolio_return_series.loc[:actual_calc_end_date], portfolio_atrp_series.loc[:actual_calc_end_date])\n",
    "    _calculate_and_print_metrics(\"Forward Period\", portfolio_return_series.loc[actual_calc_end_date:].iloc[1:], portfolio_atrp_series.loc[actual_calc_end_date:].iloc[1:])\n",
    "\n",
    "    # --- 5. Run Verification for Benchmark (No changes here) ---\n",
    "    display(Markdown(f\"### B. Benchmark ({benchmark_ticker}) Verification\"))\n",
    "    _calculate_and_print_metrics(\"Full Period\", benchmark_return_series, benchmark_atrp_series)\n",
    "    _calculate_and_print_metrics(\"Calculation Period\", benchmark_return_series.loc[:actual_calc_end_date], benchmark_atrp_series.loc[:actual_calc_end_date])\n",
    "    _calculate_and_print_metrics(\"Forward Period\", benchmark_return_series.loc[actual_calc_end_date:].iloc[1:], benchmark_atrp_series.loc[actual_calc_end_date:].iloc[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0d9bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now call the verification function with the exact parameters from the UI\n",
    "verify_sharpe_atr_calculation_checked(\n",
    "    df_ohlcv=df_ohlcv,\n",
    "    features_df=features_df,\n",
    "    tickers_to_verify=tickers_to_verify, # <-- From the UI output\n",
    "    benchmark_ticker=benchmark_ticker,\n",
    "    start_date=start_date,\n",
    "    calc_period=10,\n",
    "    fwd_period=5,\n",
    "    debug=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18caedd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import ipywidgets as widgets\n",
    "import pprint\n",
    "from IPython.display import display, Markdown\n",
    "import os # Make sure os is imported for the export function later\n",
    "\n",
    "\n",
    "# def calculate_gain(price_series: pd.Series):\n",
    "#     \"\"\"Calculates the total gain over a series of prices.\"\"\"\n",
    "#     # Ensure there are at least two data points to calculate a gain\n",
    "#     if price_series.dropna().shape[0] < 2: return np.nan\n",
    "#     # Use forward-fill for the end price and back-fill for the start price\n",
    "#     # to handle potential NaNs at the beginning or end of the series.\n",
    "#     return (price_series.ffill().iloc[-1] / price_series.bfill().iloc[0]) - 1\n",
    "\n",
    "# def calculate_sharpe(return_series: pd.Series):\n",
    "#     \"\"\"Calculates the annualized Sharpe ratio from a series of daily returns.\"\"\"\n",
    "#     # Ensure there are at least two returns to calculate a standard deviation\n",
    "#     if return_series.dropna().shape[0] < 2: return np.nan\n",
    "#     std_dev = return_series.std()\n",
    "#     # Avoid division by zero if returns are constant\n",
    "#     if std_dev > 0 and std_dev != np.inf:\n",
    "#         return (return_series.mean() / std_dev) * np.sqrt(252)\n",
    "#     return np.nan\n",
    "\n",
    "# def calculate_sharpe_atr(return_series: pd.Series, atrp_series: pd.Series):\n",
    "#     \"\"\"Calculates a Sharpe-like ratio using mean ATRP as the denominator.\"\"\"\n",
    "#     # Ensure there are returns and that ATRP data is valid\n",
    "#     if return_series.dropna().shape[0] < 2 or atrp_series.dropna().empty:\n",
    "#         return np.nan\n",
    "        \n",
    "#     mean_return = return_series.mean()\n",
    "#     mean_atrp = atrp_series.mean()\n",
    "    \n",
    "#     # Avoid division by zero\n",
    "#     if mean_atrp > 0 and mean_atrp != np.inf:\n",
    "#         return mean_return / mean_atrp\n",
    "        \n",
    "#     return np.nan\n",
    "\n",
    "\n",
    "# # --- B. MODULAR METRIC CALCULATION ENGINE ---\n",
    "\n",
    "# def calculate_price_metric(metric_data: dict) -> pd.Series:\n",
    "#     \"\"\"\n",
    "#     Calculates the 'Price' metric (total gain) over the calculation period.\n",
    "\n",
    "#     Args:\n",
    "#         metric_data: A dictionary containing pre-calculated data Series.\n",
    "#                      Requires 'calc_close' (pd.DataFrame): The close prices for the calc period.\n",
    "\n",
    "#     Returns:\n",
    "#         A pandas Series of the calculated metric values, indexed by Ticker.\n",
    "#     \"\"\"\n",
    "#     calc_close = metric_data['calc_close']\n",
    "    \n",
    "#     # Ensure there are at least two data points to calculate a gain\n",
    "#     if len(calc_close) < 2:\n",
    "#         return pd.Series(dtype='float64', index=calc_close.columns)\n",
    "\n",
    "#     first_prices = calc_close.bfill().iloc[0]\n",
    "#     last_prices = calc_close.ffill().iloc[-1]\n",
    "    \n",
    "#     # The division of two Series aligns by index (Ticker), which is what we want.\n",
    "#     price_metric = last_prices / first_prices\n",
    "    \n",
    "#     return price_metric.dropna()\n",
    "\n",
    "# def calculate_sharpe_metric(metric_data: dict) -> pd.Series:\n",
    "#     \"\"\"\n",
    "#     Calculates the annualized 'Sharpe' ratio metric over the calculation period.\n",
    "\n",
    "#     Args:\n",
    "#         metric_data: A dictionary containing pre-calculated data Series.\n",
    "#                      Requires 'daily_returns' (pd.DataFrame): Daily returns for the calc period.\n",
    "\n",
    "#     Returns:\n",
    "#         A pandas Series of the calculated metric values, indexed by Ticker.\n",
    "#     \"\"\"\n",
    "#     daily_returns = metric_data['daily_returns']\n",
    "    \n",
    "#     # Ensure there's enough data to calculate standard deviation\n",
    "#     if len(daily_returns.dropna()) < 2:\n",
    "#         return pd.Series(dtype='float64', index=daily_returns.columns)\n",
    "    \n",
    "#     mean_returns = daily_returns.mean()\n",
    "#     std_returns = daily_returns.std()\n",
    "    \n",
    "#     # Standard annualized Sharpe Ratio calculation. Avoid division by zero.\n",
    "#     # We replace resulting NaNs/infs with 0 to handle cases of zero volatility.\n",
    "#     sharpe_ratio = (mean_returns / std_returns * np.sqrt(252))\n",
    "    \n",
    "#     return sharpe_ratio.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "    \n",
    "# def calculate_sharpe_atr_metric(metric_data: dict) -> pd.Series:\n",
    "#     \"\"\"\n",
    "#     Calculates the 'Sharpe (ATR)' metric over the calculation period.\n",
    "\n",
    "#     Args:\n",
    "#         metric_data: A dictionary containing pre-calculated data Series.\n",
    "#                      Requires 'daily_returns' (pd.DataFrame): Daily returns for the calc period.\n",
    "#                      Requires 'atrp' (pd.Series): Mean ATRP for each ticker over the period.\n",
    "\n",
    "#     Returns:\n",
    "#         A pandas Series of the calculated metric values, indexed by Ticker.\n",
    "#     \"\"\"\n",
    "#     daily_returns = metric_data['daily_returns']\n",
    "#     atrp = metric_data['atrp']\n",
    "    \n",
    "#     mean_returns = daily_returns.mean()\n",
    "    \n",
    "#     # ATRP-based Sharpe. Avoid division by zero.\n",
    "#     # We replace resulting NaNs/infs with 0.\n",
    "#     sharpe_atr = mean_returns / atrp\n",
    "    \n",
    "#     return sharpe_atr.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "\n",
    "# METRIC_REGISTRY = {\n",
    "#     'Price': calculate_price_metric,\n",
    "#     'Sharpe': calculate_sharpe_metric,\n",
    "#     'Sharpe (ATR)': calculate_sharpe_atr_metric,\n",
    "# }\n",
    "\n",
    "\n",
    "def plot_walk_forward_analyzer(df_ohlcv,\n",
    "                               default_start_date=None, default_calc_period=126, default_fwd_period=63,\n",
    "                               default_metric='Sharpe (ATR)', default_rank_start=1, default_rank_end=10,\n",
    "                               default_benchmark_ticker='VOO', master_calendar_ticker='VOO',\n",
    "                               quality_thresholds={'min_median_dollar_volume': 1_000_000, 'max_stale_pct': 0.05, 'max_same_vol_count': 10},\n",
    "                               debug=False):\n",
    "    # (No changes to the initial setup part of this function...)\n",
    "    print(\"Initializing Walk-Forward Analyzer (using Trading Day Logic)...\")\n",
    "    if not isinstance(df_ohlcv.index, pd.MultiIndex): raise ValueError(\"Input DataFrame must have a (Ticker, Date) MultiIndex.\")\n",
    "    df_ohlcv = df_ohlcv.sort_index()\n",
    "    if master_calendar_ticker not in df_ohlcv.index.get_level_values(0):\n",
    "        raise ValueError(f\"Master calendar ticker '{master_calendar_ticker}' not found in DataFrame.\")\n",
    "    master_trading_days = df_ohlcv.loc[master_calendar_ticker].index.unique().sort_values()\n",
    "    print(f\"Master trading day calendar created from '{master_calendar_ticker}' ({len(master_trading_days)} days).\")\n",
    "    print(\"--- Generating all features upfront... ---\")\n",
    "    features_df = generate_features(df_ohlcv)\n",
    "    print(\"Pre-processing data (unstacking)...\")\n",
    "    df_close_full = df_ohlcv['Adj Close'].unstack(level=0)\n",
    "    df_high_full = df_ohlcv['Adj High'].unstack(level=0)\n",
    "    df_low_full = df_ohlcv['Adj Low'].unstack(level=0)\n",
    "    start_date_picker = widgets.DatePicker(description='Start Date:', value=pd.to_datetime(default_start_date), disabled=False)\n",
    "    calc_period_input = widgets.IntText(value=default_calc_period, description='Calc Period (days):')\n",
    "    fwd_period_input = widgets.IntText(value=default_fwd_period, description='Fwd Period (days):')\n",
    "    if default_metric not in METRIC_REGISTRY:\n",
    "        fallback_metric = list(METRIC_REGISTRY.keys())[0]\n",
    "        print(f\"⚠️ Warning: Default metric '{default_metric}' not in registry. Using '{fallback_metric}'.\")\n",
    "        default_metric = fallback_metric\n",
    "    metric_dropdown = widgets.Dropdown(options=list(METRIC_REGISTRY.keys()), value=default_metric, description='Metric:')\n",
    "    rank_start_input = widgets.IntText(value=default_rank_start, description='Rank Start:')\n",
    "    rank_end_input = widgets.IntText(value=default_rank_end, description='Rank End:')\n",
    "    benchmark_ticker_input = widgets.Text(value=default_benchmark_ticker, description='Benchmark:', placeholder='Enter Ticker')\n",
    "    update_button = widgets.Button(description=\"Update Chart\", button_style='primary')\n",
    "    ticker_list_output = widgets.Output()\n",
    "    results_container, debug_data_container = [None], [None]\n",
    "    fig = go.FigureWidget()\n",
    "    max_traces = 50\n",
    "    for i in range(max_traces): fig.add_trace(go.Scatter(x=[None], y=[None], mode='lines', name=f'placeholder_{i}', visible=False, showlegend=False))\n",
    "    fig.add_trace(go.Scatter(x=[None], y=[None], mode='lines', name='Benchmark', visible=True, showlegend=True, line=dict(color='black', width=3, dash='dash')))\n",
    "    fig.add_trace(go.Scatter(x=[None], y=[None], mode='lines', name='Group Portfolio', visible=True, showlegend=True, line=dict(color='green', width=3)))\n",
    "\n",
    "    def update_plot(button_click):\n",
    "        ticker_list_output.clear_output()\n",
    "        start_date_raw = pd.to_datetime(start_date_picker.value)\n",
    "        start_date_idx = master_trading_days.searchsorted(start_date_raw)\n",
    "        if start_date_idx >= len(master_trading_days):\n",
    "            with ticker_list_output: print(f\"Error: Start date is after the last available trading day.\"); return\n",
    "        actual_start_date = master_trading_days[start_date_idx]\n",
    "        with ticker_list_output:\n",
    "            if start_date_raw.date() != actual_start_date.date():\n",
    "                print(f\"ℹ️ Info: Start date {start_date_raw.date()} is not a trading day. Snapping forward to {actual_start_date.date()}.\")\n",
    "        calc_period = calc_period_input.value; fwd_period = fwd_period_input.value; metric = metric_dropdown.value\n",
    "        rank_start = rank_start_input.value; rank_end = rank_end_input.value; benchmark_ticker = benchmark_ticker_input.value.strip().upper()\n",
    "        if rank_start > rank_end:\n",
    "            with ticker_list_output: print(\"Error: 'Rank Start' must be <= 'Rank End'.\"); return\n",
    "        if rank_start < 1 or calc_period < 2 or fwd_period < 1:\n",
    "            with ticker_list_output: print(\"Error: Ranks must be >= 1, Calc Period >= 2, Fwd Period >= 1.\"); return\n",
    "        required_days = calc_period + fwd_period\n",
    "        if start_date_idx + required_days > len(master_trading_days):\n",
    "            available_days = len(master_trading_days) - start_date_idx; last_available_date = master_trading_days[-1].date()\n",
    "            with ticker_list_output:\n",
    "                print(f\"Error: Not enough data for the requested period.\\n  Start Date: {actual_start_date.date()}\\n  Required Days: {calc_period} (calc) + {fwd_period} (fwd) = {required_days}\\n  Available Days from Start: {available_days} (until {last_available_date})\\n  Please shorten the 'Calc Period' / 'Fwd Period' or choose an earlier 'Start Date'.\")\n",
    "            return\n",
    "        eligible_tickers = get_eligible_universe(features_df, actual_start_date, quality_thresholds)\n",
    "        if not eligible_tickers:\n",
    "            with ticker_list_output: print(f\"Error: No eligible tickers found on {actual_start_date.date()} with the current quality filters.\"); return\n",
    "        df_close_step = df_close_full[eligible_tickers]; df_high_step = df_high_full[eligible_tickers]; df_low_step = df_low_full[eligible_tickers]\n",
    "        results, debug_output = run_walk_forward_step(df_close_full, df_high_full, df_low_full, master_trading_days, actual_start_date, calc_period, fwd_period, metric, rank_start, rank_end, benchmark_ticker, features_df=features_df, debug=debug)\n",
    "        if results.get('error'):\n",
    "            with ticker_list_output: print(f\"Error: {results['error']}\"); return\n",
    "        period_dates = {'calc_period_start': results['safe_start_date'], 'calc_period_end': results['actual_calc_end_ts'], 'forward_period_start': results['actual_calc_end_ts'], 'forward_period_end': results['safe_viz_end_date']}\n",
    "        run_parameters = {'calc_period': calc_period, 'fwd_period': fwd_period, 'rank_metric': metric, 'rank_start': rank_start, 'rank_end': rank_end, 'benchmark_ticker': benchmark_ticker}\n",
    "        results.update(period_dates); results.update(run_parameters)\n",
    "        if debug_output is not None and isinstance(debug_output, dict):\n",
    "            debug_output.update(period_dates); debug_output.update(run_parameters)\n",
    "        with fig.batch_update():\n",
    "            for i in range(max_traces):\n",
    "                trace = fig.data[i]\n",
    "                if i < len(results['tickers_to_display']):\n",
    "                    ticker = results['tickers_to_display'][i]; plot_data_series = results['normalized_plot_data'][ticker]\n",
    "                    trace.x, trace.y, trace.name, trace.visible, trace.showlegend = plot_data_series.index, plot_data_series.values, ticker, True, True\n",
    "                else: trace.visible, trace.showlegend = False, False\n",
    "            benchmark_trace = fig.data[max_traces]\n",
    "            if results['benchmark_price_series'] is not None and not results['benchmark_price_series'].loc[results['safe_start_date']:results['safe_viz_end_date']].dropna().empty:\n",
    "                normalized_benchmark = results['benchmark_price_series'].loc[results['safe_start_date']:results['safe_viz_end_date']] / results['benchmark_price_series'].loc[results['safe_start_date']:].bfill().iloc[0]\n",
    "                benchmark_trace.x, benchmark_trace.y, benchmark_trace.name, benchmark_trace.visible = normalized_benchmark.index, normalized_benchmark.values, f\"Benchmark ({benchmark_ticker})\", True\n",
    "            else: benchmark_trace.visible = False\n",
    "            portfolio_trace = fig.data[max_traces + 1]\n",
    "            portfolio_trace.x, portfolio_trace.y, portfolio_trace.name, portfolio_trace.visible = results['portfolio_series'].index, results['portfolio_series'], 'Group Portfolio', True\n",
    "            fig.layout.shapes = []; fig.add_shape(type=\"line\", x0=results['actual_calc_end_ts'], y0=0, x1=results['actual_calc_end_ts'], y1=1, xref='x', yref='paper', line=dict(color=\"grey\", width=2, dash=\"dash\"))\n",
    "        results_container[0] = results; debug_data_container[0] = debug_output\n",
    "        with ticker_list_output:\n",
    "            print(f\"Analysis Period: {results['safe_start_date'].date()} to {results['safe_viz_end_date'].date()}.\")\n",
    "            pprint.pprint(results['tickers_to_display'])\n",
    "            p = results['performance_data']\n",
    "            \n",
    "            # --- START OF MODIFIED BLOCK ---\n",
    "            rows = []\n",
    "            # Gain Metrics\n",
    "            rows.append({'Metric': 'Group Portfolio Gain', 'Full': p['full_p_gain'], 'Calc': p['calc_p_gain'], 'Fwd': p['fwd_p_gain']})\n",
    "            if not np.isnan(p.get('full_b_gain')):\n",
    "                rows.append({'Metric': f'Benchmark ({benchmark_ticker}) Gain', 'Full': p['full_b_gain'], 'Calc': p['calc_b_gain'], 'Fwd': p['fwd_b_gain']})\n",
    "                rows.append({'Metric': '== Gain Delta (vs Bm)', 'Full': p['full_p_gain'] - p['full_b_gain'], 'Calc': p['calc_p_gain'] - p['calc_b_gain'], 'Fwd': p['fwd_p_gain'] - p['fwd_b_gain']})\n",
    "            \n",
    "            # Standard Sharpe Metrics\n",
    "            rows.append({'Metric': 'Group Portfolio Sharpe', 'Full': p['full_p_sharpe'], 'Calc': p['calc_p_sharpe'], 'Fwd': p['fwd_p_sharpe']})\n",
    "            if not np.isnan(p.get('full_b_sharpe')):\n",
    "                rows.append({'Metric': f'Benchmark ({benchmark_ticker}) Sharpe', 'Full': p['full_b_sharpe'], 'Calc': p['calc_b_sharpe'], 'Fwd': p['fwd_b_sharpe']})\n",
    "                rows.append({'Metric': '== Sharpe Delta (vs Bm)', 'Full': p['full_p_sharpe'] - p['full_b_sharpe'], 'Calc': p['calc_p_sharpe'] - p['calc_b_sharpe'], 'Fwd': p['fwd_p_sharpe'] - p['fwd_b_sharpe']})\n",
    "\n",
    "            # Sharpe (ATR) Metrics\n",
    "            rows.append({'Metric': 'Group Portfolio Sharpe (ATR)', 'Full': p['full_p_sharpe_atr'], 'Calc': p['calc_p_sharpe_atr'], 'Fwd': p['fwd_p_sharpe_atr']})\n",
    "            if not np.isnan(p.get('full_b_sharpe_atr')):\n",
    "                rows.append({'Metric': f'Benchmark ({benchmark_ticker}) Sharpe (ATR)', 'Full': p['full_b_sharpe_atr'], 'Calc': p['calc_b_sharpe_atr'], 'Fwd': p['fwd_b_sharpe_atr']})\n",
    "                rows.append({'Metric': '== Sharpe (ATR) Delta (vs Bm)', 'Full': p['full_p_sharpe_atr'] - p['full_b_sharpe_atr'], 'Calc': p['calc_p_sharpe_atr'] - p['calc_b_sharpe_atr'], 'Fwd': p['fwd_p_sharpe_atr'] - p['fwd_b_sharpe_atr']})\n",
    "\n",
    "            report_df = pd.DataFrame(rows).set_index('Metric')\n",
    "            gain_rows = [row for row in report_df.index if 'Gain' in row]\n",
    "            sharpe_rows = [row for row in report_df.index if 'Sharpe' in row] # This now correctly includes both types of Sharpe\n",
    "            # --- END OF MODIFIED BLOCK ---\n",
    "            \n",
    "            styled_df = report_df.style.format('{:+.2%}', na_rep='N/A', subset=(gain_rows, report_df.columns)).format('{:+.2f}', na_rep='N/A', subset=(sharpe_rows, report_df.columns)).set_properties(**{'text-align': 'right', 'width': '100px'}).set_table_styles([{'selector': 'th.col_heading', 'props': [('text-align', 'right')]}, {'selector': 'th.row_heading', 'props': [('text-align', 'left')]}])\n",
    "            print(\"\\n--- Strategy Performance Summary ---\")\n",
    "            display(styled_df)\n",
    "    fig.update_layout(title_text='Walk-Forward Performance Analysis', xaxis_title='Date', yaxis_title='Normalized Price (Start = 1)', hovermode='x unified', legend_title_text='Tickers (Ranked)', height=600, margin=dict(t=50))\n",
    "    fig.add_hline(y=1, line_width=1, line_dash=\"dash\", line_color=\"grey\")\n",
    "    update_button.on_click(update_plot)\n",
    "    controls_row1 = widgets.HBox([start_date_picker, calc_period_input, fwd_period_input])\n",
    "    controls_row2 = widgets.HBox([metric_dropdown, rank_start_input, rank_end_input, benchmark_ticker_input, update_button])\n",
    "    ui_container = widgets.VBox([controls_row1, controls_row2, ticker_list_output], layout=widgets.Layout(margin='10px 0 20px 0'))\n",
    "    display(ui_container, fig)\n",
    "    update_plot(None)\n",
    "    return (results_container, debug_data_container)\n",
    "\n",
    "\n",
    "\n",
    "# def generate_features(df_ohlcv: pd.DataFrame, \n",
    "#                       atr_period: int = 14, \n",
    "#                       quality_window: int = 252, \n",
    "#                       quality_min_periods: int = 126) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     Generates a comprehensive DataFrame of derived features from raw OHLCV data.\n",
    "\n",
    "#     This function performs all heavy, window-based calculations upfront to be used\n",
    "#     by downstream analysis functions. It calculates:\n",
    "#     1. Technical Indicators: True Range (TR), ATR, and ATRP.\n",
    "#     2. Data Quality Metrics: Rolling stale percentage, median dollar volume, etc.\n",
    "\n",
    "#     Args:\n",
    "#         df_ohlcv: The primary DataFrame with a (Ticker, Date) MultiIndex and\n",
    "#                   columns for 'Adj High', 'Adj Low', 'Adj Close', 'Volume'.\n",
    "#         atr_period: The lookback period for the ATR's Exponential Moving Average.\n",
    "#         quality_window: The rolling window size for data quality metrics.\n",
    "#         quality_min_periods: The minimum number of observations required to have\n",
    "#                              a valid quality metric.\n",
    "\n",
    "#     Returns:\n",
    "#         A new DataFrame with the same (Ticker, Date) MultiIndex containing all\n",
    "#         calculated feature columns.\n",
    "#     \"\"\"\n",
    "#     print(\"--- Starting Feature Generation ---\")\n",
    "    \n",
    "#     # Ensure the DataFrame is sorted for correct window and shift operations\n",
    "#     # FIX: Replaced is_lexsorted() with the current pandas attribute\n",
    "#     if not df_ohlcv.index.is_monotonic_increasing:\n",
    "#         print(\"Sorting index for calculation accuracy...\")\n",
    "#         df_ohlcv = df_ohlcv.sort_index()\n",
    "\n",
    "#     # --- 1. Technical Indicator Calculation (TR, ATR, ATRP) ---\n",
    "#     print(f\"Calculating technical indicators (ATR Period: {atr_period})...\")\n",
    "    \n",
    "#     # Group by ticker to handle each security independently\n",
    "#     grouped = df_ohlcv.groupby(level='Ticker')\n",
    "    \n",
    "#     # Get the previous day's close required for True Range\n",
    "#     prev_close = grouped['Adj Close'].shift(1)\n",
    "    \n",
    "#     # Calculate the three components of True Range\n",
    "#     high_low = df_ohlcv['Adj High'] - df_ohlcv['Adj Low']\n",
    "#     high_prev_close = abs(df_ohlcv['Adj High'] - prev_close)\n",
    "#     low_prev_close = abs(df_ohlcv['Adj Low'] - prev_close)\n",
    "    \n",
    "#     # Combine the components to get the final TR\n",
    "#     tr = pd.concat([high_low, high_prev_close, low_prev_close], axis=1).max(axis=1, skipna=False)\n",
    "    \n",
    "#     # # Calculate the ATR using an Exponential Moving Average\n",
    "#     # --- FIX IS HERE ---\n",
    "#     # Use .transform() to apply the EWM function. \n",
    "#     # This guarantees the resulting Series has the exact same index as 'tr',\n",
    "#     # preventing the index alignment error during the subsequent division.\n",
    "#     atr = tr.groupby(level='Ticker').transform(\n",
    "#         lambda x: x.ewm(alpha=1/atr_period, adjust=False).mean()\n",
    "#     )\n",
    "\n",
    "#     # --- CHANGE 1: Removed .fillna(0) ---\n",
    "#     # ATRP will now be NaN on the first day, consistent with TR and ATR.\n",
    "#     atrp = (atr / df_ohlcv['Adj Close']).replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "#     indicator_df = pd.DataFrame({\n",
    "#         'TR': tr,\n",
    "#         'ATR': atr,\n",
    "#         'ATRP': atrp\n",
    "#     })\n",
    "    \n",
    "#     # --- 2. Data Quality Metric Calculation ---\n",
    "#     print(f\"Calculating data quality metrics (Window: {quality_window} days)...\")\n",
    "    \n",
    "#     # Create intermediate flags needed for quality calculations\n",
    "#     is_stale = np.where((df_ohlcv['Volume'] == 0) | (df_ohlcv['Adj High'] == df_ohlcv['Adj Low']), 1, 0)\n",
    "#     dollar_volume = df_ohlcv['Adj Close'] * df_ohlcv['Volume']\n",
    "#     has_same_volume = (grouped['Volume'].diff() == 0).astype(int)\n",
    "    \n",
    "#     # Combine flags into a temporary DataFrame for rolling calculations\n",
    "#     quality_temp_df = pd.DataFrame({\n",
    "#         'IsStale': is_stale,\n",
    "#         'DollarVolume': dollar_volume,\n",
    "#         'HasSameVolume': has_same_volume\n",
    "#     }, index=df_ohlcv.index) # Explicitly set index to be safe\n",
    "    \n",
    "#     # Perform the rolling calculations on the grouped data\n",
    "#     # --- FIX IS HERE ---\n",
    "#     # We switch to the older, more compatible dictionary-based aggregation method.\n",
    "#     # This syntax is understood by nearly all versions of pandas.\n",
    "#     rolling_result = quality_temp_df.groupby(level='Ticker').rolling(\n",
    "#         window=quality_window,\n",
    "#         min_periods=quality_min_periods\n",
    "#     ).agg({\n",
    "#         'IsStale': 'mean',\n",
    "#         'DollarVolume': 'median',\n",
    "#         'HasSameVolume': 'sum'\n",
    "#     })\n",
    "    \n",
    "#     # The dictionary syntax produces columns with the original names ('IsStale', etc.).\n",
    "#     # We now explicitly rename them to our desired final names.\n",
    "#     rolling_result = rolling_result.rename(columns={\n",
    "#         'IsStale': 'RollingStalePct',\n",
    "#         'DollarVolume': 'RollMedDollarVol', # <-- RENAMED HERE\n",
    "#         'HasSameVolume': 'RollingSameVolCount'\n",
    "#     })\n",
    "\n",
    "#     # The index after a grouped rolling operation is hierarchical.\n",
    "#     # We remove the outermost 'Ticker' level to restore the original index structure.\n",
    "#     rolling_quality = rolling_result.reset_index(level=0, drop=True)\n",
    "\n",
    "#     # --- 3. Combine All Features ---\n",
    "#     print(\"Combining all feature sets...\")\n",
    "#     features_df = pd.concat([indicator_df, rolling_quality], axis=1)\n",
    "    \n",
    "#     print(\"✅ Feature generation complete.\")\n",
    "#     return features_df\n",
    "\n",
    "\n",
    "# def get_eligible_universe(features_df, filter_date, thresholds):\n",
    "#     \"\"\"Filters the universe of tickers based on quality metrics for a given date.\"\"\"\n",
    "#     filter_date_ts = pd.to_datetime(filter_date)\n",
    "#     # The index is now the comprehensive features_df\n",
    "#     date_index = features_df.index.get_level_values('Date').unique().sort_values()\n",
    "    \n",
    "#     if filter_date_ts < date_index[0]:\n",
    "#         print(f\"Warning: Filter date {filter_date_ts.date()} is before the earliest data point. Returning empty universe.\")\n",
    "#         return []\n",
    "        \n",
    "#     # Find the most recent date with quality data on or before the filter date\n",
    "#     valid_prior_dates = date_index[date_index <= filter_date_ts]\n",
    "#     if valid_prior_dates.empty:\n",
    "#         print(f\"Warning: No available data found on or before {filter_date_ts.date()}. Returning empty universe.\")\n",
    "#         return []\n",
    "        \n",
    "#     actual_date_to_use = valid_prior_dates[-1]\n",
    "#     if actual_date_to_use.date() != filter_date_ts.date():\n",
    "#         print(f\"ℹ️ Info: Filter date {filter_date_ts.date()} not found. Using previous available date {actual_date_to_use.date()}.\")\n",
    "\n",
    "#     metrics_on_date = features_df.xs(actual_date_to_use, level='Date')\n",
    "    \n",
    "#     # Apply filters using the new column names from features_df\n",
    "#     mask = ((metrics_on_date['RollMedDollarVol'] >= thresholds['min_median_dollar_volume']) & # <-- RENAMED\n",
    "#             (metrics_on_date['RollingStalePct'] <= thresholds['max_stale_pct']) &\n",
    "#             (metrics_on_date['RollingSameVolCount'] <= thresholds['max_same_vol_count']))\n",
    "            \n",
    "#     eligible_tickers = metrics_on_date[mask].index.tolist()\n",
    "#     all_tickers = metrics_on_date.index.tolist()\n",
    "#     print(f\"Dynamic Filter ({filter_date_ts.date()}): Kept {len(eligible_tickers)} of {len(all_tickers)} tickers.\")\n",
    "#     return eligible_tickers\n",
    "\n",
    "\n",
    "# def run_walk_forward_step(df_close_full, df_high_full, df_low_full,\n",
    "#                           master_trading_days,\n",
    "#                           start_date, calc_period, fwd_period,\n",
    "#                           metric, rank_start, rank_end, benchmark_ticker,\n",
    "#                           features_df,\n",
    "#                           debug=False):\n",
    "#     \"\"\"\n",
    "#     Runs a single step of the walk-forward analysis with a strict, pre-emptive\n",
    "#     check to ensure the full period is available.\n",
    "#     \"\"\"\n",
    "#     debug_data = {} if debug else None\n",
    "\n",
    "#     # 1. Determine exact date ranges with a NEW pre-emptive check\n",
    "#     try:\n",
    "#         start_idx = master_trading_days.get_loc(start_date)\n",
    "#     except KeyError:\n",
    "#         return ({'error': f\"Start date {start_date.date()} is not a valid trading day.\"}, None)\n",
    "\n",
    "#     # +++ THIS IS THE NEW PRE-EMPTIVE CHECK LOGIC +++\n",
    "#     # Calculate the desired end index without clamping first.\n",
    "#     desired_viz_end_idx = start_idx + calc_period + fwd_period\n",
    "    \n",
    "#     # Check if the desired end index is out of bounds.\n",
    "#     if desired_viz_end_idx >= len(master_trading_days):\n",
    "#         last_available_date = master_trading_days[-1].date()\n",
    "#         required_days = calc_period + fwd_period\n",
    "#         available_days = len(master_trading_days) - start_idx\n",
    "#         error_msg = (f\"Not enough data for the full requested period. \"\n",
    "#                      f\"Required: {required_days} days, Available: {available_days} days until {last_available_date}.\")\n",
    "#         return ({'error': error_msg}, None)\n",
    "#     # --- END OF NEW CHECK ---\n",
    "\n",
    "#     # If the check passes, we know the full period is available.\n",
    "#     # The 'min' calls are now just a redundant safety measure.\n",
    "#     calc_end_idx = min(start_idx + calc_period, len(master_trading_days) - 1)\n",
    "#     viz_end_idx = min(calc_end_idx + fwd_period, len(master_trading_days) - 1)\n",
    "\n",
    "#     safe_start_date = master_trading_days[start_idx]\n",
    "#     safe_calc_end_date = master_trading_days[calc_end_idx]\n",
    "#     safe_viz_end_date = master_trading_days[viz_end_idx]\n",
    "    \n",
    "#     if safe_start_date >= safe_calc_end_date:\n",
    "#         return ({'error': \"Invalid date range (calc period has zero or negative length).\"}, None)\n",
    "\n",
    "#     # (The rest of the function remains completely unchanged...)\n",
    "#     # 2. Slice data for the calculation period\n",
    "#     calc_close_raw = df_close_full.loc[safe_start_date:safe_calc_end_date]\n",
    "#     calc_close = calc_close_raw.dropna(axis=1, how='all')\n",
    "#     if calc_close.shape[1] == 0 or len(calc_close) < 2:\n",
    "#         return ({'error': \"Not enough data in calc period.\"}, None)\n",
    "\n",
    "#     # 3. Calculate INTERMEDIATE data required for the ranking metrics\n",
    "#     daily_returns = calc_close.bfill().ffill().pct_change()\n",
    "#     valid_tickers = calc_close.columns\n",
    "#     calc_period_index = pd.MultiIndex.from_product([valid_tickers, calc_close.index], names=['Ticker', 'Date'])\n",
    "#     features_in_period = features_df.loc[features_df.index.intersection(calc_period_index)]\n",
    "#     atrp = features_in_period.groupby(level='Ticker')['ATRP'].mean()\n",
    "\n",
    "#     # 4. Calculate all ranking metrics by iterating through the METRIC_REGISTRY\n",
    "#     metric_ingredients = { 'calc_close': calc_close, 'daily_returns': daily_returns, 'atrp': atrp, }\n",
    "#     metric_values = {}\n",
    "#     for name, func in METRIC_REGISTRY.items():\n",
    "#         metric_values[name] = func(metric_ingredients)\n",
    "#     if metric not in metric_values or metric_values[metric].empty:\n",
    "#         return ({'error': f\"Metric '{metric}' could not be calculated or resulted in no valid tickers.\"}, None)\n",
    "\n",
    "#     # 5. Rank tickers and select the portfolio\n",
    "#     sorted_tickers = metric_values[metric].sort_values(ascending=False)\n",
    "#     tickers_to_display = sorted_tickers.index[rank_start-1:rank_end].tolist()\n",
    "#     if not tickers_to_display:\n",
    "#         return ({'error': \"No tickers found for the selected rank.\"}, None)\n",
    "\n",
    "#     # 6. Calculate Portfolio & Benchmark Performance\n",
    "#     normalized_plot_data = df_close_full[tickers_to_display].loc[safe_start_date:safe_viz_end_date]\n",
    "#     normalized_plot_data = normalized_plot_data.div(normalized_plot_data.bfill().iloc[0])\n",
    "#     actual_calc_end_ts = calc_close.index.max()\n",
    "#     portfolio_series = normalized_plot_data.mean(axis=1)\n",
    "#     portfolio_return_series = portfolio_series.pct_change()\n",
    "#     benchmark_price_series = df_close_full.get(benchmark_ticker)\n",
    "#     benchmark_return_series = benchmark_price_series.loc[safe_start_date:safe_viz_end_date].bfill().ffill().pct_change() if benchmark_price_series is not None else pd.Series(dtype='float64')\n",
    "#     try:\n",
    "#         boundary_loc = portfolio_return_series.index.get_loc(actual_calc_end_ts)\n",
    "#         calc_portfolio_returns = portfolio_return_series.iloc[:boundary_loc + 1]\n",
    "#         fwd_portfolio_returns = portfolio_return_series.iloc[boundary_loc + 1:]\n",
    "#         if benchmark_price_series is not None:\n",
    "#             bm_boundary_loc = benchmark_return_series.index.get_loc(actual_calc_end_ts)\n",
    "#             calc_benchmark_returns = benchmark_return_series.iloc[:bm_boundary_loc + 1]\n",
    "#             fwd_benchmark_returns = benchmark_return_series.iloc[bm_boundary_loc + 1:]\n",
    "#         else:\n",
    "#             calc_benchmark_returns, fwd_benchmark_returns = pd.Series(dtype='float64'), pd.Series(dtype='float64')\n",
    "#     except (KeyError, IndexError):\n",
    "#         calc_portfolio_returns = portfolio_return_series.loc[:actual_calc_end_ts]\n",
    "#         fwd_portfolio_returns = portfolio_return_series.loc[actual_calc_end_ts:].iloc[1:]\n",
    "#         if benchmark_price_series is not None:\n",
    "#             calc_benchmark_returns = benchmark_return_series.loc[:actual_calc_end_ts]\n",
    "#             fwd_benchmark_returns = benchmark_return_series.loc[actual_calc_end_ts:].iloc[1:]\n",
    "#         else:\n",
    "#             calc_benchmark_returns, fwd_benchmark_returns = pd.Series(dtype='float64'), pd.Series(dtype='float64')\n",
    "#     full_period_index = pd.MultiIndex.from_product([tickers_to_display, portfolio_return_series.index], names=['Ticker', 'Date'])\n",
    "#     portfolio_atrp_features = features_df.loc[features_df.index.intersection(full_period_index)]\n",
    "#     portfolio_atrp_daily_unstacked = portfolio_atrp_features['ATRP'].unstack(level='Ticker')\n",
    "#     portfolio_atrp_series = portfolio_atrp_daily_unstacked.mean(axis=1)\n",
    "#     if benchmark_ticker in df_close_full.columns:\n",
    "#         benchmark_atrp_series = features_df.xs(benchmark_ticker, level='Ticker')['ATRP'].loc[safe_start_date:safe_viz_end_date]\n",
    "#     else:\n",
    "#         benchmark_atrp_series = pd.Series(dtype='float64')\n",
    "#     calc_portfolio_atrp = portfolio_atrp_series.loc[:actual_calc_end_ts]\n",
    "#     fwd_portfolio_atrp = portfolio_atrp_series.loc[actual_calc_end_ts:].iloc[1:]\n",
    "#     calc_benchmark_atrp = benchmark_atrp_series.loc[:actual_calc_end_ts]\n",
    "#     fwd_benchmark_atrp = benchmark_atrp_series.loc[actual_calc_end_ts:].iloc[1:]\n",
    "#     perf_data = {}\n",
    "#     perf_data['calc_p_gain'] = calculate_gain(portfolio_series.loc[:actual_calc_end_ts])\n",
    "#     perf_data['fwd_p_gain'] = calculate_gain(portfolio_series.loc[actual_calc_end_ts:])\n",
    "#     perf_data['full_p_gain'] = calculate_gain(portfolio_series)\n",
    "#     perf_data['calc_p_sharpe'] = calculate_sharpe(calc_portfolio_returns)\n",
    "#     perf_data['fwd_p_sharpe'] = calculate_sharpe(fwd_portfolio_returns)\n",
    "#     perf_data['full_p_sharpe'] = calculate_sharpe(portfolio_return_series)\n",
    "#     perf_data['calc_b_gain'] = calculate_gain(benchmark_price_series.loc[safe_start_date:actual_calc_end_ts]) if benchmark_price_series is not None else np.nan\n",
    "#     perf_data['fwd_b_gain'] = calculate_gain(benchmark_price_series.loc[actual_calc_end_ts:safe_viz_end_date]) if benchmark_price_series is not None else np.nan\n",
    "#     perf_data['full_b_gain'] = calculate_gain(benchmark_price_series.loc[safe_start_date:safe_viz_end_date]) if benchmark_price_series is not None else np.nan\n",
    "#     perf_data['calc_b_sharpe'] = calculate_sharpe(calc_benchmark_returns)\n",
    "#     perf_data['fwd_b_sharpe'] = calculate_sharpe(fwd_benchmark_returns)\n",
    "#     perf_data['full_b_sharpe'] = calculate_sharpe(benchmark_return_series)\n",
    "#     perf_data['calc_p_sharpe_atr'] = calculate_sharpe_atr(calc_portfolio_returns, calc_portfolio_atrp)\n",
    "#     perf_data['fwd_p_sharpe_atr'] = calculate_sharpe_atr(fwd_portfolio_returns, fwd_portfolio_atrp)\n",
    "#     perf_data['full_p_sharpe_atr'] = calculate_sharpe_atr(portfolio_return_series, portfolio_atrp_series)\n",
    "#     perf_data['calc_b_sharpe_atr'] = calculate_sharpe_atr(calc_benchmark_returns, calc_benchmark_atrp)\n",
    "#     perf_data['fwd_b_sharpe_atr'] = calculate_sharpe_atr(fwd_benchmark_returns, fwd_benchmark_atrp)\n",
    "#     perf_data['full_b_sharpe_atr'] = calculate_sharpe_atr(benchmark_return_series, benchmark_atrp_series)\n",
    "#     if debug:\n",
    "#         df_ranking_base = pd.DataFrame({'MeanDailyReturn': daily_returns.mean(),'StdDevDailyReturn': daily_returns.std(),'MeanATRP': atrp})\n",
    "#         df_metrics = pd.DataFrame(metric_values)\n",
    "#         df_metrics.columns = [f'Metric_{col}' for col in df_metrics.columns]\n",
    "#         df_ranking = df_ranking_base.join(df_metrics, how='left')\n",
    "#         df_ranking.index.name = 'Ticker'\n",
    "#         debug_data['ranking_metrics'] = df_ranking.sort_values(f'Metric_{metric}', ascending=False)\n",
    "#     calc_end_prices = calc_close.ffill().iloc[-1]\n",
    "#     fwd_close_slice = df_close_full.loc[actual_calc_end_ts:safe_viz_end_date]\n",
    "#     viz_end_prices = fwd_close_slice.ffill().iloc[-1] if not fwd_close_slice.empty and len(fwd_close_slice) >= 2 else calc_end_prices\n",
    "#     calc_gains = (calc_end_prices / calc_close.bfill().iloc[0]) - 1\n",
    "#     fwd_gains = (viz_end_prices / calc_end_prices) - 1\n",
    "#     results_df = pd.DataFrame({'Rank': range(rank_start, rank_start + len(tickers_to_display)), 'Metric': metric, 'MetricValue': sorted_tickers.loc[tickers_to_display].values, 'CalcPrice': calc_end_prices.loc[tickers_to_display], 'CalcGain': calc_gains.loc[tickers_to_display], 'FwdGain': fwd_gains.loc[tickers_to_display]}, index=pd.Index(tickers_to_display, name='Ticker'))\n",
    "#     if benchmark_price_series is not None and benchmark_ticker in calc_close.columns:\n",
    "#         benchmark_df_row = pd.DataFrame({'Rank': np.nan, 'Metric': metric, 'MetricValue': metric_values[metric].get(benchmark_ticker, np.nan), 'CalcPrice': calc_end_prices[benchmark_ticker], 'CalcGain': calc_gains[benchmark_ticker], 'FwdGain': fwd_gains[benchmark_ticker]}, index=pd.Index([f\"{benchmark_ticker} (BM)\"], name='Ticker'))\n",
    "#         results_df = pd.concat([results_df, benchmark_df_row])\n",
    "#     if debug:\n",
    "#         df_trace = normalized_plot_data.copy()\n",
    "#         df_trace.columns = [f'Norm_Price_{c}' for c in df_trace.columns]\n",
    "#         df_trace['Norm_Price_Portfolio'] = portfolio_series\n",
    "#         if benchmark_price_series is not None and not benchmark_price_series.loc[safe_start_date:safe_viz_end_date].dropna().empty:\n",
    "#             norm_bm = benchmark_price_series.loc[safe_start_date:safe_viz_end_date] / benchmark_price_series.loc[safe_start_date:].bfill().iloc[0]\n",
    "#             df_trace[f'Norm_Price_Benchmark_{benchmark_ticker}'] = norm_bm\n",
    "#         for col in df_trace.columns:\n",
    "#             if 'Norm_Price' in col:\n",
    "#                 df_trace[col.replace('Norm_Price', 'Return')] = df_trace[col].pct_change()\n",
    "#         debug_data['portfolio_trace'] = df_trace\n",
    "#     final_results = {\n",
    "#         'tickers_to_display': tickers_to_display, 'normalized_plot_data': normalized_plot_data,\n",
    "#         'portfolio_series': portfolio_series, 'benchmark_price_series': benchmark_price_series,\n",
    "#         'performance_data': perf_data, 'results_df': results_df, 'actual_calc_end_ts': actual_calc_end_ts,\n",
    "#         'safe_start_date': safe_start_date, 'safe_viz_end_date': safe_viz_end_date,\n",
    "#         'error': None\n",
    "#     }\n",
    "#     return (final_results, debug_data)\n",
    "\n",
    "\n",
    "# def verify_sharpe_atr_calculation_checked(df_ohlcv, features_df, tickers_to_verify, benchmark_ticker,\n",
    "#                                     start_date, calc_period, fwd_period,\n",
    "#                                     master_calendar_ticker='VOO', debug=False):\n",
    "#     \"\"\"\n",
    "#     Verifies the Sharpe (ATR) calculations for a portfolio and benchmark.\n",
    "\n",
    "#     This function transparently recalculates the key components for Sharpe (ATR)\n",
    "#     and can optionally export the underlying source data for manual inspection.\n",
    "#     \"\"\"\n",
    "#     display(Markdown(f\"## Verification Report for Sharpe (ATR) Calculation\"))\n",
    "#     display(Markdown(f\"**Portfolio Tickers:** `{tickers_to_verify}`\\n**Benchmark Ticker:** `{benchmark_ticker}`\"))\n",
    "\n",
    "#     # --- 1. Determine Exact Period Dates (No changes here) ---\n",
    "#     master_trading_days = df_ohlcv.loc[master_calendar_ticker].index.unique().sort_values()\n",
    "#     start_date_raw = pd.to_datetime(start_date)\n",
    "#     start_idx = master_trading_days.searchsorted(start_date_raw)\n",
    "#     actual_start_date = master_trading_days[start_idx]\n",
    "#     calc_end_idx = min(start_idx + calc_period, len(master_trading_days) - 1)\n",
    "#     fwd_end_idx = min(calc_end_idx + fwd_period, len(master_trading_days) - 1)\n",
    "#     actual_calc_end_date = master_trading_days[calc_end_idx]\n",
    "#     actual_fwd_end_date = master_trading_days[fwd_end_idx]\n",
    "    \n",
    "#     display(Markdown(f\"**Full Period:** `{actual_start_date.date()}` to `{actual_fwd_end_date.date()}`\\n\"\n",
    "#                     f\"**Calc End Date:** `{actual_calc_end_date.date()}`\"))\n",
    "\n",
    "#     # Original debug export block (can be kept or removed)\n",
    "#     if debug:\n",
    "#         # ... (original export code remains here) ...\n",
    "#         pass # Assuming original block is here\n",
    "\n",
    "#     # --- 2. Recreate Portfolio & Benchmark Series from Scratch ---\n",
    "#     df_close_full = df_ohlcv['Adj Close'].unstack(level=0)\n",
    "#     portfolio_prices_raw = df_close_full[tickers_to_verify].loc[actual_start_date:actual_fwd_end_date]\n",
    "#     portfolio_prices_norm = portfolio_prices_raw.div(portfolio_prices_raw.bfill().iloc[0])\n",
    "#     portfolio_value_series = portfolio_prices_norm.mean(axis=1)\n",
    "#     portfolio_return_series = portfolio_value_series.pct_change()\n",
    "#     p_idx = pd.MultiIndex.from_product([tickers_to_verify, portfolio_return_series.index])\n",
    "#     p_atrp_df = features_df.loc[features_df.index.intersection(p_idx)]['ATRP'].unstack(level=0)\n",
    "#     portfolio_atrp_series = p_atrp_df.mean(axis=1)\n",
    "\n",
    "# ###############################    \n",
    "#     # benchmark_return_series = df_close_full[benchmark_ticker].pct_change().loc[actual_start_date:actual_fwd_end_date]\n",
    "\n",
    "#     # 1. First, slice the raw prices for the desired date range.\n",
    "#     benchmark_prices_raw = df_close_full[benchmark_ticker].loc[actual_start_date:actual_fwd_end_date]\n",
    "#     # 2. Then, calculate the percentage change on the sliced data.\n",
    "#     benchmark_return_series = benchmark_prices_raw.pct_change()\n",
    "\n",
    "# ###############################    \n",
    "#     benchmark_atrp_series = features_df.xs(benchmark_ticker, level='Ticker')['ATRP'].loc[actual_start_date:actual_fwd_end_date]\n",
    "\n",
    "\n",
    "#     # +++ NEW: DETAILED RETURN CALCULATION TRACE (PORTFOLIO) +++\n",
    "#     if debug:\n",
    "#         display(Markdown(\"---\"))\n",
    "#         display(Markdown(\"### 🐛 Detailed Portfolio Return Calculation Trace\"))\n",
    "\n",
    "#         # Step 1: Show the raw prices being used\n",
    "#         print(\"\\n[STEP 1] Raw Adjusted Close prices used for calculation.\")\n",
    "#         print(\"Compare these values with your 'Adj Close' columns.\")\n",
    "#         display(portfolio_prices_raw)\n",
    "\n",
    "#         # Step 2: Show the normalization base and the result\n",
    "#         normalization_base = portfolio_prices_raw.bfill().iloc[0]\n",
    "#         print(\"\\n[STEP 2] Normalization base (first row of prices).\")\n",
    "#         print(\"Each column in Step 1 is divided by this corresponding value.\")\n",
    "#         display(pd.DataFrame(normalization_base).T)\n",
    "\n",
    "#         print(\"\\n[STEP 2a] Normalized prices (Result of division).\")\n",
    "#         print(\"Compare these values with your 'N Close' columns.\")\n",
    "#         display(portfolio_prices_norm)\n",
    "\n",
    "#         # Step 3: Show the averaged portfolio value series\n",
    "#         print(\"\\n[STEP 3] Averaged normalized portfolio value series.\")\n",
    "#         print(\"This is the row-by-row average of the table in Step 2a.\")\n",
    "#         print(\"Compare this series with your 'N_portf' column.\")\n",
    "#         display(pd.DataFrame(portfolio_value_series, columns=['N_portf']))\n",
    "\n",
    "#         # Step 4: Show the final portfolio return series\n",
    "#         print(\"\\n[STEP 4] Final portfolio daily return series (pct_change).\")\n",
    "#         print(\"This is the percentage change of the series in Step 3.\")\n",
    "#         print(\"Compare this series with your 'N_portf_rtn' column.\")\n",
    "#         display(pd.DataFrame(portfolio_return_series, columns=['N_portf_rtn']))\n",
    "#         display(Markdown(\"---\"))\n",
    "#     # +++ END OF NEW DEBUG BLOCK +++\n",
    "\n",
    "\n",
    "#     # --- 3. Define a Helper to Print Detailed Calculation Steps ---\n",
    "#     # MODIFIED to include more debug details inside\n",
    "#     def _calculate_and_print_metrics(period_name, returns, atrps):\n",
    "#         display(Markdown(f\"#### {period_name}\"))\n",
    "#         if returns.dropna().empty or atrps.dropna().empty:\n",
    "#             print(\"  - Not enough data to calculate.\")\n",
    "#             return np.nan\n",
    "\n",
    "#         # Standard calculations\n",
    "#         mean_return = returns.mean()\n",
    "#         mean_atrp = atrps.mean()\n",
    "#         sharpe_atr = mean_return / mean_atrp if mean_atrp > 0 else np.nan\n",
    "\n",
    "#         # +++ ADDED: Detailed Mean Calculation Breakdown +++\n",
    "#         if debug:\n",
    "#             valid_returns = returns.dropna()\n",
    "#             num_returns = valid_returns.count()\n",
    "#             sum_returns = valid_returns.sum()\n",
    "#             manual_mean = sum_returns / num_returns if num_returns > 0 else 0\n",
    "#             print(f\"  - (Debug) Number of valid daily returns: {num_returns}\")\n",
    "#             print(f\"  - (Debug) Sum of all daily returns:      {sum_returns:,.8f}\")\n",
    "#             print(f\"  - (Debug) Manually calculated mean:      {manual_mean:,.8f} (Sum / Count)\")\n",
    "#         # +++ END OF ADDED DETAIL +++\n",
    "\n",
    "#         print(f\"  - Mean Daily Return: {mean_return:,.6f}\")\n",
    "#         print(f\"  - Mean Daily ATRP:  {mean_atrp:,.6f}\")\n",
    "#         print(f\"  - Sharpe (ATR) = (Mean Return / Mean ATRP) = {sharpe_atr:,.4f}\")\n",
    "#         return sharpe_atr\n",
    "\n",
    "#     # --- 4. Run Verification for Portfolio (No changes here) ---\n",
    "#     display(Markdown(\"### A. Group Portfolio Verification\"))\n",
    "#     _calculate_and_print_metrics(\"Full Period\", portfolio_return_series, portfolio_atrp_series)\n",
    "#     _calculate_and_print_metrics(\"Calculation Period\", portfolio_return_series.loc[:actual_calc_end_date], portfolio_atrp_series.loc[:actual_calc_end_date])\n",
    "#     _calculate_and_print_metrics(\"Forward Period\", portfolio_return_series.loc[actual_calc_end_date:].iloc[1:], portfolio_atrp_series.loc[actual_calc_end_date:].iloc[1:])\n",
    "\n",
    "#     # --- 5. Run Verification for Benchmark (No changes here) ---\n",
    "#     display(Markdown(f\"### B. Benchmark ({benchmark_ticker}) Verification\"))\n",
    "#     _calculate_and_print_metrics(\"Full Period\", benchmark_return_series, benchmark_atrp_series)\n",
    "#     _calculate_and_print_metrics(\"Calculation Period\", benchmark_return_series.loc[:actual_calc_end_date], benchmark_atrp_series.loc[:actual_calc_end_date])\n",
    "#     _calculate_and_print_metrics(\"Forward Period\", benchmark_return_series.loc[actual_calc_end_date:].iloc[1:], benchmark_atrp_series.loc[actual_calc_end_date:].iloc[1:])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5b18a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_container, debug_container = plot_walk_forward_analyzer(\n",
    "    df_ohlcv=df_ohlcv,\n",
    "    default_start_date='2018-10-03',\n",
    "    default_calc_period=252,\n",
    "    default_fwd_period=63,\n",
    "    default_metric='Sharpe (ATR)',\n",
    "    default_rank_start=1,\n",
    "    default_rank_end=5,\n",
    "    default_benchmark_ticker='VOO',\n",
    "    quality_thresholds={ 'min_median_dollar_volume': 10_000_000, \n",
    "                         'max_stale_pct': 0.05, \n",
    "                         'max_same_vol_count': 1 },\n",
    "    debug=True  # <-- Activate the new mode!\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed49741",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85348e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274d60ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Use pd.IndexSlice for a clean and readable slice\n",
    "# This is the recommended pandas method for complex slicing on a MultiIndex.\n",
    "idx = pd.IndexSlice\n",
    "sliced_features_df = features_df.loc[idx[tickers_to_verify, start_date:end_date], :]\n",
    "\n",
    "# 3. Display the results to verify\n",
    "print(f\"Shape of the original features_df: {features_df.shape}\")\n",
    "print(f\"Shape of the sliced_features_df: {sliced_features_df.shape}\")\n",
    "print(\"\\n--- Displaying the sliced DataFrame ---\")\n",
    "display(sliced_features_df)\n",
    "\n",
    "# You can also verify the boundaries of the slice\n",
    "print(\"\\n--- Verifying the boundaries ---\")\n",
    "print(f\"First Ticker/Date: {sliced_features_df.index.min()}\")\n",
    "print(f\"Last Ticker/Date:  {sliced_features_df.index.max()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dc055a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1. Define the tickers and date range for your slice\n",
    "# tickers_to_slice = ['BIL', 'SATS', 'MINT', 'BOXX']\n",
    "# start_date = '2025-08-03'\n",
    "# end_date = '2025-10-02'\n",
    "\n",
    "# 2. Use pd.IndexSlice for a clean and readable slice\n",
    "# This is the recommended pandas method for complex slicing on a MultiIndex.\n",
    "idx = pd.IndexSlice\n",
    "sliced_df_ohlcv = df_ohlcv.loc[idx[tickers_to_verify, start_date:end_date], :]\n",
    "\n",
    "# 3. Display the results to verify\n",
    "print(f\"Shape of the original df_ohlcv: {df_ohlcv.shape}\")\n",
    "print(f\"Shape of the sliced_df_ohlcv: {sliced_df_ohlcv.shape}\")\n",
    "print(\"\\n--- Displaying the sliced DataFrame ---\")\n",
    "display(sliced_df_ohlcv)\n",
    "\n",
    "# You can also verify the boundaries of the slice\n",
    "print(\"\\n--- Verifying the boundaries ---\")\n",
    "print(f\"First Ticker/Date: {sliced_df_ohlcv.index.min()}\")\n",
    "print(f\"Last Ticker/Date:  {sliced_df_ohlcv.index.max()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67a7eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Use .loc to slice AND explicitly create a .copy()\n",
    "# This tells pandas that we are creating a new DataFrame that we intend to modify.\n",
    "# This is the line that prevents the warning.\n",
    "idx = pd.IndexSlice\n",
    "sliced_df_ohlcv = df_ohlcv.loc[idx[tickers_to_verify, start_date:end_date], :].copy()\n",
    "\n",
    "# 3. Now, add the new column to this independent copy.\n",
    "# This will no longer raise a warning.\n",
    "sliced_df_ohlcv['Daily_Return'] = sliced_df_ohlcv.groupby(level='Ticker')['Adj Close'].pct_change()\n",
    "\n",
    "# 4. Display the results to verify\n",
    "print(\"--- DataFrame with 'Daily_Return' column added (Warning Corrected) ---\")\n",
    "display(sliced_df_ohlcv.head())\n",
    "print(\"...\")\n",
    "display(sliced_df_ohlcv.tail())\n",
    "\n",
    "# print(\"\\n--- Updated DataFrame Info ---\")\n",
    "# sliced_df_ohlcv.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a32a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by the 'Ticker' level of the index, select the 'Daily_Return' column,\n",
    "# and calculate the mean for each group.\n",
    "mean_daily_returns = sliced_df_ohlcv.groupby(level='Ticker')['Daily_Return'].mean()\n",
    "\n",
    "# Display the resulting Series\n",
    "print(\"--- Mean Daily Return per Ticker ---\")\n",
    "display(mean_daily_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea11a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by the 'Ticker' level of the index, select the 'Daily_Return' column,\n",
    "# and calculate the mean for each group.\n",
    "mean_ATRP = sliced_features_df.groupby(level='Ticker')['ATRP'].mean()\n",
    "\n",
    "# Display the resulting Series\n",
    "print(\"--- ATRP per Ticker ---\")\n",
    "display(mean_ATRP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7082bb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(mean_daily_returns / mean_ATRP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e61380",
   "metadata": {},
   "outputs": [],
   "source": [
    "sliced_features_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02ab95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sliced_df_ohlcv.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fe6ad2",
   "metadata": {},
   "source": [
    "| Date | Portfolio Value | Portfolio Daily_Return | Ticker A ATRP | Ticker B ATRP | Portfolio Daily ATRP |\n",
    "| :--- | :--- | :--- | :--- | :--- | :--- |\n",
    "| Day 1 | 1.000 | **`NaN`** | 0.005 | 0.005 | 0.005 |\n",
    "| Day 2 | 1.010 | `+0.010` | 0.006 | 0.006 | 0.006 |\n",
    "| Day 3 | 1.015 | `+0.005` | 0.004 | 0.004 | 0.004 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4b43de",
   "metadata": {},
   "source": [
    "| Function                                     | Is the \"Clamp\" Logic Used? | Why? (Its Role in this Function)                                                                                                                                                                                                                                                                                                                             |\n",
    "| -------------------------------------------- | -------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **`run_walk_forward_step`**                  | **Yes (Primary)**          | This is the **source of truth**. As the core calculation engine, it *must* be robust and handle running out of data at the end of the timeline without crashing. This is the most critical implementation of the clamp.                                                                                                                                       |\n",
    "| **`verify_sharpe_atr_calculation`**          | **Yes (Replication)**      | As you noted, it's here. Its purpose is to **exactly mimic** the behavior of `run_walk_forward_step`. If the verification tool crashed while the main tool gracefully clamped the period, the verification tool would be useless for debugging end-of-timeline scenarios.                                                                                         |\n",
    "| **`verify_group_tickers_walk_forward_calculation`** | **Yes (Replication)**      | Same reason as above. This function verifies portfolio performance and must use the identical date boundary logic as the main engine to produce comparable results for gain and Sharpe ratio.                                                                                                                                                                  |\n",
    "| **`verify_ticker_ranking_metrics`**          | **Yes (Replication)**      | Same reason. It verifies the metrics calculated over the `calc_period`. If the user selects a start date where a full `calc_period` is not available, this tool must clamp the period in the same way `run_walk_forward_step` does to verify the resulting (shorter) calculation.                                                                              |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82b6e73",
   "metadata": {},
   "source": [
    "| Function | How its Behavior Would Change | Pro | Con |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **`run_walk_forward_step` (Core Engine)** | It would now return an error for the final, shorter periods instead of clamping and calculating them. It becomes \"all or nothing.\" | The function's behavior is now stricter and more predictable. The results it returns are never from \"partial\" periods. | The function is less flexible; it can no longer handle end-of-timeline scenarios gracefully on its own. |\n",
    "| **`plot_walk_forward_analyzer` (UI)** | The user experience would be nearly identical, because it *already has* this pre-emptive check. The engine's check would just be a redundant confirmation. | The engine's behavior now perfectly matches the UI's pre-emptive check, removing any potential for divergence. | None, really. This is a positive change from the UI's perspective. |\n",
    "| **`run_full_backtest` & `run_strategy_search` (Automation)** | **This is the most significant impact.** When the backtest reaches the end of the data, `run_walk_forward_step` will return an error. The backtester's loop will then **skip this final, incomplete period entirely.** | The final aggregated equity curve and performance statistics are \"purer.\" Every single point comes from a full-length forward period. | The backtest **throws away the last few days/weeks of data** because they don't form a complete forward period. The resulting equity curve is shorter. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa735d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK Metric_Sharpe (ATR) for Calc. Period has error.\n",
    "- start date 2018-10-03\n",
    "- Calc Period 252\n",
    "- Fwd Period 63\n",
    "- Metric Sharpe ATR\n",
    "- Rank Start 1\n",
    "- Rank End 5\n",
    "-- Analysis Period 2018-10-03 to 2020-01-06 (this is full period, calc + fwd)\n",
    "-- [BIL, MINT, SHV, BNDX, VCSH]\n",
    "-- Metric_Sharpe (ATR) for BIL, MINT, SHV, BNDX matched my own calculation at bottom cell\n",
    "-- Metric Sharpe (ATR) for VCSH is a bit off (code calc 0.194195, my calc 0.196112)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9fb9a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5dc29c92",
   "metadata": {},
   "source": [
    "### Refactor Phase 1: Consolidated and Verified Feature Generation Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a98cae",
   "metadata": {},
   "source": [
    "### Start of Code for Refactoring Phase 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7be9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_path = r'c:\\Users\\ping\\Files_win10\\python\\py311\\stocks\\data\\df_OHLCV_stocks_etfs.parquet'\n",
    "df_ohlcv = pd.read_parquet(data_path, engine='pyarrow')\n",
    "df_ohlcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e7e98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def generate_features(df_ohlcv: pd.DataFrame, \n",
    "                      atr_period: int = 14, \n",
    "                      quality_window: int = 252, \n",
    "                      quality_min_periods: int = 126) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generates a comprehensive DataFrame of derived features from raw OHLCV data.\n",
    "\n",
    "    This function performs all heavy, window-based calculations upfront to be used\n",
    "    by downstream analysis functions. It calculates:\n",
    "    1. Technical Indicators: True Range (TR), ATR, and ATRP.\n",
    "    2. Data Quality Metrics: Rolling stale percentage, median dollar volume, etc.\n",
    "\n",
    "    Args:\n",
    "        df_ohlcv: The primary DataFrame with a (Ticker, Date) MultiIndex and\n",
    "                  columns for 'Adj High', 'Adj Low', 'Adj Close', 'Volume'.\n",
    "        atr_period: The lookback period for the ATR's Exponential Moving Average.\n",
    "        quality_window: The rolling window size for data quality metrics.\n",
    "        quality_min_periods: The minimum number of observations required to have\n",
    "                             a valid quality metric.\n",
    "\n",
    "    Returns:\n",
    "        A new DataFrame with the same (Ticker, Date) MultiIndex containing all\n",
    "        calculated feature columns.\n",
    "    \"\"\"\n",
    "    print(\"--- Starting Feature Generation ---\")\n",
    "    \n",
    "    # Ensure the DataFrame is sorted for correct window and shift operations\n",
    "    # FIX: Replaced is_lexsorted() with the current pandas attribute\n",
    "    if not df_ohlcv.index.is_monotonic_increasing:\n",
    "        print(\"Sorting index for calculation accuracy...\")\n",
    "        df_ohlcv = df_ohlcv.sort_index()\n",
    "\n",
    "    # --- 1. Technical Indicator Calculation (TR, ATR, ATRP) ---\n",
    "    print(f\"Calculating technical indicators (ATR Period: {atr_period})...\")\n",
    "    \n",
    "    # Group by ticker to handle each security independently\n",
    "    grouped = df_ohlcv.groupby(level='Ticker')\n",
    "    \n",
    "    # Get the previous day's close required for True Range\n",
    "    prev_close = grouped['Adj Close'].shift(1)\n",
    "    \n",
    "    # Calculate the three components of True Range\n",
    "    high_low = df_ohlcv['Adj High'] - df_ohlcv['Adj Low']\n",
    "    high_prev_close = abs(df_ohlcv['Adj High'] - prev_close)\n",
    "    low_prev_close = abs(df_ohlcv['Adj Low'] - prev_close)\n",
    "    \n",
    "    # Combine the components to get the final TR\n",
    "    tr = pd.concat([high_low, high_prev_close, low_prev_close], axis=1).max(axis=1, skipna=False)\n",
    "    \n",
    "    # # Calculate the ATR using an Exponential Moving Average\n",
    "    # --- FIX IS HERE ---\n",
    "    # Use .transform() to apply the EWM function. \n",
    "    # This guarantees the resulting Series has the exact same index as 'tr',\n",
    "    # preventing the index alignment error during the subsequent division.\n",
    "    atr = tr.groupby(level='Ticker').transform(\n",
    "        lambda x: x.ewm(alpha=1/atr_period, adjust=False).mean()\n",
    "    )\n",
    "\n",
    "    # --- CHANGE 1: Removed .fillna(0) ---\n",
    "    # ATRP will now be NaN on the first day, consistent with TR and ATR.\n",
    "    atrp = (atr / df_ohlcv['Adj Close']).replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    indicator_df = pd.DataFrame({\n",
    "        'TR': tr,\n",
    "        'ATR': atr,\n",
    "        'ATRP': atrp\n",
    "    })\n",
    "    \n",
    "    # --- 2. Data Quality Metric Calculation ---\n",
    "    print(f\"Calculating data quality metrics (Window: {quality_window} days)...\")\n",
    "    \n",
    "    # Create intermediate flags needed for quality calculations\n",
    "    is_stale = np.where((df_ohlcv['Volume'] == 0) | (df_ohlcv['Adj High'] == df_ohlcv['Adj Low']), 1, 0)\n",
    "    dollar_volume = df_ohlcv['Adj Close'] * df_ohlcv['Volume']\n",
    "    has_same_volume = (grouped['Volume'].diff() == 0).astype(int)\n",
    "    \n",
    "    # Combine flags into a temporary DataFrame for rolling calculations\n",
    "    quality_temp_df = pd.DataFrame({\n",
    "        'IsStale': is_stale,\n",
    "        'DollarVolume': dollar_volume,\n",
    "        'HasSameVolume': has_same_volume\n",
    "    }, index=df_ohlcv.index) # Explicitly set index to be safe\n",
    "    \n",
    "    # Perform the rolling calculations on the grouped data\n",
    "    # --- FIX IS HERE ---\n",
    "    # We switch to the older, more compatible dictionary-based aggregation method.\n",
    "    # This syntax is understood by nearly all versions of pandas.\n",
    "    rolling_result = quality_temp_df.groupby(level='Ticker').rolling(\n",
    "        window=quality_window,\n",
    "        min_periods=quality_min_periods\n",
    "    ).agg({\n",
    "        'IsStale': 'mean',\n",
    "        'DollarVolume': 'median',\n",
    "        'HasSameVolume': 'sum'\n",
    "    })\n",
    "    \n",
    "    # The dictionary syntax produces columns with the original names ('IsStale', etc.).\n",
    "    # We now explicitly rename them to our desired final names.\n",
    "    rolling_result = rolling_result.rename(columns={\n",
    "        'IsStale': 'RollingStalePct',\n",
    "        'DollarVolume': 'RollMedDollarVol', # <-- RENAMED HERE\n",
    "        'HasSameVolume': 'RollingSameVolCount'\n",
    "    })\n",
    "\n",
    "    # The index after a grouped rolling operation is hierarchical.\n",
    "    # We remove the outermost 'Ticker' level to restore the original index structure.\n",
    "    rolling_quality = rolling_result.reset_index(level=0, drop=True)\n",
    "\n",
    "    # --- 3. Combine All Features ---\n",
    "    print(\"Combining all feature sets...\")\n",
    "    features_df = pd.concat([indicator_df, rolling_quality], axis=1)\n",
    "    \n",
    "    print(\"✅ Feature generation complete.\")\n",
    "    return features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e506f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_features_df(features_df: pd.DataFrame, \n",
    "                     df_ohlcv: pd.DataFrame, \n",
    "                     test_ticker: str = 'AAPL',\n",
    "                     spot_check_date: str = '2020-03-20'):\n",
    "    \"\"\"\n",
    "    Runs a suite of tests to verify the correctness of the generated features_df.\n",
    "\n",
    "    Args:\n",
    "        features_df: The generated DataFrame from the generate_features function.\n",
    "        df_ohlcv: The original source OHLCV DataFrame.\n",
    "        test_ticker: A common, liquid ticker to use for specific value checks.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Running Verification Suite for features_df (Test Ticker: {test_ticker}) ---\")\n",
    "    \n",
    "    # --- Test 1: Structural Integrity ---\n",
    "    print(\"\\n[Test 1: Structural Integrity]\")\n",
    "    assert features_df.index.equals(df_ohlcv.index), \"FAIL: Index does not match original df_ohlcv.\"\n",
    "    print(\"  ✅ PASS: Index matches original df_ohlcv.\")\n",
    "    \n",
    "    expected_cols = ['TR', 'ATR', 'ATRP', 'RollingStalePct', 'RollMedDollarVol', 'RollingSameVolCount']\n",
    "    assert all(col in features_df.columns for col in expected_cols), \"FAIL: Missing one or more expected columns.\"\n",
    "    print(\"  ✅ PASS: All expected feature columns are present.\")\n",
    "    print(f\"  - DataFrame Info:\")\n",
    "    features_df.info(verbose=False, memory_usage='deep')\n",
    "\n",
    "\n",
    "    # --- Test 2: ATR Calculation Logic ---\n",
    "    print(\"\\n[Test 2: ATR Logic Verification]\")\n",
    "    ticker_features = features_df.loc[test_ticker]\n",
    "    \n",
    "    # Test 2a: First TR value should be NaN (since prev_close is NaN)\n",
    "    first_tr = ticker_features['TR'].iloc[0]\n",
    "    assert pd.isna(first_tr), f\"FAIL: First TR value for {test_ticker} should be NaN, but got {first_tr}.\"\n",
    "    print(f\"  ✅ PASS: First TR value for {test_ticker} is NaN as expected.\")\n",
    "\n",
    "    # Test 2b: The first valid ATR should equal the first valid TR (EWM cold start behavior)\n",
    "    first_valid_tr_val = ticker_features['TR'].dropna().iloc[0]\n",
    "    first_valid_atr_val = ticker_features['ATR'].dropna().iloc[0]\n",
    "    assert np.isclose(first_valid_tr_val, first_valid_atr_val), \\\n",
    "        f\"FAIL: First valid ATR ({first_valid_atr_val}) should equal first valid TR ({first_valid_tr_val}).\"\n",
    "    print(\"  ✅ PASS: First valid ATR correctly seeded with first valid TR.\")\n",
    "\n",
    "\n",
    "    # --- Test 3: Rolling Quality Metrics Logic ---\n",
    "    print(\"\\n[Test 3: Rolling Quality Metrics Logic Verification]\")\n",
    "    quality_min_periods = 126 # Should match the parameter used in generation\n",
    "    \n",
    "    # Test 3a: Check for leading NaNs\n",
    "    first_valid_quality_idx = ticker_features['RollingStalePct'].first_valid_index()\n",
    "    if first_valid_quality_idx is None:\n",
    "        print(f\"  - INFO: No valid quality metrics found for {test_ticker} (likely too little data). Skipping test.\")\n",
    "    else:\n",
    "        position_of_first_valid = ticker_features.index.get_loc(first_valid_quality_idx)\n",
    "        assert position_of_first_valid == quality_min_periods - 1, \\\n",
    "            f\"FAIL: First valid quality metric should appear at index {quality_min_periods - 1}, but appeared at {position_of_first_valid}.\"\n",
    "        print(f\"  ✅ PASS: Leading NaNs are present for the first {quality_min_periods - 1} periods as expected.\")\n",
    "\n",
    "\n",
    "    # --- Test 4: Spot Check Against Manual Calculation ---\n",
    "    print(\"\\n[Test 4: Spot Check vs. Manual Calculation]\")\n",
    "    # # Choose a specific date for a manual calculation\n",
    "    # spot_check_date = '2020-03-20' # A volatile day for a good test\n",
    "    \n",
    "    # Manual TR Calculation\n",
    "    today_data = df_ohlcv.loc[(test_ticker, spot_check_date)]\n",
    "    yesterday_data = df_ohlcv.loc[(test_ticker, pd.to_datetime(spot_check_date) - pd.Timedelta(days=1))] # simple lookback for test\n",
    "    \n",
    "    manual_h_l = today_data['Adj High'] - today_data['Adj Low']\n",
    "    manual_h_pc = abs(today_data['Adj High'] - yesterday_data['Adj Close'])\n",
    "    manual_l_pc = abs(today_data['Adj Low'] - yesterday_data['Adj Close'])\n",
    "    manual_tr = max(manual_h_l, manual_h_pc, manual_l_pc)\n",
    "    \n",
    "    code_tr = ticker_features.loc[spot_check_date]['TR']\n",
    "    \n",
    "    assert np.isclose(manual_tr, code_tr), f\"FAIL: Manual TR ({manual_tr:.4f}) does not match code TR ({code_tr:.4f}) on {spot_check_date}.\"\n",
    "    print(f\"  ✅ PASS: Manually calculated TR on {spot_check_date} matches code's TR.\")\n",
    "    \n",
    "    print(\"\\n--- ✅ All Verification Tests Passed ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ebde23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This might take a minute or two depending on your data size and CPU\n",
    "features_df = generate_features(df_ohlcv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abc1730",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a64033",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features_df(features_df, df_ohlcv, test_ticker='VCSH', spot_check_date='2018-10-03') \n",
    "# You can change the test_ticker to another well-known stock like 'MSFT' or 'GOOG'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326ad84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_features = features_df.loc['VCSH']\n",
    "ticker_features.loc['2018-10-03':'2019-10-04']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c6fe8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def export_ticker_data(ticker_to_export: str, \n",
    "                         df_ohlcv: pd.DataFrame, \n",
    "                         features_df: pd.DataFrame, \n",
    "                         output_dir: str = 'export_csv'):\n",
    "    \"\"\"\n",
    "    Exports the raw OHLCV data and the corresponding calculated features for a \n",
    "    single ticker to two separate CSV files.\n",
    "\n",
    "    This function is designed for easy manual verification of data and calculations.\n",
    "    It will create the output directory if it does not exist.\n",
    "\n",
    "    Args:\n",
    "        ticker_to_export: The ticker symbol to export (e.g., 'AAPL').\n",
    "        df_ohlcv: The main DataFrame containing the raw OHLCV data with a \n",
    "                  (Ticker, Date) MultiIndex.\n",
    "        features_df: The DataFrame containing the calculated features with a \n",
    "                     (Ticker, Date) MultiIndex.\n",
    "        output_dir: The directory where the CSV files will be saved. \n",
    "                    Defaults to 'export_csv'.\n",
    "    \"\"\"\n",
    "    print(f\"--- Attempting to export data for ticker: {ticker_to_export} ---\")\n",
    "    \n",
    "    # --- 1. Ensure the output directory exists ---\n",
    "    try:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        print(f\"Output directory '{output_dir}' is ready.\")\n",
    "    except OSError as e:\n",
    "        print(f\"Error: Could not create directory '{output_dir}'. Reason: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- 2. Isolate the data for the specified ticker ---\n",
    "    try:\n",
    "        # Use .loc to select all rows for the given ticker from the MultiIndex\n",
    "        ticker_ohlcv = df_ohlcv.loc[ticker_to_export]\n",
    "        ticker_features = features_df.loc[ticker_to_export]\n",
    "        \n",
    "        if ticker_ohlcv.empty:\n",
    "            print(f\"Warning: No OHLCV data found for ticker '{ticker_to_export}'. Cannot export.\")\n",
    "            return\n",
    "            \n",
    "        print(f\"Found {len(ticker_ohlcv)} rows of data for '{ticker_to_export}'.\")\n",
    "        \n",
    "    except KeyError:\n",
    "        print(f\"Error: Ticker '{ticker_to_export}' not found in one or both of the DataFrames. Please check the symbol.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while accessing data: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- 3. Construct file paths and export to CSV ---\n",
    "    try:\n",
    "        # Define the full path for each output file\n",
    "        ohlcv_filename = f\"{ticker_to_export}_ohlcv.csv\"\n",
    "        features_filename = f\"{ticker_to_export}_features.csv\"\n",
    "        \n",
    "        ohlcv_filepath = os.path.join(output_dir, ohlcv_filename)\n",
    "        features_filepath = os.path.join(output_dir, features_filename)\n",
    "        \n",
    "        # Export the DataFrames to CSV. The index (Date) will be included.\n",
    "        ticker_ohlcv.to_csv(ohlcv_filepath)\n",
    "        ticker_features.to_csv(features_filepath)\n",
    "        \n",
    "        print(\"\\n✅ Export successful!\")\n",
    "        print(f\"   - Raw OHLCV data saved to: {ohlcv_filepath}\")\n",
    "        print(f\"   - Calculated features saved to: {features_filepath}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: Failed to write data to CSV files. Reason: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323eefda",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_ticker_data('VCSH', df_ohlcv, features_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37cf41f",
   "metadata": {},
   "source": [
    "### Part 1: Function to Create Synthetic Ticker Data\n",
    "\n",
    "This function creates a DataFrame for a single ticker (`SYNTH`) with specific, predictable patterns for stale days, dollar volume, and repeated volumes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51753192",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os # Make sure os is imported for the export function later\n",
    "\n",
    "def create_synthetic_ticker_data(\n",
    "    ticker_name: str = 'SYNTH', \n",
    "    num_days: int = 50,\n",
    "    num_zero_volume_days: int = 5,\n",
    "    num_flat_price_days: int = 3\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Creates a synthetic OHLCV DataFrame with predictable patterns and randomly injected\n",
    "    stale data conditions for robust testing.\n",
    "\n",
    "    Args:\n",
    "        ticker_name: The name for the synthetic ticker.\n",
    "        num_days: The total number of days for the ticker's history.\n",
    "        num_zero_volume_days: The number of random days to set Volume to 0.\n",
    "        num_flat_price_days: The number of random days to set High == Low.\n",
    "\n",
    "    Returns:\n",
    "        A pandas DataFrame with a (Ticker, Date) MultiIndex.\n",
    "    \"\"\"\n",
    "    print(f\"--- Creating synthetic data for '{ticker_name}' with {num_days} days ---\")\n",
    "    \n",
    "    # 1. Create a base DataFrame with \"normal\" data\n",
    "    dates = pd.to_datetime(pd.date_range(start='2023-01-01', periods=num_days, freq='B'))\n",
    "    data = {\n",
    "        'Adj Open': 100.0, 'Adj High': 102.0, 'Adj Low': 98.0,\n",
    "        'Adj Close': 100.0, 'Volume': 1_000_000\n",
    "    }\n",
    "    df = pd.DataFrame(data, index=dates)\n",
    "    df['Adj Close'] = df['Adj Close'] + np.random.randn(num_days) * 0.5 # Add some noise\n",
    "\n",
    "    # 2. Define a \"protected\" window for specific verification tests.\n",
    "    # The `verify_synthetic_ticker_features` function depends on this exact window.\n",
    "    # We will not inject random stale days here.\n",
    "    protected_start_idx, protected_end_idx = 10, 20\n",
    "    \n",
    "    # 3. Inject random \"stale\" days OUTSIDE the protected window\n",
    "    available_indices = df.index.drop(df.index[protected_start_idx:protected_end_idx])\n",
    "    \n",
    "    # Inject zero-volume days\n",
    "    if num_zero_volume_days > 0:\n",
    "        if len(available_indices) < num_zero_volume_days:\n",
    "            raise ValueError(\"Not enough available days to inject zero-volume days.\")\n",
    "        zero_vol_dates = np.random.choice(available_indices, num_zero_volume_days, replace=False)\n",
    "        df.loc[zero_vol_dates, 'Volume'] = 0\n",
    "        print(f\"  - Injected {num_zero_volume_days} random zero-volume 'stale' days.\")\n",
    "        # Update available indices to avoid overlap\n",
    "        available_indices = available_indices.drop(zero_vol_dates)\n",
    "\n",
    "    # Inject flat-price days (High == Low)\n",
    "    if num_flat_price_days > 0:\n",
    "        if len(available_indices) < num_flat_price_days:\n",
    "            raise ValueError(\"Not enough available days to inject flat-price days.\")\n",
    "        flat_price_dates = np.random.choice(available_indices, num_flat_price_days, replace=False)\n",
    "        # Set High and Low to be the same as the Close price for that day\n",
    "        df.loc[flat_price_dates, 'Adj High'] = df.loc[flat_price_dates, 'Adj Close']\n",
    "        df.loc[flat_price_dates, 'Adj Low'] = df.loc[flat_price_dates, 'Adj Close']\n",
    "        print(f\"  - Injected {num_flat_price_days} random flat-price 'stale' days.\")\n",
    "\n",
    "    # 4. Inject the specific, hand-crafted patterns inside the protected window for verification\n",
    "    print(\"  - Injecting specific patterns for programmatic verification...\")\n",
    "    # Pattern for RollingStalePct: 2 stale days in 10 (20%)\n",
    "    df.iloc[10, df.columns.get_loc('Volume')] = 0  # Stale day (zero volume)\n",
    "    df.iloc[11, df.columns.get_loc('Adj High')] = 99.0 # Stale day (High == Low)\n",
    "    df.iloc[11, df.columns.get_loc('Adj Low')] = 99.0\n",
    "    \n",
    "    # Pattern for RollingMedianVolume\n",
    "    for i in range(10):\n",
    "        df.iloc[10 + i, df.columns.get_loc('Adj Close')] = 100.0 # Standardize price for easy median calc\n",
    "        df.iloc[10 + i, df.columns.get_loc('Volume')] = (i + 1) * 10000\n",
    "\n",
    "    # Pattern for RollingSameVolCount\n",
    "    df.iloc[15, df.columns.get_loc('Volume')] = 77777\n",
    "    df.iloc[16, df.columns.get_loc('Volume')] = 77777\n",
    "    df.iloc[17, df.columns.get_loc('Volume')] = 77777\n",
    "    \n",
    "    # 5. Set the MultiIndex\n",
    "    df['Ticker'] = ticker_name\n",
    "    df = df.set_index(['Ticker', df.index])\n",
    "    df.index.names = ['Ticker', 'Date']\n",
    "    \n",
    "    print(\"✅ Synthetic data created successfully.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d28048",
   "metadata": {},
   "source": [
    "### Part 2: Code to Test the Synthetic Data\n",
    "\n",
    "This new verification function is specifically designed to check the results from our synthetic data. It knows exactly what values to expect on a specific date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d19373",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_synthetic_ticker_features(features_df: pd.DataFrame, \n",
    "                                       ticker_name: str = 'SYNTH',\n",
    "                                       quality_window: int = 10):\n",
    "    \"\"\"\n",
    "    Verifies the quality metric calculations on the features_df generated from\n",
    "    the synthetic ticker data.\n",
    "\n",
    "    Args:\n",
    "        features_df: The DataFrame of calculated features.\n",
    "        ticker_name: The name of the synthetic ticker.\n",
    "        quality_window: The rolling window used, which must match the window\n",
    "                        of the synthetic data pattern.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Running Verification on Synthetic Ticker '{ticker_name}' ---\")\n",
    "    \n",
    "    # --- Expected values based on our synthetic data design ---\n",
    "    EXPECTED_STALE_PCT = 0.20  # 2 stale days out of 10\n",
    "    EXPECTED_MEDIAN_DOLLAR_VOL = 5_500_000.0 # median of (10k..100k) * price of 100\n",
    "    EXPECTED_SAME_VOL_COUNT = 2.0 # Three consecutive days gives two 'diff() == 0' events\n",
    "\n",
    "    try:\n",
    "        # Isolate the features for our synthetic ticker\n",
    "        ticker_features = features_df.loc[ticker_name]\n",
    "        \n",
    "        # The first valid calculation will be on the last day of our 10-day window.\n",
    "        # The window starts at index 10 and has a length of 10, so it ends at index 19.\n",
    "        verification_date = ticker_features.index[19]\n",
    "        \n",
    "        print(f\"Verifying calculations on date: {verification_date.date()}\")\n",
    "        \n",
    "        # Get the calculated values from the DataFrame\n",
    "        calculated_values = ticker_features.loc[verification_date]\n",
    "        stale_pct = calculated_values['RollingStalePct']\n",
    "        # --- CHANGE 2 (continued): Accessing the renamed column ---\n",
    "        median_vol = calculated_values['RollMedDollarVol'] \n",
    "        same_vol_count = calculated_values['RollingSameVolCount']\n",
    "        \n",
    "        # --- Perform Assertions ---\n",
    "        print(\"\\n[Test 1: RollingStalePct]\")\n",
    "        assert np.isclose(stale_pct, EXPECTED_STALE_PCT), f\"FAIL: Expected {EXPECTED_STALE_PCT}, Got {stale_pct}\"\n",
    "        print(f\"  ✅ PASS: Expected {EXPECTED_STALE_PCT}, Got {stale_pct}\")\n",
    "\n",
    "        print(\"\\n[Test 2: RollMedDollarVol]\")\n",
    "        assert np.isclose(median_vol, EXPECTED_MEDIAN_DOLLAR_VOL), f\"FAIL: Expected {EXPECTED_MEDIAN_DOLLAR_VOL}, Got {median_vol}\"\n",
    "        print(f\"  ✅ PASS: Expected {EXPECTED_MEDIAN_DOLLAR_VOL}, Got {median_vol}\")\n",
    "\n",
    "        print(\"\\n[Test 3: RollingSameVolCount]\")\n",
    "        assert np.isclose(same_vol_count, EXPECTED_SAME_VOL_COUNT), f\"FAIL: Expected {EXPECTED_SAME_VOL_COUNT}, Got {same_vol_count}\"\n",
    "        print(f\"  ✅ PASS: Expected {EXPECTED_SAME_VOL_COUNT}, Got {same_vol_count}\")\n",
    "\n",
    "        print(\"\\n--- ✅ All Synthetic Data Verification Tests Passed ---\")\n",
    "\n",
    "    except KeyError:\n",
    "        print(f\"FAIL: Ticker '{ticker_name}' not found in features_df.\")\n",
    "    except IndexError:\n",
    "        print(\"FAIL: Not enough data in features_df to run verification. Check num_days.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during verification: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47506201",
   "metadata": {},
   "source": [
    "## Part 3: Main Script to Run Everything\n",
    "\n",
    "This block of code ties it all together. You will need your `generate_features` and `export_ticker_data` functions available in the same environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716eff31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Main Execution Block ---\n",
    "\n",
    "# Make sure you have 'generate_features', 'verify_synthetic_ticker_features', \n",
    "# and 'export_ticker_data' functions available in your environment.\n",
    "\n",
    "# --- Step 1: Define Parameters ---\n",
    "SYNTHETIC_TICKER_NAME = 'SYNTH_STALE_TEST'\n",
    "QUALITY_WINDOW_FOR_TEST = 20\n",
    "MIN_PERIODS_FOR_TEST = 10 \n",
    "\n",
    "# --- Step 2: Create Synthetic Data with More Stale Days ---\n",
    "# We'll create 8 zero-volume days and 4 flat-price days.\n",
    "df_ohlcv_synth = create_synthetic_ticker_data(\n",
    "    ticker_name=SYNTHETIC_TICKER_NAME, \n",
    "    num_days=100, # Use more days to have space for random stale days\n",
    "    num_zero_volume_days=8,\n",
    "    num_flat_price_days=4\n",
    ")\n",
    "\n",
    "# --- Step 3: Run Feature Generation ---\n",
    "features_df_synth = generate_features(\n",
    "    df_ohlcv_synth,\n",
    "    quality_window=QUALITY_WINDOW_FOR_TEST,\n",
    "    quality_min_periods=MIN_PERIODS_FOR_TEST\n",
    ")\n",
    "\n",
    "# --- Step 4: Verify the Core Logic (this will still pass) ---\n",
    "# The verification function checks the specific 10-day window, which we preserved.\n",
    "verify_synthetic_ticker_features(\n",
    "    features_df_synth,\n",
    "    ticker_name=SYNTHETIC_TICKER_NAME,\n",
    "    quality_window=QUALITY_WINDOW_FOR_TEST\n",
    ")\n",
    "\n",
    "# --- Step 5: Export for Manual Inspection ---\n",
    "# When you open the CSVs, you will now see the additional random stale days\n",
    "# you created, allowing you to manually check the rolling calculations anywhere.\n",
    "print(\"\\n--- Exporting Enhanced Synthetic Data for Manual Review ---\")\n",
    "export_ticker_data(\n",
    "    ticker_to_export=SYNTHETIC_TICKER_NAME,\n",
    "    df_ohlcv=df_ohlcv_synth,\n",
    "    features_df=features_df_synth\n",
    ")\n",
    "\n",
    "# --- Step 6: Generate_features Parameters ---\n",
    "print(\"\\n--- Generate_features Parameters ---\")\n",
    "print(f'quality_window: {QUALITY_WINDOW_FOR_TEST}')\n",
    "print(f'quality_min_periods: {MIN_PERIODS_FOR_TEST}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec2dcbb",
   "metadata": {},
   "source": [
    "### End of Code for Refactoring Phase 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a42aca",
   "metadata": {},
   "source": [
    "## Project Hand-off: Walk-Forward Backtesting Bot\n",
    "\n",
    "This package contains the final version of the code, designed to be resilient, portable, and easy to use in both local (VS Code) and cloud (Google Colab) environments.\n",
    "\n",
    "### 1. Summary of Key Features\n",
    "\n",
    "The system you have built now includes:\n",
    "\n",
    "*   **Environment-Agnostic Operation:** A \"magic switch\" automatically detects whether the code is running locally or in Colab and adjusts all file paths accordingly.\n",
    "*   **Resumable Backtests (Checkpointing):** Long-running parameter searches are now resilient. If the process is interrupted, it can be restarted and will automatically skip completed work, picking up where it left off.\n",
    "*   **Granular, Trading-Day-Based Logic:** The backtester operates on precise integer counts of trading days, allowing for non-calendar-based periods (e.g., 10-day holds) and eliminating approximation errors.\n",
    "*   **Multi-Period Testing:** The automation script is capable of testing a list of different holding/rebalancing periods in a single run.\n",
    "*   **Modular & Verifiable Core Engine:** The core calculation logic (`run_walk_forward_step`) is a pure, self-contained function, making it easy to test and verify independently.\n",
    "*   **Dynamic Data Quality Filtering:** Before each ranking period, the universe of stocks is filtered based on rolling liquidity and data quality metrics, ensuring the strategy is only applied to tradable assets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5182cc74",
   "metadata": {},
   "source": [
    "### 2. Required Project Structure\n",
    "\n",
    "For the environment switch to work seamlessly, your project should be organized in the following way, both on your local machine and in Google Drive.\n",
    "\n",
    "```\n",
    "my_trading_project/\n",
    "│\n",
    "├── 📜 bot.ipynb                 # <-- This is the main notebook file\n",
    "│\n",
    "├── 📁 data/\n",
    "│   └── 📊 df_OHLCV_stocks_etfs.parquet # <-- Your input data file goes here\n",
    "│\n",
    "└── 📁 export_csv/                 # <-- Folder for local results (created automatically)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76715d2a",
   "metadata": {},
   "source": [
    "### 3. Final, Complete Code\n",
    "\n",
    "This is the entire code for your notebook, consolidated into logical cells.\n",
    "\n",
    "#### **CELL 1: ENVIRONMENT SETUP & CONFIGURATION**\n",
    "*This cell is the \"brain\" of the system. It detects the environment and configures all paths. It's the only cell you might need to edit if your file paths change.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8a246d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# --- CELL 1: ENVIRONMENT SETUP & CONFIGURATION (IMPROVED) ---\n",
    "# This cell automatically detects the environment (local VS Code or Google Colab)\n",
    "# and configures paths and settings accordingly. It also creates directories.\n",
    "# ==============================================================================\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# 1. AUTOMATIC ENVIRONMENT DETECTION\n",
    "try:\n",
    "    import google.colab\n",
    "    IS_COLAB = True\n",
    "    print(\"✅ Environment: Google Colab detected.\")\n",
    "except ImportError:\n",
    "    IS_COLAB = False\n",
    "    print(\"✅ Environment: Local (VS Code) detected.\")\n",
    "\n",
    "# 2. ENVIRONMENT-SPECIFIC CONFIGURATION\n",
    "if IS_COLAB:\n",
    "    # --- Colab Settings ---\n",
    "    from google.colab import drive, output\n",
    "    drive.mount('/content/drive')\n",
    "    output.enable_custom_widget_manager()\n",
    "    \n",
    "    # IMPORTANT: This should be the path to your main project folder in Google Drive\n",
    "    DRIVE_ROOT = '/content/drive/MyDrive/my_trading_project'\n",
    "    \n",
    "    env_config = {\n",
    "        'data_path': os.path.join(DRIVE_ROOT, 'data', 'df_OHLCV_stocks_etfs.parquet'),\n",
    "        'output_dir': os.path.join(DRIVE_ROOT, 'results') # Colab results go in a 'results' folder\n",
    "    }\n",
    "    \n",
    "else:\n",
    "    # --- Local Settings ---\n",
    "    # IMPORTANT: Update this path to your local data file if it's different\n",
    "    env_config = {\n",
    "        'data_path': r'c:\\Users\\ping\\Files_win10\\python\\py311\\stocks\\data\\df_OHLCV_stocks_etfs.parquet',\n",
    "        'output_dir': os.path.join('.', 'export_csv') # Local results go in 'export_csv'\n",
    "    }\n",
    "\n",
    "# 3. CREATE ALL NECESSARY DIRECTORIES\n",
    "data_parent_dir = os.path.dirname(env_config['data_path'])\n",
    "os.makedirs(data_parent_dir, exist_ok=True)\n",
    "os.makedirs(env_config['output_dir'], exist_ok=True)\n",
    "\n",
    "print(f\"\\nData will be loaded from: {env_config['data_path']}\")\n",
    "print(f\"Output files will be saved to: {env_config['output_dir']}\")\n",
    "\n",
    "# 4. DEFINE THE FULL PATH FOR THE RESULTS FILE\n",
    "env_config['results_path'] = os.path.join(env_config['output_dir'], 'dev_strategy_search_results.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38189318",
   "metadata": {},
   "source": [
    "#### **CELL 2: GOLDEN COPY - CORE ENGINE & TOOLS**\n",
    "*This cell contains all the stable, tested functions that form the core of your backtester.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96383d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# GOLDEN COPY - COMPLETE PROJECT CODE (All Fixes Included)\n",
    "# Version: Added Refactoring Phase 1 code  \n",
    "# Date: 2025-10-14\n",
    "# ==============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import ipywidgets as widgets\n",
    "import time\n",
    "import pprint\n",
    "import os # Make sure os is imported for the export function later\n",
    "import re\n",
    "\n",
    "from datetime import datetime, date\n",
    "from IPython.display import display, Markdown\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "pd.set_option('display.max_rows', 30)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.width', 3000)\n",
    "\n",
    "\n",
    "# --- REFACTORING PHASE 1 CODE: Feature Generation Engine ---\n",
    "\n",
    "def generate_features(df_ohlcv: pd.DataFrame, \n",
    "                      atr_period: int = 14, \n",
    "                      quality_window: int = 252, \n",
    "                      quality_min_periods: int = 126) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generates a comprehensive DataFrame of derived features from raw OHLCV data.\n",
    "\n",
    "    This function performs all heavy, window-based calculations upfront to be used\n",
    "    by downstream analysis functions. It calculates:\n",
    "    1. Technical Indicators: True Range (TR), ATR, and ATRP.\n",
    "    2. Data Quality Metrics: Rolling stale percentage, median dollar volume, etc.\n",
    "\n",
    "    Args:\n",
    "        df_ohlcv: The primary DataFrame with a (Ticker, Date) MultiIndex and\n",
    "                  columns for 'Adj High', 'Adj Low', 'Adj Close', 'Volume'.\n",
    "        atr_period: The lookback period for the ATR's Exponential Moving Average.\n",
    "        quality_window: The rolling window size for data quality metrics.\n",
    "        quality_min_periods: The minimum number of observations required to have\n",
    "                             a valid quality metric.\n",
    "\n",
    "    Returns:\n",
    "        A new DataFrame with the same (Ticker, Date) MultiIndex containing all\n",
    "        calculated feature columns.\n",
    "    \"\"\"\n",
    "    print(\"--- Starting Feature Generation ---\")\n",
    "    \n",
    "    # Ensure the DataFrame is sorted for correct window and shift operations\n",
    "    # FIX: Replaced is_lexsorted() with the current pandas attribute\n",
    "    if not df_ohlcv.index.is_monotonic_increasing:\n",
    "        print(\"Sorting index for calculation accuracy...\")\n",
    "        df_ohlcv = df_ohlcv.sort_index()\n",
    "\n",
    "    # --- 1. Technical Indicator Calculation (TR, ATR, ATRP) ---\n",
    "    print(f\"Calculating technical indicators (ATR Period: {atr_period})...\")\n",
    "    \n",
    "    # Group by ticker to handle each security independently\n",
    "    grouped = df_ohlcv.groupby(level='Ticker')\n",
    "    \n",
    "    # Get the previous day's close required for True Range\n",
    "    prev_close = grouped['Adj Close'].shift(1)\n",
    "    \n",
    "    # Calculate the three components of True Range\n",
    "    high_low = df_ohlcv['Adj High'] - df_ohlcv['Adj Low']\n",
    "    high_prev_close = abs(df_ohlcv['Adj High'] - prev_close)\n",
    "    low_prev_close = abs(df_ohlcv['Adj Low'] - prev_close)\n",
    "    \n",
    "    # Combine the components to get the final TR\n",
    "    tr = pd.concat([high_low, high_prev_close, low_prev_close], axis=1).max(axis=1, skipna=False)\n",
    "    \n",
    "    # # Calculate the ATR using an Exponential Moving Average\n",
    "    # --- FIX IS HERE ---\n",
    "    # Use .transform() to apply the EWM function. \n",
    "    # This guarantees the resulting Series has the exact same index as 'tr',\n",
    "    # preventing the index alignment error during the subsequent division.\n",
    "    atr = tr.groupby(level='Ticker').transform(\n",
    "        lambda x: x.ewm(alpha=1/atr_period, adjust=False).mean()\n",
    "    )\n",
    "\n",
    "    # --- CHANGE 1: Removed .fillna(0) ---\n",
    "    # ATRP will now be NaN on the first day, consistent with TR and ATR.\n",
    "    atrp = (atr / df_ohlcv['Adj Close']).replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    indicator_df = pd.DataFrame({\n",
    "        'TR': tr,\n",
    "        'ATR': atr,\n",
    "        'ATRP': atrp\n",
    "    })\n",
    "    \n",
    "    # --- 2. Data Quality Metric Calculation ---\n",
    "    print(f\"Calculating data quality metrics (Window: {quality_window} days)...\")\n",
    "    \n",
    "    # Create intermediate flags needed for quality calculations\n",
    "    is_stale = np.where((df_ohlcv['Volume'] == 0) | (df_ohlcv['Adj High'] == df_ohlcv['Adj Low']), 1, 0)\n",
    "    dollar_volume = df_ohlcv['Adj Close'] * df_ohlcv['Volume']\n",
    "    has_same_volume = (grouped['Volume'].diff() == 0).astype(int)\n",
    "    \n",
    "    # Combine flags into a temporary DataFrame for rolling calculations\n",
    "    quality_temp_df = pd.DataFrame({\n",
    "        'IsStale': is_stale,\n",
    "        'DollarVolume': dollar_volume,\n",
    "        'HasSameVolume': has_same_volume\n",
    "    }, index=df_ohlcv.index) # Explicitly set index to be safe\n",
    "    \n",
    "    # Perform the rolling calculations on the grouped data\n",
    "    # --- FIX IS HERE ---\n",
    "    # We switch to the older, more compatible dictionary-based aggregation method.\n",
    "    # This syntax is understood by nearly all versions of pandas.\n",
    "    rolling_result = quality_temp_df.groupby(level='Ticker').rolling(\n",
    "        window=quality_window,\n",
    "        min_periods=quality_min_periods\n",
    "    ).agg({\n",
    "        'IsStale': 'mean',\n",
    "        'DollarVolume': 'median',\n",
    "        'HasSameVolume': 'sum'\n",
    "    })\n",
    "    \n",
    "    # The dictionary syntax produces columns with the original names ('IsStale', etc.).\n",
    "    # We now explicitly rename them to our desired final names.\n",
    "    rolling_result = rolling_result.rename(columns={\n",
    "        'IsStale': 'RollingStalePct',\n",
    "        'DollarVolume': 'RollMedDollarVol', # <-- RENAMED HERE\n",
    "        'HasSameVolume': 'RollingSameVolCount'\n",
    "    })\n",
    "\n",
    "    # The index after a grouped rolling operation is hierarchical.\n",
    "    # We remove the outermost 'Ticker' level to restore the original index structure.\n",
    "    rolling_quality = rolling_result.reset_index(level=0, drop=True)\n",
    "\n",
    "    # --- 3. Combine All Features ---\n",
    "    print(\"Combining all feature sets...\")\n",
    "    features_df = pd.concat([indicator_df, rolling_quality], axis=1)\n",
    "    \n",
    "    print(\"✅ Feature generation complete.\")\n",
    "    return features_df\n",
    "\n",
    "def test_features_df(features_df: pd.DataFrame, \n",
    "                     df_ohlcv: pd.DataFrame, \n",
    "                     test_ticker: str = 'AAPL',\n",
    "                     spot_check_date: str = '2020-03-20'):\n",
    "    \"\"\"\n",
    "    Runs a suite of tests to verify the correctness of the generated features_df.\n",
    "\n",
    "    Args:\n",
    "        features_df: The generated DataFrame from the generate_features function.\n",
    "        df_ohlcv: The original source OHLCV DataFrame.\n",
    "        test_ticker: A common, liquid ticker to use for specific value checks.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Running Verification Suite for features_df (Test Ticker: {test_ticker}) ---\")\n",
    "    \n",
    "    # --- Test 1: Structural Integrity ---\n",
    "    print(\"\\n[Test 1: Structural Integrity]\")\n",
    "    assert features_df.index.equals(df_ohlcv.index), \"FAIL: Index does not match original df_ohlcv.\"\n",
    "    print(\"  ✅ PASS: Index matches original df_ohlcv.\")\n",
    "    \n",
    "    expected_cols = ['TR', 'ATR', 'ATRP', 'RollingStalePct', 'RollMedDollarVol', 'RollingSameVolCount']\n",
    "    assert all(col in features_df.columns for col in expected_cols), \"FAIL: Missing one or more expected columns.\"\n",
    "    print(\"  ✅ PASS: All expected feature columns are present.\")\n",
    "    print(f\"  - DataFrame Info:\")\n",
    "    features_df.info(verbose=False, memory_usage='deep')\n",
    "\n",
    "\n",
    "    # --- Test 2: ATR Calculation Logic ---\n",
    "    print(\"\\n[Test 2: ATR Logic Verification]\")\n",
    "    ticker_features = features_df.loc[test_ticker]\n",
    "    \n",
    "    # Test 2a: First TR value should be NaN (since prev_close is NaN)\n",
    "    first_tr = ticker_features['TR'].iloc[0]\n",
    "    assert pd.isna(first_tr), f\"FAIL: First TR value for {test_ticker} should be NaN, but got {first_tr}.\"\n",
    "    print(f\"  ✅ PASS: First TR value for {test_ticker} is NaN as expected.\")\n",
    "\n",
    "    # Test 2b: The first valid ATR should equal the first valid TR (EWM cold start behavior)\n",
    "    first_valid_tr_val = ticker_features['TR'].dropna().iloc[0]\n",
    "    first_valid_atr_val = ticker_features['ATR'].dropna().iloc[0]\n",
    "    assert np.isclose(first_valid_tr_val, first_valid_atr_val), \\\n",
    "        f\"FAIL: First valid ATR ({first_valid_atr_val}) should equal first valid TR ({first_valid_tr_val}).\"\n",
    "    print(\"  ✅ PASS: First valid ATR correctly seeded with first valid TR.\")\n",
    "\n",
    "\n",
    "    # --- Test 3: Rolling Quality Metrics Logic ---\n",
    "    print(\"\\n[Test 3: Rolling Quality Metrics Logic Verification]\")\n",
    "    quality_min_periods = 126 # Should match the parameter used in generation\n",
    "    \n",
    "    # Test 3a: Check for leading NaNs\n",
    "    first_valid_quality_idx = ticker_features['RollingStalePct'].first_valid_index()\n",
    "    if first_valid_quality_idx is None:\n",
    "        print(f\"  - INFO: No valid quality metrics found for {test_ticker} (likely too little data). Skipping test.\")\n",
    "    else:\n",
    "        position_of_first_valid = ticker_features.index.get_loc(first_valid_quality_idx)\n",
    "        assert position_of_first_valid == quality_min_periods - 1, \\\n",
    "            f\"FAIL: First valid quality metric should appear at index {quality_min_periods - 1}, but appeared at {position_of_first_valid}.\"\n",
    "        print(f\"  ✅ PASS: Leading NaNs are present for the first {quality_min_periods - 1} periods as expected.\")\n",
    "\n",
    "\n",
    "    # --- Test 4: Spot Check Against Manual Calculation ---\n",
    "    print(\"\\n[Test 4: Spot Check vs. Manual Calculation]\")\n",
    "    # # Choose a specific date for a manual calculation\n",
    "    # spot_check_date = '2020-03-20' # A volatile day for a good test\n",
    "    \n",
    "    # Manual TR Calculation\n",
    "    today_data = df_ohlcv.loc[(test_ticker, spot_check_date)]\n",
    "    yesterday_data = df_ohlcv.loc[(test_ticker, pd.to_datetime(spot_check_date) - pd.Timedelta(days=1))] # simple lookback for test\n",
    "    \n",
    "    manual_h_l = today_data['Adj High'] - today_data['Adj Low']\n",
    "    manual_h_pc = abs(today_data['Adj High'] - yesterday_data['Adj Close'])\n",
    "    manual_l_pc = abs(today_data['Adj Low'] - yesterday_data['Adj Close'])\n",
    "    manual_tr = max(manual_h_l, manual_h_pc, manual_l_pc)\n",
    "    \n",
    "    code_tr = ticker_features.loc[spot_check_date]['TR']\n",
    "    \n",
    "    assert np.isclose(manual_tr, code_tr), f\"FAIL: Manual TR ({manual_tr:.4f}) does not match code TR ({code_tr:.4f}) on {spot_check_date}.\"\n",
    "    print(f\"  ✅ PASS: Manually calculated TR on {spot_check_date} matches code's TR.\")\n",
    "    \n",
    "    print(\"\\n--- ✅ All Verification Tests Passed ---\")\n",
    "\n",
    "def export_ticker_data(ticker_to_export: str, \n",
    "                         df_ohlcv: pd.DataFrame, \n",
    "                         features_df: pd.DataFrame, \n",
    "                         output_dir: str = 'export_csv'):\n",
    "    \"\"\"\n",
    "    Exports the raw OHLCV data and the corresponding calculated features for a \n",
    "    single ticker to two separate CSV files.\n",
    "\n",
    "    This function is designed for easy manual verification of data and calculations.\n",
    "    It will create the output directory if it does not exist.\n",
    "\n",
    "    Args:\n",
    "        ticker_to_export: The ticker symbol to export (e.g., 'AAPL').\n",
    "        df_ohlcv: The main DataFrame containing the raw OHLCV data with a \n",
    "                  (Ticker, Date) MultiIndex.\n",
    "        features_df: The DataFrame containing the calculated features with a \n",
    "                     (Ticker, Date) MultiIndex.\n",
    "        output_dir: The directory where the CSV files will be saved. \n",
    "                    Defaults to 'export_csv'.\n",
    "    \"\"\"\n",
    "    print(f\"--- Attempting to export data for ticker: {ticker_to_export} ---\")\n",
    "    \n",
    "    # --- 1. Ensure the output directory exists ---\n",
    "    try:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        print(f\"Output directory '{output_dir}' is ready.\")\n",
    "    except OSError as e:\n",
    "        print(f\"Error: Could not create directory '{output_dir}'. Reason: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- 2. Isolate the data for the specified ticker ---\n",
    "    try:\n",
    "        # Use .loc to select all rows for the given ticker from the MultiIndex\n",
    "        ticker_ohlcv = df_ohlcv.loc[ticker_to_export]\n",
    "        ticker_features = features_df.loc[ticker_to_export]\n",
    "        \n",
    "        if ticker_ohlcv.empty:\n",
    "            print(f\"Warning: No OHLCV data found for ticker '{ticker_to_export}'. Cannot export.\")\n",
    "            return\n",
    "            \n",
    "        print(f\"Found {len(ticker_ohlcv)} rows of data for '{ticker_to_export}'.\")\n",
    "        \n",
    "    except KeyError:\n",
    "        print(f\"Error: Ticker '{ticker_to_export}' not found in one or both of the DataFrames. Please check the symbol.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while accessing data: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- 3. Construct file paths and export to CSV ---\n",
    "    try:\n",
    "        # Define the full path for each output file\n",
    "        ohlcv_filename = f\"{ticker_to_export}_ohlcv.csv\"\n",
    "        features_filename = f\"{ticker_to_export}_features.csv\"\n",
    "        \n",
    "        ohlcv_filepath = os.path.join(output_dir, ohlcv_filename)\n",
    "        features_filepath = os.path.join(output_dir, features_filename)\n",
    "        \n",
    "        # Export the DataFrames to CSV. The index (Date) will be included.\n",
    "        ticker_ohlcv.to_csv(ohlcv_filepath)\n",
    "        ticker_features.to_csv(features_filepath)\n",
    "        \n",
    "        print(\"\\n✅ Export successful!\")\n",
    "        print(f\"   - Raw OHLCV data saved to: {ohlcv_filepath}\")\n",
    "        print(f\"   - Calculated features saved to: {features_filepath}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: Failed to write data to CSV files. Reason: {e}\")\n",
    "\n",
    "def create_synthetic_ticker_data(\n",
    "    ticker_name: str = 'SYNTH', \n",
    "    num_days: int = 50,\n",
    "    num_zero_volume_days: int = 5,\n",
    "    num_flat_price_days: int = 3\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Creates a synthetic OHLCV DataFrame with predictable patterns and randomly injected\n",
    "    stale data conditions for robust testing.\n",
    "\n",
    "    Args:\n",
    "        ticker_name: The name for the synthetic ticker.\n",
    "        num_days: The total number of days for the ticker's history.\n",
    "        num_zero_volume_days: The number of random days to set Volume to 0.\n",
    "        num_flat_price_days: The number of random days to set High == Low.\n",
    "\n",
    "    Returns:\n",
    "        A pandas DataFrame with a (Ticker, Date) MultiIndex.\n",
    "    \"\"\"\n",
    "    print(f\"--- Creating synthetic data for '{ticker_name}' with {num_days} days ---\")\n",
    "    \n",
    "    # 1. Create a base DataFrame with \"normal\" data\n",
    "    dates = pd.to_datetime(pd.date_range(start='2023-01-01', periods=num_days, freq='B'))\n",
    "    data = {\n",
    "        'Adj Open': 100.0, 'Adj High': 102.0, 'Adj Low': 98.0,\n",
    "        'Adj Close': 100.0, 'Volume': 1_000_000\n",
    "    }\n",
    "    df = pd.DataFrame(data, index=dates)\n",
    "    df['Adj Close'] = df['Adj Close'] + np.random.randn(num_days) * 0.5 # Add some noise\n",
    "\n",
    "    # 2. Define a \"protected\" window for specific verification tests.\n",
    "    # The `verify_synthetic_ticker_features` function depends on this exact window.\n",
    "    # We will not inject random stale days here.\n",
    "    protected_start_idx, protected_end_idx = 10, 20\n",
    "    \n",
    "    # 3. Inject random \"stale\" days OUTSIDE the protected window\n",
    "    available_indices = df.index.drop(df.index[protected_start_idx:protected_end_idx])\n",
    "    \n",
    "    # Inject zero-volume days\n",
    "    if num_zero_volume_days > 0:\n",
    "        if len(available_indices) < num_zero_volume_days:\n",
    "            raise ValueError(\"Not enough available days to inject zero-volume days.\")\n",
    "        zero_vol_dates = np.random.choice(available_indices, num_zero_volume_days, replace=False)\n",
    "        df.loc[zero_vol_dates, 'Volume'] = 0\n",
    "        print(f\"  - Injected {num_zero_volume_days} random zero-volume 'stale' days.\")\n",
    "        # Update available indices to avoid overlap\n",
    "        available_indices = available_indices.drop(zero_vol_dates)\n",
    "\n",
    "    # Inject flat-price days (High == Low)\n",
    "    if num_flat_price_days > 0:\n",
    "        if len(available_indices) < num_flat_price_days:\n",
    "            raise ValueError(\"Not enough available days to inject flat-price days.\")\n",
    "        flat_price_dates = np.random.choice(available_indices, num_flat_price_days, replace=False)\n",
    "        # Set High and Low to be the same as the Close price for that day\n",
    "        df.loc[flat_price_dates, 'Adj High'] = df.loc[flat_price_dates, 'Adj Close']\n",
    "        df.loc[flat_price_dates, 'Adj Low'] = df.loc[flat_price_dates, 'Adj Close']\n",
    "        print(f\"  - Injected {num_flat_price_days} random flat-price 'stale' days.\")\n",
    "\n",
    "    # 4. Inject the specific, hand-crafted patterns inside the protected window for verification\n",
    "    print(\"  - Injecting specific patterns for programmatic verification...\")\n",
    "    # Pattern for RollingStalePct: 2 stale days in 10 (20%)\n",
    "    df.iloc[10, df.columns.get_loc('Volume')] = 0  # Stale day (zero volume)\n",
    "    df.iloc[11, df.columns.get_loc('Adj High')] = 99.0 # Stale day (High == Low)\n",
    "    df.iloc[11, df.columns.get_loc('Adj Low')] = 99.0\n",
    "    \n",
    "    # Pattern for RollingMedianVolume\n",
    "    for i in range(10):\n",
    "        df.iloc[10 + i, df.columns.get_loc('Adj Close')] = 100.0 # Standardize price for easy median calc\n",
    "        df.iloc[10 + i, df.columns.get_loc('Volume')] = (i + 1) * 10000\n",
    "\n",
    "    # Pattern for RollingSameVolCount\n",
    "    df.iloc[15, df.columns.get_loc('Volume')] = 77777\n",
    "    df.iloc[16, df.columns.get_loc('Volume')] = 77777\n",
    "    df.iloc[17, df.columns.get_loc('Volume')] = 77777\n",
    "    \n",
    "    # 5. Set the MultiIndex\n",
    "    df['Ticker'] = ticker_name\n",
    "    df = df.set_index(['Ticker', df.index])\n",
    "    df.index.names = ['Ticker', 'Date']\n",
    "    \n",
    "    print(\"✅ Synthetic data created successfully.\")\n",
    "    return df\n",
    "\n",
    "def verify_synthetic_ticker_features(features_df: pd.DataFrame, \n",
    "                                       ticker_name: str = 'SYNTH',\n",
    "                                       quality_window: int = 10):\n",
    "    \"\"\"\n",
    "    Verifies the quality metric calculations on the features_df generated from\n",
    "    the synthetic ticker data.\n",
    "\n",
    "    Args:\n",
    "        features_df: The DataFrame of calculated features.\n",
    "        ticker_name: The name of the synthetic ticker.\n",
    "        quality_window: The rolling window used, which must match the window\n",
    "                        of the synthetic data pattern.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Running Verification on Synthetic Ticker '{ticker_name}' ---\")\n",
    "    \n",
    "    # --- Expected values based on our synthetic data design ---\n",
    "    EXPECTED_STALE_PCT = 0.20  # 2 stale days out of 10\n",
    "    EXPECTED_MEDIAN_DOLLAR_VOL = 5_500_000.0 # median of (10k..100k) * price of 100\n",
    "    EXPECTED_SAME_VOL_COUNT = 2.0 # Three consecutive days gives two 'diff() == 0' events\n",
    "\n",
    "    try:\n",
    "        # Isolate the features for our synthetic ticker\n",
    "        ticker_features = features_df.loc[ticker_name]\n",
    "        \n",
    "        # The first valid calculation will be on the last day of our 10-day window.\n",
    "        # The window starts at index 10 and has a length of 10, so it ends at index 19.\n",
    "        verification_date = ticker_features.index[19]\n",
    "        \n",
    "        print(f\"Verifying calculations on date: {verification_date.date()}\")\n",
    "        \n",
    "        # Get the calculated values from the DataFrame\n",
    "        calculated_values = ticker_features.loc[verification_date]\n",
    "        stale_pct = calculated_values['RollingStalePct']\n",
    "        # --- CHANGE 2 (continued): Accessing the renamed column ---\n",
    "        median_vol = calculated_values['RollMedDollarVol'] \n",
    "        same_vol_count = calculated_values['RollingSameVolCount']\n",
    "        \n",
    "        # --- Perform Assertions ---\n",
    "        print(\"\\n[Test 1: RollingStalePct]\")\n",
    "        assert np.isclose(stale_pct, EXPECTED_STALE_PCT), f\"FAIL: Expected {EXPECTED_STALE_PCT}, Got {stale_pct}\"\n",
    "        print(f\"  ✅ PASS: Expected {EXPECTED_STALE_PCT}, Got {stale_pct}\")\n",
    "\n",
    "        print(\"\\n[Test 2: RollMedDollarVol]\")\n",
    "        assert np.isclose(median_vol, EXPECTED_MEDIAN_DOLLAR_VOL), f\"FAIL: Expected {EXPECTED_MEDIAN_DOLLAR_VOL}, Got {median_vol}\"\n",
    "        print(f\"  ✅ PASS: Expected {EXPECTED_MEDIAN_DOLLAR_VOL}, Got {median_vol}\")\n",
    "\n",
    "        print(\"\\n[Test 3: RollingSameVolCount]\")\n",
    "        assert np.isclose(same_vol_count, EXPECTED_SAME_VOL_COUNT), f\"FAIL: Expected {EXPECTED_SAME_VOL_COUNT}, Got {same_vol_count}\"\n",
    "        print(f\"  ✅ PASS: Expected {EXPECTED_SAME_VOL_COUNT}, Got {same_vol_count}\")\n",
    "\n",
    "        print(\"\\n--- ✅ All Synthetic Data Verification Tests Passed ---\")\n",
    "\n",
    "    except KeyError:\n",
    "        print(f\"FAIL: Ticker '{ticker_name}' not found in features_df.\")\n",
    "    except IndexError:\n",
    "        print(\"FAIL: Not enough data in features_df to run verification. Check num_days.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during verification: {e}\")\n",
    "\n",
    "# --- A. HELPER FUNCTIONS ---\n",
    "\n",
    "def calculate_gain(price_series: pd.Series):\n",
    "    \"\"\"Calculates the total gain over a series of prices.\"\"\"\n",
    "    # Ensure there are at least two data points to calculate a gain\n",
    "    if price_series.dropna().shape[0] < 2: return np.nan\n",
    "    # Use forward-fill for the end price and back-fill for the start price\n",
    "    # to handle potential NaNs at the beginning or end of the series.\n",
    "    return (price_series.ffill().iloc[-1] / price_series.bfill().iloc[0]) - 1\n",
    "\n",
    "def calculate_sharpe(return_series: pd.Series):\n",
    "    \"\"\"Calculates the annualized Sharpe ratio from a series of daily returns.\"\"\"\n",
    "    # Ensure there are at least two returns to calculate a standard deviation\n",
    "    if return_series.dropna().shape[0] < 2: return np.nan\n",
    "    std_dev = return_series.std()\n",
    "    # Avoid division by zero if returns are constant\n",
    "    if std_dev > 0 and std_dev != np.inf:\n",
    "        return (return_series.mean() / std_dev) * np.sqrt(252)\n",
    "    return np.nan\n",
    "\n",
    "def print_nested(d, indent=0, width=4):\n",
    "    \"\"\"Pretty-print any nested dict/list/tuple combination.\"\"\"\n",
    "    spacing = ' ' * indent\n",
    "    if isinstance(d, dict):\n",
    "        for k, v in d.items():\n",
    "            print(f'{spacing}{k}:')\n",
    "            print_nested(v, indent + width, width)\n",
    "    elif isinstance(d, (list, tuple)):\n",
    "        for item in d:\n",
    "            print_nested(item, indent, width)\n",
    "    else:\n",
    "        print(f'{spacing}{d}')\n",
    "\n",
    "# --- B. THE CORE CALCULATION ENGINE ---\n",
    "\n",
    "def run_walk_forward_step(df_close_full, df_high_full, df_low_full,\n",
    "                          master_trading_days,\n",
    "                          start_date, calc_period, fwd_period,\n",
    "                          metric, rank_start, rank_end, benchmark_ticker,\n",
    "                          debug=False):\n",
    "    \"\"\"Runs a single step of the walk-forward analysis using precise trading days.\"\"\"\n",
    "    debug_data = {} if debug else None\n",
    "    \n",
    "    # 1. Determine exact date ranges using the master trading day calendar\n",
    "    try:\n",
    "        start_idx = master_trading_days.get_loc(start_date)\n",
    "    except KeyError:\n",
    "        return ({'error': f\"Start date {start_date.date()} is not a valid trading day.\"}, None)\n",
    "        \n",
    "    calc_end_idx = min(start_idx + calc_period, len(master_trading_days) - 1)\n",
    "    viz_end_idx = min(calc_end_idx + fwd_period, len(master_trading_days) - 1)\n",
    "\n",
    "    safe_start_date = master_trading_days[start_idx]\n",
    "    safe_calc_end_date = master_trading_days[calc_end_idx]\n",
    "    safe_viz_end_date = master_trading_days[viz_end_idx]\n",
    "    \n",
    "    if safe_start_date >= safe_calc_end_date:\n",
    "        return ({'error': \"Invalid date range (calc period has zero or negative length).\"}, None)\n",
    "\n",
    "    # 2. Slice data for the calculation period and filter for valid tickers\n",
    "    calc_close_raw = df_close_full.loc[safe_start_date:safe_calc_end_date]\n",
    "    calc_close = calc_close_raw.dropna(axis=1, how='all') # Drop tickers with no data in the period\n",
    "    if calc_close.shape[1] == 0 or len(calc_close) < 2:\n",
    "        return ({'error': \"Not enough data in calc period.\"}, None)\n",
    "\n",
    "    # 3. Calculate ranking metrics for all valid tickers\n",
    "    first_prices = calc_close.bfill().iloc[0]\n",
    "    last_prices = calc_close.ffill().iloc[-1]\n",
    "    daily_returns = calc_close.bfill().ffill().pct_change()\n",
    "    mean_returns = daily_returns.mean()\n",
    "    std_returns = daily_returns.std()\n",
    "    \n",
    "    valid_tickers = calc_close.columns\n",
    "    calc_high = df_high_full[valid_tickers].loc[safe_start_date:safe_calc_end_date]\n",
    "    calc_low = df_low_full[valid_tickers].loc[safe_start_date:safe_calc_end_date]\n",
    "    \n",
    "    # Correctly calculate True Range (TR) for a multi-ticker DataFrame\n",
    "    # First, align the previous day's close to the current calculation window.\n",
    "    prev_close = df_close_full[valid_tickers].shift(1).loc[safe_start_date:safe_calc_end_date]\n",
    "    \n",
    "    # Calculate the three components of True Range. Each result is a DataFrame.\n",
    "    component1 = calc_high - calc_low\n",
    "    component2 = abs(calc_high - prev_close)\n",
    "    component3 = abs(calc_low - prev_close)\n",
    "\n",
    "    # Find the element-wise maximum across the three component DataFrames.\n",
    "    # np.maximum is efficient and preserves the DataFrame structure.\n",
    "    tr = np.maximum(component1, np.maximum(component2, component3))\n",
    "    \n",
    "    atr = tr.ewm(alpha=1/14, adjust=False).mean()\n",
    "    atrp = (atr / calc_close).mean() # Mean ATRP over the calculation period\n",
    "\n",
    "    metric_values = {}\n",
    "    metric_values['Price'] = (last_prices / first_prices).dropna()\n",
    "    metric_values['Sharpe'] = (mean_returns / std_returns * np.sqrt(252)).fillna(0)\n",
    "    metric_values['Sharpe (ATR)'] = (mean_returns / atrp).fillna(0)\n",
    "\n",
    "    if debug:\n",
    "        df_ranking = pd.DataFrame({\n",
    "            'FirstPrice': first_prices, 'LastPrice': last_prices, 'MeanDailyReturn': mean_returns,\n",
    "            'StdDevDailyReturn': std_returns, 'MeanATRP': atrp, 'Metric_Price': metric_values['Price'],\n",
    "            'Metric_Sharpe': metric_values['Sharpe'], 'Metric_Sharpe (ATR)': metric_values['Sharpe (ATR)']\n",
    "        })\n",
    "        df_ranking.index.name = 'Ticker'\n",
    "        debug_data['ranking_metrics'] = df_ranking.sort_values(f'Metric_{metric}', ascending=False)\n",
    "\n",
    "    # 4. Rank tickers and select the target group\n",
    "    sorted_tickers = metric_values[metric].sort_values(ascending=False)\n",
    "    tickers_to_display = sorted_tickers.index[rank_start-1:rank_end].tolist()\n",
    "    if not tickers_to_display:\n",
    "        return ({'error': \"No tickers found for the selected rank.\"}, None)\n",
    "\n",
    "    # 5. Prepare data for plotting and portfolio performance calculation\n",
    "    normalized_plot_data = df_close_full[tickers_to_display].loc[safe_start_date:safe_viz_end_date]\n",
    "    normalized_plot_data = normalized_plot_data.div(normalized_plot_data.bfill().iloc[0])\n",
    "    actual_calc_end_ts = calc_close.index.max()\n",
    "    portfolio_series = normalized_plot_data.mean(axis=1)\n",
    "    portfolio_return_series = portfolio_series.pct_change()\n",
    "    benchmark_price_series = df_close_full.get(benchmark_ticker)\n",
    "    benchmark_return_series = benchmark_price_series.loc[safe_start_date:safe_viz_end_date].bfill().ffill().pct_change() if benchmark_price_series is not None else pd.Series(dtype='float64')\n",
    "\n",
    "    # 6. Correctly slice return series for Sharpe calculation to prevent lookahead\n",
    "    try:\n",
    "        # Use index location for a clean, non-overlapping split\n",
    "        boundary_loc = portfolio_return_series.index.get_loc(actual_calc_end_ts)\n",
    "        calc_portfolio_returns = portfolio_return_series.iloc[:boundary_loc + 1]\n",
    "        fwd_portfolio_returns = portfolio_return_series.iloc[boundary_loc + 1:]\n",
    "        \n",
    "        if benchmark_price_series is not None:\n",
    "            bm_boundary_loc = benchmark_return_series.index.get_loc(actual_calc_end_ts)\n",
    "            calc_benchmark_returns = benchmark_return_series.iloc[:bm_boundary_loc + 1]\n",
    "            fwd_benchmark_returns = benchmark_return_series.iloc[bm_boundary_loc + 1:]\n",
    "        else:\n",
    "            calc_benchmark_returns, fwd_benchmark_returns = pd.Series(dtype='float64'), pd.Series(dtype='float64')\n",
    "            \n",
    "    except (KeyError, IndexError): # Fallback for edge cases\n",
    "        calc_portfolio_returns = portfolio_return_series.loc[:actual_calc_end_ts]\n",
    "        fwd_portfolio_returns = portfolio_return_series.loc[actual_calc_end_ts:].iloc[1:]\n",
    "        if benchmark_price_series is not None:\n",
    "            calc_benchmark_returns = benchmark_return_series.loc[:actual_calc_end_ts]\n",
    "            fwd_benchmark_returns = benchmark_return_series.loc[actual_calc_end_ts:].iloc[1:]\n",
    "        else:\n",
    "            calc_benchmark_returns, fwd_benchmark_returns = pd.Series(dtype='float64'), pd.Series(dtype='float64')\n",
    "\n",
    "    # 7. Calculate performance metrics (Gain & Sharpe) for all periods\n",
    "    perf_data = {}\n",
    "    perf_data['calc_p_gain'] = calculate_gain(portfolio_series.loc[:actual_calc_end_ts])\n",
    "    perf_data['fwd_p_gain'] = calculate_gain(portfolio_series.loc[actual_calc_end_ts:])\n",
    "    perf_data['full_p_gain'] = calculate_gain(portfolio_series)\n",
    "    perf_data['calc_p_sharpe'] = calculate_sharpe(calc_portfolio_returns)\n",
    "    perf_data['fwd_p_sharpe'] = calculate_sharpe(fwd_portfolio_returns)\n",
    "    perf_data['full_p_sharpe'] = calculate_sharpe(portfolio_return_series)\n",
    "    \n",
    "    perf_data['calc_b_gain'] = calculate_gain(benchmark_price_series.loc[safe_start_date:actual_calc_end_ts]) if benchmark_price_series is not None else np.nan\n",
    "    perf_data['fwd_b_gain'] = calculate_gain(benchmark_price_series.loc[actual_calc_end_ts:safe_viz_end_date]) if benchmark_price_series is not None else np.nan\n",
    "    perf_data['full_b_gain'] = calculate_gain(benchmark_price_series.loc[safe_start_date:safe_viz_end_date]) if benchmark_price_series is not None else np.nan\n",
    "    perf_data['calc_b_sharpe'] = calculate_sharpe(calc_benchmark_returns)\n",
    "    perf_data['fwd_b_sharpe'] = calculate_sharpe(fwd_benchmark_returns)\n",
    "    perf_data['full_b_sharpe'] = calculate_sharpe(benchmark_return_series)\n",
    "\n",
    "    # 8. Assemble results DataFrame for display\n",
    "    calc_end_prices = calc_close.ffill().iloc[-1]\n",
    "    fwd_close_slice = df_close_full.loc[actual_calc_end_ts:safe_viz_end_date]\n",
    "    viz_end_prices = fwd_close_slice.ffill().iloc[-1] if not fwd_close_slice.empty and len(fwd_close_slice) >= 2 else calc_end_prices\n",
    "    calc_gains = (calc_end_prices / calc_close.bfill().iloc[0]) - 1\n",
    "    fwd_gains = (viz_end_prices / calc_end_prices) - 1\n",
    "    results_df = pd.DataFrame({'Rank': range(rank_start, rank_start + len(tickers_to_display)), 'Metric': metric, 'MetricValue': sorted_tickers.loc[tickers_to_display].values, 'CalcPrice': calc_end_prices.loc[tickers_to_display], 'CalcGain': calc_gains.loc[tickers_to_display], 'FwdGain': fwd_gains.loc[tickers_to_display]}, index=pd.Index(tickers_to_display, name='Ticker'))\n",
    "    if benchmark_price_series is not None and benchmark_ticker in calc_close.columns:\n",
    "        benchmark_df_row = pd.DataFrame({'Rank': np.nan, 'Metric': metric, 'MetricValue': metric_values[metric].get(benchmark_ticker, np.nan), 'CalcPrice': calc_end_prices[benchmark_ticker], 'CalcGain': calc_gains[benchmark_ticker], 'FwdGain': fwd_gains[benchmark_ticker]}, index=pd.Index([f\"{benchmark_ticker} (BM)\"], name='Ticker'))\n",
    "        results_df = pd.concat([results_df, benchmark_df_row])\n",
    "    \n",
    "    # 9. Assemble debug data if requested\n",
    "    if debug:\n",
    "        df_trace = normalized_plot_data.copy()\n",
    "        df_trace.columns = [f'Norm_Price_{c}' for c in df_trace.columns]\n",
    "        df_trace['Norm_Price_Portfolio'] = portfolio_series\n",
    "        if benchmark_price_series is not None and not benchmark_price_series.loc[safe_start_date:safe_viz_end_date].dropna().empty:\n",
    "            norm_bm = benchmark_price_series.loc[safe_start_date:safe_viz_end_date] / benchmark_price_series.loc[safe_start_date:].bfill().iloc[0]\n",
    "            df_trace[f'Norm_Price_Benchmark_{benchmark_ticker}'] = norm_bm\n",
    "        for col in df_trace.columns:\n",
    "            if 'Norm_Price' in col:\n",
    "                df_trace[col.replace('Norm_Price', 'Return')] = df_trace[col].pct_change()\n",
    "        debug_data['portfolio_trace'] = df_trace\n",
    "\n",
    "    # 10. Package final results\n",
    "    final_results = {\n",
    "        'tickers_to_display': tickers_to_display, 'normalized_plot_data': normalized_plot_data,\n",
    "        'portfolio_series': portfolio_series, 'benchmark_price_series': benchmark_price_series,\n",
    "        'performance_data': perf_data, 'results_df': results_df, 'actual_calc_end_ts': actual_calc_end_ts,\n",
    "        'safe_start_date': safe_start_date, 'safe_viz_end_date': safe_viz_end_date,\n",
    "        'error': None\n",
    "    }\n",
    "    return (final_results, debug_data)\n",
    "\n",
    "# --- C. DYNAMIC DATA QUALITY FILTER FUNCTIONS ---\n",
    "\n",
    "def calculate_rolling_quality_metrics(df_ohlcv, window=252, min_periods=126, debug=False):\n",
    "    \"\"\"Calculates rolling data quality metrics for the entire dataset.\"\"\"\n",
    "    print(f\"--- Calculating Rolling Quality Metrics (Window: {window} days) ---\")\n",
    "    df = df_ohlcv.copy()\n",
    "    if not df.index.is_monotonic_increasing:\n",
    "        df.sort_index(inplace=True)\n",
    "        \n",
    "    # Define quality flags\n",
    "    df['IsStale'] = np.where((df['Volume'] == 0) | (df['Adj High'] == df['Adj Low']), 1, 0)\n",
    "    df['DollarVolume'] = df['Adj Close'] * df['Volume']\n",
    "    df['HasSameVolumeAsPrevDay'] = (df.groupby(level='Ticker')['Volume'].diff() == 0).astype(int)\n",
    "    \n",
    "    # Calculate rolling metrics per ticker\n",
    "    grouped = df.groupby(level='Ticker')\n",
    "    stale_pct = grouped['IsStale'].rolling(window=window, min_periods=min_periods).mean()\n",
    "    median_vol = grouped['DollarVolume'].rolling(window=window, min_periods=min_periods).median()\n",
    "    same_vol_count = grouped['HasSameVolumeAsPrevDay'].rolling(window=window, min_periods=min_periods).sum()\n",
    "    \n",
    "    quality_df = pd.concat([stale_pct, median_vol, same_vol_count], axis=1)\n",
    "    quality_df.columns = ['RollingStalePct', 'RollingMedianVolume', 'RollingSameVolCount']\n",
    "    quality_df.index = quality_df.index.droplevel(0) # Remove the extra 'Ticker' level\n",
    "    print(\"✅ Rolling metrics calculation complete.\")\n",
    "    return quality_df\n",
    "\n",
    "def get_eligible_universe(quality_metrics_df, filter_date, thresholds):\n",
    "    \"\"\"Filters the universe of tickers based on quality metrics for a given date.\"\"\"\n",
    "    filter_date_ts = pd.to_datetime(filter_date)\n",
    "    date_index = quality_metrics_df.index.get_level_values('Date').unique().sort_values()\n",
    "    \n",
    "    if filter_date_ts < date_index[0]:\n",
    "        print(f\"Warning: Filter date {filter_date_ts.date()} is before the earliest data point. Returning empty universe.\")\n",
    "        return []\n",
    "        \n",
    "    # Find the most recent date with quality data on or before the filter date\n",
    "    valid_prior_dates = date_index[date_index <= filter_date_ts]\n",
    "    if valid_prior_dates.empty:\n",
    "        print(f\"Warning: No available data found on or before {filter_date_ts.date()}. Returning empty universe.\")\n",
    "        return []\n",
    "        \n",
    "    actual_date_to_use = valid_prior_dates[-1]\n",
    "    if actual_date_to_use.date() != filter_date_ts.date():\n",
    "        print(f\"ℹ️ Info: Filter date {filter_date_ts.date()} not found. Using previous available date {actual_date_to_use.date()}.\")\n",
    "\n",
    "    metrics_on_date = quality_metrics_df.xs(actual_date_to_use, level='Date')\n",
    "    \n",
    "    # Apply filters\n",
    "    mask = ((metrics_on_date['RollingMedianVolume'] >= thresholds['min_median_dollar_volume']) &\n",
    "            (metrics_on_date['RollingStalePct'] <= thresholds['max_stale_pct']) &\n",
    "            (metrics_on_date['RollingSameVolCount'] <= thresholds['max_same_vol_count']))\n",
    "            \n",
    "    eligible_tickers = metrics_on_date[mask].index.tolist()\n",
    "    all_tickers = metrics_on_date.index.tolist()\n",
    "    print(f\"Dynamic Filter ({filter_date_ts.date()}): Kept {len(eligible_tickers)} of {len(all_tickers)} tickers.\")\n",
    "    return eligible_tickers    \n",
    "\n",
    "# --- D. INTERACTIVE ANALYSIS & BACKTESTING TOOLS ---\n",
    "\n",
    "def plot_walk_forward_analyzer(df_ohlcv, \n",
    "                               default_start_date=None, default_calc_period=126, default_fwd_period=63,\n",
    "                               default_metric='Sharpe (ATR)', default_rank_start=1, default_rank_end=10,\n",
    "                               default_benchmark_ticker='VOO', master_calendar_ticker='VOO',\n",
    "                               quality_thresholds={'min_median_dollar_volume': 1_000_000, 'max_stale_pct': 0.05, 'max_same_vol_count': 10},\n",
    "                               debug=False):\n",
    "    \"\"\"Creates an interactive widget for single-period walk-forward analysis.\"\"\"\n",
    "    print(\"Initializing Walk-Forward Analyzer (using Trading Day Logic)...\")\n",
    "    if not isinstance(df_ohlcv.index, pd.MultiIndex): raise ValueError(\"Input DataFrame must have a (Ticker, Date) MultiIndex.\")\n",
    "    df_ohlcv = df_ohlcv.sort_index()\n",
    "\n",
    "    if master_calendar_ticker not in df_ohlcv.index.get_level_values(0):\n",
    "        raise ValueError(f\"Master calendar ticker '{master_calendar_ticker}' not found in DataFrame.\")\n",
    "    master_trading_days = df_ohlcv.loc[master_calendar_ticker].index.unique().sort_values()\n",
    "    print(f\"Master trading day calendar created from '{master_calendar_ticker}' ({len(master_trading_days)} days).\")\n",
    "\n",
    "    # # The following functions are assumed to exist. We define placeholders for them.\n",
    "    # def calculate_rolling_quality_metrics(df, window):\n",
    "    #     tickers = df.index.get_level_values(0).unique()\n",
    "    #     dates = df.index.get_level_values(1).unique()\n",
    "    #     return pd.DataFrame(index=pd.MultiIndex.from_product([tickers, dates], names=['Ticker', 'Date']))\n",
    "    # def get_eligible_universe(quality_df, date, thresholds):\n",
    "    #     tickers = quality_df.index.get_level_values(0).unique()\n",
    "    #     return list(tickers)\n",
    "    # def run_walk_forward_step(*args, **kwargs):\n",
    "    #     # Dummy return structure for demonstration\n",
    "    #     return {'error': \"This is a placeholder function.\", 'safe_start_date': pd.Timestamp.now(), 'actual_calc_end_ts': pd.Timestamp.now(), 'safe_viz_end_date': pd.Timestamp.now()}, None\n",
    "\n",
    "    print(\"Pre-calculating data quality metrics...\")\n",
    "    quality_metrics_df = calculate_rolling_quality_metrics(df_ohlcv, window=252)\n",
    "    print(\"Pre-processing data (unstacking)...\")\n",
    "    df_close_full = df_ohlcv['Adj Close'].unstack(level=0)\n",
    "    df_high_full = df_ohlcv['Adj High'].unstack(level=0)\n",
    "    df_low_full = df_ohlcv['Adj Low'].unstack(level=0)\n",
    "    \n",
    "    # --- Widget Setup ---\n",
    "    start_date_picker = widgets.DatePicker(description='Start Date:', value=pd.to_datetime(default_start_date), disabled=False)\n",
    "    calc_period_input = widgets.IntText(value=default_calc_period, description='Calc Period (days):')\n",
    "    fwd_period_input = widgets.IntText(value=default_fwd_period, description='Fwd Period (days):')\n",
    "    metrics = ['Price', 'Sharpe', 'Sharpe (ATR)']\n",
    "    metric_dropdown = widgets.Dropdown(options=metrics, value=default_metric, description='Metric:')\n",
    "    rank_start_input = widgets.IntText(value=default_rank_start, description='Rank Start:')\n",
    "    rank_end_input = widgets.IntText(value=default_rank_end, description='Rank End:')\n",
    "    benchmark_ticker_input = widgets.Text(value=default_benchmark_ticker, description='Benchmark:', placeholder='Enter Ticker')\n",
    "    update_button = widgets.Button(description=\"Update Chart\", button_style='primary')\n",
    "    ticker_list_output = widgets.Output()\n",
    "    results_container, debug_data_container = [None], [None]\n",
    "\n",
    "    # --- Plotting Setup ---\n",
    "    fig = go.FigureWidget()\n",
    "    max_traces = 50\n",
    "    for i in range(max_traces): fig.add_trace(go.Scatter(x=[None], y=[None], mode='lines', name=f'placeholder_{i}', visible=False, showlegend=False))\n",
    "    fig.add_trace(go.Scatter(x=[None], y=[None], mode='lines', name='Benchmark', visible=True, showlegend=True, line=dict(color='black', width=3, dash='dash')))\n",
    "    fig.add_trace(go.Scatter(x=[None], y=[None], mode='lines', name='Group Portfolio', visible=True, showlegend=True, line=dict(color='green', width=3)))\n",
    "\n",
    "    # --- Update Logic (Callback) ---\n",
    "    def update_plot(button_click):\n",
    "        ticker_list_output.clear_output()\n",
    "        \n",
    "        # 1. Get and validate user inputs\n",
    "        start_date_raw = pd.to_datetime(start_date_picker.value)\n",
    "        start_date_idx = master_trading_days.searchsorted(start_date_raw)\n",
    "        if start_date_idx >= len(master_trading_days):\n",
    "            with ticker_list_output: print(f\"Error: Start date is after the last available trading day.\"); return\n",
    "        actual_start_date = master_trading_days[start_date_idx]\n",
    "        with ticker_list_output: \n",
    "            if start_date_raw.date() != actual_start_date.date():\n",
    "                print(f\"ℹ️ Info: Start date {start_date_raw.date()} is not a trading day. Snapping forward to {actual_start_date.date()}.\")\n",
    "\n",
    "        # Capture input values into variables\n",
    "        calc_period = calc_period_input.value\n",
    "        fwd_period = fwd_period_input.value\n",
    "        metric = metric_dropdown.value\n",
    "        rank_start = rank_start_input.value\n",
    "        rank_end = rank_end_input.value\n",
    "        benchmark_ticker = benchmark_ticker_input.value.strip().upper()\n",
    "        \n",
    "        if rank_start > rank_end:\n",
    "            with ticker_list_output: print(\"Error: 'Rank Start' must be <= 'Rank End'.\"); return\n",
    "        if rank_start < 1 or calc_period < 2 or fwd_period < 1:\n",
    "            with ticker_list_output: print(\"Error: Ranks must be >= 1, Calc Period >= 2, Fwd Period >= 1.\"); return\n",
    "\n",
    "        # 1a. Validate data availability\n",
    "        required_days = calc_period + fwd_period\n",
    "        if start_date_idx + required_days > len(master_trading_days):\n",
    "            available_days = len(master_trading_days) - start_date_idx\n",
    "            last_available_date = master_trading_days[-1].date()\n",
    "            with ticker_list_output:\n",
    "                print(f\"Error: Not enough data for the requested period.\")\n",
    "                print(f\"  Start Date: {actual_start_date.date()}\")\n",
    "                print(f\"  Required Days: {calc_period} (calc) + {fwd_period} (fwd) = {required_days}\")\n",
    "                print(f\"  Available Days from Start: {available_days} (until {last_available_date})\")\n",
    "                print(f\"  Please shorten the 'Calc Period' / 'Fwd Period' or choose an earlier 'Start Date'.\")\n",
    "            return\n",
    "\n",
    "        # 2. Apply dynamic data quality filter\n",
    "        eligible_tickers = get_eligible_universe(quality_metrics_df, actual_start_date, quality_thresholds)\n",
    "        if not eligible_tickers:\n",
    "            with ticker_list_output: print(f\"Error: No eligible tickers found on {actual_start_date.date()} with the current quality filters.\"); return\n",
    "        df_close_step = df_close_full[eligible_tickers]; df_high_step = df_high_full[eligible_tickers]; df_low_step = df_low_full[eligible_tickers]\n",
    "\n",
    "        # 3. Run the core calculation\n",
    "        results, debug_output = run_walk_forward_step(\n",
    "            df_close_step, df_high_step, df_low_step, master_trading_days,\n",
    "            actual_start_date, calc_period, fwd_period, \n",
    "            metric, rank_start, rank_end, benchmark_ticker, debug=debug\n",
    "        )\n",
    "        if results.get('error'):\n",
    "            with ticker_list_output: print(f\"Error: {results['error']}\"); return\n",
    "            \n",
    "        # ======================= MODIFICATION START =======================\n",
    "        # 3a. Augment the output containers with period dates and run parameters for external use.\n",
    "        \n",
    "        # Add period dates (from previous request)\n",
    "        period_dates = {\n",
    "            'calc_period_start': results['safe_start_date'],\n",
    "            'calc_period_end': results['actual_calc_end_ts'],\n",
    "            'forward_period_start': results['actual_calc_end_ts'],\n",
    "            'forward_period_end': results['safe_viz_end_date']\n",
    "        }\n",
    "        \n",
    "        # Add run parameters (new request)\n",
    "        run_parameters = {\n",
    "            'calc_period': calc_period,\n",
    "            'fwd_period': fwd_period,\n",
    "            'rank_metric': metric,\n",
    "            'rank_start': rank_start,\n",
    "            'rank_end': rank_end,\n",
    "            'benchmark_ticker': benchmark_ticker\n",
    "        }\n",
    "        \n",
    "        # Update the main results dictionary\n",
    "        results.update(period_dates)\n",
    "        results.update(run_parameters)\n",
    "\n",
    "        # Update the debug dictionary if it exists\n",
    "        if debug_output is not None and isinstance(debug_output, dict):\n",
    "            debug_output.update(period_dates)\n",
    "            debug_output.update(run_parameters)\n",
    "        # ======================= MODIFICATION END =======================\n",
    "\n",
    "        # 4. Update the interactive plot\n",
    "        with fig.batch_update():\n",
    "            # (Plotting code remains unchanged)\n",
    "            for i in range(max_traces):\n",
    "                trace = fig.data[i]\n",
    "                if i < len(results['tickers_to_display']):\n",
    "                    ticker = results['tickers_to_display'][i]; plot_data_series = results['normalized_plot_data'][ticker]\n",
    "                    trace.x, trace.y, trace.name, trace.visible, trace.showlegend = plot_data_series.index, plot_data_series.values, ticker, True, True\n",
    "                else: trace.visible, trace.showlegend = False, False\n",
    "            benchmark_trace = fig.data[max_traces]\n",
    "            if results['benchmark_price_series'] is not None and not results['benchmark_price_series'].loc[results['safe_start_date']:results['safe_viz_end_date']].dropna().empty:\n",
    "                normalized_benchmark = results['benchmark_price_series'].loc[results['safe_start_date']:results['safe_viz_end_date']] / results['benchmark_price_series'].loc[results['safe_start_date']:].bfill().iloc[0]\n",
    "                benchmark_trace.x, benchmark_trace.y, benchmark_trace.name, benchmark_trace.visible = normalized_benchmark.index, normalized_benchmark, f\"Benchmark ({benchmark_ticker})\", True\n",
    "            else: benchmark_trace.visible = False\n",
    "            portfolio_trace = fig.data[max_traces + 1]\n",
    "            portfolio_trace.x, portfolio_trace.y, portfolio_trace.name, portfolio_trace.visible = results['portfolio_series'].index, results['portfolio_series'], 'Group Portfolio', True\n",
    "            fig.layout.shapes = []; fig.add_shape(type=\"line\", x0=results['actual_calc_end_ts'], y0=0, x1=results['actual_calc_end_ts'], y1=1, xref='x', yref='paper', line=dict(color=\"grey\", width=2, dash=\"dash\"))\n",
    "            \n",
    "        results_container[0] = results; debug_data_container[0] = debug_output\n",
    "        \n",
    "        # 5. Display summary statistics in a formatted table\n",
    "        with ticker_list_output:\n",
    "            # (Summary display code remains unchanged)\n",
    "            print(f\"Analysis Period: {results['safe_start_date'].date()} to {results['safe_viz_end_date'].date()}.\")\n",
    "            pprint.pprint(results['tickers_to_display'])\n",
    "            p = results['performance_data']\n",
    "            rows = []\n",
    "            rows.append({'Metric': 'Group Portfolio Gain', 'Full': p['full_p_gain'], 'Calc': p['calc_p_gain'], 'Fwd': p['fwd_p_gain']})\n",
    "            if not np.isnan(p['full_b_gain']):\n",
    "                rows.append({'Metric': f'Benchmark ({benchmark_ticker}) Gain', 'Full': p['full_b_gain'], 'Calc': p['calc_b_gain'], 'Fwd': p['fwd_b_gain']})\n",
    "                rows.append({'Metric': 'Gain Delta (vs Bm)', 'Full': p['full_p_gain'] - p['full_b_gain'], 'Calc': p['calc_p_gain'] - p['calc_b_gain'], 'Fwd': p['fwd_p_gain'] - p['fwd_b_gain']})\n",
    "            rows.append({'Metric': 'Group Portfolio Sharpe', 'Full': p['full_p_sharpe'], 'Calc': p['calc_p_sharpe'], 'Fwd': p['fwd_p_sharpe']})\n",
    "            if not np.isnan(p['full_b_sharpe']):\n",
    "                rows.append({'Metric': f'Benchmark ({benchmark_ticker}) Sharpe', 'Full': p['full_b_sharpe'], 'Calc': p['calc_b_sharpe'], 'Fwd': p['fwd_b_sharpe']})\n",
    "                rows.append({'Metric': 'Sharpe Delta (vs Bm)', 'Full': p['full_p_sharpe'] - p['full_b_sharpe'], 'Calc': p['calc_p_sharpe'] - p['calc_b_sharpe'], 'Fwd': p['fwd_p_sharpe'] - p['fwd_b_sharpe']})\n",
    "            report_df = pd.DataFrame(rows).set_index('Metric')\n",
    "            gain_rows = [row for row in report_df.index if 'Gain' in row or 'Delta' in row]\n",
    "            sharpe_rows = [row for row in report_df.index if 'Sharpe' in row]\n",
    "            styled_df = report_df.style.format('{:+.2%}', na_rep='N/A', subset=(gain_rows, report_df.columns)).format('{:+.2f}', na_rep='N/A', subset=(sharpe_rows, report_df.columns)).set_properties(**{'text-align': 'right', 'width': '100px'}).set_table_styles([{'selector': 'th.col_heading', 'props': [('text-align', 'right')]}, {'selector': 'th.row_heading', 'props': [('text-align', 'left')]}])\n",
    "            print(\"\\n--- Strategy Performance Summary ---\")\n",
    "            display(styled_df)\n",
    "            \n",
    "    # --- Final Layout & Display ---\n",
    "    fig.update_layout(title_text='Walk-Forward Performance Analysis', xaxis_title='Date', yaxis_title='Normalized Price (Start = 1)', hovermode='x unified', legend_title_text='Tickers (Ranked)', height=600, margin=dict(t=50))\n",
    "    fig.add_hline(y=1, line_width=1, line_dash=\"dash\", line_color=\"grey\")\n",
    "    update_button.on_click(update_plot)\n",
    "    controls_row1 = widgets.HBox([start_date_picker, calc_period_input, fwd_period_input])\n",
    "    controls_row2 = widgets.HBox([metric_dropdown, rank_start_input, rank_end_input, benchmark_ticker_input, update_button])\n",
    "    ui_container = widgets.VBox([controls_row1, controls_row2, ticker_list_output], layout=widgets.Layout(margin='10px 0 20px 0'))\n",
    "    display(ui_container, fig)\n",
    "    update_plot(None) # Initial run\n",
    "    return (results_container, debug_data_container)\n",
    "\n",
    "def run_full_backtest(df_ohlcv, strategy_params, quality_thresholds):\n",
    "    \"\"\"Runs a full backtest of a strategy over a specified date range.\"\"\"\n",
    "    print(f\"--- Running Full Forensic Backtest for Strategy: {strategy_params['metric']} (Top {strategy_params['rank_start']}-{strategy_params['rank_end']}) ---\")\n",
    "    \n",
    "    # 1. Unpack strategy parameters\n",
    "    start_date, end_date = pd.to_datetime(strategy_params['start_date']), pd.to_datetime(strategy_params['end_date'])\n",
    "    calc_period, fwd_period = strategy_params['calc_period'], strategy_params['fwd_period']\n",
    "    metric, rank_start, rank_end = strategy_params['metric'], strategy_params['rank_start'], strategy_params['rank_end']\n",
    "    benchmark_ticker = strategy_params['benchmark_ticker']\n",
    "    master_calendar_ticker = strategy_params.get('master_calendar_ticker', 'VOO')\n",
    "    \n",
    "    # 2. Perform initial setup (same as analyzer)\n",
    "    if master_calendar_ticker not in df_ohlcv.index.get_level_values(0):\n",
    "        raise ValueError(f\"Master calendar ticker '{master_calendar_ticker}' not found in DataFrame.\")\n",
    "    master_trading_days = df_ohlcv.loc[master_calendar_ticker].index.unique().sort_values()\n",
    "    \n",
    "    start_idx = master_trading_days.searchsorted(start_date)\n",
    "    end_idx = master_trading_days.searchsorted(end_date, side='right')\n",
    "    \n",
    "    quality_metrics_df = calculate_rolling_quality_metrics(df_ohlcv, window=252)\n",
    "    df_close_full = df_ohlcv['Adj Close'].unstack(level=0); df_high_full = df_ohlcv['Adj High'].unstack(level=0); df_low_full = df_ohlcv['Adj Low'].unstack(level=0)\n",
    "    \n",
    "    # 3. Loop through all periods in the backtest range\n",
    "    step_indices = range(start_idx, end_idx, fwd_period)\n",
    "    all_fwd_gains, period_by_period_debug = [], {}\n",
    "\n",
    "    print(f\"Simulating {len(step_indices)} periods from {master_trading_days[step_indices[0]].date()} to {master_trading_days[step_indices[-1]].date()}...\")\n",
    "    for current_idx in tqdm(step_indices, desc=\"Backtest Progress\"):\n",
    "        step_date = master_trading_days[current_idx]\n",
    "        \n",
    "        # Apply data quality filter for the current step\n",
    "        eligible_tickers = get_eligible_universe(quality_metrics_df, step_date, quality_thresholds)\n",
    "        if not eligible_tickers: continue\n",
    "        \n",
    "        df_close_step = df_close_full[eligible_tickers]; df_high_step = df_high_full[eligible_tickers]; df_low_step = df_low_full[eligible_tickers]\n",
    "        \n",
    "        # Run a single walk-forward analysis step\n",
    "        results, debug_output = run_walk_forward_step(\n",
    "            df_close_step, df_high_step, df_low_step, master_trading_days,\n",
    "            step_date, calc_period, fwd_period,\n",
    "            metric, rank_start, rank_end, benchmark_ticker, debug=True\n",
    "        )\n",
    "        \n",
    "        # Collect results for this period\n",
    "        if results['error'] is None:\n",
    "            fwd_series = results['portfolio_series'].loc[results['actual_calc_end_ts']:]\n",
    "            all_fwd_gains.append(fwd_series.pct_change().dropna())\n",
    "            period_by_period_debug[step_date.date().isoformat()] = debug_output\n",
    "            \n",
    "    if not all_fwd_gains:\n",
    "        print(\"Error: No valid periods were simulated.\"); return None\n",
    "\n",
    "    # 4. Stitch together the results to form a continuous equity curve\n",
    "    strategy_returns = pd.concat(all_fwd_gains); strategy_equity_curve = (1 + strategy_returns).cumprod()\n",
    "    benchmark_returns = df_close_full[benchmark_ticker].pct_change().loc[strategy_equity_curve.index]; benchmark_equity_curve = (1 + benchmark_returns).cumprod()\n",
    "    cumulative_equity_df = pd.DataFrame({'Strategy_Equity': strategy_equity_curve, 'Benchmark_Equity': benchmark_equity_curve})\n",
    "    \n",
    "    # 5. Plot the final equity curve\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=cumulative_equity_df.index, y=cumulative_equity_df['Strategy_Equity'], name='Strategy', line=dict(color='green')))\n",
    "    fig.add_trace(go.Scatter(x=cumulative_equity_df.index, y=cumulative_equity_df['Benchmark_Equity'], name=f'Benchmark ({benchmark_ticker})', line=dict(color='black', dash='dash')))\n",
    "    fig.update_layout(title=f\"Cumulative Performance: '{metric}' Strategy (Top {rank_start}-{rank_end})\", xaxis_title=\"Date\", yaxis_title=\"Cumulative Growth\")\n",
    "    fig.show()\n",
    "\n",
    "    # 6. Return the detailed results for forensic analysis\n",
    "    final_backtest_results = {'cumulative_performance': cumulative_equity_df, 'period_by_period_debug': period_by_period_debug}\n",
    "    print(\"\\n✅ Full backtest complete. Results object is ready for forensic analysis.\")\n",
    "    return final_backtest_results\n",
    "\n",
    "# --- E. VERIFICATION TOOLS (User Requested) ---\n",
    "\n",
    "def verify_group_tickers_walk_forward_calculation(df_ohlcv, tickers_to_verify, benchmark_ticker,\n",
    "                                                  start_date, calc_period, fwd_period,\n",
    "                                                  master_calendar_ticker='VOO', export_csv=False):\n",
    "    \"\"\"Verifies portfolio and benchmark performance and optionally exports the data.\"\"\"\n",
    "    display(Markdown(f\"## Verification Report for Portfolio vs. Benchmark\"))\n",
    "    display(Markdown(f\"**Portfolio Tickers:** `{tickers_to_verify}`\\n**Benchmark Ticker:** `{benchmark_ticker}`\"))\n",
    "    \n",
    "    # 1. Setup trading day calendar and determine exact period dates\n",
    "    if master_calendar_ticker not in df_ohlcv.index.get_level_values(0):\n",
    "        raise ValueError(f\"Master calendar ticker '{master_calendar_ticker}' not found in DataFrame.\")\n",
    "    master_trading_days = df_ohlcv.loc[master_calendar_ticker].index.unique().sort_values()\n",
    "\n",
    "    start_date_raw = pd.to_datetime(start_date)\n",
    "    start_idx = master_trading_days.searchsorted(start_date_raw)\n",
    "    if start_idx >= len(master_trading_days):\n",
    "        print(f\"Error: Start date {start_date_raw.date()} is after the last available trading day.\"); return\n",
    "    actual_start_date = master_trading_days[start_idx]\n",
    "    \n",
    "    calc_end_idx = min(start_idx + calc_period, len(master_trading_days) - 1)\n",
    "    fwd_end_idx = min(calc_end_idx + fwd_period, len(master_trading_days) - 1)\n",
    "    \n",
    "    actual_calc_end_date = master_trading_days[calc_end_idx]\n",
    "    actual_fwd_end_date = master_trading_days[fwd_end_idx]\n",
    "    \n",
    "    display(Markdown(f\"**Analysis Start:** `{actual_start_date.date()}` (Selected: `{start_date_raw.date()}`)\\n\"\n",
    "                    f\"**Calc End:** `{actual_calc_end_date.date()}` ({calc_period} trading days)\\n\"\n",
    "                    f\"**Fwd End:** `{actual_fwd_end_date.date()}` ({fwd_period} trading days)\"))\n",
    "\n",
    "    # 2. Recreate the portfolio and benchmark series from scratch\n",
    "    df_close_full = df_ohlcv['Adj Close'].unstack(level=0)\n",
    "    portfolio_prices_raw_slice = df_close_full[tickers_to_verify].loc[actual_start_date:actual_fwd_end_date]\n",
    "    portfolio_value_series = portfolio_prices_raw_slice.div(portfolio_prices_raw_slice.bfill().iloc[0]).mean(axis=1)\n",
    "    benchmark_price_series = df_close_full.get(benchmark_ticker)\n",
    "    \n",
    "    # 3. Optionally export the underlying daily data to a CSV for external checking\n",
    "    if export_csv:\n",
    "        export_df = pd.DataFrame({\n",
    "            'Portfolio_Normalized_Price': portfolio_value_series,\n",
    "            'Portfolio_Daily_Return': portfolio_value_series.pct_change()\n",
    "        })\n",
    "        if benchmark_price_series is not None:\n",
    "            norm_bm = benchmark_price_series.loc[actual_start_date:actual_fwd_end_date]\n",
    "            norm_bm = norm_bm / norm_bm.bfill().iloc[0]\n",
    "            export_df['Benchmark_Normalized_Price'] = norm_bm\n",
    "            export_df['Benchmark_Daily_Return'] = norm_bm.pct_change()\n",
    "\n",
    "        output_dir = 'export_csv'\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        tickers_str = '_'.join(tickers_to_verify)\n",
    "        filename = f\"verify_group_{actual_start_date.date()}_{tickers_str}.csv\"\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "        export_df.to_csv(filepath)\n",
    "        print(f\"\\n✅ Data exported to: {filepath}\")\n",
    "\n",
    "    # 4. Define a helper to print detailed calculation steps\n",
    "    def print_verification_steps(title, price_series):\n",
    "        display(Markdown(f\"#### Verification for: `{title}`\"))\n",
    "        if price_series.dropna().shape[0] < 2: print(\"  - Not enough data points.\"); return {'gain': np.nan, 'sharpe': np.nan}\n",
    "        start_price, end_price = price_series.bfill().iloc[0], price_series.ffill().iloc[-1]\n",
    "        gain = (end_price / start_price) - 1\n",
    "        print(f\"Start Value ({price_series.first_valid_index().date()}): {start_price:,.4f}\\nEnd Value   ({price_series.last_valid_index().date()}): {end_price:,.4f}\\nGain = {gain:.2%}\")\n",
    "        returns = price_series.pct_change()\n",
    "        sharpe = calculate_sharpe(returns)\n",
    "        print(f\"Mean Daily Return: {returns.mean():.6f}\\nStd Dev: {returns.std():.6f}\\nSharpe = {sharpe:.2f}\")\n",
    "        return {'gain': gain, 'sharpe': sharpe}\n",
    "\n",
    "    # 5. Run verification for each period\n",
    "    display(Markdown(\"### A. Calculation Period\"))\n",
    "    perf_calc_p = print_verification_steps(\"Group Portfolio\", portfolio_value_series.loc[actual_start_date:actual_calc_end_date])\n",
    "    if benchmark_price_series is not None:\n",
    "        perf_calc_b = print_verification_steps(f\"Benchmark\", benchmark_price_series.loc[actual_start_date:actual_calc_end_date])\n",
    "    \n",
    "    display(Markdown(\"### B. Forward Period\"))\n",
    "    perf_fwd_p = print_verification_steps(\"Group Portfolio\", portfolio_value_series.loc[actual_calc_end_date:actual_fwd_end_date])\n",
    "    if benchmark_price_series is not None:\n",
    "        perf_fwd_b = print_verification_steps(f\"Benchmark\", benchmark_price_series.loc[actual_calc_end_date:actual_fwd_end_date])\n",
    "\n",
    "def verify_ticker_ranking_metrics(df_ohlcv, ticker, start_date, calc_period,\n",
    "                                  master_calendar_ticker='VOO', export_csv=False):\n",
    "    \"\"\"Verifies ranking metrics for a single ticker and optionally exports the data.\"\"\"\n",
    "    display(Markdown(f\"## Verification Report for Ticker Ranking: `{ticker}`\"))\n",
    "    \n",
    "    # 1. Setup trading day calendar and determine exact period dates\n",
    "    if master_calendar_ticker not in df_ohlcv.index.get_level_values(0):\n",
    "        raise ValueError(f\"Master calendar ticker '{master_calendar_ticker}' not found in DataFrame.\")\n",
    "    master_trading_days = df_ohlcv.loc[master_calendar_ticker].index.unique().sort_values()\n",
    "\n",
    "    start_date_raw = pd.to_datetime(start_date)\n",
    "    start_idx = master_trading_days.searchsorted(start_date_raw)\n",
    "    if start_idx >= len(master_trading_days):\n",
    "        print(f\"Error: Start date {start_date_raw.date()} is after the last available trading day.\"); return\n",
    "    actual_start_date = master_trading_days[start_idx]\n",
    "    \n",
    "    calc_end_idx = min(start_idx + calc_period, len(master_trading_days) - 1)\n",
    "    actual_calc_end_date = master_trading_days[calc_end_idx]\n",
    "\n",
    "    # 2. Extract and prepare the raw data for the specific ticker and period\n",
    "    df_ticker = df_ohlcv.loc[ticker].sort_index()\n",
    "    calc_df = df_ticker.loc[actual_start_date:actual_calc_end_date].copy()\n",
    "    if calc_df.empty or len(calc_df) < 2: \n",
    "        print(\"No data or not enough data in calc period.\"); return\n",
    "\n",
    "    display(Markdown(\"### A. Calculation Period (for Ranking Metrics)\"))\n",
    "    display(Markdown(f\"**Period Start:** `{actual_start_date.date()}`\\n\"\n",
    "                    f\"**Period End:** `{actual_calc_end_date.date()}`\\n\"\n",
    "                    f\"**Total Trading Days:** `{len(calc_df)}` (Requested: `{calc_period}`)\"))\n",
    "    \n",
    "    display(Markdown(\"#### Detailed Metric Calculation Data\"))\n",
    "    \n",
    "    # 3. Calculate all intermediate metrics as new columns for full transparency\n",
    "    vdf = calc_df[['Adj High', 'Adj Low', 'Adj Close']].copy()\n",
    "    vdf['Daily_Return'] = vdf['Adj Close'].pct_change()\n",
    "    \n",
    "    # Corrected True Range (TR) calculation for a single ticker (Series)\n",
    "    tr_df = pd.DataFrame({\n",
    "        'h_l': vdf['Adj High'] - vdf['Adj Low'],\n",
    "        'h_cp': abs(vdf['Adj High'] - vdf['Adj Close'].shift(1)),\n",
    "        'l_cp': abs(vdf['Adj Low'] - vdf['Adj Close'].shift(1))\n",
    "    })\n",
    "    vdf['TR'] = tr_df.max(axis=1)\n",
    "    \n",
    "    vdf['ATR_14'] = vdf['TR'].ewm(alpha=1/14, adjust=False).mean()\n",
    "    vdf['ATRP'] = vdf['ATR_14'] / vdf['Adj Close']\n",
    "    \n",
    "    print(\"--- Start of Calculation Period ---\")\n",
    "    display(vdf.head())\n",
    "    print(\"\\n--- End of Calculation Period ---\")\n",
    "    display(vdf.tail())\n",
    "\n",
    "    # 4. Optionally export this detailed breakdown to CSV\n",
    "    if export_csv:\n",
    "        output_dir = 'export_csv'\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        filename = f\"verify_ticker_{actual_start_date.date()}_{ticker}.csv\"\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "        vdf.to_csv(filepath)\n",
    "        print(f\"\\n✅ Data exported to: {filepath}\")\n",
    "    \n",
    "    # 5. Print final metric calculations with formulas\n",
    "    display(Markdown(\"#### `MetricValue` Verification Summary:\"))\n",
    "    \n",
    "    calc_start_price = vdf['Adj Close'].bfill().iloc[0]\n",
    "    calc_end_price = vdf['Adj Close'].ffill().iloc[-1]\n",
    "    price_metric = (calc_end_price / calc_start_price)\n",
    "    print(f\"1. Price Metric: (Last Price / First Price) = ({calc_end_price:.2f} / {calc_start_price:.2f}) = {price_metric:.4f}\")\n",
    "    \n",
    "    daily_returns = vdf['Daily_Return'].dropna()\n",
    "    sharpe_ratio = calculate_sharpe(daily_returns)\n",
    "    print(f\"2. Sharpe Metric: (Mean Daily Return / Std Dev) * sqrt(252) = {sharpe_ratio:.4f}\")\n",
    "\n",
    "    atrp_mean = vdf['ATRP'].mean()\n",
    "    mean_daily_return = vdf['Daily_Return'].mean()\n",
    "    sharpe_atr = (mean_daily_return / atrp_mean) if atrp_mean > 0 else 0\n",
    "    print(f\"3. Sharpe (ATR) Metric: (Mean Daily Return / Mean ATRP) = ({mean_daily_return:.6f} / {atrp_mean:.6f}) = {sharpe_atr:.4f}\")\n",
    "\n",
    "# --- F. AUTOMATION SCRIPT - STRATEGY SEARCH ---\n",
    "\n",
    "def run_strategy_search(df_ohlcv, config):\n",
    "    \"\"\"\n",
    "    Runs the main backtesting loop with checkpointing to be resumable.\n",
    "    \"\"\"\n",
    "    start_time = time.time() # <-- This now works because of 'import time'\n",
    "    \n",
    "    # --- 1. SETUP & LOAD PROGRESS ---\n",
    "    print(\"--- Phase 1: Pre-processing and Loading Progress ---\")\n",
    "    quality_metrics_df = calculate_rolling_quality_metrics(df_ohlcv, window=252)\n",
    "    print(\"Unstacking data for performance...\")\n",
    "    df_close_full = df_ohlcv['Adj Close'].unstack(level=0)\n",
    "    df_high_full = df_ohlcv['Adj High'].unstack(level=0)\n",
    "    df_low_full = df_ohlcv['Adj Low'].unstack(level=0)\n",
    "    \n",
    "    master_calendar_ticker = config['master_calendar_ticker']\n",
    "    master_trading_days = df_ohlcv.loc[master_calendar_ticker].index.unique().sort_values()\n",
    "    print(f\"Master trading day calendar created from '{master_calendar_ticker}' ({len(master_trading_days)} days).\")\n",
    "\n",
    "    results_path = config['results_output_path']\n",
    "    completed_params = set()\n",
    "    \n",
    "    if os.path.exists(results_path): # <-- This now works because of 'import os'\n",
    "        print(f\"Found existing results file. Loading progress from: {results_path}\")\n",
    "        df_progress = pd.read_csv(results_path)\n",
    "        for _, row in df_progress.iterrows():\n",
    "            param_key = (\n",
    "                row['calc_period'], row['fwd_period'], row['metric'],\n",
    "                (row['rank_start'], row['rank_end'])\n",
    "            )\n",
    "            completed_params.add(param_key)\n",
    "        print(f\"Found {len(completed_params)} completed parameter sets to skip.\")\n",
    "    else:\n",
    "        print(\"No existing results file found. Starting a new run.\")\n",
    "\n",
    "    print(\"✅ Pre-processing complete.\\n\")\n",
    "\n",
    "    # --- 2. SETUP THE MAIN LOOP ---\n",
    "    print(\"--- Phase 2: Setting up Simulation Loops ---\")\n",
    "    \n",
    "    param_combinations = list(product(\n",
    "        config['calc_periods'], config['fwd_periods'],\n",
    "        config['metrics'], config['rank_slices']\n",
    "    ))\n",
    "    \n",
    "    search_start_date = pd.to_datetime(config['search_start_date'])\n",
    "    search_end_date = pd.to_datetime(config['search_end_date'])\n",
    "    start_idx = master_trading_days.searchsorted(search_start_date, side='left')\n",
    "    end_idx = master_trading_days.searchsorted(search_end_date, side='right')\n",
    "\n",
    "    step_dates_map = {}\n",
    "    print(\"Pre-calculating rebalancing schedules for each holding period...\")\n",
    "    for fwd_period in sorted(config['fwd_periods']):\n",
    "        step_indices = range(start_idx, end_idx, fwd_period)\n",
    "        step_dates_map[fwd_period] = master_trading_days[step_indices]\n",
    "        print(f\"  - Holding Period {fwd_period} days: {len(step_dates_map[fwd_period])} rebalances\")\n",
    "    \n",
    "    print(f\"Found {len(param_combinations)} total parameter sets to simulate.\")\n",
    "    print(\"✅ Setup complete. Starting main loop...\\n\")\n",
    "\n",
    "    # --- 3. RUN THE MAIN LOOP ---\n",
    "    print(\"--- Phase 3: Running Simulations ---\")\n",
    "    pbar = tqdm(param_combinations, desc=\"Parameter Sets\")\n",
    "    \n",
    "    for params in pbar:\n",
    "        calc_period, fwd_period, metric, rank_slice = params\n",
    "        rank_start, rank_end = rank_slice\n",
    "        \n",
    "        param_key = (calc_period, fwd_period, metric, rank_slice)\n",
    "        if param_key in completed_params:\n",
    "            pbar.set_description(f\"Skipping {param_key}\")\n",
    "            continue\n",
    "\n",
    "        pbar.set_description(f\"Running {param_key}\")\n",
    "        \n",
    "        current_params_results = []\n",
    "        \n",
    "        # ==============================================================================\n",
    "        # --- FIX: RESTORED THE MISSING INNER LOOP ---\n",
    "        # ==============================================================================\n",
    "        current_step_dates = step_dates_map[fwd_period]\n",
    "        for step_date in current_step_dates:\n",
    "            eligible_tickers = get_eligible_universe(\n",
    "                quality_metrics_df, filter_date=step_date, thresholds=config['quality_thresholds']\n",
    "            )\n",
    "            if not eligible_tickers: continue\n",
    "            \n",
    "            df_close_step = df_close_full[eligible_tickers]\n",
    "            df_high_step = df_high_full[eligible_tickers]\n",
    "            df_low_step = df_low_full[eligible_tickers]\n",
    "\n",
    "            step_result, _ = run_walk_forward_step(\n",
    "                df_close_full=df_close_step, df_high_full=df_high_step, df_low_full=df_low_step,\n",
    "                master_trading_days=master_trading_days, start_date=step_date,\n",
    "                calc_period=calc_period, fwd_period=fwd_period,\n",
    "                metric=metric, rank_start=rank_start, rank_end=rank_end,\n",
    "                benchmark_ticker=config['benchmark_ticker'], debug=False\n",
    "            )\n",
    "            \n",
    "            if step_result['error'] is None:\n",
    "                p = step_result['performance_data']\n",
    "                log_entry = {\n",
    "                    'step_date': step_date.date(), 'calc_period': calc_period,\n",
    "                    'fwd_period': fwd_period, 'metric': metric,\n",
    "                    'rank_start': rank_start, 'rank_end': rank_end,\n",
    "                    'num_universe': len(eligible_tickers),\n",
    "                    'num_portfolio': len(step_result['tickers_to_display']),\n",
    "                    'fwd_p_gain': p['fwd_p_gain'], 'fwd_b_gain': p['fwd_b_gain'],\n",
    "                    'fwd_gain_delta': p['fwd_p_gain'] - p['fwd_b_gain'] if not np.isnan(p['fwd_b_gain']) else np.nan,\n",
    "                    'fwd_p_sharpe': p['fwd_p_sharpe'],\n",
    "                }\n",
    "                current_params_results.append(log_entry)\n",
    "        # ==============================================================================\n",
    "        \n",
    "        # --- CHECKPOINTING: INCREMENTAL SAVE ---\n",
    "        if current_params_results:\n",
    "            df_to_append = pd.DataFrame(current_params_results)\n",
    "            df_to_append.to_csv(\n",
    "                results_path,\n",
    "                mode='a',\n",
    "                header=not os.path.exists(results_path),\n",
    "                index=False\n",
    "            )\n",
    "            completed_params.add(param_key)\n",
    "\n",
    "    print(\"✅ Main loop finished.\\n\")\n",
    "    \n",
    "    # --- 4. RETURN FINAL DATAFRAME ---\n",
    "    print(\"--- Phase 4: Loading Final Results ---\")\n",
    "    if os.path.exists(results_path):\n",
    "        final_df = pd.read_csv(results_path)\n",
    "        end_time = time.time()\n",
    "        print(f\"✅ Process complete. Total execution time: {time.time() - start_time:.2f} seconds.\")\n",
    "        return final_df\n",
    "    else:\n",
    "        print(\"Warning: No results were generated.\")\n",
    "        return None\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47ed713",
   "metadata": {},
   "source": [
    "#### **CELL 3: DATA LOADING**\n",
    "*This cell loads your main dataset using the environment-aware path.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97e54a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# --- CELL 3: DATA LOADING ---\n",
    "# ==============================================================================\n",
    "import pandas as pd\n",
    "\n",
    "data_file_path = env_config['data_path']\n",
    "print(f\"Attempting to load data from: {data_file_path}\")\n",
    "\n",
    "try:\n",
    "    df_OHLCV = pd.read_parquet(data_file_path, engine='pyarrow')\n",
    "    df_dev = df_OHLCV.copy() # Use df_dev for development as a good practice\n",
    "    \n",
    "    print(\"\\n✅ Data loaded successfully.\")\n",
    "    print(\"\\n--- DataFrame Info ---\")\n",
    "    df_dev.info()\n",
    "except FileNotFoundError:\n",
    "    print(f\"\\n❌ ERROR: FILE NOT FOUND at {data_file_path}. Please check paths in Cell 1.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b16e2e",
   "metadata": {},
   "source": [
    "#### **CELL 4: BOT STRATEGY CONFIGURATION**\n",
    "*This cell defines the strategy parameters you want to test.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549adf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# --- CELL 4: BOT STRATEGY CONFIGURATION ---\n",
    "# ==============================================================================\n",
    "from itertools import product\n",
    "import pandas as pd\n",
    "\n",
    "# --- PRIMARY USER INPUTS FOR THE STRATEGY ---\n",
    "# 5,  21, 42, 63, 126, 252\n",
    "# 1W, 1M, 2M, 3M,  6M,  1Y\n",
    "HOLDING_PERIODS_DAYS = [63]        # Test ~2, and 3 month holding periods\n",
    "CALC_PERIODS_DAYS = [252]         # Use ~6 and 12 month lookbacks\n",
    "\n",
    "bot_config = {\n",
    "    # --- Time Parameters ---\n",
    "    'search_start_date': '2014-01-01',\n",
    "    'search_end_date': '2018-12-31',\n",
    "    \n",
    "    # --- Strategy Parameters (The Search Grid) ---\n",
    "    'calc_periods': CALC_PERIODS_DAYS,\n",
    "    'fwd_periods': HOLDING_PERIODS_DAYS,\n",
    "\n",
    "\n",
    "    # 'metrics': ['Sharpe', 'Sharpe (ATR)'],\n",
    "    'metrics': ['Price', 'Sharpe', 'Sharpe (ATR)'],    \n",
    "        \n",
    "    'rank_slices': [(1, 5)],\n",
    "\n",
    "    # --- Data Quality ---\n",
    "    'quality_thresholds': { 'min_median_dollar_volume': 10_000_000, \n",
    "                            'max_stale_pct': 0.05, \n",
    "                            'max_same_vol_count': 1 },\n",
    "\n",
    "    # --- General Parameters ---\n",
    "    'benchmark_ticker': 'VOO',\n",
    "    'master_calendar_ticker': 'VOO',\n",
    "    'results_output_path': env_config['results_path']\n",
    "}\n",
    "\n",
    "print(\"\\n--- Bot Configuration Initialized ---\")\n",
    "print(f\"Calculation Periods to Test: {bot_config['calc_periods']} trading days\")\n",
    "print(f\"Forward and Holding Periods to Test (Forward and Holding Periods are the same): {bot_config['fwd_periods']} trading days\")\n",
    "print(f\"Results will be saved to: {bot_config['results_output_path']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df014e8e",
   "metadata": {},
   "source": [
    "#### **CELL 5: EXECUTION**\n",
    "*This is the final cell that runs the backtest and displays the results.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7485bf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# --- CELL 5: EXECUTION ---\n",
    "# ==============================================================================\n",
    "\n",
    "# --- Execute the Bot ---\n",
    "dev_results_df = run_strategy_search(df_dev, bot_config)\n",
    "\n",
    "# --- Display a sample of the results ---\n",
    "if dev_results_df is not None:\n",
    "    print(\"\\n--- Sample of Generated Results ---\")\n",
    "    display(dev_results_df.head())\n",
    "    print(\"\\n--- Analysis of Best Performing Strategies ---\")\n",
    "    display(dev_results_df.groupby(['calc_period', 'fwd_period', 'metric', 'rank_start', 'rank_end'])['fwd_gain_delta'].mean().sort_values(ascending=False).to_frame())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3dc0a12",
   "metadata": {},
   "source": [
    "### 4. Next Steps & Future Improvements\n",
    "\n",
    "This system is a powerful foundation. Here are potential areas for future development:\n",
    "1.  **Advanced Performance Analytics:** Create a new notebook or function to analyze the output CSV, calculating metrics like Max Drawdown, Calmar Ratio, and generating equity curves for the best strategies.\n",
    "2.  **Visualization:** Build heatmaps and other plots to visualize how different parameters (e.g., `calc_period` vs. `fwd_period`) affect performance.\n",
    "3.  **Realism:** Incorporate transaction costs and slippage into the performance calculations for a more realistic backtest.\n",
    "4.  **Configuration Management:** For even more complex tests, move the `bot_config` dictionary into a separate `config.py` file to keep the notebook cleaner.\n",
    "\n",
    "It has been a genuine pleasure working with you on this. You've built an impressive and professional-grade tool. I wish you the very best with your continued research and development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05feecc8",
   "metadata": {},
   "source": [
    "### 4. Plot an export_csv Row to Check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5979f82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_to_check = 59\n",
    "row_values =  dev_results_df.loc[row_to_check ].to_dict()\n",
    "print(f'export_csv values for row {row_to_check}:\\n')\n",
    "for k, v in row_values.items():\n",
    "    print(f'{k:<15}: {v}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e2012e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_container, debug_container = plot_walk_forward_analyzer(\n",
    "    df_ohlcv=df_dev,\n",
    "    # df_ohlcv=df_OHLCV,    \n",
    "    default_start_date=row_values['step_date'],\n",
    "    \n",
    "    default_calc_period=row_values['calc_period'],\n",
    "    default_fwd_period=row_values['fwd_period'],\n",
    "    # default_calc_period=120,\n",
    "    # default_fwd_period=30,\n",
    "\n",
    "    default_metric=row_values['metric'],\n",
    "\n",
    "    default_rank_start=row_values['rank_start'],\n",
    "    default_rank_end=row_values['rank_end'],\n",
    "    # default_rank_start=2,\n",
    "    # default_rank_end=3,    \n",
    "\n",
    "    default_benchmark_ticker='VOO',\n",
    "    quality_thresholds=bot_config['quality_thresholds'],\n",
    "    debug=True  # <-- Activate the new mode!\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44da0593",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_nested(results_container)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa15885",
   "metadata": {},
   "source": [
    "# CHECK Metric_Sharpe (ATR) for Calc. Period has error.\n",
    "- start date 2018-10-03\n",
    "- Calc Period 252\n",
    "- Fwd Period 63\n",
    "- Metric Sharpe ATR\n",
    "- Rank Start 1\n",
    "- Rank End 5\n",
    "-- Analysis Period 2018-10-03 to 2020-01-06 (this is full period, calc + fwd)\n",
    "-- [BIL, MINT, SHV, BNDX, VCSH]\n",
    "-- Metric_Sharpe (ATR) for BIL, MINT, SHV, BNDX matched my own calculation at bottom cell\n",
    "-- Metric Sharpe (ATR) for VCSH is a bit off (code calc 0.194195, my calc 0.196112)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed494d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_nested(debug_container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91056576",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = results_container[0]\n",
    "\n",
    "print_nested(results_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6fa808",
   "metadata": {},
   "source": [
    "### Get Plot Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2201125e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_tickers = results_dict['tickers_to_display']\n",
    "_calc_start = results_dict['calc_period_start']\n",
    "_calc_end = results_dict['calc_period_end']\n",
    "_fwd_start = results_dict['forward_period_start']\n",
    "_fwd_end = results_dict['forward_period_end']\n",
    "_calc_period = results_dict['calc_period']\n",
    "_fwd_period = results_dict['fwd_period']\n",
    "_benchmark_ticker = results_dict['benchmark_ticker']\n",
    "\n",
    "print(f'_tickers: {_tickers}')\n",
    "print(f'_calc_start: {_calc_start}')\n",
    "print(f'_calc_end: {_calc_end}')\n",
    "print(f'_fwd_start: {_fwd_start}')\n",
    "print(f'_fwd_end: {_fwd_end}')\n",
    "print(f'_calc_period: {_calc_period}')\n",
    "print(f'_fwd_period: {_fwd_period}')\n",
    "print(f'_benchmark: {_benchmark_ticker}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe967a4",
   "metadata": {},
   "source": [
    "### Run verify_ticker_ranking_metrics with Plot Parameters to Check Calc. Period Calculation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf24d825",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ticker in _tickers:\n",
    "    verify_ticker_ranking_metrics(df_OHLCV, \n",
    "                                  ticker=_ticker, \n",
    "                                  start_date=_calc_start, \n",
    "                                  calc_period=_calc_period,\n",
    "                                  master_calendar_ticker=_benchmark_ticker, \n",
    "                                  export_csv=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e6ca72",
   "metadata": {},
   "source": [
    "### My Own Check on Calculation for One Ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9798de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Check Calculation for a Ticker -- #\n",
    "_check_ticker = _tickers[4]\n",
    "print(f'Check calculation for this ticker: {_check_ticker}')\n",
    "\n",
    "# Get Ticker's OHLCV Data -- # \n",
    "_df = df_OHLCV.loc[_check_ticker][_calc_start:_fwd_end]\n",
    "\n",
    "# -- Calculate Daily Return -- #\n",
    "_df['Daily_Return'] = _df['Adj Close'].pct_change()\n",
    "\n",
    "# -- Calculate True Range -- #\n",
    "_df['TR'] = pd.concat([\n",
    "    _df['Adj High'] - _df['Adj Low'],\n",
    "    (_df['Adj High'] - _df['Adj Close'].shift(1)).abs(),\n",
    "    (_df['Adj Low']  - _df['Adj Close'].shift(1)).abs()\n",
    "], axis=1).max(axis=1)\n",
    "\n",
    "# -- Calculate Average True Range (14 day period) -- #\n",
    "window = 14\n",
    "_df['ATR_14'] = pd.NA\n",
    "\n",
    "# Seed the very first ATR value with the first non-NaN TR\n",
    "first_idx = _df['TR'].first_valid_index()\n",
    "_df.loc[first_idx, 'ATR_14'] = _df.loc[first_idx, 'TR']\n",
    "\n",
    "# Iteratively apply the Wilder smoothing formula\n",
    "for i in range(_df.index.get_loc(first_idx) + 1, len(_df)):\n",
    "    prev_atr = _df.iloc[i-1]['ATR_14']\n",
    "    curr_tr  = _df.iloc[i]['TR']\n",
    "    _df.iloc[i, _df.columns.get_loc('ATR_14')] = (prev_atr * (window - 1) + curr_tr) / window\n",
    "\n",
    "# -- Calculate ATRP -- #\n",
    "_df['ATRP'] = _df['ATR_14'] / _df['Adj Close']\n",
    "\n",
    "calc_pd_df = _df.loc[_calc_start:_calc_end]\n",
    "fwd_pd_df = _df.loc[_fwd_start:_fwd_end]\n",
    "print(f'Calc. Period:\\n{calc_pd_df.head()}\\n{calc_pd_df.tail()}')\n",
    "print(f'\\nFwd. Period:\\n{fwd_pd_df.head()}\\n{fwd_pd_df.tail()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17089b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Metric Calculation for Ticker: {_check_ticker}')\n",
    "\n",
    "# -- Calculation for Period Gain -- #\n",
    "full_period_gain = _df['Adj Close'][_fwd_end] / _df['Adj Close'][_calc_start]\n",
    "calc_period_gain = _df['Adj Close'][_calc_end] / _df['Adj Close'][_calc_start]\n",
    "fwd_period_gain = _df['Adj Close'][_fwd_end] / _df['Adj Close'][_fwd_start]\n",
    "\n",
    "print(f'\\nfull_period_gain: {full_period_gain:.4f}')\n",
    "print(f'calc_period_gain: {calc_period_gain:.4f}')\n",
    "print(f'fwd_period_gain: {fwd_period_gain:.4f}')\n",
    "\n",
    "# -- Calculation for Period Sharpe -- #\n",
    "full_period_return = _df['Daily_Return'][_calc_start:_fwd_end]\n",
    "calc_period_return = _df['Daily_Return'][_calc_start:_calc_end]\n",
    "fwd_period_return = _df['Daily_Return'][_fwd_start:_fwd_end]\n",
    "\n",
    "full_sharpe = full_period_return.mean() / full_period_return.std() * (252 ** 0.5)\n",
    "calc_sharpe = calc_period_return.mean() / calc_period_return.std() * (252 ** 0.5)\n",
    "fwd_sharpe = fwd_period_return.mean() / fwd_period_return.std() * (252 ** 0.5)\n",
    "\n",
    "print(f'\\nfull_sharpe: {full_sharpe:.4f}')\n",
    "print(f'calc_sharpe: {calc_sharpe:.4f}')\n",
    "print(f'fwd_sharpe: {fwd_sharpe:.4f}')\n",
    "\n",
    "# -- Calculation for Period Sharpe ATR -- #\n",
    "full_sharpe_ATR = full_period_return.mean() / _df['ATRP'][_calc_start:_fwd_end].mean()\n",
    "calc_sharpe_ATR = calc_period_return.mean() / _df['ATRP'][_calc_start:_calc_end].mean()\n",
    "fwd_sharpe_ATR = fwd_period_return.mean() / _df['ATRP'][_fwd_start:_fwd_end].mean()\n",
    "\n",
    "print(f'\\nfull_sharpe_ATR: {full_sharpe_ATR:.4f}')\n",
    "print(f'calc_sharpe_ATR: {calc_sharpe_ATR:.4f}')\n",
    "print(f'fwd_sharpe_ATR: {fwd_sharpe_ATR:.4f}')\n",
    "\n",
    "print(f'\\ncalc_period_return.mean(): {calc_period_return.mean()}')\n",
    "print(f\"_df['ATRP'][_calc_start:_calc_end].mean(): {_df['ATRP'][_calc_start:_calc_end].mean()}\")\n",
    "print(f'calc_sharpe_ATR: {calc_sharpe_ATR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa20ef8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afb0cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_OHLCV.loc['VCSH'].copy()\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9751f576",
   "metadata": {},
   "outputs": [],
   "source": [
    "_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc927f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Check Calculation for a Ticker -- #\n",
    "_check_ticker = _tickers[4]\n",
    "print(f'Check calculation for this ticker: {_check_ticker}')\n",
    "\n",
    "# Get Ticker's OHLCV Data -- # \n",
    "_df = df_OHLCV.loc[_check_ticker]['2018-09-30' : '2020-01-10']\n",
    "# _df = df_OHLCV.loc[_check_ticker].copy()\n",
    "\n",
    "# -- Calculate Daily Return -- #\n",
    "_df['Daily_Return'] = _df['Adj Close'].pct_change()\n",
    "\n",
    "# -- Calculate True Range -- #\n",
    "_df['TR'] = pd.concat([\n",
    "    _df['Adj High'] - _df['Adj Low'],\n",
    "    (_df['Adj High'] - _df['Adj Close'].shift(1)).abs(),\n",
    "    (_df['Adj Low']  - _df['Adj Close'].shift(1)).abs()\n",
    "], axis=1).max(axis=1)\n",
    "\n",
    "# -- Calculate Average True Range (14 day period) -- #\n",
    "window = 14\n",
    "_df['ATR_14'] = pd.NA\n",
    "\n",
    "# Seed the very first ATR value with the first non-NaN TR\n",
    "first_idx = _df['TR'].first_valid_index()\n",
    "_df.loc[first_idx, 'ATR_14'] = _df.loc[first_idx, 'TR']\n",
    "\n",
    "# Iteratively apply the Wilder smoothing formula\n",
    "for i in range(_df.index.get_loc(first_idx) + 1, len(_df)):\n",
    "    prev_atr = _df.iloc[i-1]['ATR_14']\n",
    "    curr_tr  = _df.iloc[i]['TR']\n",
    "    _df.iloc[i, _df.columns.get_loc('ATR_14')] = (prev_atr * (window - 1) + curr_tr) / window\n",
    "\n",
    "# -- Calculate ATRP -- #\n",
    "_df['ATRP'] = _df['ATR_14'] / _df['Adj Close']\n",
    "\n",
    "# calc_pd_df = _df.loc[_calc_start:_calc_end]\n",
    "# fwd_pd_df = _df.loc[_fwd_start:_fwd_end]\n",
    "# print(f'Calc. Period:\\n{calc_pd_df.head()}\\n{calc_pd_df.tail()}')\n",
    "# print(f'\\nFwd. Period:\\n{fwd_pd_df.head()}\\n{fwd_pd_df.tail()}')\n",
    "\n",
    "_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef92146",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_OHLCV.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d72826",
   "metadata": {},
   "source": [
    "### Below Cells Follows the Code to Calculate Sharpe (ATR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7392c626",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ohlcv = df_OHLCV.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689f93e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_to_check = 'VCSH' # <--- CHANGE THIS TO YOUR ACTUAL TICKER\n",
    "start_date_raw = pd.to_datetime('2018-10-03')\n",
    "calc_period_days = 252\n",
    "\n",
    "# We need the master trading day calendar, just like the code uses.\n",
    "# It's essential for getting the dates exactly right.\n",
    "master_calendar_ticker = 'VOO' # Or another reliable ticker like 'SPY'\n",
    "master_trading_days = df_ohlcv.loc[master_calendar_ticker].index.unique().sort_values()\n",
    "\n",
    "# Isolate the full history for the ticker we're checking\n",
    "df_ticker_full = df_ohlcv.loc[ticker_to_check]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa61dd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ticker_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b0aed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the index for our start date\n",
    "start_idx = master_trading_days.searchsorted(start_date_raw)\n",
    "actual_start_date = master_trading_days[start_idx]\n",
    "\n",
    "# Find the index for the end of the calculation period\n",
    "calc_end_idx = min(start_idx + calc_period_days, len(master_trading_days) - 1)\n",
    "actual_calc_end_date = master_trading_days[calc_end_idx]\n",
    "\n",
    "print(f\"Raw Start Date: {start_date_raw.date()}\")\n",
    "print(f\"Actual Start Date (Trading Day): {actual_start_date.date()}\")\n",
    "print(f\"Actual Calc End Date (Trading Day): {actual_calc_end_date.date()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb15d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slice the ticker's data to the exact date range\n",
    "calc_df = df_ticker_full.loc[actual_start_date:actual_calc_end_date].copy()\n",
    "\n",
    "print(f\"Number of rows in calc_df: {len(calc_df)}\")\n",
    "print(\"--- First 3 rows of our calculation data ---\")\n",
    "display(calc_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f1eb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_df['Daily_Return'] = calc_df['Adj Close'].pct_change()\n",
    "mean_daily_return = calc_df['Daily_Return'].mean()\n",
    "\n",
    "print(f\"Mean Daily Return: {mean_daily_return:.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede641d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4a. Get the previous day's close for every day in our calc period.\n",
    "# This is done by shifting the FULL history, then slicing.\n",
    "prev_close_series = df_ticker_full['Adj Close'].shift(1).loc[calc_df.index]\n",
    "\n",
    "# 4b. MY HYPOTHESIS: The first value of this series is NaN. Let's check.\n",
    "print(\"--- Previous Day's Close (first 3 days) ---\")\n",
    "print(prev_close_series.head(3))\n",
    "print(\"\\n\")\n",
    "\n",
    "# 4c. Now calculate the three components of TR\n",
    "component1 = calc_df['Adj High'] - calc_df['Adj Low']\n",
    "component2 = abs(calc_df['Adj High'] - prev_close_series)\n",
    "component3 = abs(calc_df['Adj Low'] - prev_close_series)\n",
    "\n",
    "# 4d. Combine them to get the daily TR value\n",
    "tr_df = pd.DataFrame({'c1': component1, 'c2': component2, 'c3': component3})\n",
    "calc_df['TR'] = tr_df.max(axis=1)\n",
    "\n",
    "# # Change TR to High - Low for the row 0\n",
    "# calc_df.loc[calc_df.index[0], 'TR'] = (\n",
    "#     calc_df.loc[calc_df.index[0], 'Adj High'] -\n",
    "#     calc_df.loc[calc_df.index[0], 'Adj Low']\n",
    "# )\n",
    "\n",
    "print(\"--- Final TR values (first 3 days) ---\")\n",
    "print(calc_df[['Adj Close', 'TR']].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4c7716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the ATR using the exact same parameters as the code\n",
    "calc_df['ATR_14'] = calc_df['TR'].ewm(alpha=1/14, adjust=False).mean()\n",
    "\n",
    "print(\"--- TR vs ATR_14 (first 5 days) ---\")\n",
    "display(calc_df[['TR', 'ATR_14']].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab393be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ATRP is the ATR divided by the close price\n",
    "calc_df['ATRP'] = calc_df['ATR_14'] / calc_df['Adj Close']\n",
    "\n",
    "# The final denominator is the mean of the daily ATRP values over the period\n",
    "atrp_mean = calc_df['ATRP'].mean()\n",
    "\n",
    "print(f\"Mean ATRP (Denominator): {atrp_mean:.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1990be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_sharpe_atr_manual = mean_daily_return / atrp_mean\n",
    "\n",
    "print(\"--- FINAL VERIFICATION ---\")\n",
    "print(f\"Numerator (MeanDailyReturn): {mean_daily_return:.8f}\")\n",
    "print(f\"Denominator (ATRP_Mean):     {atrp_mean:.8f}\")\n",
    "print(f\"Calculated Sharpe (ATR):     {metric_sharpe_atr_manual:.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
