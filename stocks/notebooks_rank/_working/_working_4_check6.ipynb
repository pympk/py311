{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 3000)\n",
    "\n",
    "# DATA_DIR = r'c:\\Users\\ping\\Files_win10\\python\\py311\\stocks\\data'\n",
    "# Manually construct the full path before loading\n",
    "full_file_path = r'c:\\Users\\ping\\Files_win10\\python\\py311\\stocks\\data\\df_OHLCV_clean_stocks_etfs.parquet'\n",
    "df_OHLCV = pd.read_parquet(full_file_path, engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique trading dates in dataset: 250\n",
      "The data will be split on the date: 2025-05-30\n",
      "\n",
      "--- Verification ---\n",
      "Original DataFrame shape: (371250, 5)\n",
      "Training set shape:   (261360, 5)\n",
      "Testing set shape:    (109890, 5)\n",
      "\n",
      "Date Ranges:\n",
      "  Training: 2024-09-17 to 2025-05-30\n",
      "  Testing:  2025-06-02 to 2025-09-16\n",
      "\n",
      "Verification successful: There is no date overlap between train and test sets.\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Find the Chronological Split Point ---\n",
    "\n",
    "# Get all unique dates from the index and sort them\n",
    "unique_dates = df_OHLCV.index.get_level_values('Date').unique().sort_values()\n",
    "\n",
    "# Determine the index for the 70% split\n",
    "split_index = int(len(unique_dates) * 0.7)\n",
    "\n",
    "# Find the actual date at that split index\n",
    "split_date = unique_dates[split_index]\n",
    "\n",
    "print(f\"Total unique trading dates in dataset: {len(unique_dates)}\")\n",
    "print(f\"The data will be split on the date: {split_date.date()}\")\n",
    "\n",
    "# --- 2. Create the Training and Testing Sets ---\n",
    "\n",
    "# The training set includes all data UP TO and INCLUDING the split_date\n",
    "df_train = df_OHLCV[df_OHLCV.index.get_level_values('Date') <= split_date]\n",
    "\n",
    "# The testing set includes all data AFTER the split_date\n",
    "df_test = df_OHLCV[df_OHLCV.index.get_level_values('Date') > split_date]\n",
    "\n",
    "\n",
    "# --- 3. Verify the Split ---\n",
    "\n",
    "print(\"\\n--- Verification ---\")\n",
    "print(f\"Original DataFrame shape: {df_OHLCV.shape}\")\n",
    "print(f\"Training set shape:   {df_train.shape}\")\n",
    "print(f\"Testing set shape:    {df_test.shape}\")\n",
    "\n",
    "print(\"\\nDate Ranges:\")\n",
    "print(f\"  Training: {df_train.index.get_level_values('Date').min().date()} to {df_train.index.get_level_values('Date').max().date()}\")\n",
    "print(f\"  Testing:  {df_test.index.get_level_values('Date').min().date()} to {df_test.index.get_level_values('Date').max().date()}\")\n",
    "\n",
    "# Final check to ensure no overlap\n",
    "assert df_train.index.get_level_values('Date').max() < df_test.index.get_level_values('Date').min()\n",
    "print(\"\\nVerification successful: There is no date overlap between train and test sets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jupyter Notebook: Verifying `analyze_ticker_trends_vectorized`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cell 1: Setup and Imports\n",
    "\n",
    "First, let's import the necessary libraries and define both versions of the function we want to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verification functions are defined.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import linregress\n",
    "\n",
    "# --- Function 1: The NEW, Correct, Log-Vectorized Version (The one we are verifying) ---\n",
    "\n",
    "def analyze_ticker_trends_log_vectorized(df_group, lookback_days=60):\n",
    "    \"\"\"\n",
    "    Vectorized analysis of trends, including penalty scores and the underlying\n",
    "    volatility metrics used to calculate them.\n",
    "    \"\"\"\n",
    "    if len(df_group) < lookback_days:\n",
    "        return None \n",
    "\n",
    "    time_index = pd.Series(np.arange(len(df_group)), index=df_group.index)\n",
    "    var_time = np.var(np.arange(lookback_days), ddof=0)\n",
    "    \n",
    "    # --- 1. TREND ANALYSIS (EXPANDED TO OHLCV) ---\n",
    "    trend_columns = ['Adj Open', 'Adj High', 'Adj Low', 'Adj Close', 'Volume']\n",
    "    trend_columns_exist = [col for col in trend_columns if col in df_group.columns]\n",
    "\n",
    "    df_results = pd.DataFrame(index=df_group.index)\n",
    "\n",
    "    for name in trend_columns_exist:\n",
    "        series = df_group[name].astype(float)\n",
    "        log_series = np.log(series + 1) if name == 'Volume' else np.log(series)\n",
    "        \n",
    "        rolling_cov = time_index.rolling(window=lookback_days).cov(log_series, ddof=0)\n",
    "        rolling_var_series = log_series.rolling(window=lookback_days).var(ddof=0)\n",
    "        \n",
    "        simple_name = name.replace('Adj ', '').lower() \n",
    "        \n",
    "        df_results[f'{simple_name}_slope'] = rolling_cov / var_time\n",
    "        denominator = (var_time * rolling_var_series) + 1e-9\n",
    "        df_results[f'{simple_name}_r_squared'] = (rolling_cov**2) / denominator\n",
    "\n",
    "    # --- 2. VOLATILITY CALCULATION ---\n",
    "    yesterday_low = df_group['Adj Low'].shift(1)\n",
    "    worst_case_returns = (df_group['Adj High'] - yesterday_low) / yesterday_low\n",
    "    unified_std_dev = worst_case_returns.rolling(window=lookback_days).std(ddof=0)\n",
    "    \n",
    "    volume_std_dev = df_group['Volume'].pct_change().rolling(window=lookback_days).std(ddof=0)\n",
    "    \n",
    "    # --- KEY CHANGE: Add the volatility columns to the output DataFrame ---\n",
    "    df_results['unified_std_dev_returns'] = unified_std_dev\n",
    "    df_results['volume_std_dev_returns'] = volume_std_dev\n",
    "\n",
    "    # --- 3. PENALTY SCORE CALCULATION ---\n",
    "    price_trend_names = ['open', 'high', 'low', 'close']\n",
    "    for name in price_trend_names:\n",
    "        r_squared_col = f'{name}_r_squared'\n",
    "        if r_squared_col in df_results.columns:\n",
    "            df_results[f'{name}_penalty_score'] = (1 - df_results[r_squared_col]) * (unified_std_dev + 1e-9)\n",
    "\n",
    "    if 'volume_r_squared' in df_results.columns:\n",
    "        df_results['volume_penalty_score'] = (1 - df_results['volume_r_squared']) * (volume_std_dev + 1e-9)\n",
    "    \n",
    "    return df_results\n",
    "\n",
    "\n",
    "print(\"Verification functions are defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def calculate_rolling_z_scores_general(df_group, columns_to_process, rolling_window=20):\n",
    "    \"\"\"\n",
    "    This function output has been verified. \n",
    "\n",
    "    Calculates rolling Z-scores for a list of specified columns.\n",
    "    \n",
    "    This is a flexible, reusable, and efficient version.\n",
    "    \n",
    "    Args:\n",
    "        df_group (pd.DataFrame): The DataFrame for a single ticker.\n",
    "        columns_to_process (list): A list of column names to calculate Z-scores for.\n",
    "        rolling_window (int): The lookback window.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with Z-score columns, prefixed with 'z_score_'.\n",
    "                      Returns NaNs for non-computable values.\n",
    "    \"\"\"\n",
    "    if df_group.empty or len(df_group) < rolling_window:\n",
    "        # Return an empty DataFrame with the expected column names for consistency\n",
    "        return pd.DataFrame(columns=[f\"z_score_{col}\" for col in columns_to_process])\n",
    "\n",
    "    # Select the subset of data to work on\n",
    "    data_subset = df_group[columns_to_process]\n",
    "    \n",
    "    # Calculate rolling stats for all columns at once.\n",
    "    # This correctly produces NaNs for the initial, incomplete windows.\n",
    "    rolling_mean = data_subset.rolling(window=rolling_window).mean()\n",
    "    rolling_std = data_subset.rolling(window=rolling_window).std()\n",
    "    \n",
    "    # Calculate Z-scores for all columns in one vectorized operation.\n",
    "    z_scores_df = (data_subset - rolling_mean) / rolling_std\n",
    "    \n",
    "    # Handle true division-by-zero errors (where std is 0)\n",
    "    z_scores_df = z_scores_df.replace([np.inf, -np.inf], 0)\n",
    "    \n",
    "    # Add a descriptive prefix to the column names (e.g., 'Adj Low' -> 'z_score_Adj Low')\n",
    "    return z_scores_df.add_prefix('z_score_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from itertools import product\n",
    "\n",
    "def run_backtest(df_ohlcv, config):\n",
    "    \"\"\"\n",
    "    Orchestrates the backtesting process with enhanced logging.\n",
    "    \"\"\"\n",
    "    # Now returns a DataFrame of features for triggered signals\n",
    "    entry_signals_features = precompute_signals(df_ohlcv, config)\n",
    "    \n",
    "    trades = []\n",
    "    open_positions = {}\n",
    "    \n",
    "    all_dates = df_ohlcv.index.get_level_values('Date').unique().sort_values()\n",
    "    start_index = max(config['lookback_days'], config['rolling_window'])\n",
    "\n",
    "    for i in tqdm(range(start_index, len(all_dates) - 1), desc=\"Backtesting\"):\n",
    "        current_date = all_dates[i]\n",
    "        next_day_date = all_dates[i+1]\n",
    "\n",
    "        closed_trades, open_positions = handle_exits_for_day(\n",
    "            current_date, next_day_date, open_positions, df_ohlcv, config\n",
    "        )\n",
    "        trades.extend(closed_trades)\n",
    "\n",
    "        # --- KEY CHANGE: Filter the features DataFrame for today's signals ---\n",
    "        signals_today = entry_signals_features[\n",
    "            entry_signals_features.index.get_level_values('Date') == current_date\n",
    "        ]\n",
    "        \n",
    "        # Pass the full signals_today DataFrame to the handler\n",
    "        open_positions = handle_entries_for_day(\n",
    "            current_date, next_day_date, signals_today, open_positions, df_ohlcv\n",
    "        )\n",
    "                \n",
    "    # --- Create the final DataFrame and reorder columns for clarity ---\n",
    "    if not trades:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    final_trades_df = pd.DataFrame(trades)\n",
    "    log_columns = [\n",
    "        'ticker', 'signal_date', 'entry_date', 'exit_signal_date', 'exit_date', 'reason',\n",
    "        'return', 'entry_price_actual', 'exit_price_actual', 'exit_trigger_price', \n",
    "        'exit_target_value', 'entry_signal_features'\n",
    "    ]\n",
    "    # Ensure all columns exist, fill missing with None\n",
    "    for col in log_columns:\n",
    "        if col not in final_trades_df.columns:\n",
    "            final_trades_df[col] = None\n",
    "            \n",
    "    return final_trades_df[log_columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "def apply_strategy_rules(features, rules, config):\n",
    "    \"\"\"\n",
    "    Applies a list of filtering rules to a features DataFrame.\n",
    "\n",
    "    Args:\n",
    "        features (pd.DataFrame): The DataFrame containing all calculated features.\n",
    "        rules (list): A list of dictionaries, where each dict defines a filtering rule.\n",
    "        config (dict): The configuration dictionary, used for dynamic thresholds.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: A boolean Series (mask) indicating which rows pass all rules.\n",
    "    \"\"\"\n",
    "    # Start with a mask that is True for all rows. We will progressively filter it.\n",
    "    final_mask = pd.Series(True, index=features.index)\n",
    "    \n",
    "    # Map operator strings to actual Python operator functions for flexibility\n",
    "    op_map = {\n",
    "        '>': operator.gt,\n",
    "        '<': operator.lt,\n",
    "        '>=': operator.ge,\n",
    "        '<=': operator.le,\n",
    "        '==': operator.eq,\n",
    "        '!=': operator.ne\n",
    "    }\n",
    "\n",
    "    for rule in rules:\n",
    "        op_func = op_map[rule['operator']]\n",
    "        \n",
    "        # --- Rule Type 1: Comparing two columns ---\n",
    "        if 'column_A' in rule and 'column_B' in rule:\n",
    "            mask = op_func(features[rule['column_A']], features[rule['column_B']])\n",
    "        \n",
    "        # --- Rule Type 2: Comparing a column to a value ---\n",
    "        elif 'column' in rule:\n",
    "            # Determine the value to compare against\n",
    "            if 'value' in rule:\n",
    "                value = rule['value']\n",
    "            elif 'value_from_config' in rule:\n",
    "                value = config[rule['value_from_config']]\n",
    "            else:\n",
    "                raise ValueError(f\"Rule missing 'value' or 'value_from_config': {rule}\")\n",
    "            \n",
    "            mask = op_func(features[rule['column']], value)\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f\"Invalid rule format: {rule}\")\n",
    "        \n",
    "        # Combine the mask for this rule with the final mask using a logical AND\n",
    "        final_mask &= mask\n",
    "            \n",
    "    return final_mask\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from itertools import product\n",
    "\n",
    "def precompute_signals(df_ohlcv, config, rules):\n",
    "    \"\"\"\n",
    "    Pre-computes a rich feature set and then applies a dynamic set of rules\n",
    "    to generate the final trading signals.\n",
    "    \"\"\"\n",
    "    print(\"Pre-computing features for this parameter set...\")\n",
    "    \n",
    "    # --- 1. FEATURE GENERATION (No changes here) ---\n",
    "    trends = df_ohlcv.groupby(level='Ticker', group_keys=False).apply(\n",
    "        analyze_ticker_trends_log_vectorized, config['lookback_days']\n",
    "    )\n",
    "    \n",
    "    z_score_columns = ['Adj Open', 'Adj High', 'Adj Low', 'Adj Close', 'Volume']\n",
    "    z_score_columns_exist = [col for col in z_score_columns if col in df_ohlcv.columns]\n",
    "    z_scores = df_ohlcv.groupby(level='Ticker', group_keys=False).apply(\n",
    "        calculate_rolling_z_scores_general, \n",
    "        columns_to_process=z_score_columns_exist,\n",
    "        rolling_window=config['rolling_window']\n",
    "    )\n",
    "    \n",
    "    features = trends.join(z_scores).dropna()\n",
    "\n",
    "    # --- 2. DYNAMIC FILTERING (KEY CHANGE HERE) ---\n",
    "    print(\"Applying dynamic strategy rules...\")\n",
    "    # Delegate the filtering logic to our new, specialized function\n",
    "    signal_mask = apply_strategy_rules(features, rules, config)\n",
    "    \n",
    "    signals = features[signal_mask]\n",
    "\n",
    "######################################    \n",
    "    # return signals\n",
    "    return signals, trends, features, df_ohlcv\n",
    "######################################    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cell 2: Create Realistic Sample Data\n",
    "\n",
    "We need some sample data that mimics your real dataset to perform the check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample DataFrame created.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Adj Open</th>\n",
       "      <th>Adj High</th>\n",
       "      <th>Adj Low</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ticker</th>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">TEST</th>\n",
       "      <th>2023-01-02</th>\n",
       "      <td>100.500000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-03</th>\n",
       "      <td>101.867014</td>\n",
       "      <td>102.367014</td>\n",
       "      <td>100.367014</td>\n",
       "      <td>101.367014</td>\n",
       "      <td>101020.408163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-04</th>\n",
       "      <td>102.999269</td>\n",
       "      <td>103.499269</td>\n",
       "      <td>101.499269</td>\n",
       "      <td>102.499269</td>\n",
       "      <td>102040.816327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-05</th>\n",
       "      <td>103.719480</td>\n",
       "      <td>104.219480</td>\n",
       "      <td>102.219480</td>\n",
       "      <td>103.219480</td>\n",
       "      <td>103061.224490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-06</th>\n",
       "      <td>103.951248</td>\n",
       "      <td>104.451248</td>\n",
       "      <td>102.451248</td>\n",
       "      <td>103.451248</td>\n",
       "      <td>104081.632653</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Adj Open    Adj High     Adj Low   Adj Close         Volume\n",
       "Ticker Date                                                                     \n",
       "TEST   2023-01-02  100.500000  101.000000   99.000000  100.000000  100000.000000\n",
       "       2023-01-03  101.867014  102.367014  100.367014  101.367014  101020.408163\n",
       "       2023-01-04  102.999269  103.499269  101.499269  102.499269  102040.816327\n",
       "       2023-01-05  103.719480  104.219480  102.219480  103.219480  103061.224490\n",
       "       2023-01-06  103.951248  104.451248  102.451248  103.451248  104081.632653"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create a date range\n",
    "dates = pd.to_datetime(pd.date_range(start='2023-01-01', periods=50, freq='B'))\n",
    "\n",
    "# Create data for Ticker 'TEST' with a clear upward trend and some noise\n",
    "price = 100 + np.linspace(0, 20, 50) + np.sin(np.arange(50)/2) * 2\n",
    "volume = 100000 + np.linspace(0, 50000, 50) # Strong positive volume slope\n",
    "df_test = pd.DataFrame({\n",
    "    'Ticker': 'TEST', 'Date': dates, \n",
    "    'Adj Open': price + 0.5, 'Adj High': price + 1, 'Adj Low': price - 1, 'Adj Close': price,\n",
    "    'Volume': volume\n",
    "})\n",
    "\n",
    "# Add a ticker with insufficient data to test edge cases\n",
    "df_short = pd.DataFrame({\n",
    "    'Ticker': 'SHORT', 'Date': dates[:5],\n",
    "    'Adj High': 10, 'Adj Low': 9, 'Adj Close': 9.5, 'Volume': 1000\n",
    "})\n",
    "\n",
    "\n",
    "# Combine and set index\n",
    "sample_df = pd.concat([df_test, df_short]).set_index(['Ticker', 'Date'])\n",
    "print(\"Sample DataFrame created.\")\n",
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jupyter Notebook: Testing `precompute_signals`\n",
    "\n",
    "#### Cell 1: Setup and Test Functions\n",
    "\n",
    "First, we need the function definitions and a small, predictable sample dataset. Testing on the full `df_train` is too difficult to verify manually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-computing features for this parameter set...\n",
      "Applying dynamic strategy rules...\n",
      "\n",
      "--- Strategy Results ---\n",
      "Found 376 signals using the dynamic rules.\n",
      "                   open_slope  open_r_squared  high_slope  high_r_squared  low_slope  low_r_squared  close_slope  close_r_squared  volume_slope  volume_r_squared  unified_std_dev_returns  volume_std_dev_returns  open_penalty_score  high_penalty_score  low_penalty_score  close_penalty_score  volume_penalty_score  z_score_Adj Open  z_score_Adj High  z_score_Adj Low  z_score_Adj Close  z_score_Volume\n",
      "Ticker Date                                                                                                                                                                                                                                                                                                                                                                                                     \n",
      "APP    2024-10-29    0.010107        0.931826    0.010439        0.933052   0.010340       0.936707     0.010431         0.938683     -0.020981          0.256388                 0.023122                0.355606            0.001576            0.001548           0.001463             0.001418              0.264433          1.847713          1.887088         1.909797           2.115803       -0.688993\n",
      "CRDO   2024-10-29    0.013583        0.924518    0.013661        0.925195   0.013623       0.939363     0.013740         0.926895     -0.003875          0.017507                 0.027425                0.344745            0.002070            0.002052           0.001663             0.002005              0.338710          1.185360          1.322637         1.410838           1.530603       -0.844842\n",
      "MSTR   2024-10-29    0.019487        0.932174    0.019380        0.936331   0.018651       0.940442     0.019212         0.935855      0.021386          0.269593                 0.051900                0.399403            0.003520            0.003304           0.003091             0.003329              0.291727          2.258589          2.052735         2.166904           1.979662        1.059287\n",
      "MSTY   2024-10-29    0.014552        0.946154    0.014261        0.956010   0.013956       0.933098     0.014260         0.942356      0.047187          0.553338                 0.037763                0.507938            0.002033            0.001661           0.002526             0.002177              0.226876          2.043333          1.936033         2.017354           1.877224       -0.040102\n",
      "OWL    2024-10-29    0.009457        0.930301    0.009559        0.927258   0.009429       0.936059     0.009487         0.930605      0.006059          0.023366                 0.017198                0.499497            0.001199            0.001251           0.001100             0.001193              0.487826          0.667025          0.764540         0.774973           0.924528       -0.278166\n",
      "\n",
      "_signals.descride():\n",
      "       open_slope  open_r_squared  high_slope  high_r_squared   low_slope  low_r_squared  close_slope  close_r_squared  volume_slope  volume_r_squared  unified_std_dev_returns  volume_std_dev_returns  open_penalty_score  high_penalty_score  low_penalty_score  close_penalty_score  volume_penalty_score  z_score_Adj Open  z_score_Adj High  z_score_Adj Low  z_score_Adj Close  z_score_Volume\n",
      "count  376.000000      376.000000  376.000000      376.000000  376.000000     376.000000   376.000000       376.000000    376.000000        376.000000               376.000000              376.000000          376.000000          376.000000         376.000000           376.000000            376.000000        376.000000        376.000000       376.000000         376.000000      376.000000\n",
      "mean     0.012039        0.918282    0.012210        0.919526    0.011900       0.923171     0.012035         0.917064      0.017193          0.182436                 0.028669                0.508126            0.002363            0.002329           0.002225             0.002406              0.418039          1.289346          1.238525         1.269833           1.209473       -0.058467\n",
      "std      0.002943        0.028468    0.002984        0.026509    0.002876       0.028088     0.002897         0.029400      0.015978          0.169993                 0.007013                0.216109            0.000986            0.000933           0.000954             0.001023              0.213702          0.469219          0.446424         0.502971           0.497340        0.963230\n",
      "min      0.008967        0.838760    0.009076        0.847416    0.009008       0.840571     0.008834         0.825013     -0.020981          0.000004                 0.015374                0.223899            0.000476            0.000294           0.000430             0.000300              0.096721         -0.674220         -0.082936        -0.497790          -0.463485       -1.926294\n",
      "25%      0.009875        0.900581    0.010091        0.899592    0.009745       0.902876     0.009889         0.894866      0.007247          0.037053                 0.023692                0.377545            0.001507            0.001596           0.001398             0.001559              0.294955          0.981693          0.908836         0.930668           0.905426       -0.692405\n",
      "50%      0.010729        0.920140    0.010775        0.920544    0.010591       0.923472     0.010662         0.920238      0.015530          0.138054                 0.027304                0.473623            0.002329            0.002331           0.002233             0.002424              0.368858          1.335077          1.254730         1.300110           1.239863       -0.256077\n",
      "75%      0.013420        0.939666    0.013517        0.936428    0.013392       0.943563     0.013433         0.938206      0.025046          0.292011                 0.032730                0.557506            0.003203            0.003157           0.003014             0.003268              0.495186          1.600011          1.540711         1.605240           1.544002        0.252711\n",
      "max      0.021211        0.980417    0.021440        0.985494    0.020957       0.978801     0.020954         0.985206      0.069064          0.669715                 0.053844                1.799467            0.004864            0.003998           0.003964             0.004634              1.798875          2.872779          2.703512         2.924393           2.711928        3.495731\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Define your configuration for thresholds ---\n",
    "config = {\n",
    "    'lookback_days': 30,\n",
    "    'rolling_window': 20,\n",
    "    'r2_thresh': 0.90,\n",
    "    'z_entry_thresh': -1.5,\n",
    "    # You can add new thresholds here anytime\n",
    "    'max_penalty_score': 0.004 \n",
    "}\n",
    "\n",
    "# --- 2. Define your strategy as a list of rules ---\n",
    "# This is where you can be creative and easily swap rules in and out.\n",
    "strategy_1_rules = [\n",
    "    # Static value comparison: volume trend must be positive\n",
    "    {'column': 'low_slope', 'operator': '>', 'value': 0.009},\n",
    "    \n",
    "    # # Dynamic threshold from config: R-squared must be high enough\n",
    "    # {'column': 'low_r_squared', 'operator': '>', 'value_from_config': 'r2_thresh'},\n",
    "    \n",
    "    # # Dynamic threshold from config: Look for a price dip\n",
    "    # {'column': 'z_score_Adj Low', 'operator': '<', 'value_from_config': 'z_entry_thresh'},\n",
    "    \n",
    "    # Column-vs-column comparison: Highs are trending up faster than lows\n",
    "    {'column_A': 'high_slope', 'operator': '>', 'column_B': 'low_slope'},\n",
    "\n",
    "    {'column': 'low_penalty_score', 'operator': '<', 'value_from_config': 'max_penalty_score'},\n",
    "    {'column': 'high_penalty_score', 'operator': '<', 'value_from_config': 'max_penalty_score'},      \n",
    "]\n",
    "\n",
    "# --- 3. Run the pre-computation with your chosen strategy ---\n",
    "# To test a new strategy, you would just pass a different list of rules!\n",
    "_signals, _trends, _features, _df_ohlcv= precompute_signals(df_train, config, rules=strategy_1_rules)\n",
    "\n",
    "print(\"\\n--- Strategy Results ---\")\n",
    "print(f\"Found {len(_signals)} signals using the dynamic rules.\")\n",
    "print(_signals.head())\n",
    "print(f'\\n_signals.descride():\\n{_signals.describe()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   open_slope  open_r_squared  high_slope  high_r_squared  low_slope  low_r_squared  close_slope  close_r_squared  volume_slope  volume_r_squared  unified_std_dev_returns  volume_std_dev_returns  open_penalty_score  high_penalty_score  low_penalty_score  close_penalty_score  volume_penalty_score  z_score_Adj Open  z_score_Adj High  z_score_Adj Low  z_score_Adj Close  z_score_Volume\n",
      "Ticker Date                                                                                                                                                                                                                                                                                                                                                                                                     \n",
      "DDS    2025-05-30    0.010494        0.926170    0.010673        0.896950   0.010567       0.925339     0.010302         0.878128      0.018398          0.400466                 0.027735                0.273516            0.002048            0.002858           0.002071             0.003380              0.163982          0.339400          0.195477         0.436923           0.169038        1.621153\n",
      "LTM    2025-05-30    0.009361        0.964422    0.009567        0.963529   0.009209       0.968498     0.009528         0.965099     -0.010467          0.012937                 0.015374                1.793592            0.000547            0.000561           0.000484             0.000537              1.770389          1.074293          1.003572         1.061038           1.135693       -0.714706\n",
      "SYM    2025-05-30    0.014677        0.942949    0.015157        0.934360   0.014796       0.938074     0.014855         0.929314      0.027467          0.341662                 0.041539                0.451783            0.002370            0.002727           0.002572             0.002936              0.297426          0.797954          0.571806         0.912191           0.750387       -0.503166\n",
      "URA    2025-05-30    0.011625        0.919958    0.011700        0.923302   0.011556       0.922706     0.011701         0.923597      0.027988          0.272973                 0.030151                0.853105            0.002413            0.002313           0.002330             0.002304              0.620231          1.374963          1.308791         1.438409           1.428928       -0.060549\n",
      "XP     2025-05-30    0.011499        0.899012    0.011434        0.895816   0.011394       0.900447     0.011103         0.890818      0.018200          0.088154                 0.031705                0.542651            0.003202            0.003303           0.003156             0.003462              0.494815          0.958179          0.816929         0.983414           1.016139       -0.572133\n"
     ]
    }
   ],
   "source": [
    "print(_signals.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open_slope</th>\n",
       "      <th>open_r_squared</th>\n",
       "      <th>high_slope</th>\n",
       "      <th>high_r_squared</th>\n",
       "      <th>low_slope</th>\n",
       "      <th>low_r_squared</th>\n",
       "      <th>close_slope</th>\n",
       "      <th>close_r_squared</th>\n",
       "      <th>volume_slope</th>\n",
       "      <th>volume_r_squared</th>\n",
       "      <th>unified_std_dev_returns</th>\n",
       "      <th>volume_std_dev_returns</th>\n",
       "      <th>open_penalty_score</th>\n",
       "      <th>high_penalty_score</th>\n",
       "      <th>low_penalty_score</th>\n",
       "      <th>close_penalty_score</th>\n",
       "      <th>volume_penalty_score</th>\n",
       "      <th>z_score_Adj Open</th>\n",
       "      <th>z_score_Adj High</th>\n",
       "      <th>z_score_Adj Low</th>\n",
       "      <th>z_score_Adj Close</th>\n",
       "      <th>z_score_Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8040.000000</td>\n",
       "      <td>8040.000000</td>\n",
       "      <td>8040.000000</td>\n",
       "      <td>8040.000000</td>\n",
       "      <td>8040.000000</td>\n",
       "      <td>8040.000000</td>\n",
       "      <td>8040.000000</td>\n",
       "      <td>8040.000000</td>\n",
       "      <td>8040.000000</td>\n",
       "      <td>8.040000e+03</td>\n",
       "      <td>8040.000000</td>\n",
       "      <td>8040.000000</td>\n",
       "      <td>8040.000000</td>\n",
       "      <td>8040.000000</td>\n",
       "      <td>8040.000000</td>\n",
       "      <td>8040.000000</td>\n",
       "      <td>8040.000000</td>\n",
       "      <td>8040.000000</td>\n",
       "      <td>8040.000000</td>\n",
       "      <td>8040.000000</td>\n",
       "      <td>8040.000000</td>\n",
       "      <td>8040.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.009015</td>\n",
       "      <td>0.743279</td>\n",
       "      <td>0.009274</td>\n",
       "      <td>0.751379</td>\n",
       "      <td>0.008813</td>\n",
       "      <td>0.744099</td>\n",
       "      <td>0.009057</td>\n",
       "      <td>0.741965</td>\n",
       "      <td>0.016456</td>\n",
       "      <td>1.652826e-01</td>\n",
       "      <td>0.036925</td>\n",
       "      <td>0.588101</td>\n",
       "      <td>0.010732</td>\n",
       "      <td>0.010468</td>\n",
       "      <td>0.010705</td>\n",
       "      <td>0.010844</td>\n",
       "      <td>0.490797</td>\n",
       "      <td>1.031928</td>\n",
       "      <td>0.976339</td>\n",
       "      <td>0.996583</td>\n",
       "      <td>0.937000</td>\n",
       "      <td>0.055427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.004834</td>\n",
       "      <td>0.152250</td>\n",
       "      <td>0.004966</td>\n",
       "      <td>0.145476</td>\n",
       "      <td>0.004727</td>\n",
       "      <td>0.158818</td>\n",
       "      <td>0.004850</td>\n",
       "      <td>0.152959</td>\n",
       "      <td>0.017151</td>\n",
       "      <td>1.595178e-01</td>\n",
       "      <td>0.021663</td>\n",
       "      <td>0.289762</td>\n",
       "      <td>0.011675</td>\n",
       "      <td>0.011287</td>\n",
       "      <td>0.011912</td>\n",
       "      <td>0.011760</td>\n",
       "      <td>0.263029</td>\n",
       "      <td>0.869015</td>\n",
       "      <td>0.870889</td>\n",
       "      <td>0.908754</td>\n",
       "      <td>0.910232</td>\n",
       "      <td>1.025774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.004408</td>\n",
       "      <td>0.033617</td>\n",
       "      <td>0.005012</td>\n",
       "      <td>0.037358</td>\n",
       "      <td>0.005001</td>\n",
       "      <td>0.029733</td>\n",
       "      <td>0.004035</td>\n",
       "      <td>0.035997</td>\n",
       "      <td>-0.049511</td>\n",
       "      <td>6.339136e-09</td>\n",
       "      <td>0.007653</td>\n",
       "      <td>0.206328</td>\n",
       "      <td>0.000278</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.000306</td>\n",
       "      <td>0.000287</td>\n",
       "      <td>0.088978</td>\n",
       "      <td>-3.254827</td>\n",
       "      <td>-2.803485</td>\n",
       "      <td>-3.406237</td>\n",
       "      <td>-3.397731</td>\n",
       "      <td>-2.663222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.006029</td>\n",
       "      <td>0.664200</td>\n",
       "      <td>0.006221</td>\n",
       "      <td>0.676489</td>\n",
       "      <td>0.005875</td>\n",
       "      <td>0.663197</td>\n",
       "      <td>0.006070</td>\n",
       "      <td>0.663832</td>\n",
       "      <td>0.004875</td>\n",
       "      <td>3.047089e-02</td>\n",
       "      <td>0.021988</td>\n",
       "      <td>0.420230</td>\n",
       "      <td>0.003652</td>\n",
       "      <td>0.003480</td>\n",
       "      <td>0.003513</td>\n",
       "      <td>0.003609</td>\n",
       "      <td>0.337961</td>\n",
       "      <td>0.574733</td>\n",
       "      <td>0.524000</td>\n",
       "      <td>0.545446</td>\n",
       "      <td>0.472331</td>\n",
       "      <td>-0.606835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.007370</td>\n",
       "      <td>0.779088</td>\n",
       "      <td>0.007570</td>\n",
       "      <td>0.783430</td>\n",
       "      <td>0.007196</td>\n",
       "      <td>0.782853</td>\n",
       "      <td>0.007400</td>\n",
       "      <td>0.776690</td>\n",
       "      <td>0.015315</td>\n",
       "      <td>1.144041e-01</td>\n",
       "      <td>0.030509</td>\n",
       "      <td>0.518065</td>\n",
       "      <td>0.006968</td>\n",
       "      <td>0.006852</td>\n",
       "      <td>0.006859</td>\n",
       "      <td>0.007093</td>\n",
       "      <td>0.436702</td>\n",
       "      <td>1.104643</td>\n",
       "      <td>1.057384</td>\n",
       "      <td>1.103731</td>\n",
       "      <td>1.038716</td>\n",
       "      <td>-0.198650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.009998</td>\n",
       "      <td>0.858597</td>\n",
       "      <td>0.010270</td>\n",
       "      <td>0.859726</td>\n",
       "      <td>0.009751</td>\n",
       "      <td>0.864601</td>\n",
       "      <td>0.010033</td>\n",
       "      <td>0.856142</td>\n",
       "      <td>0.027044</td>\n",
       "      <td>2.632138e-01</td>\n",
       "      <td>0.044607</td>\n",
       "      <td>0.668380</td>\n",
       "      <td>0.012909</td>\n",
       "      <td>0.012635</td>\n",
       "      <td>0.012936</td>\n",
       "      <td>0.013104</td>\n",
       "      <td>0.569279</td>\n",
       "      <td>1.575373</td>\n",
       "      <td>1.519392</td>\n",
       "      <td>1.586656</td>\n",
       "      <td>1.524373</td>\n",
       "      <td>0.452402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.042944</td>\n",
       "      <td>0.980417</td>\n",
       "      <td>0.045192</td>\n",
       "      <td>0.985494</td>\n",
       "      <td>0.041779</td>\n",
       "      <td>0.978801</td>\n",
       "      <td>0.043204</td>\n",
       "      <td>0.985206</td>\n",
       "      <td>0.100791</td>\n",
       "      <td>8.116920e-01</td>\n",
       "      <td>0.166839</td>\n",
       "      <td>3.846569</td>\n",
       "      <td>0.146593</td>\n",
       "      <td>0.146026</td>\n",
       "      <td>0.147183</td>\n",
       "      <td>0.146232</td>\n",
       "      <td>3.814260</td>\n",
       "      <td>4.098381</td>\n",
       "      <td>4.138848</td>\n",
       "      <td>4.108230</td>\n",
       "      <td>4.045460</td>\n",
       "      <td>4.218908</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        open_slope  open_r_squared   high_slope  high_r_squared    low_slope  low_r_squared  close_slope  close_r_squared  volume_slope  volume_r_squared  unified_std_dev_returns  volume_std_dev_returns  open_penalty_score  high_penalty_score  low_penalty_score  close_penalty_score  volume_penalty_score  z_score_Adj Open  z_score_Adj High  z_score_Adj Low  z_score_Adj Close  z_score_Volume\n",
       "count  8040.000000     8040.000000  8040.000000     8040.000000  8040.000000    8040.000000  8040.000000      8040.000000   8040.000000      8.040000e+03              8040.000000             8040.000000         8040.000000         8040.000000        8040.000000          8040.000000           8040.000000       8040.000000       8040.000000      8040.000000        8040.000000     8040.000000\n",
       "mean      0.009015        0.743279     0.009274        0.751379     0.008813       0.744099     0.009057         0.741965      0.016456      1.652826e-01                 0.036925                0.588101            0.010732            0.010468           0.010705             0.010844              0.490797          1.031928          0.976339         0.996583           0.937000        0.055427\n",
       "std       0.004834        0.152250     0.004966        0.145476     0.004727       0.158818     0.004850         0.152959      0.017151      1.595178e-01                 0.021663                0.289762            0.011675            0.011287           0.011912             0.011760              0.263029          0.869015          0.870889         0.908754           0.910232        1.025774\n",
       "min       0.004408        0.033617     0.005012        0.037358     0.005001       0.029733     0.004035         0.035997     -0.049511      6.339136e-09                 0.007653                0.206328            0.000278            0.000222           0.000306             0.000287              0.088978         -3.254827         -2.803485        -3.406237          -3.397731       -2.663222\n",
       "25%       0.006029        0.664200     0.006221        0.676489     0.005875       0.663197     0.006070         0.663832      0.004875      3.047089e-02                 0.021988                0.420230            0.003652            0.003480           0.003513             0.003609              0.337961          0.574733          0.524000         0.545446           0.472331       -0.606835\n",
       "50%       0.007370        0.779088     0.007570        0.783430     0.007196       0.782853     0.007400         0.776690      0.015315      1.144041e-01                 0.030509                0.518065            0.006968            0.006852           0.006859             0.007093              0.436702          1.104643          1.057384         1.103731           1.038716       -0.198650\n",
       "75%       0.009998        0.858597     0.010270        0.859726     0.009751       0.864601     0.010033         0.856142      0.027044      2.632138e-01                 0.044607                0.668380            0.012909            0.012635           0.012936             0.013104              0.569279          1.575373          1.519392         1.586656           1.524373        0.452402\n",
       "max       0.042944        0.980417     0.045192        0.985494     0.041779       0.978801     0.043204         0.985206      0.100791      8.116920e-01                 0.166839                3.846569            0.146593            0.146026           0.147183             0.146232              3.814260          4.098381          4.138848         4.108230           4.045460        4.218908"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_signals.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "_signals.descride():\n",
      "        open_slope  open_r_squared   high_slope  high_r_squared    low_slope  low_r_squared  close_slope  close_r_squared  volume_slope  volume_r_squared  unified_std_dev_returns  volume_std_dev_returns  open_penalty_score  high_penalty_score  low_penalty_score  close_penalty_score  volume_penalty_score  z_score_Adj Open  z_score_Adj High  z_score_Adj Low  z_score_Adj Close  z_score_Volume\n",
      "count  8040.000000     8040.000000  8040.000000     8040.000000  8040.000000    8040.000000  8040.000000      8040.000000   8040.000000      8.040000e+03              8040.000000             8040.000000         8040.000000         8040.000000        8040.000000          8040.000000           8040.000000       8040.000000       8040.000000      8040.000000        8040.000000     8040.000000\n",
      "mean      0.009015        0.743279     0.009274        0.751379     0.008813       0.744099     0.009057         0.741965      0.016456      1.652826e-01                 0.036925                0.588101            0.010732            0.010468           0.010705             0.010844              0.490797          1.031928          0.976339         0.996583           0.937000        0.055427\n",
      "std       0.004834        0.152250     0.004966        0.145476     0.004727       0.158818     0.004850         0.152959      0.017151      1.595178e-01                 0.021663                0.289762            0.011675            0.011287           0.011912             0.011760              0.263029          0.869015          0.870889         0.908754           0.910232        1.025774\n",
      "min       0.004408        0.033617     0.005012        0.037358     0.005001       0.029733     0.004035         0.035997     -0.049511      6.339136e-09                 0.007653                0.206328            0.000278            0.000222           0.000306             0.000287              0.088978         -3.254827         -2.803485        -3.406237          -3.397731       -2.663222\n",
      "25%       0.006029        0.664200     0.006221        0.676489     0.005875       0.663197     0.006070         0.663832      0.004875      3.047089e-02                 0.021988                0.420230            0.003652            0.003480           0.003513             0.003609              0.337961          0.574733          0.524000         0.545446           0.472331       -0.606835\n",
      "50%       0.007370        0.779088     0.007570        0.783430     0.007196       0.782853     0.007400         0.776690      0.015315      1.144041e-01                 0.030509                0.518065            0.006968            0.006852           0.006859             0.007093              0.436702          1.104643          1.057384         1.103731           1.038716       -0.198650\n",
      "75%       0.009998        0.858597     0.010270        0.859726     0.009751       0.864601     0.010033         0.856142      0.027044      2.632138e-01                 0.044607                0.668380            0.012909            0.012635           0.012936             0.013104              0.569279          1.575373          1.519392         1.586656           1.524373        0.452402\n",
      "max       0.042944        0.980417     0.045192        0.985494     0.041779       0.978801     0.043204         0.985206      0.100791      8.116920e-01                 0.166839                3.846569            0.146593            0.146026           0.147183             0.146232              3.814260          4.098381          4.138848         4.108230           4.045460        4.218908\n"
     ]
    }
   ],
   "source": [
    "print(f'\\n_signals.descride():\\n{_signals.describe()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df.to_csv(r'C:\\Users\\ping\\Desktop\\sample_df.csv', index=True)\n",
    "_trends.to_csv(r'C:\\Users\\ping\\Desktop\\_trends.csv', index=True)\n",
    "_features.to_csv(r'C:\\Users\\ping\\Desktop\\_features.csv', index=True)\n",
    "_df_ohlcv.to_csv(r'C:\\Users\\ping\\Desktop\\_df_ohlcv.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSFT = df_train.loc['MSFT'].copy()\n",
    "print(f'MSFT.head(3):\\n{MSFT.head(3)}')\n",
    "print(f'\\nMSFT.tail(3):\\n{MSFT.tail(3)}')\n",
    "print(f'\\nlen(MSFT): {len(MSFT)}')\n",
    "\n",
    "MSFT.to_csv(r'C:\\Users\\ping\\Desktop\\MSFT.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Parameters for our Test ---\n",
    "TICKER_TO_CHECK = 'MSFT'\n",
    "DATE_TO_CHECK = pd.to_datetime('2024-09-26')\n",
    "LOOKBACK_DAYS = 10\n",
    "\n",
    "print(f\"--- Verifying LOG-TRANSFORMED calculations for '{TICKER_TO_CHECK}' on {DATE_TO_CHECK.date()} ---\\n\")\n",
    "\n",
    "# --- 1. Run the FAST Log-Vectorized function ---\n",
    "ticker_history = df_train.loc[TICKER_TO_CHECK]\n",
    "vectorized_results_full = analyze_ticker_trends_log_vectorized(ticker_history, LOOKBACK_DAYS)\n",
    "vectorized_result_today = vectorized_results_full.loc[DATE_TO_CHECK]\n",
    "\n",
    "# --- 2. Run the SLOW Original (Normalized) function ---\n",
    "historical_slice = ticker_history.loc[:DATE_TO_CHECK]\n",
    "original_result_today = analyze_ticker_trends_original(historical_slice, LOOKBACK_DAYS)\n",
    "\n",
    "# --- 3. Compare the results ---\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Log-Vectorized': vectorized_result_today,\n",
    "    'Original (Normalized)': original_result_today\n",
    "}).dropna()\n",
    "\n",
    "print(\"### Important Note ###\")\n",
    "print(\"The slope of a log(price) series is mathematically very similar to the slope of a normalized (price/price_0) series.\")\n",
    "print(\"Therefore, both the SLOPES and R-SQUARED values should now be very close.\\n\")\n",
    "\n",
    "print(\"### Side-by-Side Comparison ###\")\n",
    "print(comparison_df)\n",
    "\n",
    "# --- 4. Programmatic Check for ALL values ---\n",
    "try:\n",
    "    # We now test BOTH slope and r_squared\n",
    "    pd.testing.assert_series_equal(\n",
    "        comparison_df['Log-Vectorized'],\n",
    "        comparison_df['Original (Normalized)'],\n",
    "        atol=0.05 # Use a slightly larger tolerance for slope approximation\n",
    "    )\n",
    "    print(\"\\n[SUCCESS]: Log-vectorized results closely match the original normalized results!\")\n",
    "except AssertionError as e:\n",
    "    print(\"\\n[FAILURE]: Results do not match.\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_results_full.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_results_full.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refactored Code\n",
    "\n",
    "Here is the complete, refactored solution. I've included the previously refactored functions with slight modifications to accept the new configuration structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_scores = df_train.groupby(level='Ticker', group_keys=False).apply(\n",
    "    calculate_rolling_z_scores_general, df_train.columns, rolling_window=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_MSFT_w10 = z_scores.loc['MSFT']\n",
    "z_MSFT_w10.to_csv(r'C:\\Users\\ping\\Desktop\\z_MSFT_w10.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Refined Core Backtesting Functions\n",
    "\n",
    "We'll modify the function signatures to accept a single `config` dictionary. This makes them more modular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define the full configuration\n",
    "config = {\n",
    "    'lookback_days': 30,\n",
    "    'rolling_window': 10,  # Set the value you want to test\n",
    "    'slope_thresh': 0.05,\n",
    "    'r2_thresh': 0.50,\n",
    "    'z_entry_thresh': -1.5,\n",
    "    # ... other params if needed\n",
    "}\n",
    "\n",
    "# 2. Call the function correctly\n",
    "signals_df, features_df = precompute_signals(df_train, config) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Test Configuration ---\n",
    "test_config = {\n",
    "    'lookback_days': 10,\n",
    "    'rolling_window': 5,\n",
    "    'slope_thresh': 0, # Set low to ensure we get some signals\n",
    "    'r2_thresh': 0.3,\n",
    "    'z_entry_thresh': -0.5,\n",
    "    'volume_thresh': 0,\n",
    "}\n",
    "\n",
    "# --- Run the special testing function ---\n",
    "all_features, signals_from_test_func, trends = precompute_signals_for_testing(sample_df, test_config)\n",
    "\n",
    "\n",
    "print(\"--- Test 1: Inspecting the full 'features' DataFrame ---\")\n",
    "print(f\"Shape of the features DataFrame: {all_features.shape}\")\n",
    "print(\"Note: The first (lookback_days - 1) rows should be missing due to NaNs.\")\n",
    "print(\"The ticker 'SHORT' should not appear at all.\\n\")\n",
    "\n",
    "# Display the head and tail to check values\n",
    "print(\"Head of features:\")\n",
    "print(all_features.head())\n",
    "print(\"\\nTail of features:\")\n",
    "print(all_features.tail())\n",
    "\n",
    "# --- Verification Checks for 'features' ---\n",
    "assert 'SHORT' not in all_features.index.get_level_values('Ticker'), \"FAIL: Ticker with insufficient data was not filtered out.\"\n",
    "print(\"\\n[SUCCESS]: Ticker with insufficient data was correctly ignored.\")\n",
    "\n",
    "assert not all_features.isnull().values.any(), \"FAIL: The features DataFrame contains unexpected NaNs after dropna().\"\n",
    "print(\"[SUCCESS]: Features DataFrame contains no NaNs.\")\n",
    "\n",
    "print(\"\\n--- Sanity Check Passed for Feature Generation ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signals = features[\n",
    "    (features['low_slope'] > config['slope_thresh']) &\n",
    "    (features['low_r_squared'] > config['r2_thresh']) &\n",
    "    (features['volume_slope'] > 0) &\n",
    "    (features['z_score_Adj Low'] < config['z_entry_thresh'])\n",
    "]\n",
    "\n",
    "test_config = {\n",
    "    'lookback_days': 10,\n",
    "    'rolling_window': 5,\n",
    "    'slope_thresh': 0.005, # Set low to ensure we get some signals\n",
    "    'r2_thresh': 0.3,\n",
    "    'z_entry_thresh': -0.5,\n",
    "    'volume_thresh': 0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'trends:\\n{trends}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'sample_df:\\n{sample_df}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: \"Black-Box\" Test - Verify the Final `signals` DataFrame\n",
    "\n",
    "Now we test the real function. Our goal is to prove that **every single row** in the final `signals_df` meets the filtering criteria defined in our `test_config`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Run the REAL function to get the final output ---\n",
    "signals_df = precompute_signals(sample_df, test_config)\n",
    "\n",
    "print(f\"\\n--- Test 2: Verifying the final 'signals' DataFrame ---\")\n",
    "print(f\"Found {len(signals_df)} potential signals to verify.\")\n",
    "\n",
    "# --- Programmatic Verification ---\n",
    "# For every signal found, we cross-reference it with the 'all_features' DataFrame\n",
    "# and assert that its values meet the criteria.\n",
    "\n",
    "for idx, signal_row in signals_df.iterrows():\n",
    "    # Find the original, unfiltered features for this specific signal\n",
    "    original_features = all_features.loc[idx]\n",
    "\n",
    "    # Assert that each condition is met\n",
    "    try:\n",
    "        assert original_features['low_slope'] > test_config['slope_thresh']\n",
    "        assert original_features['low_r_squared'] > test_config['r2_thresh']\n",
    "        assert original_features['volume_slope'] > test_config['volume_thresh']\n",
    "        assert original_features['z_score_Adj Low'] < test_config['z_entry_thresh']\n",
    "    except AssertionError as e:\n",
    "        print(f\"\\n[FAILURE]: Verification failed for signal at index {idx}!\")\n",
    "        print(\"Signal Row:\")\n",
    "        print(signal_row)\n",
    "        print(\"\\nThresholds:\")\n",
    "        print(test_config)\n",
    "        raise e\n",
    "\n",
    "if not signals_df.empty:\n",
    "    print(\"\\n[SUCCESS]: All rows in the final signals DataFrame correctly meet the filter criteria.\")\n",
    "else:\n",
    "    print(\"\\n[INFO]: No signals were generated with this config, test passed vacuously.\")\n",
    "\n",
    "print(\"\\n--- Sanity Check Passed for Signal Filtering ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'signals_df:\\n{signals_df}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# penalty_scores are not calculated???\n",
    "* penalty_score = (1 - r_squared) * (unified_std_dev + 1e-9)  \n",
    "* volume_penalty_score = (1 - volume_r_squared) * (volume_std_dev + 1e-9)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signals_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: New Encapsulated Helper Functions\n",
    "\n",
    "These new functions isolate the logic for performance analysis and the optimization loop itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_performance(trade_results):\n",
    "    \"\"\"\n",
    "    Calculates performance metrics from a DataFrame of trades.\n",
    "    \n",
    "    Returns a dictionary of key metrics.\n",
    "    \"\"\"\n",
    "    if trade_results.empty:\n",
    "        return {'num_trades': 0, 'win_rate': 0, 'avg_return': 0, 'total_return': 0}\n",
    "    \n",
    "    win_rate = (trade_results['return'] > 0).mean()\n",
    "    total_return = (1 + trade_results['return']).prod() - 1\n",
    "    avg_return = trade_results['return'].mean()\n",
    "    \n",
    "    return {\n",
    "        'num_trades': len(trade_results),\n",
    "        'win_rate': win_rate,\n",
    "        'avg_return': avg_return,\n",
    "        'total_return': total_return\n",
    "    }\n",
    "\n",
    "def run_parameter_optimization(df, param_grid, static_params):\n",
    "    \"\"\"\n",
    "    Orchestrates the entire parameter optimization process.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The OHLCV data.\n",
    "        param_grid (dict): Dictionary with lists of parameters to test.\n",
    "        static_params (dict): Dictionary of parameters that are not being optimized.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A summary of results for each parameter combination.\n",
    "    \"\"\"\n",
    "    results_log = []\n",
    "    \n",
    "    # Use itertools.product to create a clean generator for all combinations\n",
    "    keys, values = zip(*param_grid.items())\n",
    "    param_combinations = [dict(zip(keys, v)) for v in product(*values)]\n",
    "\n",
    "    print(f\"Starting optimization for {len(param_combinations)} combinations...\")\n",
    "    \n",
    "    for param_set in tqdm(param_combinations, desc=\"Optimization Progress\"):\n",
    "        # Combine static and dynamic parameters into a single config for this run\n",
    "        current_config = {**static_params, **param_set}\n",
    "        \n",
    "        # 1. Run the backtest with the current configuration\n",
    "        trade_results = run_backtest(df, current_config)\n",
    "        \n",
    "        # 2. Analyze the performance of this run\n",
    "        performance_metrics = analyze_performance(trade_results)\n",
    "        \n",
    "        # 3. Log the results\n",
    "        log_entry = {**param_set, **performance_metrics}\n",
    "        results_log.append(log_entry)\n",
    "        \n",
    "    return pd.DataFrame(results_log)\n",
    "\n",
    "def handle_entries_for_day(current_date, next_day_date, signals_today, open_positions, df_ohlcv):\n",
    "    \"\"\"\n",
    "    Processes entries and stores signal details in the open_positions dict.\n",
    "    \"\"\"\n",
    "    # --- KEY CHANGE: Loop through the signals DataFrame ---\n",
    "    for ticker, signal_row in signals_today.iterrows():\n",
    "        # The ticker is now in the index of signal_row, so we use its name\n",
    "        ticker_name = ticker[0] \n",
    "        \n",
    "        if ticker_name not in open_positions:\n",
    "            try:\n",
    "                entry_price = df_ohlcv.loc[(ticker_name, next_day_date), 'Adj High']\n",
    "                \n",
    "                # --- LOGGING: Store more info about the entry signal ---\n",
    "                open_positions[ticker_name] = {\n",
    "                    'entry_date': next_day_date,\n",
    "                    'entry_price': entry_price,\n",
    "                    'signal_date': current_date,\n",
    "                    'signal_features': signal_row.to_dict() # Store all features that triggered the signal\n",
    "                }\n",
    "            except KeyError:\n",
    "                pass\n",
    "                \n",
    "    return open_positions\n",
    "\n",
    "def handle_exits_for_day(current_date, next_day_date, open_positions, df_ohlcv, config):\n",
    "    \"\"\"\n",
    "    Checks for exits and logs detailed information about the exit trigger.\n",
    "    Corrected version with valid syntax for the if/elif chain.\n",
    "    \"\"\"\n",
    "    closed_trades = []\n",
    "    positions_to_close = []\n",
    "\n",
    "    for ticker, pos in open_positions.items():\n",
    "        try:\n",
    "            current_close_price = df_ohlcv.loc[(ticker, current_date), 'Adj Close']\n",
    "        except KeyError:\n",
    "            continue \n",
    "\n",
    "        exit_reason = None\n",
    "        exit_target_value = None \n",
    "        \n",
    "        # --- SYNTAX FIX: Calculate all threshold values *before* the conditional block ---\n",
    "        profit_target_price = pos['entry_price'] * (1 + config['profit_target'])\n",
    "        stop_loss_price = pos['entry_price'] * (1 - config['stop_loss'])\n",
    "        days_held = (current_date.to_pydatetime().date() - pos['entry_date'].to_pydatetime().date()).days\n",
    "        \n",
    "        # --- Now, check conditions in a contiguous if/elif/elif block ---\n",
    "        if current_close_price >= profit_target_price:\n",
    "            exit_reason = \"Profit Target\"\n",
    "            exit_target_value = profit_target_price \n",
    "\n",
    "        elif current_close_price <= stop_loss_price:\n",
    "            exit_reason = \"Stop-Loss\"\n",
    "            exit_target_value = stop_loss_price \n",
    "\n",
    "        elif days_held >= config['time_hold_days']:\n",
    "            exit_reason = \"Time Hold\"\n",
    "            exit_target_value = days_held \n",
    "\n",
    "        if exit_reason:\n",
    "            try:\n",
    "                exit_price = df_ohlcv.loc[(ticker, next_day_date), 'Adj Low']\n",
    "                trade_return = (exit_price - pos['entry_price']) / pos['entry_price']\n",
    "                \n",
    "                trade_log = {\n",
    "                    'ticker': ticker, \n",
    "                    'entry_date': pos['entry_date'], \n",
    "                    'exit_date': next_day_date,\n",
    "                    'return': trade_return, \n",
    "                    'reason': exit_reason,\n",
    "                    'signal_date': pos['signal_date'],\n",
    "                    'entry_signal_features': pos['signal_features'],\n",
    "                    'entry_price_actual': pos['entry_price'],\n",
    "                    'exit_signal_date': current_date,\n",
    "                    'exit_trigger_price': current_close_price,\n",
    "                    'exit_target_value': exit_target_value,\n",
    "                    'exit_price_actual': exit_price,\n",
    "                }\n",
    "                closed_trades.append(trade_log)\n",
    "                positions_to_close.append(ticker)\n",
    "            except KeyError:\n",
    "                pass\n",
    "                \n",
    "    for ticker in positions_to_close:\n",
    "        del open_positions[ticker]\n",
    "        \n",
    "    return closed_trades, open_positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: The New, Clean Top-Level Script\n",
    "\n",
    "Your main script is now incredibly simple and readable. It's all about configuration and orchestration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. DEFINE CONFIGURATION ---\n",
    "\n",
    "# Parameters to be optimized, defining the search space\n",
    "optimization_grid = {\n",
    "    'lookback_days': [30, 60, 90],\n",
    "    'rolling_window': [15, 20]\n",
    "}\n",
    "\n",
    "# Static strategy parameters that do not change during optimization\n",
    "strategy_params = {\n",
    "    'slope_thresh': 1.0,\n",
    "    'r2_thresh': 0.50,\n",
    "    'z_entry_thresh': 0,\n",
    "    'profit_target': 0.10,\n",
    "    'stop_loss': 0.05,\n",
    "    'time_hold_days': 20\n",
    "}\n",
    "\n",
    "\n",
    "# --- 2. RUN ORCHESTRATOR ---\n",
    "\n",
    "# The main call is now a single, descriptive function\n",
    "optimization_results = run_parameter_optimization(\n",
    "    df_train, optimization_grid, strategy_params\n",
    ")\n",
    "\n",
    "\n",
    "# --- 3. ANALYZE RESULTS ---\n",
    "\n",
    "print(\"\\n\\n--- Optimization Complete ---\")\n",
    "print(optimization_results.sort_values(by='total_return', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: The \"One-Trade\" Deep Dive\n",
    "\n",
    "The most powerful debugging technique is to isolate a single trade and follow it from signal generation to exit. If the logic holds for one trade, it's likely correct for all of them.\n",
    "\n",
    "1.  **Pick a Winning Trade and a Losing Trade:** Run one of the backtests again (e.g., the one with `lookback=30`, `rolling=20`) and save the `trade_results` DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Pick a configuration to analyze\n",
    "config_to_test = {\n",
    "    **strategy_params, \n",
    "    'lookback_days': 30, \n",
    "    'rolling_window': 20\n",
    "}\n",
    "\n",
    "# 2. Run the backtest to get the detailed trade log\n",
    "trade_log_df = run_backtest(df_train, config_to_test)\n",
    "\n",
    "# 3. Isolate and inspect a single trade\n",
    "if not trade_log_df.empty:\n",
    "    # Get the first losing trade\n",
    "    losing_trade = trade_log_df[trade_log_df['return'] < 0].iloc[0]\n",
    "\n",
    "    print(\"--- Detailed Log for a Single Losing Trade ---\")\n",
    "    # Using .T transposes the Series for easy vertical reading\n",
    "    print(losing_trade.T)\n",
    "else:\n",
    "    print(\"No trades were made for this configuration.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losing_trade.entry_signal_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df_trends = analyze_ticker_trends_vectorized(df_train, lookback_days=30)\n",
    "print(f'_df_trends:\\n{_df_trends}')\n",
    "# _df_trends.to_csv('C:\\\\Users\\\\ping\\\\Desktop\\\\_df_trends.csv', index=True)\n",
    "_df_trends.index.names = ['Ticker', 'Date']\n",
    "_df_trends.reset_index().to_csv(r'C:\\Users\\ping\\Desktop\\_df_trends.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.loc['MSFT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to CSV\n",
    "_df_trends.to_csv(r'C:\\Users\\ping\\Desktop\\_df_trends.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df_trends.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loss = trade_log_df[trade_log_df['return'] < 0]\n",
    "print(df_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_win = trade_log_df[trade_log_df['return'] > 0]\n",
    "print(df_win)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loss.to_csv('C:\\\\Users\\\\ping\\\\Desktop\\\\df_loss.csv', index=True)\n",
    "df_win.to_csv('C:\\\\Users\\\\ping\\\\Desktop\\\\df_win.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimization_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a configuration to analyze\n",
    "config_to_test = {**strategy_params, 'lookback_days': 30, 'rolling_window': 20}\n",
    "\n",
    "# Run a single backtest and get the detailed trade log\n",
    "single_run_trades = run_backtest(df_train, config_to_test)\n",
    "\n",
    "# Find a winning and a losing trade to investigate\n",
    "print(\"Sample winning trade:\")\n",
    "print(single_run_trades[single_run_trades['return'] > 0].head(1))\n",
    "\n",
    "print(\"\\nSample losing trade:\")\n",
    "print(single_run_trades[single_run_trades['return'] < 0].head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trends = analyze_ticker_trends_vectorized(df_train, lookback_days=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trends.loc['FIX', '2024-10-25']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
