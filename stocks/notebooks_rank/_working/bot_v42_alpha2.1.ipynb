{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9ae2708",
   "metadata": {},
   "source": [
    "# Here is the refactored solution. I have separated the concerns into three distinct layers:\n",
    "1.  **The Data Contract:** explicit `dataclasses` defining exactly what goes in and comes out.\n",
    "2.  **The Engine:** A purely mathematical class (`AlphaEngine`) containing the logic, with no widget/plotting dependencies.\n",
    "3.  **The UI:** A cleaned-up dashboard function that simply sends inputs to the Engine and visualizes the Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc79d79",
   "metadata": {},
   "source": [
    "### Following is the reverse chronological fix log (most recent entry is at the top )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e7f873",
   "metadata": {},
   "source": [
    "v38  \n",
    "Added Momentum and Pullback strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1ecc32",
   "metadata": {},
   "source": [
    "v37  \n",
    "For an automated bot (and for rigorous scientific testing), **Silent Auto-Correction (Clamping)** is dangerous. If the bot asks for \"2080\" and receives data for \"2025\", it creates \"Data Hallucinations.\"\n",
    "\n",
    "We need to replace the \"Clamping\" logic with **\"Strict Validation\" logic**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d784b62",
   "metadata": {},
   "source": [
    "v36  \n",
    "The step `self.df_close = df_ohlcv['Adj Close'].unstack(level=0)` is an expensive \"Pivot\" operation. It takes a \"Tall and Skinny\" table (1 million rows) and reshapes it into a \"Short and Wide\" matrix (Dates x Tickers). Pandas hates doing this repeatedly.\n",
    "\n",
    "Yes, we can pre-compute this matrix and pass it in, just like we did with the featur"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92185d85",
   "metadata": {},
   "source": [
    "v35  \n",
    "This is a great optimization step. In data science, this is called **\"Memoization\"** or **\"Caching\"**‚Äîdo the heavy math once, save it, and reuse it.\n",
    "\n",
    "To achieve this, we need to modify the **`AlphaEngine` constructor** to accept pre-calculated features, and update the **`plot_walk_forward_analyzer`** to pass them down.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7578ec93",
   "metadata": {},
   "source": [
    "v34  \n",
    "I have refactored **SECTION D (`AlphaEngine.run`)** to implement the \"Decision Anchor\" logic, and **SECTION E (`plot_walk_forward_analyzer`)** to implement the new \"Timeline\" UI layout and renaming.\n",
    "\n",
    "### Key Changes:\n",
    "1.  **Engine Logic:** The `start_date` input is now treated as the **Decision Date (T0)**. The engine calculates backward for the Lookback period and forward for the Holding period.\n",
    "2.  **Universe Selection:** Tickers are now filtered based on liquidity **on the Decision Date**, not the start of history.\n",
    "3.  **UI Layout:** Inputs are arranged horizontally: `[Lookback] <-> [Decision Date] <-> [Holding]`.\n",
    "4.  **Renaming:** \"Metric\" is now **\"Strategy\"**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17abb82d",
   "metadata": {},
   "source": [
    "v33  \n",
    "**Action Item to make it perfect:**\n",
    "Change the \"Fwd Gain\" calculation in `AlphaEngine.run` to skip one day, or accept that your results are slightly optimistic due to the \"Overnight Gap\" bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68b1a41",
   "metadata": {},
   "source": [
    "```\n",
    "To see the full dataframe of all tickers (both those that passed and those that failed) for a specific date, we need to capture a snapshot of the universe inside the `_get_eligible_universe` method.\n",
    "\n",
    "I have updated the **`AlphaEngine`** class below.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30d1f60",
   "metadata": {},
   "source": [
    "```\n",
    "To verify that the relative percentile logic is working, we can modify the `AlphaEngine` to report exactly **how the cutoff was calculated** for the specific start date.\n",
    "\n",
    "We want to see evidence that:\n",
    "1.  In earlier years (e.g., 2005), the volume cutoff is lower (e.g., $200k).\n",
    "2.  In later years (e.g., 2024), the volume cutoff is higher (e.g., $5M).\n",
    "\n",
    "Here is the updated `AlphaEngine` and `UI` code. I have added a **\"Audit Log\"** feature. When you run the tool, it will now print exactly what the Dollar Volume Threshold was for that specific day.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50de4e99",
   "metadata": {},
   "source": [
    "```\n",
    "The best way to solve this is to switch from a **Fixed Dollar Threshold** (e.g., \"$1 Million\") to a **Relative Percentile Threshold** (e.g., \"Top 50% of the market\").\n",
    "\n",
    "In 2004, a stock trading $200k might have been in the top 50% of liquid stocks. In 2024, that same $200k is illiquid garbage. Using a percentile automatically adjusts for inflation and market growth over time.\n",
    "\n",
    "Here is how to modify your code to support this.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e8a3b9",
   "metadata": {},
   "source": [
    "```\n",
    "To fix this, we need to pass the **actual** calculated start date (the trading day the engine \"snapped\" to) back from the `AlphaEngine` to the UI. Then, the UI can compare the *Requested Date* vs. the *Actual Date* and display the warning message if they differ.\n",
    "\n",
    "Here is the plan:\n",
    "1.  **Update `EngineOutput`**: Add a `start_date` field to the dataclass.\n",
    "2.  **Update `AlphaEngine.run`**: Populate this new field with `safe_start_date`.\n",
    "3.  **Update `plot_walk_forward_analyzer`**: Add logic to compare the user's input date with the engine's returned date and print the \"Info\" message if they are different.\n",
    "\n",
    "Here is the updated code (Sections C, D, and E have changed):\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb918f61",
   "metadata": {},
   "source": [
    "```\n",
    "I have updated the `AlphaEngine.run` method. specifically inside the `if inputs.mode == 'Manual List':` block. It now iterates through every manual ticker and performs two checks:\n",
    "1.  **Existence**: Is the ticker in the database?\n",
    "2.  **Availability**: Does the ticker have a valid price on the specific `Start Date`?\n",
    "\n",
    "If any ticker fails, it compiles a specific error message explaining why (e.g., \"No price data on start date\") and aborts the calculation immediately.  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b728a8",
   "metadata": {},
   "source": [
    "```\n",
    "The `snapshot_df` contains **every single feature** calculated by your `generate_features` function for that specific day, plus the new audit columns we added.\n",
    "\n",
    "Here is exactly what is inside that DataFrame:\n",
    "\n",
    "### 1. The Core Features (from `generate_features`)\n",
    "*   **`TR`**: True Range\n",
    "*   **`ATR`**: Average True Range\n",
    "*   **`ATRP`**: Average True Range Percent (Volatility)\n",
    "*   **`RollingStalePct`**: How often the price didn't move or volume was 0.\n",
    "*   **`RollMedDollarVol`**: Median Daily Dollar Volume (Liquidity).\n",
    "*   **`RollingSameVolCount`**: Data quality check for repeated volume numbers.\n",
    "\n",
    "### 2. The Audit Columns (Added during filtering)\n",
    "*   **`Calculated_Cutoff`**: The specific dollar amount required to pass on that day.\n",
    "*   **`Passed_Vol_Check`**: `True` if the ticker met the liquidity requirement.\n",
    "*   **`Passed_Final`**: `True` if it passed **all** checks (Liquidity + Stale + Quality).\n",
    "\n",
    "=========================================\n",
    "\n",
    "Here are the formulas translated directly into the Python `pandas` code used in your `generate_features` function.\n",
    "\n",
    "I have simplified the code slightly to assume a single ticker context (removing the `groupby` wrapper) so you can see the raw math clearly.\n",
    "\n",
    "### 1. True Range (TR)\n",
    "Calculates the maximum of the three price differences.\n",
    "\n",
    "prev_close = df_ohlcv['Adj Close'].shift(1)\n",
    "\n",
    "# The three components\n",
    "diff1 = df_ohlcv['Adj High'] - df_ohlcv['Adj Low']\n",
    "diff2 = (df_ohlcv['Adj High'] - prev_close).abs()\n",
    "diff3 = (df_ohlcv['Adj Low'] - prev_close).abs()\n",
    "\n",
    "# Taking the max of the three\n",
    "tr = pd.concat([diff1, diff2, diff3], axis=1).max(axis=1)\n",
    "\n",
    "### 2. Average True Range (ATR)\n",
    "Uses an Exponential Weighted Mean (EWM) with a specific alpha smoothing factor.\n",
    "\n",
    "# N = atr_period (e.g., 14)\n",
    "# alpha = 1 / N\n",
    "atr = tr.ewm(alpha=1/14, adjust=False).mean()\n",
    "\n",
    "### 3. ATR Percent (ATRP)\n",
    "Simple division to normalize volatility.\n",
    "\n",
    "atrp = atr / df_ohlcv['Adj Close']\n",
    "\n",
    "### 4. Rolling Stale Percentage\n",
    "Checks if volume is 0 OR if High equals Low (price didn't move), then averages that 1 or 0 signal over the window.\n",
    "\n",
    "# 1. Define the Stale Signal (1 for stale, 0 for active)\n",
    "is_stale = np.where(\n",
    "    (df_ohlcv['Volume'] == 0) | (df_ohlcv['Adj High'] == df_ohlcv['Adj Low']), \n",
    "    1,  \n",
    "    0\n",
    ")\n",
    "\n",
    "# 2. Calculate average over window (W=252)\n",
    "rolling_stale_pct = pd.Series(is_stale).rolling(window=252).mean()\n",
    "\n",
    "### 5. Rolling Median Dollar Volume\n",
    "Calculates raw dollar volume, then finds the median over the window.\n",
    "\n",
    "# 1. Calculate Daily Dollar Volume\n",
    "dollar_volume = df_ohlcv['Adj Close'] * df_ohlcv['Volume']\n",
    "\n",
    "# 2. Get Median over window (W=252)\n",
    "roll_med_dollar_vol = dollar_volume.rolling(window=252).median()\n",
    "\n",
    "### 6. Rolling Same Volume Count\n",
    "Checks if today's volume is exactly the same as yesterday's (a sign of bad data), then sums those occurrences.\n",
    "\n",
    "# 1. Check if Volume(t) - Volume(t-1) equals 0\n",
    "# .diff() calculates current row minus previous row\n",
    "has_same_volume = (df_ohlcv['Volume'].diff() == 0).astype(int)\n",
    "\n",
    "# 2. Sum the errors over window (W=252)\n",
    "rolling_same_vol_count = has_same_volume.rolling(window=252).sum()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb417c7",
   "metadata": {},
   "source": [
    "================================  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1110f2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import ipywidgets as widgets\n",
    "import pprint\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Optional, Any, Union\n",
    "from collections import Counter\n",
    "from datetime import datetime, date\n",
    "from pandas.testing import assert_series_equal\n",
    "\n",
    "# pd.set_option('display.max_rows', None)  display all rows\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "pd.set_option('display.precision', 4)\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION A: CORE HELPER FUNCTIONS & FEATURE GENERATION\n",
    "# (Unchanged from previous version)\n",
    "# ==============================================================================\n",
    "# ... (Keep generate_features, calculate_gain, calculate_sharpe, \n",
    "#      calculate_sharpe_atr, calculate_buy_and_hold_performance as is) ...\n",
    "\n",
    "def generate_features(df_ohlcv: pd.DataFrame, atr_period: int = 14, quality_window: int = 252, quality_min_periods: int = 126) -> pd.DataFrame:\n",
    "    # 1. Sort and Group\n",
    "    if not df_ohlcv.index.is_monotonic_increasing: \n",
    "        df_ohlcv = df_ohlcv.sort_index()\n",
    "    grouped = df_ohlcv.groupby(level='Ticker')\n",
    "    \n",
    "    # 2. ATR Calculation (Existing)\n",
    "    prev_close = grouped['Adj Close'].shift(1)\n",
    "    tr = pd.concat([\n",
    "        df_ohlcv['Adj High'] - df_ohlcv['Adj Low'], \n",
    "        abs(df_ohlcv['Adj High'] - prev_close), \n",
    "        abs(df_ohlcv['Adj Low'] - prev_close)\n",
    "    ], axis=1).max(axis=1, skipna=False)\n",
    "    \n",
    "    atr = tr.groupby(level='Ticker').transform(lambda x: x.ewm(alpha=1/atr_period, adjust=False).mean())\n",
    "    atrp = (atr / df_ohlcv['Adj Close']).replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    # 3. --- NEW: MOMENTUM / RETURN FEATURES ---\n",
    "    # We calculate percentage change for specific windows useful for Pullbacks (3D, 5D) and Trends (21D)\n",
    "    # Note: We use grouped.pct_change to respect Ticker boundaries\n",
    "    roc_1 = grouped['Adj Close'].pct_change(1)\n",
    "    roc_3 = grouped['Adj Close'].pct_change(3)\n",
    "    roc_5 = grouped['Adj Close'].pct_change(5)\n",
    "    roc_10 = grouped['Adj Close'].pct_change(10)\n",
    "    roc_21 = grouped['Adj Close'].pct_change(21)\n",
    "\n",
    "    indicator_df = pd.DataFrame({\n",
    "        'TR': tr, \n",
    "        'ATR': atr, \n",
    "        'ATRP': atrp,\n",
    "        'ROC_1': roc_1,\n",
    "        'ROC_3': roc_3,\n",
    "        'ROC_5': roc_5,\n",
    "        'ROC_10': roc_10,\n",
    "        'ROC_21': roc_21\n",
    "    })\n",
    "\n",
    "    # 4. Quality/Liquidity Features (Existing)\n",
    "    quality_temp_df = pd.DataFrame({\n",
    "        'IsStale': np.where((df_ohlcv['Volume'] == 0) | (df_ohlcv['Adj High'] == df_ohlcv['Adj Low']), 1, 0), \n",
    "        'DollarVolume': df_ohlcv['Adj Close'] * df_ohlcv['Volume'], \n",
    "        'HasSameVolume': (grouped['Volume'].diff() == 0).astype(int)\n",
    "    }, index=df_ohlcv.index)\n",
    "    \n",
    "    rolling_result = quality_temp_df.groupby(level='Ticker').rolling(window=quality_window, min_periods=quality_min_periods).agg({\n",
    "        'IsStale': 'mean', \n",
    "        'DollarVolume': 'median', \n",
    "        'HasSameVolume': 'sum'\n",
    "    }).rename(columns={\n",
    "        'IsStale': 'RollingStalePct', \n",
    "        'DollarVolume': 'RollMedDollarVol', \n",
    "        'HasSameVolume': 'RollingSameVolCount'\n",
    "    }).reset_index(level=0, drop=True)\n",
    "    \n",
    "    # 5. Merge\n",
    "    return pd.concat([indicator_df, rolling_result], axis=1)\n",
    "\n",
    "def calculate_gain(price_series): \n",
    "    if price_series.dropna().shape[0] < 2: return np.nan\n",
    "    return (price_series.ffill().iloc[-1] / price_series.bfill().iloc[0]) - 1\n",
    "\n",
    "def calculate_sharpe(return_series):\n",
    "    if return_series.dropna().shape[0] < 2: return np.nan\n",
    "    std = return_series.std()\n",
    "    return (return_series.mean() / std * np.sqrt(252)) if std > 0 else 0.0\n",
    "\n",
    "def calculate_sharpe_atr(return_series, atrp_series):\n",
    "    if return_series.dropna().shape[0] < 2 or atrp_series.dropna().empty: return np.nan\n",
    "    mean_atrp = atrp_series.mean()\n",
    "    return (return_series.mean() / mean_atrp) if mean_atrp > 0 else 0.0\n",
    "\n",
    "def calculate_buy_and_hold_performance(df_close, features_df, tickers, start_date, end_date):\n",
    "    if not tickers: return pd.Series(dtype=float), pd.Series(dtype=float), pd.Series(dtype=float)\n",
    "    ticker_counts = Counter(tickers)\n",
    "    initial_weights = pd.Series({t: c / len(tickers) for t, c in ticker_counts.items()})\n",
    "    prices_raw = df_close[initial_weights.index.tolist()].loc[start_date:end_date]\n",
    "    if prices_raw.dropna(how='all').empty: return pd.Series(dtype=float), pd.Series(dtype=float), pd.Series(dtype=float)\n",
    "    prices_norm = prices_raw.div(prices_raw.bfill().iloc[0])\n",
    "    weighted_growth = prices_norm.mul(initial_weights, axis='columns')\n",
    "    value_series = weighted_growth.sum(axis=1)\n",
    "    return_series = value_series.ffill().pct_change()\n",
    "    full_idx = pd.MultiIndex.from_product([initial_weights.index.tolist(), return_series.index], names=['Ticker', 'Date'])\n",
    "    feat_subset = features_df.reindex(full_idx)['ATRP'].unstack(level='Ticker')\n",
    "    atrp_series = (weighted_growth.div(value_series, axis='index').align(feat_subset, join='inner', axis=1)[0] * weighted_growth.div(value_series, axis='index').align(feat_subset, join='inner', axis=1)[1]).sum(axis=1)\n",
    "    return value_series, return_series, atrp_series\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION B: METRIC REGISTRY (UPDATED VARIABLES)\n",
    "# ==============================================================================\n",
    "\n",
    "# Note: We now expect 'lookback_close' instead of 'calc_close' in the dictionary\n",
    "def metric_price(d): return calculate_gain(d['lookback_close'])\n",
    "def metric_sharpe(d): \n",
    "    r = d['lookback_returns'] # Was daily_returns\n",
    "    return (r.mean() / r.std() * np.sqrt(252)).replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "def metric_sharpe_atr(d):\n",
    "    return (d['lookback_returns'].mean() / d['atrp']).replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "METRIC_REGISTRY = {\n",
    "    'Price': metric_price,\n",
    "    'Sharpe': metric_sharpe,\n",
    "    'Sharpe (ATR)': metric_sharpe_atr,\n",
    "    # Momentum / Pullback (These use features, so they remain the same)\n",
    "    'Momentum 1D': lambda d: d['roc_1'],\n",
    "    'Momentum 3D': lambda d: d['roc_3'],\n",
    "    'Momentum 5D': lambda d: d['roc_5'],\n",
    "    'Momentum 10D': lambda d: d['roc_10'],\n",
    "    'Momentum 1M': lambda d: d['roc_21'],\n",
    "    'Pullback 1D': lambda d: -d['roc_1'],\n",
    "    'Pullback 3D': lambda d: -d['roc_3'],\n",
    "    'Pullback 5D': lambda d: -d['roc_5'],\n",
    "    'Pullback 10D': lambda d: -d['roc_10'],\n",
    "    'Pullback 1M': lambda d: -d['roc_21'],\n",
    "}\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION C: DATA CONTRACTS (UPDATED v2.2 - Verification Ready)\n",
    "# ==============================================================================\n",
    "\n",
    "@dataclass\n",
    "class EngineInput:\n",
    "    mode: str\n",
    "    start_date: pd.Timestamp\n",
    "    lookback_period: int\n",
    "    holding_period: int\n",
    "    metric: str\n",
    "    benchmark_ticker: str\n",
    "    rank_start: int = 1\n",
    "    rank_end: int = 10\n",
    "    quality_thresholds: Dict[str, float] = field(default_factory=lambda: {'min_median_dollar_volume': 1_000_000, 'max_stale_pct': 0.05, 'max_same_vol_count': 10})\n",
    "    manual_tickers: List[str] = field(default_factory=list)\n",
    "    debug: bool = False\n",
    "\n",
    "@dataclass\n",
    "class EngineOutput:\n",
    "    portfolio_series: pd.Series\n",
    "    benchmark_series: pd.Series\n",
    "    normalized_plot_data: pd.DataFrame\n",
    "    tickers: List[str]\n",
    "    initial_weights: pd.Series\n",
    "    perf_metrics: Dict[str, float]\n",
    "    results_df: pd.DataFrame\n",
    "    \n",
    "    # Dates\n",
    "    start_date: pd.Timestamp \n",
    "    decision_date: pd.Timestamp \n",
    "    buy_date: pd.Timestamp       \n",
    "    holding_end_date: pd.Timestamp \n",
    "    \n",
    "    error_msg: Optional[str] = None\n",
    "    debug_data: Optional[Dict[str, Any]] = None\n",
    "\n",
    "class AlphaEngine:\n",
    "    def __init__(self, df_ohlcv: pd.DataFrame, features_df: pd.DataFrame = None, df_close_wide: pd.DataFrame = None, master_ticker: str = 'SPY'):\n",
    "        print(\"--- ‚öôÔ∏è Initializing AlphaEngine v2.2 (Transparent Verification) ---\")\n",
    "        \n",
    "        # 1. Setup Features\n",
    "        if features_df is not None: \n",
    "            self.features_df = features_df\n",
    "        else:\n",
    "            print(\"üê¢ Calculating Features from scratch...\")\n",
    "            self.features_df = generate_features(df_ohlcv)\n",
    "            \n",
    "        # 2. Setup Prices\n",
    "        if df_close_wide is not None: \n",
    "            self.df_close = df_close_wide\n",
    "        else:\n",
    "            print(\"üê¢ Pivoting Price Data (Slow)...\")\n",
    "            self.df_close = df_ohlcv['Adj Close'].unstack(level=0)\n",
    "        \n",
    "        # 3. Setup Calendar\n",
    "        if master_ticker not in self.df_close.columns: \n",
    "            master_ticker = self.df_close.columns[0]\n",
    "        self.trading_calendar = self.df_close[master_ticker].dropna().index.unique().sort_values()\n",
    "\n",
    "    def run(self, inputs: EngineInput) -> EngineOutput:\n",
    "        \n",
    "        # --- Step 1: Validate Timeline ---\n",
    "        dates, error = self._validate_timeline(inputs)\n",
    "        if error: return self._error_result(error)\n",
    "        (safe_start, safe_decision, safe_buy, safe_end) = dates\n",
    "\n",
    "        # --- Step 2: Select Assets ---\n",
    "        tickers_to_trade, results_table, debug_dict, error = self._select_tickers(inputs, safe_start, safe_decision)\n",
    "        if error: return self._error_result(error)\n",
    "\n",
    "        # --- Step 3: Generate Equity Curves ---\n",
    "        p_val, p_ret, p_atrp = calculate_buy_and_hold_performance(\n",
    "            self.df_close, self.features_df, tickers_to_trade, safe_start, safe_end\n",
    "        )\n",
    "        b_val, b_ret, b_atrp = calculate_buy_and_hold_performance(\n",
    "            self.df_close, self.features_df, [inputs.benchmark_ticker], safe_start, safe_end\n",
    "        )\n",
    "\n",
    "        # --- Step 4: Calculate Unified Metrics & CAPTURE SLICES ---\n",
    "        metrics = {}\n",
    "        \n",
    "        # Portfolio Calculation\n",
    "        p_metrics, p_slices = self._calculate_period_metrics(p_val, p_ret, p_atrp, safe_decision, safe_buy, prefix='p')\n",
    "        metrics.update(p_metrics)\n",
    "        \n",
    "        # Benchmark Calculation\n",
    "        b_metrics, b_slices = self._calculate_period_metrics(b_val, b_ret, b_atrp, safe_decision, safe_buy, prefix='b')\n",
    "        metrics.update(b_metrics)\n",
    "\n",
    "        # Store Verification Data\n",
    "        debug_dict['verification'] = {\n",
    "            'portfolio': p_slices,\n",
    "            'benchmark': b_slices\n",
    "        }\n",
    "\n",
    "        # --- Step 5: Final Packaging ---\n",
    "        plot_data = self._get_normalized_plot_data(tickers_to_trade, safe_start, safe_end)\n",
    "        \n",
    "        if not plot_data.empty and not results_table.empty:\n",
    "            holding_period_slice = plot_data.loc[safe_buy:]\n",
    "            if len(holding_period_slice) > 0:\n",
    "                gains = (holding_period_slice.iloc[-1] / holding_period_slice.iloc[0]) - 1\n",
    "                results_table['Holding Gain'] = results_table.index.map(gains)\n",
    "\n",
    "        ticker_counts = Counter(tickers_to_trade)\n",
    "        weights = pd.Series({t: c/len(tickers_to_trade) for t, c in ticker_counts.items()})\n",
    "\n",
    "        return EngineOutput(\n",
    "            portfolio_series=p_val,\n",
    "            benchmark_series=b_val,\n",
    "            normalized_plot_data=plot_data,\n",
    "            tickers=tickers_to_trade,\n",
    "            initial_weights=weights,\n",
    "            perf_metrics=metrics,\n",
    "            results_df=results_table,\n",
    "            start_date=safe_start,\n",
    "            decision_date=safe_decision,\n",
    "            buy_date=safe_buy,\n",
    "            holding_end_date=safe_end,\n",
    "            error_msg=None,\n",
    "            debug_data=debug_dict\n",
    "        )\n",
    "\n",
    "    # ==============================================================================\n",
    "    # INTERNAL LOGIC MODULES\n",
    "    # ==============================================================================\n",
    "\n",
    "    def _validate_timeline(self, inputs: EngineInput):\n",
    "        cal = self.trading_calendar\n",
    "        if len(cal) <= inputs.lookback_period: return None, f\"Dataset too small. Need > {inputs.lookback_period} days.\"\n",
    "        min_decision_date = cal[inputs.lookback_period]\n",
    "        \n",
    "        if inputs.start_date < cal[0]: return None, f\"‚ùå Date Out of Range. Earliest date is {cal[0].date()}.\"\n",
    "        if inputs.start_date < min_decision_date: return None, f\"‚ùå Not enough history. Try {min_decision_date.date()} or later.\"\n",
    "        if inputs.start_date > cal[-1]: return None, f\"‚ùå Date Out of Range. Dataset ends on {cal[-1].date()}.\"\n",
    "\n",
    "        decision_idx = cal.searchsorted(inputs.start_date)\n",
    "        if decision_idx >= len(cal): decision_idx = len(cal) - 1\n",
    "        \n",
    "        entry_idx = decision_idx + 1\n",
    "        if entry_idx >= len(cal): return None, f\"‚ùå Cannot execute T+1. Decision date {cal[decision_idx].date()} is the last available day.\"\n",
    "            \n",
    "        required_end_idx = entry_idx + inputs.holding_period\n",
    "        if required_end_idx >= len(cal):\n",
    "            valid_idx = len(cal) - 1 - inputs.holding_period - 1\n",
    "            valid_date = cal[valid_idx] if valid_idx >= 0 else \"N/A\"\n",
    "            return None, (f\"‚ùå Not enough future data. Latest valid Decision Date: {valid_date.date()}\")\n",
    "\n",
    "        start_idx = decision_idx - inputs.lookback_period\n",
    "        return (cal[start_idx], cal[decision_idx], cal[entry_idx], cal[required_end_idx]), None\n",
    "\n",
    "    def _select_tickers(self, inputs: EngineInput, start_date, decision_date):\n",
    "        debug_dict = {}\n",
    "        if inputs.mode == 'Manual List':\n",
    "            validation_errors = []\n",
    "            valid_tickers = []\n",
    "            for t in inputs.manual_tickers:\n",
    "                if t not in self.df_close.columns: validation_errors.append(f\"‚ùå {t}: Not found.\"); continue\n",
    "                if pd.isna(self.df_close.at[start_date, t]): validation_errors.append(f\"‚ö†Ô∏è {t}: No data on start date.\"); continue\n",
    "                valid_tickers.append(t)\n",
    "            \n",
    "            if validation_errors: return [], pd.DataFrame(), {}, \"\\n\".join(validation_errors)\n",
    "            if not valid_tickers: return [], pd.DataFrame(), {}, \"No valid tickers found.\"\n",
    "            return valid_tickers, pd.DataFrame(index=valid_tickers), {}, None\n",
    "\n",
    "        else: # Ranking\n",
    "            audit_info = {}\n",
    "            eligible_tickers = self._filter_universe(decision_date, inputs.quality_thresholds, audit_info)\n",
    "            debug_dict['audit_liquidity'] = audit_info\n",
    "            \n",
    "            if not eligible_tickers: return [], pd.DataFrame(), debug_dict, \"No tickers passed quality filters.\"\n",
    "\n",
    "            lookback_close = self.df_close.loc[start_date:decision_date, eligible_tickers]\n",
    "            idx_product = pd.MultiIndex.from_product([eligible_tickers, lookback_close.index], names=['Ticker', 'Date'])\n",
    "            \n",
    "            feat_slice_current = self.features_df.xs(decision_date, level='Date').reindex(eligible_tickers)\n",
    "            feat_slice_period = self.features_df.loc[(slice(None), lookback_close.index), :].reindex(idx_product)\n",
    "            atrp_mean = feat_slice_period['ATRP'].groupby(level='Ticker').mean()\n",
    "\n",
    "            ingredients = { \n",
    "                'lookback_close': lookback_close,\n",
    "                'lookback_returns': lookback_close.ffill().pct_change(),\n",
    "                'atrp': atrp_mean,\n",
    "                'roc_1': feat_slice_current['ROC_1'],\n",
    "                'roc_3': feat_slice_current['ROC_3'],\n",
    "                'roc_5': feat_slice_current['ROC_5'],\n",
    "                'roc_10': feat_slice_current['ROC_10'],\n",
    "                'roc_21': feat_slice_current['ROC_21']\n",
    "            }\n",
    "\n",
    "            if inputs.metric not in METRIC_REGISTRY: return [], pd.DataFrame(), {}, f\"Strategy '{inputs.metric}' not found.\"\n",
    "            metric_vals = METRIC_REGISTRY[inputs.metric](ingredients)\n",
    "            sorted_tickers = metric_vals.sort_values(ascending=False)\n",
    "            \n",
    "            start_r = max(0, inputs.rank_start - 1)\n",
    "            end_r = inputs.rank_end\n",
    "            selected_tickers = sorted_tickers.iloc[start_r:end_r].index.tolist()\n",
    "            \n",
    "            if not selected_tickers: return [], pd.DataFrame(), debug_dict, \"No tickers generated from ranking.\"\n",
    "\n",
    "            results_table = pd.DataFrame({\n",
    "                'Rank': range(inputs.rank_start, inputs.rank_start + len(selected_tickers)),\n",
    "                'Ticker': selected_tickers,\n",
    "                'Strategy Value': sorted_tickers.loc[selected_tickers].values\n",
    "            }).set_index('Ticker')\n",
    "\n",
    "            return selected_tickers, results_table, debug_dict, None\n",
    "\n",
    "    def _filter_universe(self, date_ts, thresholds, audit_container=None):\n",
    "        avail_dates = self.features_df.index.get_level_values('Date').unique().sort_values()\n",
    "        valid_dates = avail_dates[avail_dates <= date_ts]\n",
    "        if valid_dates.empty: return []\n",
    "        target_date = valid_dates[-1]\n",
    "        day_features = self.features_df.xs(target_date, level='Date')\n",
    "\n",
    "        vol_cutoff = thresholds.get('min_median_dollar_volume', 0)\n",
    "        percentile_used = \"N/A\"\n",
    "        if 'min_liquidity_percentile' in thresholds:\n",
    "            percentile_used = thresholds['min_liquidity_percentile']\n",
    "            dynamic_val = day_features['RollMedDollarVol'].quantile(percentile_used)\n",
    "            vol_cutoff = max(vol_cutoff, dynamic_val)\n",
    "\n",
    "        mask = (\n",
    "            (day_features['RollMedDollarVol'] >= vol_cutoff) &\n",
    "            (day_features['RollingStalePct'] <= thresholds['max_stale_pct']) &\n",
    "            (day_features['RollingSameVolCount'] <= thresholds['max_same_vol_count'])\n",
    "        )\n",
    "\n",
    "        if audit_container is not None:\n",
    "            audit_container['date'] = target_date\n",
    "            audit_container['total_tickers_available'] = len(day_features)\n",
    "            audit_container['percentile_setting'] = percentile_used\n",
    "            audit_container['final_cutoff_usd'] = vol_cutoff\n",
    "            audit_container['tickers_passed'] = mask.sum()\n",
    "            snapshot = day_features.copy()\n",
    "            snapshot['Calculated_Cutoff'] = vol_cutoff\n",
    "            snapshot['Passed_Vol_Check'] = snapshot['RollMedDollarVol'] >= vol_cutoff\n",
    "            snapshot['Passed_Final'] = mask\n",
    "            snapshot = snapshot.sort_values('RollMedDollarVol', ascending=False)\n",
    "            audit_container['universe_snapshot'] = snapshot\n",
    "\n",
    "        return day_features[mask].index.tolist()\n",
    "\n",
    "    def _calculate_period_metrics(self, val_series, ret_series, atrp_series, decision_date, buy_date, prefix):\n",
    "        \"\"\"\n",
    "        Returns (metrics_dict, verification_slices_dict)\n",
    "        \"\"\"\n",
    "        metrics = {}\n",
    "        slices = {} # Store the exact Series used for math\n",
    "        \n",
    "        if val_series.empty: return metrics, slices\n",
    "\n",
    "        def get_gain(s):\n",
    "            return (s.iloc[-1] / s.iloc[0]) - 1 if len(s) > 0 else 0\n",
    "\n",
    "        # --- A. Define Slices ---\n",
    "        \n",
    "        # 1. Full\n",
    "        full_val = val_series\n",
    "        full_ret = ret_series\n",
    "        full_atrp = atrp_series\n",
    "\n",
    "        # 2. Lookback (Start -> T0)\n",
    "        lookback_val = val_series.loc[:decision_date]\n",
    "        lookback_ret = ret_series.loc[:decision_date]\n",
    "        lookback_atrp = atrp_series.loc[lookback_ret.index]\n",
    "\n",
    "        # 3. Holding (T1 -> End)\n",
    "        holding_val = val_series.loc[buy_date:]\n",
    "        if not holding_val.empty:\n",
    "            holding_ret = holding_val.pct_change()\n",
    "            holding_atrp = atrp_series.reindex(holding_ret.index)\n",
    "        else:\n",
    "            holding_ret = pd.Series(dtype=float)\n",
    "            holding_atrp = pd.Series(dtype=float)\n",
    "\n",
    "        # --- B. Calculate Metrics ---\n",
    "        metrics[f'full_{prefix}_gain'] = get_gain(full_val)\n",
    "        metrics[f'lookback_{prefix}_gain'] = get_gain(lookback_val)\n",
    "        metrics[f'holding_{prefix}_gain'] = get_gain(holding_val)\n",
    "\n",
    "        metrics[f'full_{prefix}_sharpe'] = calculate_sharpe(full_ret)\n",
    "        metrics[f'lookback_{prefix}_sharpe'] = calculate_sharpe(lookback_ret)\n",
    "        metrics[f'holding_{prefix}_sharpe'] = calculate_sharpe(holding_ret)\n",
    "\n",
    "        metrics[f'full_{prefix}_sharpe_atr'] = calculate_sharpe_atr(full_ret, full_atrp)\n",
    "        metrics[f'lookback_{prefix}_sharpe_atr'] = calculate_sharpe_atr(lookback_ret, lookback_atrp)\n",
    "        metrics[f'holding_{prefix}_sharpe_atr'] = calculate_sharpe_atr(holding_ret, holding_atrp)\n",
    "\n",
    "        # --- C. Populate Slices for Verification ---\n",
    "        slices['lookback_val'] = lookback_val\n",
    "        slices['lookback_ret'] = lookback_ret\n",
    "        slices['holding_val'] = holding_val\n",
    "        slices['holding_ret'] = holding_ret # This is the exact series passed to sharpe()\n",
    "        slices['holding_atrp'] = holding_atrp\n",
    "\n",
    "        return metrics, slices\n",
    "\n",
    "    def _get_normalized_plot_data(self, tickers, start_date, end_date):\n",
    "        if not tickers: return pd.DataFrame()\n",
    "        data = self.df_close[list(set(tickers))].loc[start_date:end_date]\n",
    "        if data.empty: return pd.DataFrame()\n",
    "        return data / data.bfill().iloc[0]\n",
    "\n",
    "    def _error_result(self, msg):\n",
    "        return EngineOutput(\n",
    "            portfolio_series=pd.Series(dtype=float), \n",
    "            benchmark_series=pd.Series(dtype=float), \n",
    "            normalized_plot_data=pd.DataFrame(), \n",
    "            tickers=[], \n",
    "            initial_weights=pd.Series(dtype=float), \n",
    "            perf_metrics={}, \n",
    "            results_df=pd.DataFrame(), \n",
    "            start_date=pd.Timestamp.min, \n",
    "            decision_date=pd.Timestamp.min, \n",
    "            buy_date=pd.Timestamp.min,      \n",
    "            holding_end_date=pd.Timestamp.min, \n",
    "            error_msg=msg\n",
    "        )\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION E: THE UI (Visualization) - UPDATED v2.4 (Complete Timeline)\n",
    "# ==============================================================================\n",
    "\n",
    "def plot_walk_forward_analyzer(df_ohlcv, \n",
    "                               precomputed_features=None,\n",
    "                               precomputed_close=None, \n",
    "                               default_start_date='2020-01-01', \n",
    "                               default_lookback=126, \n",
    "                               default_holding=63,\n",
    "                               default_strategy='Sharpe (ATR)', \n",
    "                               default_rank_start=1, \n",
    "                               default_rank_end=10,\n",
    "                               default_benchmark_ticker='SPY', \n",
    "                               master_calendar_ticker='XOM', \n",
    "                               quality_thresholds=None, \n",
    "                               debug=False):\n",
    "    \n",
    "    engine = AlphaEngine(\n",
    "        df_ohlcv, \n",
    "        features_df=precomputed_features, \n",
    "        df_close_wide=precomputed_close, \n",
    "        master_ticker=master_calendar_ticker\n",
    "    )\n",
    "    \n",
    "    # Initialize containers\n",
    "    results_container = [None]\n",
    "    debug_container = [{}] \n",
    "\n",
    "    if quality_thresholds is None:\n",
    "        quality_thresholds = {\n",
    "            'min_median_dollar_volume': 100_000, \n",
    "            'min_liquidity_percentile': 0.50,    \n",
    "            'max_stale_pct': 0.05, \n",
    "            'max_same_vol_count': 10\n",
    "        }\n",
    "\n",
    "    # --- Widgets ---\n",
    "    mode_selector = widgets.RadioButtons(options=['Ranking', 'Manual List'], value='Ranking', description='Mode:', layout={'width': 'max-content'}, style={'description_width': 'initial'})\n",
    "    lookback_input = widgets.IntText(value=default_lookback, description='Lookback (Days):', layout={'width': '200px'}, style={'description_width': 'initial'})\n",
    "    decision_date_picker = widgets.DatePicker(description='Decision Date:', value=pd.to_datetime(default_start_date), layout={'width': 'auto'}, style={'description_width': 'initial'})\n",
    "    holding_input = widgets.IntText(value=default_holding, description='Holding (Days):', layout={'width': '200px'}, style={'description_width': 'initial'})\n",
    "    strategy_dropdown = widgets.Dropdown(options=list(METRIC_REGISTRY.keys()), value=default_strategy, description='Strategy:', layout={'width': '220px'}, style={'description_width': 'initial'})\n",
    "    benchmark_input = widgets.Text(value=default_benchmark_ticker, description='Benchmark:', placeholder='Enter Ticker', layout={'width': '180px'}, style={'description_width': 'initial'})\n",
    "    rank_start_input = widgets.IntText(value=default_rank_start, description='Rank Start:', layout={'width': '150px'}, style={'description_width': 'initial'})\n",
    "    rank_end_input = widgets.IntText(value=default_rank_end, description='Rank End:', layout={'width': '150px'}, style={'description_width': 'initial'})\n",
    "    manual_tickers_input = widgets.Textarea(value='', placeholder='Enter tickers...', description='Manual Tickers:', layout={'width': '400px', 'height': '80px'}, style={'description_width': 'initial'})\n",
    "    update_button = widgets.Button(description=\"Run Simulation\", button_style='primary')\n",
    "    ticker_list_output = widgets.Output()\n",
    "\n",
    "    # --- Layouts ---\n",
    "    timeline_box = widgets.HBox([lookback_input, decision_date_picker, holding_input], layout=widgets.Layout(justify_content='space-between', border='1px solid #ddd', padding='10px', margin='5px'))\n",
    "    strategy_box = widgets.HBox([strategy_dropdown, benchmark_input])\n",
    "    ranking_box = widgets.HBox([rank_start_input, rank_end_input])\n",
    "    \n",
    "    def on_mode_change(c):\n",
    "        ranking_box.layout.display = 'flex' if c['new'] == 'Ranking' else 'none'\n",
    "        manual_tickers_input.layout.display = 'none' if c['new'] == 'Ranking' else 'flex'\n",
    "        strategy_dropdown.disabled = (c['new'] == 'Manual List')\n",
    "    mode_selector.observe(on_mode_change, names='value')\n",
    "    on_mode_change({'new': mode_selector.value})\n",
    "\n",
    "    ui = widgets.VBox([\n",
    "        widgets.HTML(\"<b>1. Timeline Configuration:</b> (Past <--- Decision ---> Future)\"),\n",
    "        timeline_box,\n",
    "        widgets.HTML(\"<b>2. Strategy Settings:</b>\"),\n",
    "        widgets.HBox([mode_selector, strategy_box]),\n",
    "        ranking_box,\n",
    "        manual_tickers_input,\n",
    "        widgets.HTML(\"<hr>\"),\n",
    "        update_button,\n",
    "        ticker_list_output\n",
    "    ], layout=widgets.Layout(margin='10px 0 20px 0'))\n",
    "    \n",
    "    fig = go.FigureWidget()\n",
    "    fig.update_layout(title='Event-Driven Walk-Forward Analysis', height=600, template=\"plotly_white\", hovermode='x unified')\n",
    "    for i in range(50): fig.add_trace(go.Scatter(visible=False, line=dict(width=2)))\n",
    "    fig.add_trace(go.Scatter(name='Benchmark', visible=True, line=dict(color='black', width=3, dash='dash')))\n",
    "    fig.add_trace(go.Scatter(name='Group Portfolio', visible=True, line=dict(color='green', width=3)))\n",
    "\n",
    "    # --- Update Logic ---\n",
    "    def update_plot(b):\n",
    "        ticker_list_output.clear_output()\n",
    "        manual_list = [t.strip().upper() for t in manual_tickers_input.value.split(',') if t.strip()]\n",
    "        decision_date_raw = pd.to_datetime(decision_date_picker.value)\n",
    "        \n",
    "        inputs = EngineInput(\n",
    "            mode=mode_selector.value,\n",
    "            start_date=decision_date_raw, \n",
    "            lookback_period=lookback_input.value, \n",
    "            holding_period=holding_input.value,  \n",
    "            metric=strategy_dropdown.value,\n",
    "            benchmark_ticker=benchmark_input.value.strip().upper(),\n",
    "            rank_start=rank_start_input.value,\n",
    "            rank_end=rank_end_input.value,\n",
    "            quality_thresholds=quality_thresholds,\n",
    "            manual_tickers=manual_list,\n",
    "            debug=debug\n",
    "        )\n",
    "        \n",
    "        # --- CAPTURE INPUTS FOR AUDIT ---\n",
    "        debug_container[0]['inputs'] = inputs\n",
    "        \n",
    "        with ticker_list_output:\n",
    "            res = engine.run(inputs)\n",
    "            results_container[0] = res\n",
    "            \n",
    "            # --- MERGE ENGINE DEBUG DATA ---\n",
    "            if res.debug_data:\n",
    "                debug_container[0].update(res.debug_data)\n",
    "                \n",
    "            if res.error_msg: \n",
    "                print(f\"‚ö†Ô∏è Simulation Stopped: {res.error_msg}\")\n",
    "                return\n",
    "\n",
    "            # Plotting\n",
    "            with fig.batch_update():\n",
    "                cols = res.normalized_plot_data.columns.tolist()\n",
    "                for i in range(50):\n",
    "                    if i < len(cols): fig.data[i].update(x=res.normalized_plot_data.index, y=res.normalized_plot_data[cols[i]], name=cols[i], visible=True)\n",
    "                    else: fig.data[i].visible = False\n",
    "                \n",
    "                fig.data[50].update(x=res.benchmark_series.index, y=res.benchmark_series.values, name=f\"Benchmark ({inputs.benchmark_ticker})\", visible=not res.benchmark_series.empty)\n",
    "                fig.data[51].update(x=res.portfolio_series.index, y=res.portfolio_series.values, visible=True)\n",
    "                \n",
    "                # Visual Lines\n",
    "                fig.layout.shapes = [\n",
    "                    dict(type=\"line\", x0=res.decision_date, y0=0, x1=res.decision_date, y1=1, xref='x', yref='paper', line=dict(color=\"red\", width=2, dash=\"dash\")),\n",
    "                    dict(type=\"line\", x0=res.buy_date, y0=0, x1=res.buy_date, y1=1, xref='x', yref='paper', line=dict(color=\"blue\", width=2, dash=\"dot\"))\n",
    "                ]\n",
    "                \n",
    "                fig.layout.annotations = [\n",
    "                    dict(x=res.decision_date, y=0.05, xref=\"x\", yref=\"paper\", text=\"DECISION\", showarrow=False, bgcolor=\"red\", font=dict(color=\"white\")),\n",
    "                    dict(x=res.buy_date, y=1.0, xref=\"x\", yref=\"paper\", text=\"ENTRY (T+1)\", showarrow=False, bgcolor=\"blue\", font=dict(color=\"white\"))\n",
    "                ]\n",
    "\n",
    "            start_date = res.start_date.date()\n",
    "            act_date = res.decision_date.date()\n",
    "            entry_date = res.buy_date.date()\n",
    "            \n",
    "            # Liquidity Audit Print\n",
    "            if inputs.mode == 'Ranking' and res.debug_data and 'audit_liquidity' in res.debug_data:\n",
    "                audit = res.debug_data['audit_liquidity']\n",
    "                if audit:\n",
    "                    pct_str = f\"{audit.get('percentile_setting', 0)*100:.0f}%\"\n",
    "                    cut_val = audit.get('final_cutoff_usd', 0)\n",
    "                    print(\"-\" * 60)\n",
    "                    print(f\"üîç LIQUIDITY CHECK (On Decision Date: {act_date})\")\n",
    "                    print(f\"   Universe Size: {audit.get('total_tickers_available')} tickers\")\n",
    "                    print(f\"   Filtering: Top {pct_str} of Market\")\n",
    "                    print(f\"   Calculated Cutoff: ${cut_val:,.0f} / day\")\n",
    "                    print(f\"   Tickers Remaining: {audit.get('tickers_passed')}\")\n",
    "                    print(\"-\" * 60)\n",
    "            \n",
    "            # --- UPDATED TIMELINE PRINT ---\n",
    "            print(f\"Timeline: Start [ {start_date} ] --> Decision [ {act_date} ] --> Cash (1d) --> Entry [ {entry_date} ] --> End [ {res.holding_end_date.date()} ]\")\n",
    "            \n",
    "            if inputs.mode == 'Ranking': \n",
    "                print(f\"Ranked Tickers ({len(res.tickers)}):\")\n",
    "                for i in range(0, len(res.tickers), 10):\n",
    "                    print(\", \".join(res.tickers[i:i+10]))\n",
    "            else: \n",
    "                print(\"Manual Portfolio Tickers:\")\n",
    "                for i in range(0, len(res.tickers), 10):\n",
    "                    print(\", \".join(res.tickers[i:i+10]))\n",
    "            \n",
    "            m = res.perf_metrics\n",
    "            \n",
    "            rows = [\n",
    "                # Gain\n",
    "                {'Metric': 'Group Portfolio Gain', 'Full': m.get('full_p_gain'), 'Lookback': m.get('lookback_p_gain'), 'Holding': m.get('holding_p_gain')},\n",
    "                {'Metric': f'Benchmark ({inputs.benchmark_ticker}) Gain', 'Full': m.get('full_b_gain'), 'Lookback': m.get('lookback_b_gain'), 'Holding': m.get('holding_b_gain')},\n",
    "                {'Metric': '== Gain Delta', 'Full': m.get('full_p_gain',0)-m.get('full_b_gain',0), 'Lookback': m.get('lookback_p_gain',0)-m.get('lookback_b_gain',0), 'Holding': m.get('holding_p_gain',0)-m.get('holding_b_gain',0)},\n",
    "                \n",
    "                # Sharpe\n",
    "                {'Metric': 'Group Sharpe', 'Full': m.get('full_p_sharpe'), 'Lookback': m.get('lookback_p_sharpe'), 'Holding': m.get('holding_p_sharpe')},\n",
    "                {'Metric': f'Benchmark Sharpe', 'Full': m.get('full_b_sharpe'), 'Lookback': m.get('lookback_b_sharpe'), 'Holding': m.get('holding_b_sharpe')},\n",
    "                {'Metric': '== Sharpe Delta', 'Full': m.get('full_p_sharpe',0)-m.get('full_b_sharpe',0), 'Lookback': m.get('lookback_p_sharpe',0)-m.get('lookback_b_sharpe',0), 'Holding': m.get('holding_p_sharpe',0)-m.get('holding_b_sharpe',0)},\n",
    "\n",
    "                # Sharpe ATR\n",
    "                {'Metric': 'Group Sharpe (ATR)', 'Full': m.get('full_p_sharpe_atr'), 'Lookback': m.get('lookback_p_sharpe_atr'), 'Holding': m.get('holding_p_sharpe_atr')},\n",
    "                {'Metric': f'Benchmark Sharpe (ATR)', 'Full': m.get('full_b_sharpe_atr'), 'Lookback': m.get('lookback_b_sharpe_atr'), 'Holding': m.get('holding_b_sharpe_atr')},\n",
    "                {'Metric': '== Sharpe (ATR) Delta', 'Full': m.get('full_p_sharpe_atr',0)-m.get('full_b_sharpe_atr',0), 'Lookback': m.get('lookback_p_sharpe_atr',0)-m.get('lookback_b_sharpe_atr',0), 'Holding': m.get('holding_p_sharpe_atr',0)-m.get('holding_b_sharpe_atr',0)}\n",
    "            ]\n",
    "            \n",
    "            display(pd.DataFrame(rows).set_index('Metric').style.format(\"{:+.4f}\", na_rep=\"N/A\"))\n",
    "\n",
    "    update_button.on_click(update_plot)\n",
    "    update_plot(None)\n",
    "    display(ui, fig)\n",
    "    return results_container, debug_container\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a48017a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# SECTION F: UTILITIES\n",
    "# ==============================================================================\n",
    "\n",
    "def print_nested(d, indent=0, width=4):\n",
    "    \"\"\"Pretty-print any nested dict/list/tuple combination.\"\"\"\n",
    "    spacing = ' ' * indent\n",
    "    if isinstance(d, dict):\n",
    "        for k, v in d.items():\n",
    "            print(f'{spacing}{k}:')\n",
    "            print_nested(v, indent + width, width)\n",
    "    elif isinstance(d, (list, tuple)):\n",
    "        for item in d:\n",
    "            print_nested(item, indent, width)\n",
    "    else:\n",
    "        print(f'{spacing}{d}')\n",
    "\n",
    "def get_ticker_OHLCV(\n",
    "    df_ohlcv: pd.DataFrame,\n",
    "    tickers: Union[str, List[str]],\n",
    "    date_start: str,\n",
    "    date_end: str,\n",
    "    return_format: str = \"dataframe\",\n",
    "    verbose: bool = True\n",
    ") -> Union[pd.DataFrame, dict]:\n",
    "    \"\"\"\n",
    "    Get OHLCV data for specified tickers within a date range.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_ohlcv : pd.DataFrame\n",
    "        DataFrame with MultiIndex of (ticker, date) and OHLCV columns\n",
    "    tickers : str or list of str\n",
    "        Ticker symbol(s) to retrieve\n",
    "    date_start : str\n",
    "        Start date in 'YYYY-MM-DD' format\n",
    "    date_end : str\n",
    "        End date in 'YYYY-MM-DD' format\n",
    "    return_format : str, optional\n",
    "        Format to return data in. Options: \n",
    "        - 'dataframe': Single DataFrame with MultiIndex (default)\n",
    "        - 'dict': Dictionary with tickers as keys and DataFrames as values\n",
    "        - 'separate': List of separate DataFrames for each ticker\n",
    "    verbose : bool, optional\n",
    "        Whether to print summary information (default: True)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Union[pd.DataFrame, dict, list]\n",
    "        Filtered OHLCV data in specified format\n",
    "    \n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If input parameters are invalid\n",
    "    KeyError\n",
    "        If tickers not found in DataFrame\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> # Get data for single ticker\n",
    "    >>> vlo_data = get_ticker_OHLCV(df_ohlcv, 'VLO', '2025-08-13', '2025-09-04')\n",
    "    \n",
    "    >>> # Get data for multiple tickers\n",
    "    >>> multi_data = get_ticker_OHLCV(df_ohlcv, ['VLO', 'JPST'], '2025-08-13', '2025-09-04')\n",
    "    \n",
    "    >>> # Get data as dictionary\n",
    "    >>> data_dict = get_ticker_OHLCV(df_ohlcv, ['VLO', 'JPST'], '2025-08-13', \n",
    "    ...                              '2025-09-04', return_format='dict')\n",
    "    \"\"\"\n",
    "    \n",
    "    # Input validation\n",
    "    if not isinstance(df_ohlcv, pd.DataFrame):\n",
    "        raise TypeError(\"df_ohlcv must be a pandas DataFrame\")\n",
    "    \n",
    "    if not isinstance(df_ohlcv.index, pd.MultiIndex):\n",
    "        raise ValueError(\"DataFrame must have MultiIndex of (ticker, date)\")\n",
    "    \n",
    "    if len(df_ohlcv.index.levels) != 2:\n",
    "        raise ValueError(\"MultiIndex must have exactly 2 levels: (ticker, date)\")\n",
    "    \n",
    "    # Convert single ticker to list for consistent processing\n",
    "    if isinstance(tickers, str):\n",
    "        tickers = [tickers]\n",
    "    elif not isinstance(tickers, list):\n",
    "        raise TypeError(\"tickers must be a string or list of strings\")\n",
    "    \n",
    "    # Convert dates to Timestamps\n",
    "    try:\n",
    "        start_date = pd.Timestamp(date_start)\n",
    "        end_date = pd.Timestamp(date_end)\n",
    "    except ValueError as e:\n",
    "        raise ValueError(f\"Invalid date format. Use 'YYYY-MM-DD': {e}\")\n",
    "    \n",
    "    if start_date > end_date:\n",
    "        raise ValueError(\"date_start must be before or equal to date_end\")\n",
    "    \n",
    "    # Check if tickers exist in the DataFrame\n",
    "    available_tickers = df_ohlcv.index.get_level_values(0).unique()\n",
    "    missing_tickers = [t for t in tickers if t not in available_tickers]\n",
    "    \n",
    "    if missing_tickers:\n",
    "        raise KeyError(f\"Ticker(s) not found in DataFrame: {missing_tickers}\")\n",
    "    \n",
    "    # Filter the data using MultiIndex slicing\n",
    "    try:\n",
    "        filtered_data = df_ohlcv.loc[(tickers, slice(date_start, date_end)), :]\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error filtering data: {e}\")\n",
    "    \n",
    "    # Handle empty results\n",
    "    if filtered_data.empty:\n",
    "        if verbose:\n",
    "            print(f\"No data found for tickers {tickers} in date range {date_start} to {date_end}\")\n",
    "        return filtered_data\n",
    "    \n",
    "    # Print summary if verbose\n",
    "    if verbose:\n",
    "        print(f\"Data retrieved for {len(tickers)} ticker(s) from {date_start} to {date_end}\")\n",
    "        print(f\"Total rows: {len(filtered_data)}\")\n",
    "        print(f\"Date range in data: {filtered_data.index.get_level_values(1).min()} to \"\n",
    "              f\"{filtered_data.index.get_level_values(1).max()}\")\n",
    "        \n",
    "        # Print ticker-specific counts\n",
    "        ticker_counts = filtered_data.index.get_level_values(0).value_counts()\n",
    "        for ticker in tickers:\n",
    "            count = ticker_counts.get(ticker, 0)\n",
    "            if count > 0:\n",
    "                print(f\"  {ticker}: {count} rows\")\n",
    "            else:\n",
    "                print(f\"  {ticker}: No data in range\")\n",
    "    \n",
    "    # Return in requested format\n",
    "    if return_format == \"dict\":\n",
    "        result = {}\n",
    "        for ticker in tickers:\n",
    "            try:\n",
    "                result[ticker] = filtered_data.xs(ticker, level=0).loc[date_start:date_end]\n",
    "            except KeyError:\n",
    "                result[ticker] = pd.DataFrame()\n",
    "        return result\n",
    "    \n",
    "    elif return_format == \"separate\":\n",
    "        result = []\n",
    "        for ticker in tickers:\n",
    "            try:\n",
    "                result.append(filtered_data.xs(ticker, level=0).loc[date_start:date_end])\n",
    "            except KeyError:\n",
    "                result.append(pd.DataFrame())\n",
    "        return result\n",
    "    \n",
    "    elif return_format == \"dataframe\":\n",
    "        return filtered_data\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Invalid return_format: {return_format}. \"\n",
    "                         f\"Must be 'dataframe', 'dict', or 'separate'\")\n",
    "\n",
    "def get_ticker_features(\n",
    "    features_df: pd.DataFrame,\n",
    "    tickers: Union[str, List[str]],\n",
    "    date_start: str,\n",
    "    date_end: str,\n",
    "    return_format: str = \"dataframe\",\n",
    "    verbose: bool = True\n",
    ") -> Union[pd.DataFrame, dict]:\n",
    "    \"\"\"\n",
    "    Get features data for specified tickers within a date range.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    features_df : pd.DataFrame\n",
    "        DataFrame with MultiIndex of (ticker, date) and feature columns\n",
    "    tickers : str or list of str\n",
    "        Ticker symbol(s) to retrieve\n",
    "    date_start : str\n",
    "        Start date in 'YYYY-MM-DD' format\n",
    "    date_end : str\n",
    "        End date in 'YYYY-MM-DD' format\n",
    "    return_format : str, optional\n",
    "        Format to return data in. Options: \n",
    "        - 'dataframe': Single DataFrame with MultiIndex (default)\n",
    "        - 'dict': Dictionary with tickers as keys and DataFrames as values\n",
    "        - 'separate': List of separate DataFrames for each ticker\n",
    "    verbose : bool, optional\n",
    "        Whether to print summary information (default: True)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Union[pd.DataFrame, dict, list]\n",
    "        Filtered features data in specified format\n",
    "    \"\"\"\n",
    "    # Convert single ticker to list for consistent processing\n",
    "    if isinstance(tickers, str):\n",
    "        tickers = [tickers]\n",
    "    \n",
    "    # Filter the data using MultiIndex slicing\n",
    "    try:\n",
    "        filtered_data = features_df.loc[(tickers, slice(date_start, date_end)), :]\n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(f\"Error filtering data: {e}\")\n",
    "        return pd.DataFrame() if return_format == \"dataframe\" else {}\n",
    "    \n",
    "    # Handle empty results\n",
    "    if filtered_data.empty:\n",
    "        if verbose:\n",
    "            print(f\"No data found for tickers {tickers} in date range {date_start} to {date_end}\")\n",
    "        return filtered_data\n",
    "    \n",
    "    # Print summary if verbose\n",
    "    if verbose:\n",
    "        print(f\"Features data retrieved for {len(tickers)} ticker(s) from {date_start} to {date_end}\")\n",
    "        print(f\"Total rows: {len(filtered_data)}\")\n",
    "        print(f\"Date range in data: {filtered_data.index.get_level_values(1).min()} to \"\n",
    "              f\"{filtered_data.index.get_level_values(1).max()}\")\n",
    "        print(f\"Available features: {', '.join(filtered_data.columns.tolist())}\")\n",
    "        \n",
    "        # Print ticker-specific counts\n",
    "        ticker_counts = filtered_data.index.get_level_values(0).value_counts()\n",
    "        for ticker in tickers:\n",
    "            count = ticker_counts.get(ticker, 0)\n",
    "            if count > 0:\n",
    "                print(f\"  {ticker}: {count} rows\")\n",
    "            else:\n",
    "                print(f\"  {ticker}: No data in range\")\n",
    "    \n",
    "    # Return in requested format\n",
    "    if return_format == \"dict\":\n",
    "        result = {}\n",
    "        for ticker in tickers:\n",
    "            try:\n",
    "                result[ticker] = filtered_data.xs(ticker, level=0).loc[date_start:date_end]\n",
    "            except KeyError:\n",
    "                result[ticker] = pd.DataFrame()\n",
    "        return result\n",
    "    \n",
    "    elif return_format == \"separate\":\n",
    "        result = []\n",
    "        for ticker in tickers:\n",
    "            try:\n",
    "                result.append(filtered_data.xs(ticker, level=0).loc[date_start:date_end])\n",
    "            except KeyError:\n",
    "                result.append(pd.DataFrame())\n",
    "        return result\n",
    "    \n",
    "    elif return_format == \"dataframe\":\n",
    "        return filtered_data\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Invalid return_format: {return_format}. \"\n",
    "                         f\"Must be 'dataframe', 'dict', or 'separate'\")\n",
    "\n",
    "def create_combined_dict(\n",
    "    df_ohlcv: pd.DataFrame,\n",
    "    features_df: pd.DataFrame,\n",
    "    tickers: Union[str, List[str]],\n",
    "    date_start: str,\n",
    "    date_end: str,\n",
    "    verbose: bool = True\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Create a combined dictionary with both OHLCV and features data for each ticker.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_ohlcv : pd.DataFrame\n",
    "        DataFrame with OHLCV data (MultiIndex: ticker, date)\n",
    "    features_df : pd.DataFrame\n",
    "        DataFrame with features data (MultiIndex: ticker, date)\n",
    "    tickers : str or list of str\n",
    "        Ticker symbol(s) to retrieve\n",
    "    date_start : str\n",
    "        Start date in 'YYYY-MM-DD' format\n",
    "    date_end : str\n",
    "        End date in 'YYYY-MM-DD' format\n",
    "    verbose : bool, optional\n",
    "        Whether to print progress information (default: True)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with tickers as keys and combined DataFrames (OHLCV + features) as values\n",
    "    \"\"\"\n",
    "    # Convert single ticker to list\n",
    "    if isinstance(tickers, str):\n",
    "        tickers = [tickers]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Creating combined dictionary for {len(tickers)} ticker(s)\")\n",
    "        print(f\"Date range: {date_start} to {date_end}\")\n",
    "        print(\"=\" * 60)\n",
    "    \n",
    "    # Get OHLCV data as dictionary\n",
    "    ohlcv_dict = get_ticker_OHLCV(\n",
    "        df_ohlcv, tickers, date_start, date_end, \n",
    "        return_format='dict', verbose=verbose\n",
    "    )\n",
    "    \n",
    "    # Get features data as dictionary\n",
    "    features_dict = get_ticker_features(\n",
    "        features_df, tickers, date_start, date_end,\n",
    "        return_format='dict', verbose=verbose\n",
    "    )\n",
    "    \n",
    "    # Create combined_dict\n",
    "    combined_dict = {}\n",
    "    \n",
    "    for ticker in tickers:\n",
    "        if verbose:\n",
    "            print(f\"\\nProcessing {ticker}...\")\n",
    "        \n",
    "        # Check if ticker exists in both dictionaries\n",
    "        if ticker in ohlcv_dict and ticker in features_dict:\n",
    "            ohlcv_data = ohlcv_dict[ticker]\n",
    "            features_data = features_dict[ticker]\n",
    "            \n",
    "            # Check if both dataframes have data\n",
    "            if not ohlcv_data.empty and not features_data.empty:\n",
    "                # Combine OHLCV and features data\n",
    "                # Note: Both dataframes have the same index (dates), so we can concatenate\n",
    "                combined_df = pd.concat([ohlcv_data, features_data], axis=1)\n",
    "                \n",
    "                # Ensure proper index naming\n",
    "                combined_df.index.name = 'Date'\n",
    "                \n",
    "                # Store in combined_dict\n",
    "                combined_dict[ticker] = combined_df\n",
    "                \n",
    "                if verbose:\n",
    "                    print(f\"  ‚úì Successfully combined data\")\n",
    "                    print(f\"  OHLCV shape: {ohlcv_data.shape}\")\n",
    "                    print(f\"  Features shape: {features_data.shape}\")\n",
    "                    print(f\"  Combined shape: {combined_df.shape}\")\n",
    "                    print(f\"  Date range: {combined_df.index.min()} to {combined_df.index.max()}\")\n",
    "            else:\n",
    "                if verbose:\n",
    "                    print(f\"  ‚úó Cannot combine: One or both dataframes are empty\")\n",
    "                    print(f\"    OHLCV empty: {ohlcv_data.empty}\")\n",
    "                    print(f\"    Features empty: {features_data.empty}\")\n",
    "                combined_dict[ticker] = pd.DataFrame()\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(f\"  ‚úó Ticker not found in both dictionaries\")\n",
    "                if ticker not in ohlcv_dict:\n",
    "                    print(f\"    Not in OHLCV data\")\n",
    "                if ticker not in features_dict:\n",
    "                    print(f\"    Not in features data\")\n",
    "            combined_dict[ticker] = pd.DataFrame()\n",
    "    \n",
    "    # Print summary\n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"SUMMARY\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Total tickers processed: {len(tickers)}\")\n",
    "        \n",
    "        tickers_with_data = [ticker for ticker, df in combined_dict.items() if not df.empty]\n",
    "        print(f\"Tickers with combined data: {len(tickers_with_data)}\")\n",
    "        \n",
    "        if tickers_with_data:\n",
    "            print(\"\\nTicker details:\")\n",
    "            for ticker in tickers_with_data:\n",
    "                df = combined_dict[ticker]\n",
    "                print(f\"  {ticker}: {df.shape} - {df.index.min()} to {df.index.max()}\")\n",
    "                print(f\"    Columns: {len(df.columns)}\")\n",
    "        \n",
    "        empty_tickers = [ticker for ticker, df in combined_dict.items() if df.empty]\n",
    "        if empty_tickers:\n",
    "            print(f\"\\nTickers with no data: {', '.join(empty_tickers)}\")\n",
    "    \n",
    "    return combined_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb31c4e",
   "metadata": {},
   "source": [
    "### Unit Test for Generated Features by Function generate_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2989c8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STARTING UNIT TESTS\n",
      "============================================================\n",
      "Running test_math_metrics...\n",
      "‚úÖ Gain Calc (Positive): Passed\n",
      "‚úÖ Gain Calc (Negative): Passed\n",
      "‚úÖ Sharpe (Zero Volatility): Passed (Returns 0.0)\n",
      "\n",
      "Running test_engine_lag_logic...\n",
      "--- ‚öôÔ∏è Initializing AlphaEngine v2.2 (Transparent Verification) ---\n",
      "üê¢ Calculating Features from scratch...\n",
      "üê¢ Pivoting Price Data (Slow)...\n",
      "  Decision Date: 2024-01-02\n",
      "  Buy Date (T+1): 2024-01-03\n",
      "  Holding Gain: 0.1000\n",
      "‚úÖ LAG VERIFIED: Holding Gain is 10.00%.\n",
      "\n",
      "============================================================\n",
      "TEST SUMMARY\n",
      "============================================================\n",
      "‚úÖ PASS: Math Metrics\n",
      "‚úÖ PASS: Engine Lag Logic\n",
      "\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_true_range_calculation():\n",
    "    \"\"\"Test TR = max(High-Low, |High-PrevClose|, |Low-PrevClose|) using robust pandas testing\"\"\"\n",
    "    print(\"Running test_true_range_calculation (Robust Version)...\")\n",
    "    \n",
    "    # 1. SETUP: Create test data\n",
    "    test_data = {\n",
    "        'Adj Open': [100, 105, 95, 98, 102],\n",
    "        'Adj High': [105, 108, 97, 102, 105],\n",
    "        'Adj Low': [95, 103, 93, 100, 98],\n",
    "        'Adj Close': [100, 106, 96, 99, 103],\n",
    "        'Volume': [1000, 1200, 800, 900, 1100]\n",
    "    }\n",
    "    \n",
    "    index = pd.MultiIndex.from_tuples(\n",
    "        [\n",
    "            ('TEST', pd.Timestamp('2024-01-01')),\n",
    "            ('TEST', pd.Timestamp('2024-01-02')),\n",
    "            ('TEST', pd.Timestamp('2024-01-03')),\n",
    "            ('TEST', pd.Timestamp('2024-01-04')),\n",
    "            ('TEST', pd.Timestamp('2024-01-05')),\n",
    "        ],\n",
    "        names=['Ticker', 'Date']\n",
    "    )\n",
    "    \n",
    "    df_test = pd.DataFrame(test_data, index=index)\n",
    "    print(f'test_true_range_df input:\\n{df_test}\\n')\n",
    "    \n",
    "    # 2. EXECUTION: Run function\n",
    "    result = generate_features(df_test, quality_window=5, quality_min_periods=2)\n",
    "    \n",
    "    # 3. ASSERTION: Define EXACT expected series\n",
    "    # Day 1: NaN (No prev close)\n",
    "    # Day 2: 8.0 (PrevClose=100, High=108, Low=103. |108-100|=8)\n",
    "    # Day 3: 13.0 (PrevClose=106, High=97, Low=93. |93-106|=13)\n",
    "    # Day 4: 6.0 (PrevClose=96, High=102, Low=100. |102-96|=6)\n",
    "    # Day 5: 7.0 (PrevClose=99, High=105, Low=98. High-Low=7)\n",
    "    expected_tr_values = [np.nan, 8.0, 13.0, 6.0, 7.0]\n",
    "    \n",
    "    expected_series = pd.Series(\n",
    "        expected_tr_values, \n",
    "        index=result.index, \n",
    "        name='TR',\n",
    "        dtype='float64'\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Check Series Equality\n",
    "        # check_exact=False allows for minor floating point differences (rtol=1e-4)\n",
    "        assert_series_equal(result['TR'], expected_series, check_exact=False, rtol=1e-4)\n",
    "        \n",
    "        print(\"‚úÖ PD Testing Assertion Passed! All TR values match expected logic.\")\n",
    "        return True\n",
    "        \n",
    "    except AssertionError as e:\n",
    "        print(f\"‚ùå PD Testing Assertion Failed: {e}\")\n",
    "        \n",
    "        # Helper output to see what went wrong\n",
    "        print(\"\\nDetailed Comparison (Actual vs Expected):\")\n",
    "        comparison = pd.concat([result['TR'], expected_series], axis=1, keys=['Actual_TR', 'Expected_TR'])\n",
    "        comparison['Diff'] = comparison['Actual_TR'] - comparison['Expected_TR']\n",
    "        print(comparison)\n",
    "        \n",
    "        return False\n",
    "\n",
    "def test_atr_calculation():\n",
    "    \"\"\"Test ATR = EWMA of TR with alpha=1/period\"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Running test_atr_calculation...\")\n",
    "    \n",
    "    # Test data with 5 days\n",
    "    test_data = {\n",
    "        'Adj Open': [100, 102, 103, 110, 108],\n",
    "        'Adj High': [101, 103, 103, 112, 110],\n",
    "        'Adj Low': [99, 101, 103, 108, 107],\n",
    "        'Adj Close': [100, 102, 103, 111, 109],\n",
    "        'Volume': [1000, 1000, 1000, 1000, 1000]  # All non-zero for simplicity\n",
    "    }\n",
    "    \n",
    "    index = pd.MultiIndex.from_tuples(\n",
    "        [('TEST', pd.Timestamp(f'2024-01-{i:02d}')) for i in range(1, 6)],\n",
    "        names=['Ticker', 'Date']\n",
    "    )\n",
    "    \n",
    "    df_test = pd.DataFrame(test_data, index=index)\n",
    "\n",
    "    print(f'test_true_range_df:\\n{df_test}\\n')\n",
    "\n",
    "    result = generate_features(df_test, atr_period=14)\n",
    "    \n",
    "    print(\"\\nATR Calculation Results:\")\n",
    "    print(result[['TR', 'ATR', 'ATRP']])\n",
    "    \n",
    "    # Manual calculation from our earlier example\n",
    "    # CORRECTED EXPECTED VALUES WITH MORE PRECISION\n",
    "    expected_atr = {\n",
    "        '2024-01-02': 3.0,\n",
    "        '2024-01-03': 40/14,  # ‚âà 2.857142857142857\n",
    "        '2024-01-04': 646/196,  # ‚âà 3.2959183673469388\n",
    "        '2024-01-05': 9182/2744,  # ‚âà 3.3462099125364433\n",
    "    }\n",
    "\n",
    "    all_passed = True\n",
    "    for date_str, expected in expected_atr.items():\n",
    "        actual = result.loc[('TEST', pd.Timestamp(date_str)), 'ATR']\n",
    "        if abs(actual - expected) < 0.0001:\n",
    "            print(f\"‚úì {date_str} ATR: {actual:.6f} ‚âà {expected:.6f}\")\n",
    "        else:\n",
    "            print(f\"‚úó {date_str} ATR: {actual:.6f} != {expected:.6f}\")\n",
    "            all_passed = False\n",
    "    \n",
    "    if all_passed:\n",
    "        print(\"\\n‚úÖ All ATR tests passed!\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå Some ATR tests failed!\")\n",
    "    \n",
    "    return all_passed\n",
    "\n",
    "def test_is_stale_calculation():\n",
    "    \"\"\"Test IsStale = 1 when Volume=0 OR High=Low\"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Running test_is_stale_calculation...\")\n",
    "    \n",
    "    test_data = {\n",
    "        'Adj Open': [100, 102, 103, 104],\n",
    "        'Adj High': [101, 103, 103, 105],  # Day 3: High=Low\n",
    "        'Adj Low': [99, 101, 103, 104],\n",
    "        'Adj Close': [100, 102, 103, 105],\n",
    "        'Volume': [1000, 0, 500, 1000]  # Day 2: Volume=0\n",
    "    }\n",
    "    \n",
    "    index = pd.MultiIndex.from_tuples(\n",
    "        [('TEST', pd.Timestamp(f'2024-01-{i:02d}')) for i in range(1, 5)],\n",
    "        names=['Ticker', 'Date']\n",
    "    )\n",
    "    \n",
    "    df_test = pd.DataFrame(test_data, index=index)\n",
    "\n",
    "    print(f'test_is_stale_df:\\n{df_test}\\n')\n",
    "\n",
    "    # Create IsStale manually to verify\n",
    "    is_stale_manual = np.where(\n",
    "        (df_test['Volume'] == 0) | (df_test['Adj High'] == df_test['Adj Low']),\n",
    "        1, 0\n",
    "    )\n",
    "    \n",
    "    print(\"\\nüìä Manual IsStale Calculation:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"IsStale = 1 if EITHER condition is true:\")\n",
    "    print(\"  1. Volume == 0\")\n",
    "    print(\"  2. Adj High == Adj Low (no price movement)\")\n",
    "    print(\"Otherwise, IsStale = 0\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Create a temporary DataFrame to display the calculation clearly\n",
    "    manual_calc_df = df_test.copy()\n",
    "    manual_calc_df['IsStale_Manual'] = is_stale_manual\n",
    "    manual_calc_df['Volume==0'] = manual_calc_df['Volume'] == 0\n",
    "    manual_calc_df['High==Low'] = manual_calc_df['Adj High'] == manual_calc_df['Adj Low']\n",
    "    \n",
    "    print(\"\\nCalculation details:\")\n",
    "    for idx, row in manual_calc_df.iterrows():\n",
    "        ticker_date = f\"{idx[0]}, {idx[1].strftime('%Y-%m-%d')}\"\n",
    "        conditions = []\n",
    "        if row['Volume==0']:\n",
    "            conditions.append(\"Volume=0\")\n",
    "        if row['High==Low']:\n",
    "            conditions.append(\"High=Low\")\n",
    "        \n",
    "        condition_str = \" OR \".join(conditions) if conditions else \"None (both False)\"\n",
    "        result = row['IsStale_Manual']\n",
    "        \n",
    "        print(f\"  {ticker_date}:\")\n",
    "        print(f\"    Volume={row['Volume']}, High={row['Adj High']}, Low={row['Adj Low']}\")\n",
    "        print(f\"    Conditions met: {condition_str}\")\n",
    "        print(f\"    ‚Üí IsStale = {result}\")\n",
    "        print()\n",
    "\n",
    "    expected = [0, 1, 1, 0]  # Day 1: normal, Day 2: vol=0, Day 3: high=low, Day 4: normal\n",
    "    \n",
    "    print(f\"\\nManual IsStale calculation: {is_stale_manual}\")\n",
    "    print(f\"Expected: {expected}\")\n",
    "    \n",
    "    if list(is_stale_manual) == expected:\n",
    "        print(\"‚úì IsStale calculation logic is correct\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"‚úó IsStale calculation failed. Got {is_stale_manual}, expected {expected}\")\n",
    "        return False\n",
    "\n",
    "def test_multiple_tickers():\n",
    "    \"\"\"Test that calculations don't mix data between tickers\"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Running test_multiple_tickers...\")\n",
    "    \n",
    "    test_data = {\n",
    "        'Adj Open': [100, 102, 50, 51],\n",
    "        'Adj High': [101, 103, 52, 53],\n",
    "        'Adj Low': [99, 101, 48, 49],\n",
    "        'Adj Close': [100, 102, 49, 52],\n",
    "        'Volume': [1000, 1000, 2000, 2000]\n",
    "    }\n",
    "    \n",
    "    index = pd.MultiIndex.from_tuples([\n",
    "        ('A', pd.Timestamp('2024-01-01')),\n",
    "        ('A', pd.Timestamp('2024-01-02')),\n",
    "        ('B', pd.Timestamp('2024-01-01')),\n",
    "        ('B', pd.Timestamp('2024-01-02')),\n",
    "    ], names=['Ticker', 'Date'])\n",
    "    \n",
    "    df_test = pd.DataFrame(test_data, index=index)\n",
    "\n",
    "    print(f'test_multiple_tickers_df:\\n{df_test}\\n')\n",
    "\n",
    "    result = generate_features(df_test)\n",
    "    \n",
    "    print(\"\\nMultiple Ticker Results:\")\n",
    "    print(result[['TR', 'ATR']])\n",
    "    \n",
    "    # Ticker A day 2 TR should use A day 1 close, not B day 1 close\n",
    "    tr_a2 = result.loc[('A', '2024-01-02'), 'TR']\n",
    "    expected_a2 = 3.0  # max(103-101=2, |103-100|=3, |101-100|=1) = 3\n",
    "    \n",
    "    tr_b2 = result.loc[('B', '2024-01-02'), 'TR']\n",
    "    expected_b2 = 4.0  # max(53-49=4, |53-49|=4, |49-49|=0) = 4\n",
    "    \n",
    "    tests_passed = 0\n",
    "    total_tests = 2\n",
    "    \n",
    "    if abs(tr_a2 - expected_a2) < 0.0001:\n",
    "        print(f\"‚úì Ticker A TR: {tr_a2} (expected {expected_a2})\")\n",
    "        tests_passed += 1\n",
    "    else:\n",
    "        print(f\"‚úó Ticker A TR: {tr_a2} != {expected_a2}\")\n",
    "    \n",
    "    if abs(tr_b2 - expected_b2) < 0.0001:\n",
    "        print(f\"‚úì Ticker B TR: {tr_b2} (expected {expected_b2})\")\n",
    "        tests_passed += 1\n",
    "    else:\n",
    "        print(f\"‚úó Ticker B TR: {tr_b2} != {expected_b2}\")\n",
    "    \n",
    "    if tests_passed == total_tests:\n",
    "        print(\"‚úÖ Ticker separation test passed!\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"‚ùå Ticker separation test failed: {tests_passed}/{total_tests} passed\")\n",
    "        return False\n",
    "\n",
    "def test_edge_cases():\n",
    "    \"\"\"Test edge cases like zero price, single row, etc.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Running test_edge_cases...\")\n",
    "    \n",
    "    all_passed = True\n",
    "    \n",
    "    # Test 1: Very low price (penny stock)\n",
    "    print(\"\\n1. Testing penny stock with low price...\")\n",
    "    test_data = {\n",
    "        'Adj Open': [0.10, 0.11],\n",
    "        'Adj High': [0.10, 0.11],\n",
    "        'Adj Low': [0.10, 0.11],\n",
    "        'Adj Close': [0.10, 0.11],\n",
    "        'Volume': [1000, 1000]\n",
    "    }\n",
    "    \n",
    "    index = pd.MultiIndex.from_tuples([\n",
    "        ('PENNY', pd.Timestamp('2024-01-01')),\n",
    "        ('PENNY', pd.Timestamp('2024-01-02')),\n",
    "    ], names=['Ticker', 'Date'])\n",
    "    \n",
    "    df_penny = pd.DataFrame(test_data, index=index)\n",
    "\n",
    "    print(f'df_penny_stock:\\n{df_penny}\\n')\n",
    "\n",
    "    result = generate_features(df_penny)\n",
    "    \n",
    "    # Check ATRP is reasonable (not inf/nan)\n",
    "    atrp_val = result.loc[('PENNY', '2024-01-02'), 'ATRP']\n",
    "    if pd.isna(atrp_val) or np.isinf(atrp_val):\n",
    "        print(f\"‚úó Penny stock ATRP is {atrp_val} (should be finite)\")\n",
    "        all_passed = False\n",
    "    else:\n",
    "        print(f\"‚úì Penny stock ATRP is {atrp_val:.4f}\")\n",
    "    \n",
    "    # Test 2: Single row\n",
    "    print(\"\\n2. Testing single row data...\")\n",
    "    test_data_single = {\n",
    "        'Adj Open': [100],\n",
    "        'Adj High': [101],\n",
    "        'Adj Low': [99],\n",
    "        'Adj Close': [100],\n",
    "        'Volume': [1000]\n",
    "    }\n",
    "    \n",
    "    index_single = pd.MultiIndex.from_tuples(\n",
    "        [('SINGLE', pd.Timestamp('2024-01-01'))],\n",
    "        names=['Ticker', 'Date']\n",
    "    )\n",
    "    \n",
    "    df_single = pd.DataFrame(test_data_single, index=index_single)\n",
    "\n",
    "    print(f'df_single:\\n{df_single}\\n')\n",
    "\n",
    "    result_single = generate_features(df_single, quality_window=3, quality_min_periods=2)\n",
    "    \n",
    "    # TR should be NaN (no previous close)\n",
    "    if pd.isna(result_single.loc[('SINGLE', '2024-01-01'), 'TR']):\n",
    "        print(\"‚úì Single row TR is NaN (correct)\")\n",
    "    else:\n",
    "        print(f\"‚úó Single row TR should be NaN but got {result_single.loc[('SINGLE', '2024-01-01'), 'TR']}\")\n",
    "        all_passed = False\n",
    "    \n",
    "    # Rolling metrics should be NaN with min_periods=2\n",
    "    if pd.isna(result_single.loc[('SINGLE', '2024-01-01'), 'RollingStalePct']):\n",
    "        print(\"‚úì Single row rolling metrics are NaN (correct - insufficient periods)\")\n",
    "    else:\n",
    "        print(f\"‚úó Rolling metrics should be NaN but got {result_single.loc[('SINGLE', '2024-01-01'), 'RollingStalePct']}\")\n",
    "        all_passed = False\n",
    "    \n",
    "    if all_passed:\n",
    "        print(\"\\n‚úÖ All edge case tests passed!\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå Some edge case tests failed!\")\n",
    "    \n",
    "    return all_passed\n",
    "\n",
    "def test_zero_division_protection():\n",
    "    \"\"\"Test that Zero Price doesn't cause Inf values in ATRP\"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Running test_zero_division_protection...\")\n",
    "\n",
    "    test_data = {\n",
    "        'Adj Open': [10, 10, 10],\n",
    "        'Adj High': [12, 12, 12],\n",
    "        'Adj Low': [8, 8, 8],\n",
    "        'Adj Close': [10, 0, 10], # Day 2 Price is ZERO\n",
    "        'Volume': [1000, 1000, 1000]\n",
    "    }\n",
    "    index = pd.MultiIndex.from_tuples(\n",
    "        [('ZERO', pd.Timestamp(f'2024-01-0{i}')) for i in range(1, 4)],\n",
    "        names=['Ticker', 'Date']\n",
    "    )\n",
    "    df_test = pd.DataFrame(test_data, index=index)\n",
    "\n",
    "    print(f'test_zero_division_protection_df:\\n{df_test}\\n')\n",
    "\n",
    "    # Run features\n",
    "    result = generate_features(df_test, atr_period=2)\n",
    "    \n",
    "    # Check Day 2 ATRP\n",
    "    atrp_val = result.loc[('ZERO', '2024-01-02'), 'ATRP']\n",
    "    \n",
    "    if pd.isna(atrp_val):\n",
    "        print(\"‚úÖ Zero Division Test Passed: ATRP is NaN when Close is 0.\")\n",
    "        return True\n",
    "    elif np.isinf(atrp_val):\n",
    "        print(f\"‚ùå Zero Division Test Failed: ATRP is Infinite ({atrp_val}).\")\n",
    "        return False\n",
    "    else:\n",
    "        print(f\"‚ùå Zero Division Test Failed: Unexpected value {atrp_val}\")\n",
    "        return False\n",
    "\n",
    "def test_unsorted_input_handling():\n",
    "    \"\"\"Test that function handles unsorted dates correctly via sorting\"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)    \n",
    "    print(\"Running test_unsorted_input_handling...\")\n",
    "    \n",
    "    # Data is out of order: Day 2, Day 1, Day 3\n",
    "    index = pd.MultiIndex.from_tuples([\n",
    "        ('A', pd.Timestamp('2024-01-02')), \n",
    "        ('A', pd.Timestamp('2024-01-01')), \n",
    "        ('A', pd.Timestamp('2024-01-03'))\n",
    "    ], names=['Ticker', 'Date'])\n",
    "    \n",
    "    # Prices: 100 -> 105 -> 110\n",
    "    # If processed in order given: \n",
    "    # 1. 105 (No prev)\n",
    "    # 2. 100 (Prev is 105) -> Change -5\n",
    "    # 3. 110 (Prev is 100) -> Change +10\n",
    "    \n",
    "    # If sorted correctly:\n",
    "    # 1. 100 (No prev)\n",
    "    # 2. 105 (Prev is 100) -> Change +5\n",
    "    # 3. 110 (Prev is 105) -> Change +5\n",
    "    \n",
    "    test_data = {\n",
    "        'Adj Open': [100, 100, 100],\n",
    "        'Adj High': [105, 100, 110], \n",
    "        'Adj Low': [105, 100, 110],\n",
    "        'Adj Close': [105, 100, 110], # 105, 100, 110\n",
    "        'Volume': [100, 100, 100]\n",
    "    }\n",
    "    \n",
    "    df_test = pd.DataFrame(test_data, index=index)\n",
    "\n",
    "    print(f'test_unsorted_input_handling_df:\\n{df_test}\\n')\n",
    "\n",
    "    # Run features\n",
    "    result = generate_features(df_test)\n",
    "    \n",
    "    # Inspect 2024-01-02 (Should be Day 2 in sorted order)\n",
    "    # Prev close (Jan 1) was 100. Current High 105. TR should be roughly 5.\n",
    "    tr_day_2 = result.loc[('A', '2024-01-02'), 'TR']\n",
    "    \n",
    "    # If it wasn't sorted, Day 2 would be treated as the first row (TR=NaN) \n",
    "    # or compared against whatever came before it in memory.\n",
    "    \n",
    "    if pd.isna(tr_day_2):\n",
    "         print(\"‚ùå Sorting Test Failed: Day 2 TR is NaN (likely treated as first row).\")\n",
    "         return False\n",
    "    elif abs(tr_day_2 - 5.0) < 0.1:\n",
    "         print(\"‚úÖ Sorting Test Passed: Logic applied in correct chronological order.\")\n",
    "         return True\n",
    "    else:\n",
    "         print(f\"‚ùå Sorting Test Failed: Day 2 TR is {tr_day_2}, expected ~5.0\")\n",
    "         return False\n",
    "\n",
    "    \"\"\"Test TR = max(High-Low, |High-PrevClose|, |Low-PrevClose|) using robust pandas testing\"\"\"\n",
    "    print(\"Running test_true_range_calculation (Robust Version)...\")\n",
    "    \n",
    "    # 1. SETUP: Create test data\n",
    "    test_data = {\n",
    "        'Adj Open': [100, 105, 95, 98, 102],\n",
    "        'Adj High': [105, 108, 97, 102, 105],\n",
    "        'Adj Low': [95, 103, 93, 100, 98],\n",
    "        'Adj Close': [100, 106, 96, 99, 103],\n",
    "        'Volume': [1000, 1200, 800, 900, 1100]\n",
    "    }\n",
    "    \n",
    "    index = pd.MultiIndex.from_tuples(\n",
    "        [\n",
    "            ('TEST', pd.Timestamp('2024-01-01')),\n",
    "            ('TEST', pd.Timestamp('2024-01-02')),\n",
    "            ('TEST', pd.Timestamp('2024-01-03')),\n",
    "            ('TEST', pd.Timestamp('2024-01-04')),\n",
    "            ('TEST', pd.Timestamp('2024-01-05')),\n",
    "        ],\n",
    "        names=['Ticker', 'Date']\n",
    "    )\n",
    "    \n",
    "    df_test = pd.DataFrame(test_data, index=index)\n",
    "    print(f'test_true_range_df input:\\n{df_test}\\n')\n",
    "    \n",
    "    # 2. EXECUTION: Run function\n",
    "    result = generate_features(df_test, quality_window=5, quality_min_periods=2)\n",
    "    \n",
    "    # 3. ASSERTION: Define EXACT expected series\n",
    "    # Day 1: NaN (No prev close)\n",
    "    # Day 2: 8.0 (PrevClose=100, High=108, Low=103. |108-100|=8)\n",
    "    # Day 3: 13.0 (PrevClose=106, High=97, Low=93. |93-106|=13)\n",
    "    # Day 4: 6.0 (PrevClose=96, High=102, Low=100. |102-96|=6)\n",
    "    # Day 5: 7.0 (PrevClose=99, High=105, Low=98. High-Low=7)\n",
    "    expected_tr_values = [np.nan, 8.0, 13.0, 6.0, 7.0]\n",
    "    \n",
    "    expected_series = pd.Series(\n",
    "        expected_tr_values, \n",
    "        index=result.index, \n",
    "        name='TR',\n",
    "        dtype='float64'\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Check Series Equality\n",
    "        # check_exact=False allows for minor floating point differences (rtol=1e-4)\n",
    "        assert_series_equal(result['TR'], expected_series, check_exact=False, rtol=1e-4)\n",
    "        \n",
    "        print(\"‚úÖ PD Testing Assertion Passed! All TR values match expected logic.\")\n",
    "        return True\n",
    "        \n",
    "    except AssertionError as e:\n",
    "        print(f\"‚ùå PD Testing Assertion Failed: {e}\")\n",
    "        \n",
    "        # Helper output to see what went wrong\n",
    "        print(\"\\nDetailed Comparison (Actual vs Expected):\")\n",
    "        comparison = pd.concat([result['TR'], expected_series], axis=1, keys=['Actual_TR', 'Expected_TR'])\n",
    "        comparison['Diff'] = comparison['Actual_TR'] - comparison['Expected_TR']\n",
    "        print(comparison)\n",
    "        \n",
    "        return False\n",
    "\n",
    "def test_quality_rolling_features():\n",
    "    \"\"\"\n",
    "    Test RollingStalePct, RollMedDollarVol, and RollingSameVolCount \n",
    "    verifying logic for Stale(Vol=0), Stale(H=L), SameVolume, and Median calculations.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Running test_quality_rolling_features...\")\n",
    "\n",
    "    # 1. SETUP: Create specific test data\n",
    "    # We set up 5 days to test a window of 4\n",
    "    test_data = {\n",
    "        # Day 1: Normal Base Day. $Vol = 10*100 = 1000.\n",
    "        # Day 2: Same Volume as D1. $Vol = 10*100 = 1000.\n",
    "        # Day 3: Stale (Volume=0). $Vol = 20*0 = 0.\n",
    "        # Day 4: Stale (High=Low). $Vol = 20*50 = 1000.\n",
    "        # Day 5: Normal High Vol. $Vol = 30*200 = 6000.\n",
    "        \n",
    "        'Adj Open':  [10,   10,   20,   20,   30],\n",
    "        'Adj High':  [12,   12,   22,   20,   35], # Day 4 High=20\n",
    "        'Adj Low':   [8,    8,    18,   20,   25], # Day 4 Low=20 (H=L)\n",
    "        'Adj Close': [10,   10,   20,   20,   30],\n",
    "        'Volume':    [100,  100,  0,    50,   200] # Day 2 same as D1, Day 3 is 0\n",
    "    }\n",
    "\n",
    "    index = pd.MultiIndex.from_tuples(\n",
    "        [('TEST', pd.Timestamp(f'2024-01-0{i}')) for i in range(1, 6)],\n",
    "        names=['Ticker', 'Date']\n",
    "    )\n",
    "\n",
    "    df_test = pd.DataFrame(test_data, index=index)\n",
    "    print(f'Input Data:\\n{df_test}')\n",
    "\n",
    "    # 2. EXECUTION: Use window=4, min_periods=2 to capture partial rolling\n",
    "    # We expect Day 1 to be NaN (count=1 < min_periods=2)\n",
    "    result = generate_features(df_test, quality_window=4, quality_min_periods=2)\n",
    "    \n",
    "    # 3. VERIFICATION\n",
    "\n",
    "    # --- A. Test RollingStalePct ---\n",
    "    print(\"\\n--- Testing RollingStalePct ---\")\n",
    "    # Logic:\n",
    "    # Day 1: IsStale=0. Result=NaN (min_periods)\n",
    "    # Day 2: IsStale=0. Window=[0,0]. Mean=0.0\n",
    "    # Day 3: IsStale=1 (Vol=0). Window=[0,0,1]. Mean=1/3 (~0.333)\n",
    "    # Day 4: IsStale=1 (High=Low). Window=[0,0,1,1]. Mean=2/4 = 0.5\n",
    "    # Day 5: IsStale=0. Window=[0,1,1,0] (Day 1 drops off). Mean=2/4 = 0.5\n",
    "    \n",
    "    expected_stale = pd.Series(\n",
    "        [np.nan, 0.0, 1/3, 0.5, 0.5],\n",
    "        index=result.index, name='RollingStalePct'\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        assert_series_equal(result['RollingStalePct'], expected_stale, check_exact=False, rtol=1e-4)\n",
    "        print(\"‚úÖ RollingStalePct Passed\")\n",
    "    except AssertionError as e:\n",
    "        print(f\"‚ùå RollingStalePct Failed: {e}\")\n",
    "        return False\n",
    "\n",
    "    # --- B. Test RollMedDollarVol ---\n",
    "    print(\"\\n--- Testing RollMedDollarVol ---\")\n",
    "    # Logic: $Vol = Close * Volume\n",
    "    # D1: 1000. Result=NaN\n",
    "    # D2: 1000. Window=[1000, 1000]. Median=1000\n",
    "    # D3: 0.    Window=[1000, 1000, 0]. Sorted=[0, 1000, 1000]. Median=1000\n",
    "    # D4: 1000. Window=[1000, 1000, 0, 1000]. Sorted=[0, 1000, 1000, 1000]. Median=(1000+1000)/2 = 1000\n",
    "    # D5: 6000. Window=[1000, 0, 1000, 6000]. Sorted=[0, 1000, 1000, 6000]. Median=1000\n",
    "    \n",
    "    expected_dollar = pd.Series(\n",
    "        [np.nan, 1000.0, 1000.0, 1000.0, 1000.0],\n",
    "        index=result.index, name='RollMedDollarVol'\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        assert_series_equal(result['RollMedDollarVol'], expected_dollar, check_exact=False, rtol=1e-4)\n",
    "        print(\"‚úÖ RollMedDollarVol Passed\")\n",
    "    except AssertionError as e:\n",
    "        print(f\"‚ùå RollMedDollarVol Failed: {e}\")\n",
    "        return False\n",
    "\n",
    "    # --- C. Test RollingSameVolCount ---\n",
    "    print(\"\\n--- Testing RollingSameVolCount ---\")\n",
    "    # Logic: HasSameVolume = (Volume == PrevVolume)\n",
    "    # D1: NaN/0 (First row diff is NaN, astype(int) -> 0). Result=NaN (min_periods)\n",
    "    # D2: 1 (100==100). Window=[0, 1]. Sum=1\n",
    "    # D3: 0 (0!=100).   Window=[0, 1, 0]. Sum=1\n",
    "    # D4: 0 (50!=0).    Window=[0, 1, 0, 0]. Sum=1\n",
    "    # D5: 0 (200!=50).  Window=[1, 0, 0, 0] (D1 drops). Sum=1\n",
    "    \n",
    "    expected_same_vol = pd.Series(\n",
    "        [np.nan, 1.0, 1.0, 1.0, 1.0],\n",
    "        index=result.index, name='RollingSameVolCount'\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        assert_series_equal(result['RollingSameVolCount'], expected_same_vol, check_exact=False, rtol=1e-4)\n",
    "        print(\"‚úÖ RollingSameVolCount Passed\")\n",
    "    except AssertionError as e:\n",
    "        print(f\"‚ùå RollingSameVolCount Failed: {e}\")\n",
    "        return False\n",
    "\n",
    "    print(\"\\nüéâ All Rolling Quality Feature Tests Passed!\")\n",
    "    return True\n",
    "\n",
    "def test_math_metrics():\n",
    "    \"\"\"Verifies calculate_gain and calculate_sharpe are mathematically precise.\"\"\"\n",
    "    print(\"Running test_math_metrics...\")\n",
    "    all_passed = True\n",
    "\n",
    "    # --- 1. Test calculate_gain ---\n",
    "    s_gain = pd.Series([100, 105, 110])\n",
    "    res_gain = calculate_gain(s_gain)\n",
    "    if abs(res_gain - 0.10) < 1e-9:\n",
    "        print(\"‚úÖ Gain Calc (Positive): Passed\")\n",
    "    else:\n",
    "        print(f\"‚ùå Gain Calc (Positive): Failed. Got {res_gain}\")\n",
    "        all_passed = False\n",
    "\n",
    "    s_loss = pd.Series([100, 95, 90])\n",
    "    res_loss = calculate_gain(s_loss)\n",
    "    if abs(res_loss - (-0.10)) < 1e-9:\n",
    "        print(\"‚úÖ Gain Calc (Negative): Passed\")\n",
    "    else:\n",
    "        print(f\"‚ùå Gain Calc (Negative): Failed. Got {res_loss}\")\n",
    "        all_passed = False\n",
    "\n",
    "    # --- 2. Test calculate_sharpe ---\n",
    "    s_flat = pd.Series([0.01, 0.01, 0.01]) \n",
    "    res_sharpe_flat = calculate_sharpe(s_flat)\n",
    "    if res_sharpe_flat == 0.0:\n",
    "        print(\"‚úÖ Sharpe (Zero Volatility): Passed (Returns 0.0)\")\n",
    "    else:\n",
    "        print(f\"‚ùå Sharpe (Zero Volatility): Failed. Got {res_sharpe_flat}\")\n",
    "        all_passed = False\n",
    "    \n",
    "    return all_passed\n",
    "\n",
    "def test_engine_lag_logic():\n",
    "    \"\"\"\n",
    "    CRITICAL TEST: Verifies the '1-Day Lag' logic using NEW Variable Names.\n",
    "    \"\"\"\n",
    "    print(\"\\nRunning test_engine_lag_logic...\")\n",
    "    \n",
    "    # 1. Setup Mock Data\n",
    "    dates = pd.to_datetime(['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04'])\n",
    "    # Prices: \n",
    "    # Jan 1: 100\n",
    "    # Jan 2: 100 (Decision T0)\n",
    "    # Jan 3: 110 (Entry T1 - We buy HERE)\n",
    "    # Jan 4: 121 (Exit - We sell HERE)\n",
    "    prices = [100.0, 100.0, 110.0, 121.0] \n",
    "    \n",
    "    df_ohlcv = pd.DataFrame({\n",
    "        'Adj Close': prices,\n",
    "        'Adj High': prices,\n",
    "        'Adj Low': prices, \n",
    "        'Adj Open': prices,\n",
    "        'Volume': [1000]*4\n",
    "    }, index=pd.MultiIndex.from_product([['MOCK'], dates], names=['Ticker', 'Date']))\n",
    "\n",
    "    # 2. Initialize Engine\n",
    "    engine = AlphaEngine(df_ohlcv)\n",
    "    \n",
    "    # 3. Run Strategy\n",
    "    # Decision Date = 2024-01-02.\n",
    "    inputs = EngineInput(\n",
    "        mode='Manual List',\n",
    "        start_date=pd.Timestamp('2024-01-02'),\n",
    "        lookback_period=1,  \n",
    "        holding_period=1,   # <--- FIXED: Set to 1 to fit the 4-day dataset\n",
    "        metric='Price', \n",
    "        benchmark_ticker='MOCK', \n",
    "        manual_tickers=['MOCK']\n",
    "    )\n",
    "    \n",
    "    res = engine.run(inputs)\n",
    "\n",
    "    # --- SAFETY CHECK ---\n",
    "    if res.error_msg:\n",
    "        print(f\"‚ùå TEST FAILED: Engine returned error: {res.error_msg}\")\n",
    "        return False\n",
    "    \n",
    "    # 4. Analyze Results\n",
    "    metrics = res.perf_metrics\n",
    "    \n",
    "    print(f\"  Decision Date: {res.decision_date.date()}\")\n",
    "    print(f\"  Buy Date (T+1): {res.buy_date.date()}\")\n",
    "    \n",
    "    # Use the NEW metric key\n",
    "    if 'holding_p_gain' in metrics:\n",
    "        holding_gain = metrics['holding_p_gain']\n",
    "        print(f\"  Holding Gain: {holding_gain:.4f}\")\n",
    "    else:\n",
    "        print(\"‚ùå 'holding_p_gain' not found in metrics!\")\n",
    "        return False\n",
    "    \n",
    "    # Verification Logic\n",
    "    # Buy @ Jan 3 Close (110) -> Sell @ Jan 4 Close (121)\n",
    "    # Math: (121 / 110) - 1 = 1.1 - 1 = 0.10\n",
    "    expected_gain = 0.10\n",
    "    \n",
    "    if abs(holding_gain - expected_gain) < 1e-4:\n",
    "        print(f\"‚úÖ LAG VERIFIED: Holding Gain is {holding_gain:.2%}.\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"‚ùå LAG FAILURE: Holding Gain is {holding_gain:.2%}.\")\n",
    "        print(f\"   Expected {expected_gain:.2%} (based on 121/110).\")\n",
    "        return False\n",
    "\n",
    "def run_all_tests():\n",
    "    \"\"\"Run all tests\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"STARTING UNIT TESTS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Run the math tests\n",
    "    results['Math Metrics'] = test_math_metrics()\n",
    "    \n",
    "    # Run the engine logic test (Requires updated Engine code)\n",
    "    try:\n",
    "        results['Engine Lag Logic'] = test_engine_lag_logic()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Engine Logic crashed: {e}\")\n",
    "        results['Engine Lag Logic'] = False\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TEST SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    passed = sum(results.values())\n",
    "    total = len(results)\n",
    "    \n",
    "    for test_name, result in results.items():\n",
    "        status = \"‚úÖ PASS\" if result else \"‚ùå FAIL\"\n",
    "        print(f\"{status}: {test_name}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    \n",
    "    return passed == total\n",
    "\n",
    "run_all_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "232740e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 1309921 entries, ('ACN', Timestamp('2001-07-19 00:00:00')) to ('ZS', Timestamp('2025-12-12 00:00:00'))\n",
      "Data columns (total 5 columns):\n",
      " #   Column     Non-Null Count    Dtype  \n",
      "---  ------     --------------    -----  \n",
      " 0   Adj Open   1309921 non-null  float64\n",
      " 1   Adj High   1309921 non-null  float64\n",
      " 2   Adj Low    1309921 non-null  float64\n",
      " 3   Adj Close  1309921 non-null  float64\n",
      " 4   Volume     1309921 non-null  int64  \n",
      "dtypes: float64(4), int64(1)\n",
      "memory usage: 55.6+ MB\n",
      "df_ohlcv.info():\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Adj Open</th>\n",
       "      <th>Adj High</th>\n",
       "      <th>Adj Low</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ticker</th>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">ACN</th>\n",
       "      <th>2001-07-19</th>\n",
       "      <td>10.5544</td>\n",
       "      <td>10.6872</td>\n",
       "      <td>10.4845</td>\n",
       "      <td>10.6034</td>\n",
       "      <td>50065558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001-07-20</th>\n",
       "      <td>10.5195</td>\n",
       "      <td>10.5195</td>\n",
       "      <td>10.3447</td>\n",
       "      <td>10.4915</td>\n",
       "      <td>13217313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001-07-23</th>\n",
       "      <td>10.4845</td>\n",
       "      <td>10.4915</td>\n",
       "      <td>10.1700</td>\n",
       "      <td>10.4845</td>\n",
       "      <td>10731510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001-07-24</th>\n",
       "      <td>10.4496</td>\n",
       "      <td>10.4636</td>\n",
       "      <td>10.2749</td>\n",
       "      <td>10.3867</td>\n",
       "      <td>5060735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001-07-25</th>\n",
       "      <td>10.2749</td>\n",
       "      <td>10.4496</td>\n",
       "      <td>10.2399</td>\n",
       "      <td>10.4496</td>\n",
       "      <td>6020432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">ZS</th>\n",
       "      <th>2025-12-08</th>\n",
       "      <td>243.5000</td>\n",
       "      <td>249.6400</td>\n",
       "      <td>243.4600</td>\n",
       "      <td>244.8800</td>\n",
       "      <td>2077300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-12-09</th>\n",
       "      <td>244.0100</td>\n",
       "      <td>244.7200</td>\n",
       "      <td>241.2900</td>\n",
       "      <td>243.0100</td>\n",
       "      <td>1518200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-12-10</th>\n",
       "      <td>242.2400</td>\n",
       "      <td>245.1200</td>\n",
       "      <td>238.8500</td>\n",
       "      <td>243.2900</td>\n",
       "      <td>1692700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-12-11</th>\n",
       "      <td>241.2000</td>\n",
       "      <td>243.4900</td>\n",
       "      <td>237.8700</td>\n",
       "      <td>242.0800</td>\n",
       "      <td>1294300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-12-12</th>\n",
       "      <td>241.6250</td>\n",
       "      <td>243.0650</td>\n",
       "      <td>233.9380</td>\n",
       "      <td>236.2800</td>\n",
       "      <td>1826614</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1309921 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Adj Open  Adj High   Adj Low  Adj Close    Volume\n",
       "Ticker Date                                                         \n",
       "ACN    2001-07-19   10.5544   10.6872   10.4845    10.6034  50065558\n",
       "       2001-07-20   10.5195   10.5195   10.3447    10.4915  13217313\n",
       "       2001-07-23   10.4845   10.4915   10.1700    10.4845  10731510\n",
       "       2001-07-24   10.4496   10.4636   10.2749    10.3867   5060735\n",
       "       2001-07-25   10.2749   10.4496   10.2399    10.4496   6020432\n",
       "...                     ...       ...       ...        ...       ...\n",
       "ZS     2025-12-08  243.5000  249.6400  243.4600   244.8800   2077300\n",
       "       2025-12-09  244.0100  244.7200  241.2900   243.0100   1518200\n",
       "       2025-12-10  242.2400  245.1200  238.8500   243.2900   1692700\n",
       "       2025-12-11  241.2000  243.4900  237.8700   242.0800   1294300\n",
       "       2025-12-12  241.6250  243.0650  233.9380   236.2800   1826614\n",
       "\n",
       "[1309921 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = r'c:\\Users\\ping\\Files_win10\\python\\py311\\stocks\\data\\df_OHLCV_stocks_etfs.parquet'\n",
    "df_ohlcv = pd.read_parquet(data_path, engine='pyarrow')\n",
    "print(f'df_ohlcv.info():\\n{df_ohlcv.info()}')\n",
    "df_ohlcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "957c5c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating features... this might take few minutes...\n",
      "1. Calculating Features...\n",
      "2. Pivoting Price Matrix...\n",
      "‚úÖ Optimization Complete. Ready for UI.\n"
     ]
    }
   ],
   "source": [
    "# Calculate features ONCE and store them in a variable\n",
    "print(\"Calculating features... this might take few minutes...\")\n",
    "print(\"1. Calculating Features...\")\n",
    "my_features = generate_features(\n",
    "    df_ohlcv=df_ohlcv, \n",
    "    atr_period=14,\n",
    "    quality_window=252,\n",
    "    quality_min_periods=126\n",
    ")\n",
    "\n",
    "print(\"2. Pivoting Price Matrix...\")\n",
    "# This is the line that takes 12 seconds, but now we only run it ONCE.\n",
    "my_close_matrix = df_ohlcv['Adj Close'].unstack(level=0)\n",
    "\n",
    "print(\"‚úÖ Optimization Complete. Ready for UI.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eaa8a33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_features:\n",
      "                       TR      ATR    ATRP   ROC_1   ROC_3   ROC_5  ROC_10  ROC_21  RollingStalePct  RollMedDollarVol  RollingSameVolCount\n",
      "Ticker Date                                                                                                                               \n",
      "ACN    2001-07-19     NaN      NaN     NaN     NaN     NaN     NaN     NaN     NaN              NaN               NaN                  NaN\n",
      "       2001-07-20  0.2587   0.2587  0.0247 -0.0106     NaN     NaN     NaN     NaN              NaN               NaN                  NaN\n",
      "       2001-07-23  0.3215   0.2632  0.0251 -0.0007     NaN     NaN     NaN     NaN              NaN               NaN                  NaN\n",
      "       2001-07-24  0.2096   0.2594  0.0250 -0.0093 -0.0204     NaN     NaN     NaN              NaN               NaN                  NaN\n",
      "       2001-07-25  0.2097   0.2558  0.0245  0.0061 -0.0040     NaN     NaN     NaN              NaN               NaN                  NaN\n",
      "...                   ...      ...     ...     ...     ...     ...     ...     ...              ...               ...                  ...\n",
      "ZS     2025-12-08  6.9600  10.2403  0.0418  0.0091  0.0050  0.0066 -0.1096 -0.2297              0.0        4.2652e+08                  0.0\n",
      "       2025-12-09  3.5900   9.7653  0.0402 -0.0076  0.0048  0.0055 -0.1332 -0.2406              0.0        4.2202e+08                  0.0\n",
      "       2025-12-10  6.2700   9.5157  0.0391  0.0012  0.0025 -0.0015 -0.1603 -0.2603              0.0        4.2071e+08                  0.0\n",
      "       2025-12-11  5.6200   9.2374  0.0382 -0.0050 -0.0114  0.0010 -0.0393 -0.2693              0.0        4.2071e+08                  0.0\n",
      "       2025-12-12  9.1270   9.2295  0.0391 -0.0240 -0.0277 -0.0264 -0.0605 -0.2548              0.0        4.2202e+08                  0.0\n",
      "\n",
      "[1309921 rows x 11 columns]\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 1309921 entries, ('ACN', Timestamp('2001-07-19 00:00:00')) to ('ZS', Timestamp('2025-12-12 00:00:00'))\n",
      "Data columns (total 11 columns):\n",
      " #   Column               Non-Null Count    Dtype  \n",
      "---  ------               --------------    -----  \n",
      " 0   TR                   1309744 non-null  float64\n",
      " 1   ATR                  1309744 non-null  float64\n",
      " 2   ATRP                 1309463 non-null  float64\n",
      " 3   ROC_1                1309525 non-null  float64\n",
      " 4   ROC_3                1309175 non-null  float64\n",
      " 5   ROC_5                1308830 non-null  float64\n",
      " 6   ROC_10               1307941 non-null  float64\n",
      " 7   ROC_21               1306008 non-null  float64\n",
      " 8   RollingStalePct      1287814 non-null  float64\n",
      " 9   RollMedDollarVol     1287814 non-null  float64\n",
      " 10  RollingSameVolCount  1287814 non-null  float64\n",
      "dtypes: float64(11)\n",
      "memory usage: 115.6+ MB\n"
     ]
    }
   ],
   "source": [
    "print(f'my_features:\\n{my_features}\\n')\n",
    "my_features.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "adf704ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ‚öôÔ∏è Initializing AlphaEngine v2.2 (Transparent Verification) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd6c798970fc40508328b926f1f3dfba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<b>1. Timeline Configuration:</b> (Past <--- Decision ---> Future)'), HBox(children‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "736ae7d3aed44af5b0a124508f46071f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'line': {'width': 2},\n",
       "              'name': 'VT',\n",
       "              'type': 'scatter',\n",
       "              'uid': '76b1f8e8-7230-4868-9eea-891b32c3bd47',\n",
       "              'visible': True,\n",
       "              'x': array([datetime.datetime(2025, 7, 30, 0, 0),\n",
       "                          datetime.datetime(2025, 7, 31, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 1, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 4, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 5, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 6, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 7, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 8, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 11, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 12, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 13, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 14, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 15, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 18, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 19, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 20, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 21, 0, 0)], dtype=object),\n",
       "              'y': array([1.        , 0.99471448, 0.98307559, 0.99685788, 0.99486813, 1.00137516,\n",
       "                          1.00321126, 1.00911137, 1.0064302 , 1.01844554, 1.0235774 , 1.02112671,\n",
       "                          1.02112671, 1.02166448, 1.01699355, 1.01569522, 1.01201533])},\n",
       "             {'line': {'width': 2},\n",
       "              'name': 'JNJ',\n",
       "              'type': 'scatter',\n",
       "              'uid': '4670e771-a55c-4a6e-b42e-4eb03cf4dc03',\n",
       "              'visible': True,\n",
       "              'x': array([datetime.datetime(2025, 7, 30, 0, 0),\n",
       "                          datetime.datetime(2025, 7, 31, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 1, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 4, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 5, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 6, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 7, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 8, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 11, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 12, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 13, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 14, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 15, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 18, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 19, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 20, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 21, 0, 0)], dtype=object),\n",
       "              'y': array([1.        , 0.98493279, 1.0004182 , 1.02259476, 1.02080682, 1.01990981,\n",
       "                          1.0255282 , 1.03628617, 1.03921961, 1.0330012 , 1.04280762, 1.04460162,\n",
       "                          1.05608083, 1.05374741, 1.06301441, 1.06923282, 1.06977223])},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '1d79ab13-7d6d-473d-9437-4f797c2fb3b3', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '3a688d13-bc45-48a8-a65c-0994058cc276', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '7dc42cca-c967-442d-97c4-755d2052cae0', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '090441e9-1cf5-475e-a6c8-e6fe770a00ec', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '7cbfcd8b-e630-4ad8-abe6-b3c9f6b8d3d9', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': 'cfa54fa4-69f1-47bb-ad0c-bd45e70309bc', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': 'a6cfa971-50ea-40d2-bf8e-1ca5d2d6f9ad', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': 'fb111ecc-26f7-4a8e-a629-f6f4b049d1ce', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '39485ab4-968a-4b1c-95ce-41660a4913d6', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '996997d1-fb78-4fc9-8f85-d0df15fe30b3', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': 'd0b01424-e5dd-484b-a60e-456fc92e656e', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': 'ff073291-a2a4-4064-a7f6-6ef6362c0484', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '6f376da3-2f5e-4e84-b74e-a092624723d7', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '3f2b9eed-a2b9-4443-91ce-3984edd03815', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '4ca06de3-30e0-47bc-b369-ba541b813ea1', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '3929d343-4b1a-45ba-b029-ab7619b62ee8', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '7883e40b-944f-4dfb-8219-abe1af1f77b4', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '93d90fba-5a1a-4f52-91da-5c360a6bf979', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': 'f6c118ee-8090-4c82-b59a-4ce7f7441b26', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '39c7a65b-b78f-4ce6-acc1-681e42cc7b74', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '418998c7-722f-4ccb-9768-2cff4092ac67', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': 'bc897805-d6f3-4573-85bc-b0ddb182de1a', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': 'cfc61860-8b26-49e3-997f-35b07e10424e', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': 'a19ac54d-dc44-46ca-ae85-b3be178619fd', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': 'add56a16-e853-479b-aff4-539434c7518e', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '8e9448df-2a32-4d64-9b64-30bee620b2d6', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '36a2f109-4bc7-440b-9fc0-e825452a4150', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '80850363-3b07-42fe-8cc0-f1de6d6ae481', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '56dcaea0-08e6-439b-bcbe-63190c06f283', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '73e95b00-eb03-4671-a542-72a76b492cb9', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '18a9625e-54cb-4ceb-9557-d382f9f6d2e4', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': 'e942a4a6-90e1-4f21-8a42-b523f89b96a6', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '51296ae6-5ea4-468f-9254-5c2ae53c9c62', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': 'ee53762d-67d3-42d6-bbae-02a11dfc33c1', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '604928f2-0146-4396-bdf4-0566ed5a101a', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '37248370-7e5e-472a-944c-46542235549f', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '22018bd7-be72-4071-80dc-88872ebad433', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '4b8d4094-508c-4cd5-bc40-6ca84471a1a4', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': 'b0351453-fbd4-4381-bfb6-3c4aeed96d33', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': 'a59ab2e6-b59d-468f-8e58-6f712d89ead2', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '99557c9b-5540-41b9-a2d2-02e602e8c778', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': 'c9219122-795a-4497-8bf3-a7a5bea9f359', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': 'd68653e0-af43-49c6-b06b-f13533a599be', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': 'feca2ab2-e4d2-4662-b4d4-3dcd07812b14', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '86bb539b-0bfc-4a75-a506-42ff808bb6f4', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': 'd8fbddbb-dea8-4c7a-9011-351d51f8f0f4', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': 'afd81479-0ff3-4e97-9e8c-3d1ad5ce27b8', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '45791e10-418d-4055-93c5-fa12a71ee012', 'visible': False},\n",
       "             {'line': {'color': 'black', 'dash': 'dash', 'width': 3},\n",
       "              'name': 'Benchmark (SPY)',\n",
       "              'type': 'scatter',\n",
       "              'uid': 'de6e8b01-a0b2-4b99-9aa2-71d33b1bea85',\n",
       "              'visible': True,\n",
       "              'x': array([datetime.datetime(2025, 7, 30, 0, 0),\n",
       "                          datetime.datetime(2025, 7, 31, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 1, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 4, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 5, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 6, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 7, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 8, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 11, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 12, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 13, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 14, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 15, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 18, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 19, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 20, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 21, 0, 0)], dtype=object),\n",
       "              'y': array([1.        , 0.99624786, 0.97991958, 0.99481434, 0.98977092, 0.99735264,\n",
       "                          0.99651655, 1.00428635, 1.00230123, 1.01297127, 1.01643891, 1.01653375,\n",
       "                          1.01415349, 1.01393222, 1.00843204, 1.00575307, 1.00171802])},\n",
       "             {'line': {'color': 'green', 'width': 3},\n",
       "              'name': 'Group Portfolio',\n",
       "              'type': 'scatter',\n",
       "              'uid': '9e78c42d-ea39-4153-8004-212337a72f98',\n",
       "              'visible': True,\n",
       "              'x': array([datetime.datetime(2025, 7, 30, 0, 0),\n",
       "                          datetime.datetime(2025, 7, 31, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 1, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 4, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 5, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 6, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 7, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 8, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 11, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 12, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 13, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 14, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 15, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 18, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 19, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 20, 0, 0),\n",
       "                          datetime.datetime(2025, 8, 21, 0, 0)], dtype=object),\n",
       "              'y': array([1.        , 0.98982363, 0.99174689, 1.00972632, 1.00783747, 1.01064249,\n",
       "                          1.01436973, 1.02269877, 1.0228249 , 1.02572337, 1.03319251, 1.03286416,\n",
       "                          1.03860377, 1.03770594, 1.04000398, 1.04246402, 1.04089378])}],\n",
       "    'layout': {'annotations': [{'bgcolor': 'red',\n",
       "                                'font': {'color': 'white'},\n",
       "                                'showarrow': False,\n",
       "                                'text': 'DECISION',\n",
       "                                'x': Timestamp('2025-08-13 00:00:00'),\n",
       "                                'xref': 'x',\n",
       "                                'y': 0.05,\n",
       "                                'yref': 'paper'},\n",
       "                               {'bgcolor': 'blue',\n",
       "                                'font': {'color': 'white'},\n",
       "                                'showarrow': False,\n",
       "                                'text': 'ENTRY (T+1)',\n",
       "                                'x': Timestamp('2025-08-14 00:00:00'),\n",
       "                                'xref': 'x',\n",
       "                                'y': 1.0,\n",
       "                                'yref': 'paper'}],\n",
       "               'height': 600,\n",
       "               'hovermode': 'x unified',\n",
       "               'shapes': [{'line': {'color': 'red', 'dash': 'dash', 'width': 2},\n",
       "                           'type': 'line',\n",
       "                           'x0': Timestamp('2025-08-13 00:00:00'),\n",
       "                           'x1': Timestamp('2025-08-13 00:00:00'),\n",
       "                           'xref': 'x',\n",
       "                           'y0': 0,\n",
       "                           'y1': 1,\n",
       "                           'yref': 'paper'},\n",
       "                          {'line': {'color': 'blue', 'dash': 'dot', 'width': 2},\n",
       "                           'type': 'line',\n",
       "                           'x0': Timestamp('2025-08-14 00:00:00'),\n",
       "                           'x1': Timestamp('2025-08-14 00:00:00'),\n",
       "                           'xref': 'x',\n",
       "                           'y0': 0,\n",
       "                           'y1': 1,\n",
       "                           'yref': 'paper'}],\n",
       "               'template': '...',\n",
       "               'title': {'text': 'Event-Driven Walk-Forward Analysis'}}\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_container, debug_container = plot_walk_forward_analyzer(\n",
    "    df_ohlcv=df_ohlcv,\n",
    "    precomputed_features=my_features, # Fast Features\n",
    "    precomputed_close=my_close_matrix, # Fast Prices    \n",
    "    default_start_date='2025-08-13',\n",
    "    default_lookback=10,     # Was default_calc_period\n",
    "    default_holding=5,       # Was default_fwd_period\n",
    "    default_strategy='Sharpe (ATR)', # Was default_metric\n",
    "    default_rank_start=15,\n",
    "    default_rank_end=16,\n",
    "    default_benchmark_ticker='SPY', \n",
    "    master_calendar_ticker='XOM',    \n",
    "    quality_thresholds = { \n",
    "        'min_median_dollar_volume': 100_000, # A low \"hard floor\" to filter absolute errors/garbage\n",
    "        # If min_liquidity_percentile is 0.8 (Top 20%), we want values > the 0.8 quantile.            \n",
    "        'min_liquidity_percentile': 0.50,    # Dynamic: Only keep the top 50% of stocks by volume\n",
    "        'max_stale_pct': 0.05, \n",
    "        'max_same_vol_count': 10\n",
    "    },\n",
    "    debug=True  # <-- Activate the new mode!\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a158ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_ticker_ranking_logic(df_close_wide, features_df, inputs, res_container):\n",
    "    \"\"\"\n",
    "    Shadow Ranker:\n",
    "    1. Re-applies Liquidity Filters on the full universe.\n",
    "    2. Re-calculates the Strategy Metric for ALL eligible stocks.\n",
    "    3. Sorts them independently.\n",
    "    4. Slices the Top N.\n",
    "    5. Compares the \"Shadow List\" against the \"Engine List\".\n",
    "    \"\"\"\n",
    "    if not res_container[0]:\n",
    "        print(\"‚ö†Ô∏è No results found. Run simulation first.\")\n",
    "        return\n",
    "\n",
    "    res = res_container[0]\n",
    "    decision_date = res.decision_date\n",
    "    print(f\"--- üïµÔ∏è TICKER SELECTION AUDIT ---\")\n",
    "    print(f\"Decision Date: {decision_date.date()}\")\n",
    "    print(f\"Strategy:      {inputs.metric}\")\n",
    "    print(f\"Rank Window:   {inputs.rank_start} to {inputs.rank_end}\")\n",
    "\n",
    "    # --- STEP 1: SHADOW FILTERING (Replicating Universe Selection) ---\n",
    "    # We manually inspect the feature dataframe at the decision date\n",
    "    day_feats = features_df.xs(decision_date, level='Date')\n",
    "    \n",
    "    # Re-apply thresholds manually\n",
    "    thresh = inputs.quality_thresholds\n",
    "    \n",
    "    # Handle dynamic percentile if present\n",
    "    vol_cutoff = thresh.get('min_median_dollar_volume', 0)\n",
    "    if 'min_liquidity_percentile' in thresh:\n",
    "        dynamic_val = day_feats['RollMedDollarVol'].quantile(thresh['min_liquidity_percentile'])\n",
    "        vol_cutoff = max(vol_cutoff, dynamic_val)\n",
    "\n",
    "    # Boolean Mask\n",
    "    mask = (\n",
    "        (day_feats['RollMedDollarVol'] >= vol_cutoff) &\n",
    "        (day_feats['RollingStalePct'] <= thresh['max_stale_pct']) &\n",
    "        (day_feats['RollingSameVolCount'] <= thresh['max_same_vol_count'])\n",
    "    )\n",
    "    \n",
    "    shadow_universe = day_feats[mask].index.tolist()\n",
    "    \n",
    "    print(f\"\\n1. UNIVERSE FILTERING:\")\n",
    "    print(f\"   Total Tickers: {len(day_feats)}\")\n",
    "    print(f\"   Eligible:      {len(shadow_universe)}\")\n",
    "    \n",
    "    # --- STEP 2: SHADOW SCORING (Re-calculating the Metric) ---\n",
    "    # We need to build the 'ingredients' dictionary for the metric function\n",
    "    # but ONLY for the eligible universe\n",
    "    \n",
    "    start_date = res.start_date # Start of lookback\n",
    "    \n",
    "    # Pull Raw Data\n",
    "    lookback_close = df_close_wide.loc[start_date:decision_date, shadow_universe]\n",
    "    \n",
    "    # Re-assemble Ingredients (Independent of Engine internals)\n",
    "    # We grab features specifically for the ranking\n",
    "    feat_slice_period = features_df.loc[(slice(None), lookback_close.index), :].reindex(\n",
    "        pd.MultiIndex.from_product([shadow_universe, lookback_close.index], names=['Ticker', 'Date'])\n",
    "    )\n",
    "    atrp_mean = feat_slice_period['ATRP'].groupby(level='Ticker').mean()\n",
    "    current_feats = day_feats.loc[shadow_universe]\n",
    "\n",
    "    shadow_ingredients = {\n",
    "        'lookback_close': lookback_close,\n",
    "        'lookback_returns': lookback_close.ffill().pct_change(),\n",
    "        'atrp': atrp_mean,\n",
    "        'roc_1': current_feats['ROC_1'],\n",
    "        'roc_3': current_feats['ROC_3'],\n",
    "        'roc_5': current_feats['ROC_5'],\n",
    "        'roc_10': current_feats['ROC_10'],\n",
    "        'roc_21': current_feats['ROC_21']\n",
    "    }\n",
    "    \n",
    "    # Calculate Score\n",
    "    if inputs.metric not in METRIC_REGISTRY:\n",
    "        print(f\"‚ùå Cannot verify unknown metric: {inputs.metric}\")\n",
    "        return\n",
    "\n",
    "    shadow_scores = METRIC_REGISTRY[inputs.metric](shadow_ingredients)\n",
    "    shadow_scores = shadow_scores.sort_values(ascending=False)\n",
    "    \n",
    "    # --- STEP 3: SHADOW RANKING ---\n",
    "    # Apply the Rank Window (e.g., 1 to 10)\n",
    "    # Python index is 0-based, so Rank 1 is index 0.\n",
    "    start_idx = max(0, inputs.rank_start - 1)\n",
    "    end_idx = inputs.rank_end\n",
    "    \n",
    "    shadow_picks = shadow_scores.iloc[start_idx:end_idx]\n",
    "    shadow_tickers = shadow_picks.index.tolist()\n",
    "    \n",
    "    # --- STEP 4: COMPARISON ---\n",
    "    engine_tickers = res.tickers\n",
    "    \n",
    "    print(f\"\\n2. SELECTION COMPARISON:\")\n",
    "    \n",
    "    match = (shadow_tickers == engine_tickers)\n",
    "    \n",
    "    comparison_df = pd.DataFrame({\n",
    "        'Rank': range(inputs.rank_start, inputs.rank_start + len(shadow_tickers)),\n",
    "        'Shadow_Ticker': shadow_tickers,\n",
    "        'Shadow_Score': shadow_picks.values,\n",
    "        'Engine_Ticker': engine_tickers + [''] * (len(shadow_tickers) - len(engine_tickers)) if len(shadow_tickers) > len(engine_tickers) else engine_tickers[:len(shadow_tickers)]\n",
    "    }).set_index('Rank')\n",
    "    \n",
    "    # Check for mismatches\n",
    "    comparison_df['MATCH'] = comparison_df['Shadow_Ticker'] == comparison_df['Engine_Ticker']\n",
    "    \n",
    "    print(comparison_df)\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    if match:\n",
    "        print(\"‚úÖ SUCCESS: Ticker Selection Logic Verified (Exact Match).\")\n",
    "    else:\n",
    "        print(\"‚ùå FAILURE: Ticker Mismatch detected.\")\n",
    "        # Debugging hint\n",
    "        if len(shadow_tickers) != len(engine_tickers):\n",
    "             print(f\"   Count Mismatch: Shadow {len(shadow_tickers)} vs Engine {len(engine_tickers)}\")\n",
    "        else:\n",
    "             print(\"   Order or Identity Mismatch. Check Strategy Logic.\")\n",
    "\n",
    "# Need to reconstruct inputs from the UI state to run this independently\n",
    "# Or simply pass the 'inputs' object used in the last run if available.\n",
    "# Assuming 'engine_inputs' was the variable passed to engine.run():\n",
    "\n",
    "# Example Usage:\n",
    "# verify_ticker_ranking_logic(precomputed_close, precomputed_features, current_inputs, results_container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4acad2a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üïµÔ∏è TICKER SELECTION AUDIT ---\n",
      "Decision Date: 2025-12-04\n",
      "Strategy:      Price\n",
      "Rank Window:   1 to 20\n",
      "\n",
      "1. UNIVERSE FILTERING:\n",
      "   Total Tickers: 177\n",
      "   Eligible:      87\n",
      "\n",
      "2. SELECTION COMPARISON:\n",
      "     Shadow_Ticker  Shadow_Score Engine_Ticker  MATCH\n",
      "Rank                                                 \n",
      "1              APP        0.2900           APP   True\n",
      "2             COHR        0.2407          COHR   True\n",
      "3              ADI        0.1941           ADI   True\n",
      "4             NXPI        0.1899          NXPI   True\n",
      "5              TER        0.1816           TER   True\n",
      "6             AMAT        0.1482          AMAT   True\n",
      "7              TXN        0.1466           TXN   True\n",
      "8              ACN        0.1172           ACN   True\n",
      "9             ROST        0.1037          ROST   True\n",
      "10             TPR        0.1020           TPR   True\n",
      "11            SPXL        0.0951          SPXL   True\n",
      "12             MOH        0.0901           MOH   True\n",
      "13              BA        0.0871            BA   True\n",
      "14            GOOG        0.0867          GOOG   True\n",
      "15            VRTX        0.0863          VRTX   True\n",
      "16           GOOGL        0.0847         GOOGL   True\n",
      "17             ROK        0.0843           ROK   True\n",
      "18             CMI        0.0839           CMI   True\n",
      "19              FI        0.0799            FI   True\n",
      "20               C        0.0797             C   True\n",
      "------------------------------------------------------------\n",
      "‚úÖ SUCCESS: Ticker Selection Logic Verified (Exact Match).\n"
     ]
    }
   ],
   "source": [
    "verify_ticker_ranking_logic(\n",
    "    df_close_wide=my_close_matrix, \n",
    "    features_df=my_features, \n",
    "    inputs=debug_container[0]['inputs'], # Retrieve inputs used for the last run\n",
    "    res_container=results_container,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8192d9a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üèóÔ∏è PORTFOLIO CONSTRUCTION AUDIT ---\n",
      "Period: 2025-11-19 to 2025-12-12\n",
      "Tickers (20): APP, COHR, ADI, NXPI, TER, AMAT, TXN, ACN, ROST, TPR, SPXL, MOH, BA, GOOG, VRTX, GOOGL, ROK, CMI, FI, C\n",
      "\n",
      "1. WEIGHT DISTRIBUTION:\n",
      "APP      0.05\n",
      "COHR     0.05\n",
      "ADI      0.05\n",
      "NXPI     0.05\n",
      "TER      0.05\n",
      "AMAT     0.05\n",
      "TXN      0.05\n",
      "ACN      0.05\n",
      "ROST     0.05\n",
      "TPR      0.05\n",
      "SPXL     0.05\n",
      "MOH      0.05\n",
      "BA       0.05\n",
      "GOOG     0.05\n",
      "VRTX     0.05\n",
      "GOOGL    0.05\n",
      "ROK      0.05\n",
      "CMI      0.05\n",
      "FI       0.05\n",
      "C        0.05\n",
      "‚úÖ Weights sum to 1.0\n",
      "\n",
      "2. SERIES COMPARISON:\n",
      "\n",
      "--- Start of Simulation (Head) ---\n",
      "            Shadow_Calc  Engine_Output  Difference\n",
      "Date                                              \n",
      "2025-11-19       1.0000         1.0000         0.0\n",
      "2025-11-20       0.9746         0.9746         0.0\n",
      "2025-11-21       1.0026         1.0026         0.0\n",
      "2025-11-24       1.0270         1.0270         0.0\n",
      "2025-11-25       1.0421         1.0421         0.0\n",
      "\n",
      "--- Entry Date (T+1: 2025-12-05) ---\n",
      "            Shadow_Calc  Engine_Output  Difference\n",
      "Date                                              \n",
      "2025-12-05       1.1363         1.1363         0.0\n",
      "\n",
      "--- End of Simulation (Tail) ---\n",
      "            Shadow_Calc  Engine_Output  Difference\n",
      "Date                                              \n",
      "2025-12-08       1.1352         1.1352         0.0\n",
      "2025-12-09       1.1382         1.1382         0.0\n",
      "2025-12-10       1.1543         1.1543         0.0\n",
      "2025-12-11       1.1610         1.1610         0.0\n",
      "2025-12-12       1.1387         1.1387         0.0\n",
      "------------------------------------------------------------\n",
      "‚úÖ SUCCESS: Portfolio Construction Verified (Max Error: 0.000000000000)\n"
     ]
    }
   ],
   "source": [
    "def verify_portfolio_construction(df_close_wide, res_container):\n",
    "    \"\"\"\n",
    "    Independent Auditor:\n",
    "    1. Reads Tickers & Weights from the Engine Output.\n",
    "    2. Re-pulls raw price data from the source.\n",
    "    3. Re-calculates the weighted sum (Portfolio Value).\n",
    "    4. Compares it against the Engine's reported Portfolio Series.\n",
    "    \"\"\"\n",
    "    if not res_container[0]:\n",
    "        print(\"‚ö†Ô∏è No results found. Run simulation first.\")\n",
    "        return\n",
    "\n",
    "    res = res_container[0]\n",
    "    \n",
    "    print(f\"--- üèóÔ∏è PORTFOLIO CONSTRUCTION AUDIT ---\")\n",
    "    print(f\"Period: {res.start_date.date()} to {res.holding_end_date.date()}\")\n",
    "    print(f\"Tickers ({len(res.tickers)}): {', '.join(res.tickers)}\")\n",
    "    \n",
    "    # 1. RECONSTRUCT WEIGHTS\n",
    "    # We verify that weights sum to 1.0\n",
    "    weights = res.initial_weights\n",
    "    print(f\"\\n1. WEIGHT DISTRIBUTION:\")\n",
    "    print(weights.to_string())\n",
    "    if abs(weights.sum() - 1.0) > 1e-6:\n",
    "        print(f\"‚ö†Ô∏è WARNING: Weights do not sum to 1.0! Sum: {weights.sum()}\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Weights sum to 1.0\")\n",
    "\n",
    "    # 2. RECONSTRUCT RAW DATA\n",
    "    # We pull the exact slice the engine claimed to use\n",
    "    raw_slice = df_close_wide[res.tickers].loc[res.start_date : res.holding_end_date]\n",
    "    \n",
    "    # 3. RECONSTRUCT NORMALIZATION (The \"Index\" Logic)\n",
    "    # The engine normalizes everyone to start at 1.0 based on the first valid price\n",
    "    start_prices = raw_slice.bfill().iloc[0]\n",
    "    normalized_prices = raw_slice / start_prices\n",
    "    \n",
    "    # 4. APPLY WEIGHTS\n",
    "    # Multiply every column by its weight\n",
    "    weighted_components = normalized_prices.mul(weights)\n",
    "    \n",
    "    # 5. CALCULATE SHADOW SERIES\n",
    "    # Sum across the row\n",
    "    shadow_series = weighted_components.sum(axis=1)\n",
    "    \n",
    "    # 6. COMPARE VS ENGINE\n",
    "    engine_series = res.portfolio_series\n",
    "    \n",
    "    # Align indexes just in case\n",
    "    shadow_series, engine_series = shadow_series.align(engine_series, join='inner')\n",
    "    \n",
    "    delta = shadow_series - engine_series\n",
    "    max_error = delta.abs().max()\n",
    "    \n",
    "    print(f\"\\n2. SERIES COMPARISON:\")\n",
    "    df_audit = pd.DataFrame({\n",
    "        'Shadow_Calc': shadow_series,\n",
    "        'Engine_Output': engine_series,\n",
    "        'Difference': delta\n",
    "    })\n",
    "    \n",
    "    # Show Head (Start of sim)\n",
    "    print(\"\\n--- Start of Simulation (Head) ---\")\n",
    "    print(df_audit.head())\n",
    "    \n",
    "    # Show T1 (The Entry Point)\n",
    "    if res.buy_date in df_audit.index:\n",
    "        print(f\"\\n--- Entry Date (T+1: {res.buy_date.date()}) ---\")\n",
    "        print(df_audit.loc[[res.buy_date]])\n",
    "\n",
    "    # Show Tail (End of sim)\n",
    "    print(\"\\n--- End of Simulation (Tail) ---\")\n",
    "    print(df_audit.tail())\n",
    "    \n",
    "    print(\"-\" * 60)\n",
    "    if max_error < 1e-9:\n",
    "        print(f\"‚úÖ SUCCESS: Portfolio Construction Verified (Max Error: {max_error:.12f})\")\n",
    "    else:\n",
    "        print(f\"‚ùå FAILURE: Series Mismatch (Max Error: {max_error:.12f})\")\n",
    "        \n",
    "    return df_audit\n",
    "\n",
    "# Execute Audit\n",
    "audit_df = verify_portfolio_construction(\n",
    "    df_close_wide=my_close_matrix,\n",
    "    res_container=results_container)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "332368a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METRIC                         | REPORTED     | CALCULATED   | DIFF         | STATUS\n",
      "-------------------------------------------------------------------------------------\n",
      "lookback_p_gain                |     0.128599 |     0.128599 |  0.000000000 | ‚úÖ PASS\n",
      "lookback_p_sharpe              |    12.044670 |    12.044670 |  0.000000000 | ‚úÖ PASS\n",
      "holding_p_gain                 |     0.002122 |     0.002122 |  0.000000000 | ‚úÖ PASS\n",
      "holding_p_sharpe               |     0.624537 |     0.624537 |  0.000000000 | ‚úÖ PASS\n",
      "holding_p_sharpe_atr           |     0.015249 |     0.015249 |  0.000000000 | ‚úÖ PASS\n",
      "lookback_b_gain                |     0.032839 |     0.032839 |  0.000000000 | ‚úÖ PASS\n",
      "lookback_b_sharpe              |     6.133590 |     6.133590 |  0.000000000 | ‚úÖ PASS\n",
      "holding_b_gain                 |    -0.005731 |    -0.005731 |  0.000000000 | ‚úÖ PASS\n",
      "holding_b_sharpe               |    -2.769886 |    -2.769886 |  0.000000000 | ‚úÖ PASS\n",
      "holding_b_sharpe_atr           |    -0.104184 |    -0.104184 |  0.000000000 | ‚úÖ PASS\n"
     ]
    }
   ],
   "source": [
    "def verify_engine_integrity(res_container, debug_container):\n",
    "    \"\"\"\n",
    "    Loops through reported metrics and recalculates them using the \n",
    "    raw verification series stored in debug_container.\n",
    "    \"\"\"\n",
    "    if not res_container[0] or not debug_container[0]:\n",
    "        print(\"‚ö†Ô∏è No results found. Run the simulation first.\")\n",
    "        return\n",
    "\n",
    "    metrics = res_container[0].perf_metrics\n",
    "    verification_data = debug_container[0]['verification']\n",
    "    \n",
    "    print(f\"{'METRIC':<30} | {'REPORTED':<12} | {'CALCULATED':<12} | {'DIFF':<12} | {'STATUS'}\")\n",
    "    print(\"-\" * 85)\n",
    "\n",
    "    # 1. Map prefixes to the keys in the verification dictionary\n",
    "    targets = [('p', 'portfolio'), ('b', 'benchmark')]\n",
    "    \n",
    "    # 2. Define the periods we explicitly saved in v2.2\n",
    "    # (Note: v2.2 saved Lookback and Holding. Full is implied but not explicitly saved in slices)\n",
    "    periods = ['lookback', 'holding']\n",
    "\n",
    "    for prefix, target_key in targets:\n",
    "        slices = verification_data[target_key]\n",
    "        \n",
    "        for period in periods:\n",
    "            # --- A. VERIFY GAIN ---\n",
    "            metric_name = f\"{period}_{prefix}_gain\"\n",
    "            val_key = f\"{period}_val\"\n",
    "            \n",
    "            if val_key in slices and not slices[val_key].empty:\n",
    "                s = slices[val_key]\n",
    "                # Gain Formula: (End / Start) - 1\n",
    "                calc_gain = (s.iloc[-1] / s.iloc[0]) - 1\n",
    "                reported_gain = metrics.get(metric_name, 0.0)\n",
    "                \n",
    "                diff = abs(calc_gain - reported_gain)\n",
    "                status = \"‚úÖ PASS\" if diff < 1e-8 else \"‚ùå FAIL\"\n",
    "                print(f\"{metric_name:<30} | {reported_gain:12.6f} | {calc_gain:12.6f} | {diff:12.9f} | {status}\")\n",
    "\n",
    "            # --- B. VERIFY SHARPE ---\n",
    "            metric_name = f\"{period}_{prefix}_sharpe\"\n",
    "            ret_key = f\"{period}_ret\"\n",
    "            \n",
    "            if ret_key in slices and not slices[ret_key].empty:\n",
    "                s = slices[ret_key]\n",
    "                # Sharpe Formula: (Mean / Std) * sqrt(252)\n",
    "                # Note: Pandas std() uses ddof=1 by default, which is correct\n",
    "                if s.std() > 0:\n",
    "                    calc_sharpe = (s.mean() / s.std()) * np.sqrt(252)\n",
    "                else:\n",
    "                    calc_sharpe = 0.0\n",
    "                    \n",
    "                reported_sharpe = metrics.get(metric_name, 0.0)\n",
    "                \n",
    "                diff = abs(calc_sharpe - reported_sharpe)\n",
    "                status = \"‚úÖ PASS\" if diff < 1e-8 else \"‚ùå FAIL\"\n",
    "                print(f\"{metric_name:<30} | {reported_sharpe:12.6f} | {calc_sharpe:12.6f} | {diff:12.9f} | {status}\")\n",
    "\n",
    "            # --- C. VERIFY SHARPE (ATR) ---\n",
    "            metric_name = f\"{period}_{prefix}_sharpe_atr\"\n",
    "            atrp_key = f\"{period}_atrp\"\n",
    "            \n",
    "            # Only verify if we saved the ATRP slice (v2.2 saves holding_atrp)\n",
    "            if ret_key in slices and atrp_key in slices:\n",
    "                s_ret = slices[ret_key]\n",
    "                s_atrp = slices[atrp_key]\n",
    "                \n",
    "                if not s_ret.empty and not s_atrp.empty:\n",
    "                    # Sharpe ATR Formula: Mean(Returns) / Mean(ATRP)\n",
    "                    mean_atrp = s_atrp.mean()\n",
    "                    if mean_atrp > 0:\n",
    "                        calc_sharpe_atr = s_ret.mean() / mean_atrp\n",
    "                    else:\n",
    "                        calc_sharpe_atr = 0.0\n",
    "                        \n",
    "                    reported_satr = metrics.get(metric_name, 0.0)\n",
    "                    \n",
    "                    diff = abs(calc_sharpe_atr - reported_satr)\n",
    "                    status = \"‚úÖ PASS\" if diff < 1e-8 else \"‚ùå FAIL\"\n",
    "                    print(f\"{metric_name:<30} | {reported_satr:12.6f} | {calc_sharpe_atr:12.6f} | {diff:12.9f} | {status}\")\n",
    "\n",
    "# Run the verifier\n",
    "verify_engine_integrity(results_container, debug_container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3bf4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a244281",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_nested(debug_container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "551b239f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EngineOutput(portfolio_series=Date\n",
      "2025-07-30    1.0000\n",
      "2025-07-31    0.9880\n",
      "2025-08-01    0.9892\n",
      "2025-08-04    1.0054\n",
      "2025-08-05    1.0033\n",
      "2025-08-06    1.0016\n",
      "2025-08-07    1.0094\n",
      "2025-08-08    1.0197\n",
      "2025-08-11    1.0276\n",
      "2025-08-12    1.0484\n",
      "2025-08-13    1.0612\n",
      "2025-08-14    1.0583\n",
      "2025-08-15    1.0528\n",
      "2025-08-18    1.0529\n",
      "2025-08-19    1.0575\n",
      "2025-08-20    1.0538\n",
      "2025-08-21    1.0525\n",
      "dtype: float64, benchmark_series=Date\n",
      "2025-07-30    1.0000\n",
      "2025-07-31    0.9962\n",
      "2025-08-01    0.9799\n",
      "2025-08-04    0.9948\n",
      "2025-08-05    0.9898\n",
      "2025-08-06    0.9974\n",
      "2025-08-07    0.9965\n",
      "2025-08-08    1.0043\n",
      "2025-08-11    1.0023\n",
      "2025-08-12    1.0130\n",
      "2025-08-13    1.0164\n",
      "2025-08-14    1.0165\n",
      "2025-08-15    1.0142\n",
      "2025-08-18    1.0139\n",
      "2025-08-19    1.0084\n",
      "2025-08-20    1.0058\n",
      "2025-08-21    1.0017\n",
      "dtype: float64, normalized_plot_data=Ticker         JNJ    LRCX     SPG      VT     DUK     AZO     BLK     MUB      SE     ALB\n",
      "Date                                                                                      \n",
      "2025-07-30  1.0000  1.0000  1.0000  1.0000  1.0000  1.0000  1.0000  1.0000  1.0000  1.0000\n",
      "2025-07-31  0.9849  0.9571  0.9899  0.9947  1.0096  0.9806  0.9922  1.0023  0.9837  0.9849\n",
      "2025-08-01  1.0004  0.9725  0.9711  0.9831  1.0202  1.0041  0.9795  1.0073  0.9684  0.9856\n",
      "2025-08-04  1.0226  0.9931  0.9908  0.9969  1.0306  1.0324  1.0013  1.0071  0.9842  0.9952\n",
      "2025-08-05  1.0208  0.9757  1.0233  0.9949  1.0292  1.0438  0.9955  1.0080  0.9510  0.9907\n",
      "2025-08-06  1.0199  0.9682  1.0125  1.0014  1.0393  1.0598  1.0053  1.0066  0.9265  0.9769\n",
      "2025-08-07  1.0255  1.0006  1.0054  1.0032  1.0452  1.0565  0.9984  1.0070  0.9348  1.0170\n",
      "2025-08-08  1.0363  1.0268  0.9968  1.0091  1.0402  1.0500  1.0093  1.0065  0.9269  1.0957\n",
      "2025-08-11  1.0392  1.0294  0.9984  1.0064  1.0422  1.0504  1.0119  1.0077  0.9182  1.1723\n",
      "2025-08-12  1.0330  1.0625  1.0227  1.0184  1.0331  1.0410  1.0403  1.0077  1.0934  1.1320\n",
      "2025-08-13  1.0428  1.0772  1.0431  1.0236  1.0434  1.0503  1.0407  1.0084  1.0968  1.1855\n",
      "2025-08-14  1.0446  1.0837  1.0407  1.0211  1.0314  1.0389  1.0408  1.0067  1.0927  1.1820\n",
      "2025-08-15  1.0561  1.0042  1.0473  1.0211  1.0251  1.0426  1.0183  1.0068  1.1132  1.1931\n",
      "2025-08-18  1.0537  0.9979  1.0429  1.0217  1.0168  1.0512  1.0193  1.0066  1.1132  1.2061\n",
      "2025-08-19  1.0630  1.0125  1.0569  1.0170  1.0363  1.0743  1.0124  1.0072  1.1256  1.1693\n",
      "2025-08-20  1.0692  1.0006  1.0524  1.0157  1.0421  1.0820  1.0124  1.0067  1.1224  1.1340\n",
      "2025-08-21  1.0698  0.9931  1.0482  1.0120  1.0393  1.0740  1.0091  1.0054  1.1278  1.1459, tickers=['MUB', 'SE', 'ALB', 'DUK', 'VT', 'JNJ', 'LRCX', 'AZO', 'BLK', 'SPG'], initial_weights=MUB     0.1\n",
      "SE      0.1\n",
      "ALB     0.1\n",
      "DUK     0.1\n",
      "VT      0.1\n",
      "JNJ     0.1\n",
      "LRCX    0.1\n",
      "AZO     0.1\n",
      "BLK     0.1\n",
      "SPG     0.1\n",
      "dtype: float64, perf_metrics={'full_p_gain': 0.052454863275849695, 'lookback_p_gain': 0.06117894173159, 'holding_p_gain': -0.005485096439526882, 'full_p_sharpe': 5.987736342973743, 'lookback_p_sharpe': 9.829445472536221, 'holding_p_sharpe': -4.777218986972209, 'full_p_sharpe_atr': 0.14414232669068186, 'lookback_p_sharpe_atr': 0.2654655681661157, 'holding_p_sharpe_atr': -0.04936930969057878, 'full_b_gain': 0.0017180175310491652, 'lookback_b_gain': 0.01643891475661685, 'holding_b_gain': -0.014574752746953856, 'full_b_sharpe': 0.2826048793092599, 'lookback_b_sharpe': 2.883471297880455, 'holding_b_sharpe': -23.889572215373096, 'full_b_sharpe_atr': 0.01545079767425351, 'lookback_b_sharpe_atr': 0.18814433667953198, 'holding_b_sharpe_atr': -0.35519796915407403}, results_df=        Rank  Strategy Value  Holding Gain\n",
      "Ticker                                    \n",
      "MUB       11          0.3238       -0.0014\n",
      "SE        12          0.2989        0.0321\n",
      "ALB       13          0.2961       -0.0306\n",
      "DUK       14          0.2831        0.0077\n",
      "VT        15          0.2802       -0.0089\n",
      "JNJ       16          0.2708        0.0241\n",
      "LRCX      17          0.2635       -0.0835\n",
      "AZO       18          0.2511        0.0339\n",
      "BLK       19          0.2307       -0.0305\n",
      "SPG       20          0.2279        0.0072, start_date=Timestamp('2025-07-30 00:00:00'), decision_date=Timestamp('2025-08-13 00:00:00'), buy_date=Timestamp('2025-08-14 00:00:00'), holding_end_date=Timestamp('2025-08-21 00:00:00'), error_msg=None, debug_data={'audit_liquidity': {'date': Timestamp('2025-08-13 00:00:00'), 'total_tickers_available': 177, 'percentile_setting': 0.5, 'final_cutoff_usd': 191026498.8075, 'tickers_passed': 87, 'universe_snapshot':             TR     ATR    ATRP   ROC_1   ROC_3   ROC_5  ROC_10  ROC_21  RollingStalePct  RollMedDollarVol  RollingSameVolCount  Calculated_Cutoff  Passed_Vol_Check  Passed_Final\n",
      "Ticker                                                                                                                                                                           \n",
      "SPY     3.5000  5.6850  0.0088  0.0034  0.0121  0.0191  0.0164  0.0366              0.0        3.3654e+10                  0.0         1.9103e+08              True          True\n",
      "QQQ     4.3750  6.4681  0.0112  0.0005  0.0101  0.0230  0.0217  0.0424              0.0        1.8455e+10                  0.0         1.9103e+08              True          True\n",
      "PLTR    5.8700  6.4146  0.0348 -0.0139 -0.0139  0.0269  0.1624  0.2409              0.0        7.5872e+09                  0.0         1.9103e+08              True          True\n",
      "GOOGL   7.0100  4.3318  0.0215 -0.0068  0.0027  0.0299  0.0276  0.1097              0.0        5.1081e+09                  0.0         1.9103e+08              True          True\n",
      "GOOG    6.7800  4.2912  0.0212 -0.0055  0.0046  0.0310  0.0283  0.1088              0.0        3.3522e+09                  0.0         1.9103e+08              True          True\n",
      "...        ...     ...     ...     ...     ...     ...     ...     ...              ...               ...                  ...                ...               ...           ...\n",
      "DSI     0.6890  1.1068  0.0091  0.0031  0.0105  0.0165  0.0096  0.0368              0.0        1.1068e+07                  0.0         1.9103e+08             False         False\n",
      "BKLC    0.7180  1.0924  0.0089  0.0028  0.0112  0.0177  0.0152  0.0360              0.0        1.0064e+07                  0.0         1.9103e+08             False         False\n",
      "EBR     0.1770  0.1975  0.0255 -0.0154  0.0171  0.1671  0.2131  0.1771              0.0        7.5485e+06                  0.0         1.9103e+08             False         False\n",
      "CWEN-A  0.2622  0.6297  0.0235  0.0036 -0.0209 -0.0265 -0.0895 -0.0907              0.0        4.8484e+06                  0.0         1.9103e+08             False         False\n",
      "SBIL    0.0266  0.0382  0.0004  0.0000  0.0003  0.0007  0.0017  0.0032              NaN               NaN                  NaN         1.9103e+08             False         False\n",
      "\n",
      "[177 rows x 14 columns]}, 'verification': {'portfolio': {'lookback_val': Date\n",
      "2025-07-30    1.0000\n",
      "2025-07-31    0.9880\n",
      "2025-08-01    0.9892\n",
      "2025-08-04    1.0054\n",
      "2025-08-05    1.0033\n",
      "2025-08-06    1.0016\n",
      "2025-08-07    1.0094\n",
      "2025-08-08    1.0197\n",
      "2025-08-11    1.0276\n",
      "2025-08-12    1.0484\n",
      "2025-08-13    1.0612\n",
      "dtype: float64, 'lookback_ret': Date\n",
      "2025-07-30       NaN\n",
      "2025-07-31   -0.0120\n",
      "2025-08-01    0.0012\n",
      "2025-08-04    0.0164\n",
      "2025-08-05   -0.0021\n",
      "2025-08-06   -0.0016\n",
      "2025-08-07    0.0077\n",
      "2025-08-08    0.0103\n",
      "2025-08-11    0.0077\n",
      "2025-08-12    0.0202\n",
      "2025-08-13    0.0122\n",
      "dtype: float64, 'holding_val': Date\n",
      "2025-08-14    1.0583\n",
      "2025-08-15    1.0528\n",
      "2025-08-18    1.0529\n",
      "2025-08-19    1.0575\n",
      "2025-08-20    1.0538\n",
      "2025-08-21    1.0525\n",
      "dtype: float64, 'holding_ret': Date\n",
      "2025-08-14       NaN\n",
      "2025-08-15   -0.0052\n",
      "2025-08-18    0.0002\n",
      "2025-08-19    0.0043\n",
      "2025-08-20   -0.0035\n",
      "2025-08-21   -0.0012\n",
      "dtype: float64, 'holding_atrp': Date\n",
      "2025-08-14    0.0228\n",
      "2025-08-15    0.0228\n",
      "2025-08-18    0.0222\n",
      "2025-08-19    0.0218\n",
      "2025-08-20    0.0219\n",
      "2025-08-21    0.0214\n",
      "dtype: float64}, 'benchmark': {'lookback_val': Date\n",
      "2025-07-30    1.0000\n",
      "2025-07-31    0.9962\n",
      "2025-08-01    0.9799\n",
      "2025-08-04    0.9948\n",
      "2025-08-05    0.9898\n",
      "2025-08-06    0.9974\n",
      "2025-08-07    0.9965\n",
      "2025-08-08    1.0043\n",
      "2025-08-11    1.0023\n",
      "2025-08-12    1.0130\n",
      "2025-08-13    1.0164\n",
      "dtype: float64, 'lookback_ret': Date\n",
      "2025-07-30       NaN\n",
      "2025-07-31   -0.0038\n",
      "2025-08-01   -0.0164\n",
      "2025-08-04    0.0152\n",
      "2025-08-05   -0.0051\n",
      "2025-08-06    0.0077\n",
      "2025-08-07   -0.0008\n",
      "2025-08-08    0.0078\n",
      "2025-08-11   -0.0020\n",
      "2025-08-12    0.0106\n",
      "2025-08-13    0.0034\n",
      "dtype: float64, 'holding_val': Date\n",
      "2025-08-14    1.0165\n",
      "2025-08-15    1.0142\n",
      "2025-08-18    1.0139\n",
      "2025-08-19    1.0084\n",
      "2025-08-20    1.0058\n",
      "2025-08-21    1.0017\n",
      "dtype: float64, 'holding_ret': Date\n",
      "2025-08-14       NaN\n",
      "2025-08-15   -0.0023\n",
      "2025-08-18   -0.0002\n",
      "2025-08-19   -0.0054\n",
      "2025-08-20   -0.0027\n",
      "2025-08-21   -0.0040\n",
      "dtype: float64, 'holding_atrp': Date\n",
      "2025-08-14    0.0086\n",
      "2025-08-15    0.0084\n",
      "2025-08-18    0.0080\n",
      "2025-08-19    0.0081\n",
      "2025-08-20    0.0083\n",
      "2025-08-21    0.0082\n",
      "dtype: float64}}})\n",
      "====================\n",
      "inputs:\n",
      "    EngineInput(mode='Ranking', start_date=Timestamp('2025-08-13 00:00:00'), lookback_period=10, holding_period=5, metric='Sharpe (ATR)', benchmark_ticker='SPY', rank_start=11, rank_end=20, quality_thresholds={'min_median_dollar_volume': 100000, 'min_liquidity_percentile': 0.5, 'max_stale_pct': 0.05, 'max_same_vol_count': 10}, manual_tickers=[], debug=True)\n",
      "audit_liquidity:\n",
      "    date:\n",
      "        2025-08-13 00:00:00\n",
      "    total_tickers_available:\n",
      "        177\n",
      "    percentile_setting:\n",
      "        0.5\n",
      "    final_cutoff_usd:\n",
      "        191026498.8075\n",
      "    tickers_passed:\n",
      "        87\n",
      "    universe_snapshot:\n",
      "                    TR     ATR    ATRP   ROC_1   ROC_3   ROC_5  ROC_10  ROC_21  RollingStalePct  RollMedDollarVol  RollingSameVolCount  Calculated_Cutoff  Passed_Vol_Check  Passed_Final\n",
      "Ticker                                                                                                                                                                           \n",
      "SPY     3.5000  5.6850  0.0088  0.0034  0.0121  0.0191  0.0164  0.0366              0.0        3.3654e+10                  0.0         1.9103e+08              True          True\n",
      "QQQ     4.3750  6.4681  0.0112  0.0005  0.0101  0.0230  0.0217  0.0424              0.0        1.8455e+10                  0.0         1.9103e+08              True          True\n",
      "PLTR    5.8700  6.4146  0.0348 -0.0139 -0.0139  0.0269  0.1624  0.2409              0.0        7.5872e+09                  0.0         1.9103e+08              True          True\n",
      "GOOGL   7.0100  4.3318  0.0215 -0.0068  0.0027  0.0299  0.0276  0.1097              0.0        5.1081e+09                  0.0         1.9103e+08              True          True\n",
      "GOOG    6.7800  4.2912  0.0212 -0.0055  0.0046  0.0310  0.0283  0.1088              0.0        3.3522e+09                  0.0         1.9103e+08              True          True\n",
      "...        ...     ...     ...     ...     ...     ...     ...     ...              ...               ...                  ...                ...               ...           ...\n",
      "DSI     0.6890  1.1068  0.0091  0.0031  0.0105  0.0165  0.0096  0.0368              0.0        1.1068e+07                  0.0         1.9103e+08             False         False\n",
      "BKLC    0.7180  1.0924  0.0089  0.0028  0.0112  0.0177  0.0152  0.0360              0.0        1.0064e+07                  0.0         1.9103e+08             False         False\n",
      "EBR     0.1770  0.1975  0.0255 -0.0154  0.0171  0.1671  0.2131  0.1771              0.0        7.5485e+06                  0.0         1.9103e+08             False         False\n",
      "CWEN-A  0.2622  0.6297  0.0235  0.0036 -0.0209 -0.0265 -0.0895 -0.0907              0.0        4.8484e+06                  0.0         1.9103e+08             False         False\n",
      "SBIL    0.0266  0.0382  0.0004  0.0000  0.0003  0.0007  0.0017  0.0032              NaN               NaN                  NaN         1.9103e+08             False         False\n",
      "\n",
      "[177 rows x 14 columns]\n",
      "verification:\n",
      "    portfolio:\n",
      "        lookback_val:\n",
      "            Date\n",
      "2025-07-30    1.0000\n",
      "2025-07-31    0.9880\n",
      "2025-08-01    0.9892\n",
      "2025-08-04    1.0054\n",
      "2025-08-05    1.0033\n",
      "2025-08-06    1.0016\n",
      "2025-08-07    1.0094\n",
      "2025-08-08    1.0197\n",
      "2025-08-11    1.0276\n",
      "2025-08-12    1.0484\n",
      "2025-08-13    1.0612\n",
      "dtype: float64\n",
      "        lookback_ret:\n",
      "            Date\n",
      "2025-07-30       NaN\n",
      "2025-07-31   -0.0120\n",
      "2025-08-01    0.0012\n",
      "2025-08-04    0.0164\n",
      "2025-08-05   -0.0021\n",
      "2025-08-06   -0.0016\n",
      "2025-08-07    0.0077\n",
      "2025-08-08    0.0103\n",
      "2025-08-11    0.0077\n",
      "2025-08-12    0.0202\n",
      "2025-08-13    0.0122\n",
      "dtype: float64\n",
      "        holding_val:\n",
      "            Date\n",
      "2025-08-14    1.0583\n",
      "2025-08-15    1.0528\n",
      "2025-08-18    1.0529\n",
      "2025-08-19    1.0575\n",
      "2025-08-20    1.0538\n",
      "2025-08-21    1.0525\n",
      "dtype: float64\n",
      "        holding_ret:\n",
      "            Date\n",
      "2025-08-14       NaN\n",
      "2025-08-15   -0.0052\n",
      "2025-08-18    0.0002\n",
      "2025-08-19    0.0043\n",
      "2025-08-20   -0.0035\n",
      "2025-08-21   -0.0012\n",
      "dtype: float64\n",
      "        holding_atrp:\n",
      "            Date\n",
      "2025-08-14    0.0228\n",
      "2025-08-15    0.0228\n",
      "2025-08-18    0.0222\n",
      "2025-08-19    0.0218\n",
      "2025-08-20    0.0219\n",
      "2025-08-21    0.0214\n",
      "dtype: float64\n",
      "    benchmark:\n",
      "        lookback_val:\n",
      "            Date\n",
      "2025-07-30    1.0000\n",
      "2025-07-31    0.9962\n",
      "2025-08-01    0.9799\n",
      "2025-08-04    0.9948\n",
      "2025-08-05    0.9898\n",
      "2025-08-06    0.9974\n",
      "2025-08-07    0.9965\n",
      "2025-08-08    1.0043\n",
      "2025-08-11    1.0023\n",
      "2025-08-12    1.0130\n",
      "2025-08-13    1.0164\n",
      "dtype: float64\n",
      "        lookback_ret:\n",
      "            Date\n",
      "2025-07-30       NaN\n",
      "2025-07-31   -0.0038\n",
      "2025-08-01   -0.0164\n",
      "2025-08-04    0.0152\n",
      "2025-08-05   -0.0051\n",
      "2025-08-06    0.0077\n",
      "2025-08-07   -0.0008\n",
      "2025-08-08    0.0078\n",
      "2025-08-11   -0.0020\n",
      "2025-08-12    0.0106\n",
      "2025-08-13    0.0034\n",
      "dtype: float64\n",
      "        holding_val:\n",
      "            Date\n",
      "2025-08-14    1.0165\n",
      "2025-08-15    1.0142\n",
      "2025-08-18    1.0139\n",
      "2025-08-19    1.0084\n",
      "2025-08-20    1.0058\n",
      "2025-08-21    1.0017\n",
      "dtype: float64\n",
      "        holding_ret:\n",
      "            Date\n",
      "2025-08-14       NaN\n",
      "2025-08-15   -0.0023\n",
      "2025-08-18   -0.0002\n",
      "2025-08-19   -0.0054\n",
      "2025-08-20   -0.0027\n",
      "2025-08-21   -0.0040\n",
      "dtype: float64\n",
      "        holding_atrp:\n",
      "            Date\n",
      "2025-08-14    0.0086\n",
      "2025-08-15    0.0084\n",
      "2025-08-18    0.0080\n",
      "2025-08-19    0.0081\n",
      "2025-08-20    0.0083\n",
      "2025-08-21    0.0082\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print_nested(results_container)\n",
    "print('='*20)\n",
    "print_nested(debug_container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cc1ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_tickers = ['VOO', 'VLO', 'JPST']\n",
    "# my_tickers = ['VT']\n",
    "my_tickers = ['SPY', 'APP']\n",
    "my_tickers = ['COHR', 'APP']\n",
    "date_start = '2025-07-30'\n",
    "date_end = '2025-08-21'\n",
    "\n",
    "# Create combined dictionary\n",
    "combined_dict = create_combined_dict(\n",
    "    df_ohlcv=df_ohlcv,\n",
    "    features_df=my_features,\n",
    "    tickers=my_tickers,\n",
    "    date_start=date_start,\n",
    "    date_end=date_end,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print_nested(combined_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74880ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5e1e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Access the data inside the container list\n",
    "current_debug_data = debug_container[0]\n",
    "\n",
    "# 2. Check if the audit data exists (it is created only in 'Ranking' mode)\n",
    "if current_debug_data and 'audit_liquidity' in current_debug_data:\n",
    "    audit = current_debug_data['audit_liquidity']\n",
    "    snapshot_df = audit['universe_snapshot']\n",
    "    \n",
    "    print(f\"üìÖ Date: {audit['date'].date()}\")\n",
    "    print(f\"üí∞ Calculated Cutoff: ${audit['final_cutoff_usd']:,.0f}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "# 3. View the tickers right around the cutoff point\n",
    "# Find the index where 'Passed_Vol_Check' switches from True to False\n",
    "    try:\n",
    "        # Get the integer location (iloc) of the last True value\n",
    "        last_pass_iloc = np.where(snapshot_df['Passed_Vol_Check'])[0][-1]\n",
    "        \n",
    "        # Show 5 rows before and 5 rows after the cutoff\n",
    "        start = max(0, last_pass_iloc - 5)\n",
    "        end = min(len(snapshot_df), last_pass_iloc + 6)\n",
    "        \n",
    "        display(snapshot_df.iloc[start:end].style.format({\n",
    "            'RollMedDollarVol': '${:,.0f}',\n",
    "            'Calculated_Cutoff': '${:,.0f}',\n",
    "            'RollingStalePct': '{:.1%}'\n",
    "        }))\n",
    "    except IndexError:\n",
    "        print(\"Could not determine cutoff boundary (maybe all passed or all failed).\")\n",
    "        display(snapshot_df.head())\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No audit data found. Make sure you are in 'Ranking' mode and have clicked 'Update Chart'.\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec43fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32da9680",
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot_df.to_csv('./export_csv/snapshot_df.csv')\n",
    "print(f\"‚úÖ Snapshot exported to: ./export_csv/snapshot_df.csv\")\n",
    "print(f\"   Shape: {snapshot_df.shape}\")\n",
    "print(f\"   Columns: {list(snapshot_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e0c43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have run the variables setup from the previous step\n",
    "snapshot_df = debug_container[0]['audit_liquidity']['universe_snapshot']\n",
    "\n",
    "if 'AAPL' in snapshot_df.index:\n",
    "    display(snapshot_df.loc[['AAPL']])\n",
    "else:\n",
    "    print(\"AAPL was not present in the data for this date.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d210b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "================================  \n",
    "================================  \n",
    "================================  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96683b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ohlcv.loc['SPY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd30b84c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd55374",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dict['JPST']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a082cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e7267c",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tickers = ['SPY', 'AAPL', 'IWM', 'QQQ', 'META', 'EEM', 'BABA']\n",
    "my_tickers = ['NTES', 'LII',]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b6b19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(snapshot_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32e1b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your data is loaded in 'df_ohlcv'\n",
    "print(\"--- üöÄ LAUNCHING APP ---\")\n",
    "\n",
    "# 1. We clear any old features so it calculates the new ROC columns\n",
    "# 2. We pass 'debug=True' so we can see the audit logs in the console\n",
    "results, debug_data = plot_walk_forward_analyzer(\n",
    "    df_ohlcv, \n",
    "    precomputed_features=None, # Force recalculation of new features\n",
    "    default_strategy='Pullback 5D', # Try one of your new strategies!\n",
    "    debug=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05cafb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
