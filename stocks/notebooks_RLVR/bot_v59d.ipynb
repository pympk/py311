{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9617f9b0",
   "metadata": {},
   "source": [
    "v59  \n",
    "- verify macro_df calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bc23af",
   "metadata": {},
   "source": [
    "v57, v58  \n",
    "added marco subplotsThe macro regime framework is now fully documented with:\n",
    "- Trend (SMA200 deviation) â†’ Where we are in the cycle  \n",
    "- Trend Velocity (Z) â†’ How fast we're moving relative to normal\n",
    "- VIX-Z â†’ Market fear/complacency levels  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f77acd",
   "metadata": {},
   "source": [
    "v56  \n",
    "\n",
    "- De-coupled features_df and macro_df\n",
    "- generate_features and audit_feature_engineering_integrity use GLOBAL_SETTINGS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195e6ee3",
   "metadata": {},
   "source": [
    "v55  \n",
    "Added\n",
    "- audit_feature_engineering_integrity (check calculation in features_df)  \n",
    "\n",
    "These are the metrics in plot  \n",
    "- --- 1. LEGACY / SANITY CHECKS ---\n",
    "- \"Price Gain\": lambda obs: QuantUtils.calculate_gain(obs[\"lookback_close\"]),\n",
    "- \"Sharpe\": lambda obs: QuantUtils.calculate_sharpe(obs[\"lookback_returns\"]),\n",
    "- \"Sharpe (ATRP)\": lambda obs: QuantUtils.calculate_sharpe_vol(\n",
    "        obs[\"lookback_returns\"], obs[\"atrp\"]\n",
    "    ),\n",
    "- \"Sharpe (TRP)\": lambda obs: QuantUtils.calculate_sharpe_vol(\n",
    "        obs[\"lookback_returns\"], obs[\"trp\"]\n",
    "    ),\n",
    "- --- 2. NEW QUANT METRICS ---\n",
    "- \"Momentum (21d)\": lambda obs: obs[\"mom_21\"],\n",
    "- \"Information Ratio (IR)\": lambda obs: obs[\"ir_63\"],  # Kept this one\n",
    "- \"Consistency (WinRate)\": lambda obs: obs[\"consistency\"],\n",
    "- \"Oversold (RSI)\": lambda obs: -obs[\"rsi\"],\n",
    "- \"Dip Buyer (Drawdown)\": lambda obs: -obs[\"dd_21\"],\n",
    "- \"Low Volatility\": lambda obs: -obs[\"atrp\"],\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b219d0f",
   "metadata": {},
   "source": [
    "v54\n",
    "-  **Replaced plot_walk_forward_analyzer with create_walk_forward_analyzer**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68de115e",
   "metadata": {},
   "source": [
    "v53  \n",
    "Looking at this registry with a quant lens, the list is **comprehensive but bloated**â€”we have **momentum measured five times under different names** (rocâ‚, rocâ‚ƒ, rocâ‚…, rocâ‚â‚€, rocâ‚‚â‚ and their negative twins â€œPullbackâ€).  \n",
    "Thatâ€™s **10 slots** telling us almost the same story at slightly different lags; in a rank-based engine they will **crowd the signal space** and inflate turnover without adding IC.\n",
    "\n",
    "Duplicate / redundant cluster  \n",
    "- Momentum 1 D â†” Pullback 1 D (perfect mirror)  \n",
    "- Same for 3 D, 5 D, 10 D, 21 D.  \n",
    "**Keep one side only**â€”momentum is enough; the portfolio constructor can always **reverse the rank** if it wants â€œoversoldâ€.\n",
    "\n",
    "Close cousins that can be merged  \n",
    "- â€œSharpeâ€ vs â€œSharpe (ATRP)â€ â€“ both are return / vol; keep **ATRP version** because it is regime-aware and smoother.  \n",
    "- â€œRVolâ€ vs â€œVol_Regimeâ€ â€“ both capture vol expansion; keep the **longer-memory one** (Vol_Regime) and drop the intraday snapshot.\n",
    "\n",
    "Gaps that matter to a quant  \n",
    "1. **Consistency sensor**: nowhere do we ask â€œhow often did the ticker close higher than it opened?â€ â€“ add **5-day win-rate** or **up-day hit-ratio**.  \n",
    "2. **Risk-adjusted intraday strength**: no **Sharpe(on-balance volume)** or **volume-momentum efficiency**; OBV_Score is raw, not risk-scaled.  \n",
    "3. **Benchmark-relative consistency**: â€œAlpha (RelStrength)â€ is cumulative; add **rolling information ratio vs SPY** to catch *sustained* alpha, not one gap.  \n",
    "4. **Tail flag**: no **skew** or **max-drawdown** metric; a single 20 % gap stock can poison the book.  \n",
    "5. **Macro regime overlay**: no **beta-to-SPY** or **correlation-break** sensor; mid-2022 macro swings showed that low-beta names behaved like a different asset class.\n",
    "\n",
    "Recommended minimal clean set (â‰¤ 12 metrics)\n",
    "\n",
    "1. Sharpe(ATRP) â€“ strategic anchor  \n",
    "2. Momentum 21 D â€“ slow trend  \n",
    "3. Momentum 5 D â€“ fast trend  \n",
    "4. 5-day win-rate â€“ consistency  \n",
    "5. RSI(Trend) â€“ strength confirmation  \n",
    "6. OBV_Score â€“ volume conviction  \n",
    "7. Vol_Regime â€“ vol expansion filter  \n",
    "8. Alpha(RelStrength) 63-day IR â€“ benchmark consistency  \n",
    "9. Max 21-day drawdown â€“ tail guard  \n",
    "10. Beta-to-SPY â€“ macro regime tag  \n",
    "\n",
    "Drop everything else; the freed-up slots reduce collinearity, cut turnover, and leave head-room for **interaction terms** (e.g. momentum Ã— consistency) that actually add orthogonal signal.\n",
    "\n",
    "\n",
    "\n",
    "Below is a single, fully-vectorised block that adds the **five gap metrics** to your existing MultiIndex OHLCV frame.  \n",
    "It never loops over tickers; everything is done with `groupby(level=0).rolling(...)` so it runs in C-speed and keeps the same index.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ----------  CONFIG  -------------------------------------------------\n",
    "LKB_RET   = 21          # look-back for return-based metrics\n",
    "LKB_CONS  = 5           # consistency window (days)\n",
    "LKB_IR    = 63          # IR window\n",
    "LKB_BETA  = 63          # beta window\n",
    "LKB_TAIL  = 21          # max-drawdown window\n",
    "BENCH     = 'SPY'       # ticker that exists in your universe\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "# 1.  DAILY RETURNS ----------------------------------------------------\n",
    "df['ret'] = df.groupby(level=0)['Adj Close'].pct_change()\n",
    "\n",
    "# 2.  CONSISTENCY SENSOR  (5-day win-rate) -----------------------------\n",
    "df['up']  = df['ret'].gt(0).astype(int)\n",
    "df['consistency_5d'] = (df.groupby(level=0)['up']\n",
    "                          .rolling(LKB_CONS).mean()\n",
    "                          .reset_index(level=0, drop=True))\n",
    "\n",
    "# 3.  BENCHMARK-RELATIVE CONSISTENCY  (63-day IR vs SPY) ---------------\n",
    "# need benchmark return\n",
    "bench_ret = df.xs(BENCH, level=0)['ret'].rename('bench_ret')\n",
    "df = df.join(bench_ret, how='left')          # broadcast to all tickers\n",
    "\n",
    "df['active'] = df['ret'] - df['bench_ret']\n",
    "g = df.groupby(level=0)\n",
    "active_mean  = g['active'].rolling(LKB_IR).mean()\n",
    "active_std   = g['active'].rolling(LKB_IR).std()\n",
    "df['IR_63d'] = active_mean / active_std      # Information Ratio\n",
    "\n",
    "# 4.  TAIL FLAG  (21-day max drawdown) ---------------------------------\n",
    "roll_max = g['Adj Close'].rolling(LKB_TAIL).max()\n",
    "dd = (df['Adj Close'] - roll_max) / roll_max\n",
    "df['max_dd_21d'] = dd.groupby(level=0).rolling(LKB_TAIL).min()\n",
    "\n",
    "# 5.  MACRO REGIME OVERLAY  (beta to SPY) ------------------------------\n",
    "cov  = g['ret'].rolling(LKB_BETA).cov(df['bench_ret'])\n",
    "var  = df['bench_ret'].groupby(level=0).rolling(LKB_BETA).var()\n",
    "df['beta_SPY'] = cov / var\n",
    "\n",
    "# 6.  RISK-ADJUSTED INTRADAY STRENGTH  (OBV Sharpe) --------------------\n",
    "# OBV\n",
    "df['close_chg'] = df.groupby(level=0)['Adj Close'].diff()\n",
    "df['vol_dir']   = np.where(df['close_chg'] > 0,  df['Volume'],\n",
    "                   np.where(df['close_chg'] < 0, -df['Volume'], 0))\n",
    "df['obv'] = df.groupby(level=0)['vol_dir'].cumsum()\n",
    "\n",
    "# OBV return & vol\n",
    "df['obv_ret'] = df.groupby(level=0)['obv'].pct_change()\n",
    "obv_mean = g['obv_ret'].rolling(LKB_RET).mean()\n",
    "obv_std  = g['obv_ret'].rolling(LKB_RET).std()\n",
    "df['OBV_Sharpe_21d'] = obv_mean / obv_std\n",
    "\n",
    "# drop helper columns --------------------------------------------------\n",
    "df.drop(columns=['up','bench_ret','active','close_chg','vol_dir'], inplace=True)\n",
    "```\n",
    "\n",
    "After the block you have five new columns:\n",
    "\n",
    "- `consistency_5d`      â€“ 5-day win-rate (0-1)  \n",
    "- `IR_63d`              â€“ 63-day Information Ratio vs SPY  \n",
    "- `max_dd_21d`          â€“ 21-day maximum drawdown (â‰¤ 0)  \n",
    "- `beta_SPY`            â€“ rolling beta to SPY  \n",
    "- `OBV_Sharpe_21d`      â€“ OBV risk-adjusted momentum  \n",
    "\n",
    "All are aligned to the original MultiIndex and ready to be ranked or z-scored inside your Alpha Engine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53db751",
   "metadata": {},
   "source": [
    "v52  \n",
    "- **Cascase Filter results `AGREED` with bot_v54i.ipynb**\n",
    "- **Cascade Filter works with df_ohlcv_subset**\n",
    "- **verify_engine_results_short_form**\n",
    "- **verify_engine_results_long_form**\n",
    "-  **The Temporal Alignment Fix:** We synchronized the \"Reward\" (Returns) and \"Risk\" (Volatility) by implementing the $N-1$ denominator logic. This ensures that Day 1's volatility no longer dilutes your Sharpe scores.\n",
    "-  **The Event-Driven Re-normalization:** We verified that the Engine correctly resets capital and weights at the start of the Holding period, giving you an accurate \"Fresh Start\" performance metric.\n",
    "-  **The Double-Blind Verification:** We proved that the Engine's True Range (TRP) math is flawless by recreating it from raw High/Low/Close data and achieving an 8-decimal match.\n",
    "-  **Mathematical Fortification:** We centralized all logic into a polymorphic `QuantUtils` kernel that handles both single-portfolio reports and whole-universe rankings with built-in numerical safety.\n",
    "-  **Volatility Evolution:** We successfully added `TRP` (True Range Percent) and the `Sharpe (TRP)` metric, giving you a raw, high-frequency alternative to the smoothed ATR.\n",
    "-  **Data Integrity:** We implemented the \"Momentum Collapse\" tripwire (`verify_ranking_integrity`) to ensure that your risk-adjusted rankings never accidentally devolve into simple price momentum.\n",
    "-  **The \"Audit Pack\" Architecture:** We collapsed fragmented results into a single, atomic container, ensuring that your inputs, results, and debug data are always perfectly synchronized.\n",
    "-  **Total Transparency:** We replaced scattered CSV files with a unified **Excel Audit Report**, allowing for 1-to-1 manual verification of every calculation in the system.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9817eb05",
   "metadata": {},
   "source": [
    "v51\n",
    "\n",
    "UNDO v50, Calculate Sharpe(ATR) using mean over lookback period.  \n",
    "\n",
    "Comment out ``# --- PINPOINT START: ATRP SWITCH ---`` in function ``_select_tickers`` can switch between ``Averaged ATRP over lookback period`` and ``Current ATRP``  \n",
    "    # --- PINPOINT START: ATRP SWITCH ---  \n",
    "    # To switch between Old (Averaged ATRP) and New (Current ATRP):  \n",
    "    # 1. Comment out the logic you DON'T want.  \n",
    "    # 2. Uncomment the logic you DO want.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15eb349d",
   "metadata": {},
   "source": [
    "v50\n",
    "\n",
    "Ticker selection based on atrp_value_for_obs based on decision day, was based on average over lookback period. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31dde13",
   "metadata": {},
   "source": [
    "v48  \n",
    "### Summary of what you just accomplished:\n",
    "1.  **Strict Math:** `QuantUtils` now contains an `assert` that prevents any dev (or AI) from filling the first day with 0.0.\n",
    "2.  **Semantic Protection:** Variables are now named `returns_WITH_BOUNDARY_NAN`, signaling to the AI that the Null value is part of its identity.\n",
    "3.  **Complete SOLID Separation:** The Engine CONDUCTS the simulation, while `QuantUtils` CALCULATES the results. They no longer share logic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61923b0e",
   "metadata": {},
   "source": [
    "**1. Data Flow of `plot_walk_forward_analyzer`**\n",
    "The function acts as a **UI wrapper** around the `AlphaEngine` class. The flow is:\n",
    "1.  **Input:** User selects parameters (Dates, Lookback, Strategy).\n",
    "2.  **State Construction:** `AlphaEngine` slices the historical data (`df_ohlcv`, `df_atrp`) up to the `decision_date`.\n",
    "3.  **Policy Execution (Hardcoded):** The engine applies the logic (e.g., `METRIC_REGISTRY['Sharpe']`) to rank stocks based *only* on the Lookback window.\n",
    "4.  **Environment Step:** It simulates a \"Buy\" at `decision_date + 1` and calculates the returns over the `holding_period`.\n",
    "5.  **Reward Generation:** It outputs performance metrics (`holding_p_gain`, `holding_p_sharpe`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e7572bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ðŸ›¡ï¸ Starting Final Integrity Audit ---\n",
      "âœ… Series Boundary: OK\n",
      "âœ… DataFrame Boundary: OK\n",
      "âœ… AUDIT PASSED: Mathematical boundaries are strictly enforced.\n",
      "\n",
      "--- ðŸ›¡ï¸ Starting Feature Engineering Audit ---\n",
      "âš¡ Generating Decoupled Features (Benchmark: SPY)...\n",
      "Audit Values:\n",
      "[ nan 25.  17.5]\n",
      "âœ… FEATURE INTEGRITY PASSED: Wilder's ATR logic is strictly enforced.\n",
      "--- ðŸ›¡ï¸ Starting Ranking Kernel Audit ---\n",
      "âœ… RANKING INTEGRITY PASSED: Volatility normalization is strictly enforced.\n",
      "\n",
      "--- ðŸ›¡ï¸ Starting Volatility Alignment Audit ---\n",
      "âœ… Series Temporal Coupling: OK\n",
      "âœ… DataFrame Temporal Coupling: OK\n",
      "âœ… AUDIT PASSED: Reward and Risk are strictly synchronized.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from dataclasses import dataclass, field, fields, asdict, is_dataclass\n",
    "from typing import List, Dict, Optional, Any, Union, TypedDict, Tuple\n",
    "from collections import Counter\n",
    "from datetime import datetime, date\n",
    "from pandas.testing import assert_series_equal\n",
    "from plotly.subplots import make_subplots\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "# pd.set_option('display.max_rows', None)  display all rows\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", 1000)\n",
    "pd.set_option(\"display.max_colwidth\", 50)\n",
    "pd.set_option(\"display.precision\", 4)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# GLOBAL SETTINGS: The \"Control Panel\" for the Strategy\n",
    "# ==============================================================================\n",
    "\n",
    "GLOBAL_SETTINGS = {\n",
    "    # ENVIRONMENT (The \"Where\")\n",
    "    \"benchmark_ticker\": \"SPY\",\n",
    "    \"calendar_ticker\": \"SPY\",  # Used as the \"Master Clock\" for trading days\n",
    "    # DATA SANITIZER (The \"Glitches & Gaps\" Protector)\n",
    "    \"handle_zeros_as_nan\": True,  # Convert 0.0 prices to NaN to prevent math errors\n",
    "    \"max_data_gap_ffill\": 1,  # Max consecutive days to \"Forward Fill\" missing data\n",
    "    # IMPLICATION OF nan_price_replacement:\n",
    "    # - This defines what happens if the \"Forward Fill\" limit is exceeded.\n",
    "    # - If set to 0.0: A permanent data gap will look like a \"total loss\" (-100%).\n",
    "    #   The equity curve will plummet. Good for \"disaster detection.\"\n",
    "    #   Sharpe and Sharpe(ATR) drop because: return (gets smaller) / std (gets larger)\n",
    "    # - If set to np.nan: A permanent gap will cause portfolio calculations to return NaN.\n",
    "    #   The chart may break or show gaps. Good for \"math integrity.\"\n",
    "    \"nan_price_replacement\": 0.0,\n",
    "    # STRATEGY & MATH\n",
    "    \"annual_period\": 252,  # Replaces hardcoded 252 in Sharpe calculations\n",
    "    \"atr_period\": 14,  # Used for volatility normalization\n",
    "    \"rsi_period\": 14,  # <--- NEW: Control for RSI logic\n",
    "    # FEATURE ENGINE WINDOWS\n",
    "    \"5d_window\": 5,  # Replaces hardcoded 5 (\"Weekly\" anchor)\n",
    "    \"21d_window\": 21,  # Replaces hardcoded 21 (\"Monthly\" anchor)\n",
    "    \"63d_window\": 63,  # Replaces hardcoded 63 (\"3 Monthly\" anchor)\n",
    "    # FEATURE GUARDRAILS (CLIPS)\n",
    "    \"feature_zscore_clip\": 4.0,  # Replaces hardcoded 4.0 in OBV Z-Scores\n",
    "    \"feature_ratio_clip\": 10.0,  # Replaces hardcoded 10.0 in RVol ratios\n",
    "    # QUALITY/LIQUIDITY\n",
    "    \"quality_window\": 252,  # 1 year lookback for liquidity/quality stats\n",
    "    \"quality_min_periods\": 126,  # min period that ticker has to meet quality thresholds\n",
    "    # QUALITY THRESHOLDS (The \"Rules\")\n",
    "    \"thresholds\": {\n",
    "        # HARD LIQUIDITY FLOOR\n",
    "        # Logic: Calculates (Adj Close * Volume) daily, then takes the ROLLING MEDIAN\n",
    "        # over the quality_window (252 days). Filters out stocks where the\n",
    "        # typical daily dollar turnover is below this absolute value.\n",
    "        \"min_median_dollar_volume\": 1_000_000,\n",
    "        # DYNAMIC LIQUIDITY CUTOFF (Relative to Universe)\n",
    "        # Logic: On the decision date, the engine calculates the X-quantile\n",
    "        # of 'RollMedDollarVol' across ALL available stocks.\n",
    "        # Setting this to 0.40 calculates the 60th percentile and requires\n",
    "        # stocks to be above itâ€”effectively keeping only the TOP 60% of the market.\n",
    "        \"min_liquidity_percentile\": 0.40,\n",
    "        # PRICE/VOLUME STALENESS\n",
    "        # Logic: Creates a binary flag (1 if Volume is 0 OR High equals Low).\n",
    "        # It then calculates the ROLLING MEAN of this flag.\n",
    "        # A value of 0.05 means the stock is rejected if it was \"stale\"\n",
    "        # for more than 5% of the trading days in the rolling window.\n",
    "        \"max_stale_pct\": 0.05,\n",
    "        # DATA INTEGRITY (FROZEN VOLUME)\n",
    "        # Logic: Checks if Volume is identical to the previous day (Volume.diff() == 0).\n",
    "        # It calculates the ROLLING SUM of these occurrences over the window.\n",
    "        # If the exact same volume is reported more than 10 times, the stock\n",
    "        # is rejected as having \"frozen\" or low-quality data.\n",
    "        \"max_same_vol_count\": 10,\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION A: CORE KERNELS & QUANT UTILITIES (THE SAFE ROOM)\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "class QuantUtils:\n",
    "    \"\"\"\n",
    "    MATHEMATICAL KERNEL REGISTRY: THE SINGLE SOURCE OF TRUTH.\n",
    "    Handles both pd.Series (Report) and pd.DataFrame (Ranking) robustly.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_returns(\n",
    "        data: Union[pd.Series, pd.DataFrame],\n",
    "    ) -> Union[pd.Series, pd.DataFrame]:\n",
    "        return data.pct_change().replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_gain(data: Union[pd.Series, pd.DataFrame]) -> Union[float, pd.Series]:\n",
    "        if data.empty:\n",
    "            return 0.0\n",
    "        res = (data.ffill().iloc[-1] / data.bfill().iloc[0]) - 1\n",
    "\n",
    "        if isinstance(res, (pd.Series, pd.DataFrame)):\n",
    "            return res.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "        return float(res) if np.isfinite(res) else 0.0\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_sharpe(\n",
    "        data: Union[pd.Series, pd.DataFrame],\n",
    "        periods: int = None,  # Default to None to trigger global lookup\n",
    "    ) -> Union[float, pd.Series]:\n",
    "        if periods is None:\n",
    "            periods = GLOBAL_SETTINGS[\"annual_period\"]\n",
    "        mu, std = data.mean(), data.std()\n",
    "        # Use np.maximum for universal floor (works on scalars and Series)\n",
    "        res = (mu / np.maximum(std, 1e-8)) * np.sqrt(periods)\n",
    "\n",
    "        if isinstance(res, (pd.Series, pd.DataFrame)):\n",
    "            return res.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "        return float(res) if np.isfinite(res) else 0.0\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_sharpe_vol(\n",
    "        returns: Union[pd.Series, pd.DataFrame],\n",
    "        vol_data: Union[pd.Series, pd.DataFrame],\n",
    "    ) -> Union[float, pd.Series]:\n",
    "        \"\"\"\n",
    "        Aligned Reward / Risk.\n",
    "        Filters out volatility observations where no return exists (e.g. Day 1 NaN).\n",
    "        \"\"\"\n",
    "        # 1. Identify valid timestamps (Pandas .mean() skips NaNs in returns)\n",
    "        # but we must manually force the volatility denominator to skip those same rows.\n",
    "        mask = returns.notna()\n",
    "        avg_ret = returns.mean()\n",
    "\n",
    "        # 2. Handle Logic Branches\n",
    "        if isinstance(returns, pd.DataFrame) and isinstance(vol_data, pd.Series):\n",
    "            # RANKING MODE: vol_data is usually a pre-calculated snapshot Series\n",
    "            avg_vol = vol_data\n",
    "        else:\n",
    "            # REPORT MODE (Series) or Cross-Sectional DataFrame\n",
    "            # Filter vol_data to only include rows where returns exist\n",
    "            avg_vol = vol_data.where(mask).mean()\n",
    "\n",
    "        # 3. Final Division\n",
    "        res = avg_ret / np.maximum(avg_vol, 1e-8)\n",
    "\n",
    "        if isinstance(res, (pd.Series, pd.DataFrame)):\n",
    "            return res.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "        return float(res) if np.isfinite(res) else 0.0\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_portfolio_stats(\n",
    "        prices: pd.DataFrame,\n",
    "        atrp_matrix: pd.DataFrame,\n",
    "        trp_matrix: pd.DataFrame,\n",
    "        weights: pd.Series,\n",
    "    ) -> Tuple[pd.Series, pd.Series, pd.Series, pd.Series]:\n",
    "        \"\"\"\n",
    "        MATRIX KERNEL: Calculates equity curve and weighted volatility.\n",
    "        \"\"\"\n",
    "        # 1. Equity Curve Logic (Price-Weighted Drift)\n",
    "        norm_prices = prices.div(prices.bfill().iloc[0])\n",
    "        weighted_components = norm_prices.mul(weights, axis=1)\n",
    "        equity_curve = weighted_components.sum(axis=1)\n",
    "\n",
    "        # MANDATORY: Use internal compute_returns to preserve boundary NaN\n",
    "        returns_WITH_BOUNDARY_NAN = QuantUtils.compute_returns(equity_curve)\n",
    "\n",
    "        # 2. Portfolio Volatility Logic (Weighted Average)\n",
    "        # We calculate current_weights (rebalanced daily by price drift)\n",
    "        current_weights = weighted_components.div(equity_curve, axis=0)\n",
    "\n",
    "        # Weighted average of ATRP and TRP\n",
    "        portfolio_atrp = (current_weights * atrp_matrix).sum(axis=1, min_count=1)\n",
    "        portfolio_trp = (current_weights * trp_matrix).sum(axis=1, min_count=1)\n",
    "\n",
    "        return equity_curve, returns_WITH_BOUNDARY_NAN, portfolio_atrp, portfolio_trp\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION B: STRATEGY HELPERS & FEATURES\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "def generate_features(\n",
    "    df_ohlcv: pd.DataFrame,\n",
    "    df_indices: pd.DataFrame = None,\n",
    "    benchmark_ticker: str = GLOBAL_SETTINGS[\"benchmark_ticker\"],\n",
    "    atr_period: int = GLOBAL_SETTINGS[\"atr_period\"],\n",
    "    rsi_period: int = GLOBAL_SETTINGS[\"rsi_period\"],\n",
    "    win_5d: int = GLOBAL_SETTINGS[\"5d_window\"],\n",
    "    win_21d: int = GLOBAL_SETTINGS[\"21d_window\"],\n",
    "    win_63d: int = GLOBAL_SETTINGS[\"63d_window\"],\n",
    "    feature_zscore_clip: float = GLOBAL_SETTINGS[\"feature_zscore_clip\"],\n",
    "    quality_window: int = GLOBAL_SETTINGS[\"quality_window\"],\n",
    "    quality_min_periods: int = GLOBAL_SETTINGS[\"quality_min_periods\"],\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "\n",
    "    print(f\"âš¡ Generating Decoupled Features (Benchmark: {benchmark_ticker})...\")\n",
    "\n",
    "    # --- 0. PREP ---\n",
    "    df_ohlcv = df_ohlcv.sort_index(level=[\"Ticker\", \"Date\"])\n",
    "    all_dates = df_ohlcv.index.get_level_values(\"Date\").unique().sort_values()\n",
    "\n",
    "    # --- 1. MACRO ENGINE ---\n",
    "    macro_df = pd.DataFrame(index=all_dates)\n",
    "    if benchmark_ticker in df_ohlcv.index.get_level_values(\"Ticker\"):\n",
    "        mkt_close = (\n",
    "            df_ohlcv.xs(benchmark_ticker, level=\"Ticker\")[\"Adj Close\"]\n",
    "            .reindex(all_dates)\n",
    "            .ffill()\n",
    "        )\n",
    "        macro_df[\"Mkt_Ret\"] = mkt_close.pct_change().fillna(0.0)\n",
    "        macro_df[\"Macro_Trend\"] = (mkt_close / mkt_close.rolling(200).mean()) - 1.0\n",
    "    else:\n",
    "        macro_df[\"Mkt_Ret\"] = 0.0\n",
    "        macro_df[\"Macro_Trend\"] = 0.0\n",
    "\n",
    "    # --- TREND VELOCITY & MOMENTUM ---\n",
    "    macro_df[\"Macro_Trend_Vel\"] = macro_df[\"Macro_Trend\"].diff(win_21d)\n",
    "    macro_df[\"Macro_Trend_Vel_Z\"] = (\n",
    "        macro_df[\"Macro_Trend_Vel\"] / macro_df[\"Macro_Trend\"].rolling(win_63d).std()\n",
    "    ).clip(-feature_zscore_clip, feature_zscore_clip)\n",
    "    macro_df[\"Macro_Trend_Mom\"] = (\n",
    "        np.sign(macro_df[\"Macro_Trend\"])\n",
    "        * np.sign(macro_df[\"Macro_Trend_Vel\"])\n",
    "        * np.abs(macro_df[\"Macro_Trend_Vel\"])\n",
    "    ).fillna(0)\n",
    "\n",
    "    # VIX Extraction (Same as before)\n",
    "    macro_df[\"Macro_Vix_Z\"] = 0.0\n",
    "    macro_df[\"Macro_Vix_Ratio\"] = 1.0\n",
    "    if df_indices is not None:\n",
    "        idx_names = df_indices.index.get_level_values(0).unique()\n",
    "        if \"^VIX\" in idx_names:\n",
    "            v = df_indices.xs(\"^VIX\", level=0)[\"Adj Close\"].reindex(all_dates).ffill()\n",
    "            macro_df[\"Macro_Vix_Z\"] = (\n",
    "                (v - v.rolling(63).mean()) / v.rolling(63).std()\n",
    "            ).clip(-feature_zscore_clip, feature_zscore_clip)\n",
    "        if \"^VIX\" in idx_names and \"^VIX3M\" in idx_names:\n",
    "            v3 = (\n",
    "                df_indices.xs(\"^VIX3M\", level=0)[\"Adj Close\"].reindex(all_dates).ffill()\n",
    "            )\n",
    "            macro_df[\"Macro_Vix_Ratio\"] = (v / v3).fillna(1.0)\n",
    "    macro_df.fillna(0.0, inplace=True)\n",
    "\n",
    "    # --- 2. TICKER ENGINE ---\n",
    "    grouped = df_ohlcv.groupby(level=\"Ticker\")\n",
    "    rets = grouped[\"Adj Close\"].pct_change()\n",
    "    mkt_ret_series = macro_df[\"Mkt_Ret\"]  # The \"Master\" market vector\n",
    "\n",
    "    # A. Hybrid Metrics (Beta & IR)\n",
    "    # 1. IR_63 (Passed previously, kept same logic)\n",
    "    active_ret = rets.sub(mkt_ret_series, axis=0, level=\"Date\")\n",
    "    roll_active = active_ret.groupby(level=\"Ticker\").rolling(win_63d)\n",
    "    ir_63 = (\n",
    "        (roll_active.mean() / roll_active.std())\n",
    "        .reset_index(level=0, drop=True)\n",
    "        .fillna(0)\n",
    "    )\n",
    "\n",
    "    # 2. Beta_63 (Optimized: Pre-compute market variance, audit-exact calculation)\n",
    "    mkt_var = mkt_ret_series.rolling(win_63d).var()\n",
    "\n",
    "    def calc_rolling_beta(ticker_rets):\n",
    "        dates = ticker_rets.index.get_level_values(\"Date\")\n",
    "        m = mkt_ret_series.reindex(dates)\n",
    "        return ticker_rets.rolling(win_63d).cov(m) / mkt_var.reindex(dates)\n",
    "\n",
    "    beta_63 = (\n",
    "        rets.groupby(level=\"Ticker\", group_keys=False)\n",
    "        .apply(calc_rolling_beta)\n",
    "        .fillna(1.0)\n",
    "    )\n",
    "\n",
    "    # B. Volatility (ATR / TRP) - Optimized\n",
    "    prev_close = grouped[\"Adj Close\"].shift(1)\n",
    "\n",
    "    # Vectorized True Range without pd.concat memory overhead\n",
    "    high_low = df_ohlcv[\"Adj High\"] - df_ohlcv[\"Adj Low\"]\n",
    "    high_close = (df_ohlcv[\"Adj High\"] - prev_close).abs()\n",
    "    low_close = (df_ohlcv[\"Adj Low\"] - prev_close).abs()\n",
    "\n",
    "    # Nested np.maximum avoids creating a 3-column DataFrame\n",
    "    tr = np.maximum(np.maximum(high_low, high_close), low_close)\n",
    "\n",
    "    atr = (\n",
    "        tr.groupby(level=\"Ticker\")\n",
    "        .ewm(alpha=1 / atr_period, adjust=False)\n",
    "        .mean()\n",
    "        .reset_index(level=0, drop=True)\n",
    "    )\n",
    "    natr = (atr / df_ohlcv[\"Adj Close\"]).fillna(0)\n",
    "    trp = (tr / df_ohlcv[\"Adj Close\"]).fillna(0)\n",
    "\n",
    "    # C. Momentum & Consistency\n",
    "    mom_21 = grouped[\"Adj Close\"].pct_change(win_21d)\n",
    "    consistency = (\n",
    "        (rets > 0)\n",
    "        .astype(float)\n",
    "        .groupby(level=\"Ticker\")\n",
    "        .rolling(win_5d)\n",
    "        .mean()\n",
    "        .reset_index(level=0, drop=True)\n",
    "    )\n",
    "    dd_21 = (\n",
    "        df_ohlcv[\"Adj Close\"]\n",
    "        / grouped[\"Adj Close\"].rolling(win_21d).max().reset_index(level=0, drop=True)\n",
    "    ) - 1.0\n",
    "\n",
    "    # D. RSI (Wilder's Logic)\n",
    "    delta = grouped[\"Adj Close\"].diff()\n",
    "    up, down = delta.clip(lower=0), -1 * delta.clip(upper=0)\n",
    "    ma_up = (\n",
    "        up.groupby(level=\"Ticker\")\n",
    "        .ewm(alpha=1 / rsi_period, adjust=False)\n",
    "        .mean()\n",
    "        .reset_index(level=0, drop=True)\n",
    "    )\n",
    "    ma_down = (\n",
    "        down.groupby(level=\"Ticker\")\n",
    "        .ewm(alpha=1 / rsi_period, adjust=False)\n",
    "        .mean()\n",
    "        .reset_index(level=0, drop=True)\n",
    "    )\n",
    "    # FIX: Allow division by zero (i.e. no down day) to create inf (correct RSI=100),\n",
    "    # infâ†’100, -infâ†’0, NaNâ†’50\n",
    "    # then clean up remaining NaNs (initial periods/no movement)\n",
    "    # - Initial periods: Before the 14-day lookback is filled, the EWM mean is undefined â†’ NaN.\n",
    "    # - Flat prices: If price doesn't move (Avg Up = 0 and Avg Down = 0), RS is 0/0 â†’ NaN.\n",
    "    # - By convention, RSI is set to 50 (neutral) when there is no directional momentum.\n",
    "    rs = ma_up / ma_down  # Keep zero denominator â†’ inf\n",
    "    raw_rsi = 100 - (100 / (1 + rs))\n",
    "    rsi = raw_rsi.replace({np.inf: 100, -np.inf: 0}).fillna(50)\n",
    "\n",
    "    # E. Assemble Features\n",
    "    features_df = pd.DataFrame(\n",
    "        {\n",
    "            \"ATR\": atr,\n",
    "            \"ATRP\": natr,\n",
    "            \"TRP\": trp,\n",
    "            \"RSI\": rsi,\n",
    "            \"Mom_21\": mom_21,\n",
    "            \"Consistency\": consistency,\n",
    "            \"IR_63\": ir_63,\n",
    "            \"Beta_63\": beta_63,\n",
    "            \"DD_21\": dd_21.fillna(0),\n",
    "            \"Ret_1d\": rets,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # F. Quality (Universe Filtering) - Optimized\n",
    "    quality_temp = pd.DataFrame(\n",
    "        {\n",
    "            \"IsStale\": np.where(\n",
    "                (df_ohlcv[\"Volume\"] == 0)\n",
    "                | (df_ohlcv[\"Adj High\"] == df_ohlcv[\"Adj Low\"]),\n",
    "                1,\n",
    "                0,\n",
    "            ),\n",
    "            \"DollarVolume\": df_ohlcv[\"Adj Close\"] * df_ohlcv[\"Volume\"],\n",
    "            \"HasSameVolume\": (grouped[\"Volume\"].diff() == 0).astype(int),\n",
    "        },\n",
    "        index=df_ohlcv.index,\n",
    "    )\n",
    "\n",
    "    # Calculate rolling stats separately (avoid slow dict agg) and use .values to bypass index alignment overhead\n",
    "    grp = quality_temp.groupby(level=\"Ticker\")\n",
    "    rolling_quality = pd.DataFrame(\n",
    "        {\n",
    "            \"RollingStalePct\": grp[\"IsStale\"]\n",
    "            .rolling(window=quality_window, min_periods=quality_min_periods)\n",
    "            .mean()\n",
    "            .values,\n",
    "            \"RollMedDollarVol\": grp[\"DollarVolume\"]\n",
    "            .rolling(window=quality_window, min_periods=quality_min_periods)\n",
    "            .median()\n",
    "            .values,\n",
    "            \"RollingSameVolCount\": grp[\"HasSameVolume\"]\n",
    "            .rolling(window=quality_window, min_periods=quality_min_periods)\n",
    "            .sum()\n",
    "            .values,\n",
    "        },\n",
    "        index=quality_temp.index,\n",
    "    )\n",
    "\n",
    "    return pd.concat([features_df, rolling_quality], axis=1).sort_index(), macro_df\n",
    "\n",
    "\n",
    "def _prepare_initial_weights(tickers: List[str]) -> pd.Series:\n",
    "    \"\"\"\n",
    "    METADATA: Converts a list of tickers into a weight map.\n",
    "    Example: ['AAPL', 'AAPL', 'TSLA'] -> {'AAPL': 0.66, 'TSLA': 0.33}\n",
    "    \"\"\"\n",
    "    ticker_counts = Counter(tickers)\n",
    "    total = len(tickers)\n",
    "    return pd.Series({t: c / total for t, c in ticker_counts.items()})\n",
    "\n",
    "\n",
    "def calculate_buy_and_hold_performance(\n",
    "    df_close_wide: pd.DataFrame,  # Use the WIDE version\n",
    "    df_atrp_wide: pd.DataFrame,  # Use the WIDE version\n",
    "    df_trp_wide: pd.DataFrame,  # <--- Added\n",
    "    tickers: List[str],\n",
    "    start_date: pd.Timestamp,\n",
    "    end_date: pd.Timestamp,\n",
    "):\n",
    "    if not tickers:\n",
    "        return pd.Series(), pd.Series(), pd.Series()\n",
    "\n",
    "    initial_weights = _prepare_initial_weights(tickers)\n",
    "\n",
    "    # SLICE (Fix Part B)\n",
    "    ticker_list = initial_weights.index.tolist()\n",
    "    p_slice = df_close_wide.reindex(columns=ticker_list).loc[start_date:end_date]\n",
    "    a_slice = df_atrp_wide.reindex(columns=ticker_list).loc[start_date:end_date]\n",
    "    t_slice = df_trp_wide.reindex(columns=ticker_list).loc[start_date:end_date]\n",
    "    # KERNEL - Pure Math\n",
    "    return QuantUtils.compute_portfolio_stats(\n",
    "        p_slice, a_slice, t_slice, initial_weights\n",
    "    )\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION C: METRIC REGISTRY\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "class MarketObservation(TypedDict):\n",
    "    \"\"\"\n",
    "    The 'STATE' (Observation) in Reinforcement Learning.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- The Movie (Time Series) ---\n",
    "    lookback_returns: pd.DataFrame\n",
    "    lookback_close: pd.DataFrame\n",
    "\n",
    "    # --- The Snapshot (Scalar values at Decision Time) ---\n",
    "    atrp: pd.Series\n",
    "    trp: pd.Series  # <--- RESTORED\n",
    "\n",
    "    # NEW SENSORS\n",
    "    atr: pd.Series  # <--- RESTORED (Optional, but good for debug)\n",
    "    rsi: pd.Series\n",
    "    consistency: pd.Series\n",
    "    mom_21: pd.Series\n",
    "    ir_63: pd.Series\n",
    "    beta_63: pd.Series\n",
    "    dd_21: pd.Series\n",
    "\n",
    "    # MACRO CONTEXT\n",
    "    macro_trend: pd.Series\n",
    "    macro_trend_vel: pd.Series  # <-- ADD (Raw 21d slope)\n",
    "    macro_trend_vel_z: pd.Series  # <-- ADD (Z-score normalized)\n",
    "    macro_vix_z: pd.Series\n",
    "    macro_vix_ratio: pd.Series\n",
    "\n",
    "\n",
    "METRIC_REGISTRY = {\n",
    "    # --- 1. LEGACY / SANITY CHECKS ---\n",
    "    \"Price Gain\": lambda obs: QuantUtils.calculate_gain(obs[\"lookback_close\"]),\n",
    "    \"Sharpe\": lambda obs: QuantUtils.calculate_sharpe(obs[\"lookback_returns\"]),\n",
    "    \"Sharpe (ATRP)\": lambda obs: QuantUtils.calculate_sharpe_vol(\n",
    "        obs[\"lookback_returns\"], obs[\"atrp\"]\n",
    "    ),\n",
    "    \"Sharpe (TRP)\": lambda obs: QuantUtils.calculate_sharpe_vol(\n",
    "        obs[\"lookback_returns\"], obs[\"trp\"]\n",
    "    ),\n",
    "    # --- 2. NEW QUANT METRICS ---\n",
    "    \"Momentum (21d)\": lambda obs: obs[\"mom_21\"],\n",
    "    \"Info Ratio (Stdev_Alpha, 63d)\": lambda obs: obs[\"ir_63\"],\n",
    "    \"Consistency (WinRate 5d)\": lambda obs: obs[\"consistency\"],\n",
    "    \"Oversold (-RSI)\": lambda obs: -obs[\"rsi\"],\n",
    "    \"Dip Buyer (Drawdown -dd_21)\": lambda obs: -obs[\"dd_21\"],\n",
    "    \"Low Volatility (-ATRP)\": lambda obs: -obs[\"atrp\"],\n",
    "}\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION D: DATA CONTRACTS\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EngineInput:\n",
    "    mode: str\n",
    "    start_date: pd.Timestamp\n",
    "    lookback_period: int\n",
    "    holding_period: int\n",
    "    metric: str\n",
    "    benchmark_ticker: str\n",
    "    rank_start: int = 1\n",
    "    rank_end: int = 10\n",
    "    # Default factory pulls from Global thresholds\n",
    "    quality_thresholds: Dict[str, float] = field(\n",
    "        default_factory=lambda: GLOBAL_SETTINGS[\"thresholds\"].copy()\n",
    "    )\n",
    "    manual_tickers: List[str] = field(default_factory=list)\n",
    "    debug: bool = False\n",
    "    universe_subset: Optional[List[str]] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EngineOutput:\n",
    "    # 1. CORE DATA (Required - No Defaults)\n",
    "    portfolio_series: pd.Series\n",
    "    benchmark_series: pd.Series\n",
    "    normalized_plot_data: pd.DataFrame\n",
    "    tickers: List[str]\n",
    "    initial_weights: pd.Series\n",
    "    perf_metrics: Dict[str, float]\n",
    "    results_df: pd.DataFrame\n",
    "\n",
    "    # 2. TIMELINE (Required - No Defaults)\n",
    "    start_date: pd.Timestamp\n",
    "    decision_date: pd.Timestamp\n",
    "    buy_date: pd.Timestamp\n",
    "    holding_end_date: pd.Timestamp\n",
    "\n",
    "    # 3. OPTIONAL / AUDIT DATA (Must be at the bottom because they have defaults)\n",
    "    portfolio_atrp_series: Optional[pd.Series] = None\n",
    "    benchmark_atrp_series: Optional[pd.Series] = None\n",
    "    portfolio_trp_series: Optional[pd.Series] = None\n",
    "    benchmark_trp_series: Optional[pd.Series] = None\n",
    "    error_msg: Optional[str] = None\n",
    "    debug_data: Optional[Dict[str, Any]] = None\n",
    "    macro_df: Optional[pd.DataFrame] = None  # <-- ADD THIS\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class FilterPack:\n",
    "    \"\"\"The 'Saved List' and state for the second filter pass.\"\"\"\n",
    "\n",
    "    decision_date: Optional[pd.Timestamp] = None\n",
    "    eligible_pool: List[str] = field(default_factory=list)  # Survivors of Stage 1\n",
    "    selected_tickers: List[str] = field(default_factory=list)  # Final output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"FilterPack(Date: {self.decision_date}, Eligible: {len(self.eligible_pool)}, Selected: {len(self.selected_tickers)})\"\n",
    "\n",
    "\n",
    "class AlphaEngine:\n",
    "    def __init__(\n",
    "        self,\n",
    "        df_ohlcv: pd.DataFrame,\n",
    "        features_df: pd.DataFrame,\n",
    "        macro_df: pd.DataFrame,\n",
    "        df_close_wide: pd.DataFrame = None,\n",
    "        df_atrp_wide: pd.DataFrame = None,\n",
    "        df_trp_wide: pd.DataFrame = None,\n",
    "        master_ticker: str = GLOBAL_SETTINGS[\"calendar_ticker\"],\n",
    "    ):\n",
    "        self.df_ohlcv_raw = df_ohlcv\n",
    "        self.features_df = features_df\n",
    "        self.macro_df = macro_df\n",
    "        self.df_close = (\n",
    "            df_close_wide\n",
    "            if df_close_wide is not None\n",
    "            else df_ohlcv[\"Adj Close\"].unstack(level=0)\n",
    "        )\n",
    "        self.df_atrp = (\n",
    "            df_atrp_wide\n",
    "            if df_atrp_wide is not None\n",
    "            else self.features_df[\"ATRP\"].unstack(level=0)\n",
    "        )\n",
    "        self.df_trp = (\n",
    "            df_trp_wide\n",
    "            if df_trp_wide is not None\n",
    "            else self.features_df[\"TRP\"].unstack(level=0)\n",
    "        )\n",
    "\n",
    "        # Alignment & Sanitization\n",
    "        common_idx = self.df_close.index\n",
    "        common_cols = self.df_close.columns\n",
    "        self.df_atrp = self.df_atrp.reindex(index=common_idx, columns=common_cols)\n",
    "        self.df_trp = self.df_trp.reindex(index=common_idx, columns=common_cols)\n",
    "\n",
    "        if GLOBAL_SETTINGS[\"handle_zeros_as_nan\"]:\n",
    "            self.df_close = self.df_close.replace(0, np.nan)\n",
    "        self.df_close = self.df_close.ffill(limit=GLOBAL_SETTINGS[\"max_data_gap_ffill\"])\n",
    "        self.df_close = self.df_close.fillna(GLOBAL_SETTINGS[\"nan_price_replacement\"])\n",
    "\n",
    "        if master_ticker not in self.df_close.columns:\n",
    "            master_ticker = self.df_close.columns[0]\n",
    "        self.trading_calendar = (\n",
    "            self.df_close[master_ticker].dropna().index.unique().sort_values()\n",
    "        )\n",
    "\n",
    "    def _build_observation(\n",
    "        self,\n",
    "        decision_date: pd.Timestamp,\n",
    "        candidates: List[str],\n",
    "        start_date: pd.Timestamp,\n",
    "    ) -> MarketObservation:\n",
    "        \"\"\"\n",
    "        STATE BUILDER: Correctly aggregates volatility over the lookback window.\n",
    "        \"\"\"\n",
    "        # 1. SLICE WINDOW FOR VOLATILITY AGGREGATION\n",
    "        idx = pd.IndexSlice\n",
    "        feat_window = self.features_df.loc[idx[candidates, start_date:decision_date], :]\n",
    "\n",
    "        # Calculate MEAN volatility over the lookback (Legacy behavior)\n",
    "        atrp_lb = feat_window[\"ATRP\"].groupby(level=\"Ticker\").mean()\n",
    "        trp_lb = feat_window[\"TRP\"].groupby(level=\"Ticker\").mean()\n",
    "\n",
    "        # 2. SLICE CURRENT SNAPSHOT FOR MOMENTUM/RSI\n",
    "        feat_now = self.features_df.xs(decision_date, level=\"Date\").reindex(candidates)\n",
    "\n",
    "        # 3. MACRO SNAPSHOT\n",
    "        macro_snapshot = self.macro_df.loc[decision_date]\n",
    "\n",
    "        # 4. PRICE WINDOW\n",
    "        lookback_close = self.df_close.loc[start_date:decision_date, candidates]\n",
    "\n",
    "        return {\n",
    "            \"lookback_close\": lookback_close,\n",
    "            \"lookback_returns\": lookback_close.ffill().pct_change(),\n",
    "            # --- VOLATILITY (Lookback Averages) ---\n",
    "            \"atrp\": atrp_lb,\n",
    "            \"trp\": trp_lb,\n",
    "            \"atr\": feat_now.get(\"ATR\"),\n",
    "            # --- QUANT METRICS (Snapshots) ---\n",
    "            \"rsi\": feat_now.get(\"RSI\"),\n",
    "            \"consistency\": feat_now.get(\"Consistency\", 0.0),\n",
    "            \"mom_21\": feat_now.get(\"Mom_21\"),\n",
    "            \"ir_63\": feat_now.get(\"IR_63\"),\n",
    "            \"beta_63\": feat_now.get(\"Beta_63\"),\n",
    "            \"dd_21\": feat_now.get(\"DD_21\"),\n",
    "            # --- MACRO CONTEXT ---\n",
    "            \"macro_trend\": macro_snapshot.get(\"Macro_Trend\"),\n",
    "            \"macro_trend_vel\": macro_snapshot.get(\"Macro_Trend_Vel\"),\n",
    "            \"macro_trend_vel_z\": macro_snapshot.get(\"Macro_Trend_Vel_Z\"),\n",
    "            \"macro_vix_z\": macro_snapshot.get(\"Macro_Vix_Z\"),\n",
    "            \"macro_vix_ratio\": macro_snapshot.get(\"Macro_Vix_Ratio\"),\n",
    "        }\n",
    "\n",
    "    def _execute_strategy(\n",
    "        self, observation: MarketObservation, metric_name: str\n",
    "    ) -> pd.Series:\n",
    "        return METRIC_REGISTRY[metric_name](observation)\n",
    "\n",
    "    def _rank_and_slice(self, raw_scores, inputs, observation):\n",
    "        sorted_tickers = raw_scores.sort_values(ascending=False)\n",
    "        selected_tickers = sorted_tickers.iloc[\n",
    "            max(0, inputs.rank_start - 1) : inputs.rank_end\n",
    "        ].index.tolist()\n",
    "\n",
    "        debug_artifact = pd.DataFrame(\n",
    "            {\n",
    "                \"Strategy_Score\": raw_scores,\n",
    "                \"Lookback_Return_Ann\": observation[\"lookback_returns\"].mean() * 252,\n",
    "                \"Lookback_ATRP\": observation[\"atrp\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "        results_table = pd.DataFrame(\n",
    "            {\n",
    "                \"Rank\": range(\n",
    "                    inputs.rank_start, inputs.rank_start + len(selected_tickers)\n",
    "                ),\n",
    "                \"Ticker\": selected_tickers,\n",
    "                \"Strategy Value\": sorted_tickers.loc[selected_tickers].values,\n",
    "            }\n",
    "        ).set_index(\"Ticker\")\n",
    "\n",
    "        return (\n",
    "            selected_tickers,\n",
    "            results_table,\n",
    "            {\"full_universe_ranking\": debug_artifact},\n",
    "        )\n",
    "\n",
    "    def _select_tickers(self, inputs, start_date, decision_date):\n",
    "        debug_dict = {}\n",
    "        if inputs.mode == \"Manual List\":\n",
    "            valid = [t for t in inputs.manual_tickers if t in self.df_close.columns]\n",
    "            return valid, pd.DataFrame(index=valid), {}, None\n",
    "\n",
    "        audit_info = {}\n",
    "        if inputs.universe_subset is not None:\n",
    "            candidates = [\n",
    "                t for t in inputs.universe_subset if t in self.df_close.columns\n",
    "            ]\n",
    "            if inputs.debug:\n",
    "                debug_dict[\"audit_liquidity\"] = {\n",
    "                    \"mode\": \"Cascade\",\n",
    "                    \"tickers_passed\": len(candidates),\n",
    "                    \"forced_list\": True,\n",
    "                }\n",
    "        else:\n",
    "            candidates = self._filter_universe(\n",
    "                decision_date, inputs.quality_thresholds, audit_info\n",
    "            )\n",
    "            if inputs.debug:\n",
    "                debug_dict[\"audit_liquidity\"] = audit_info\n",
    "\n",
    "        if not candidates:\n",
    "            return [], pd.DataFrame(), debug_dict, \"No survivors.\"\n",
    "\n",
    "        obs = self._build_observation(decision_date, candidates, start_date)\n",
    "        scores = self._execute_strategy(obs, inputs.metric)\n",
    "        sel, table, r_debug = self._rank_and_slice(scores, inputs, obs)\n",
    "        debug_dict.update(r_debug)\n",
    "        return sel, table, debug_dict, None\n",
    "\n",
    "    def run(self, inputs: EngineInput) -> EngineOutput:\n",
    "        dates, error = self._validate_timeline(inputs)\n",
    "        if error:\n",
    "            return self._error_result(error)\n",
    "        (safe_start, safe_decision, safe_buy, safe_end) = dates\n",
    "\n",
    "        tickers_to_trade, results_table, debug_dict, error = self._select_tickers(\n",
    "            inputs, safe_start, safe_decision\n",
    "        )\n",
    "        if error:\n",
    "            return self._error_result(error)\n",
    "\n",
    "        # Performance Calculations\n",
    "        p_res = calculate_buy_and_hold_performance(\n",
    "            self.df_close,\n",
    "            self.df_atrp,\n",
    "            self.df_trp,\n",
    "            tickers_to_trade,\n",
    "            safe_start,\n",
    "            safe_end,\n",
    "        )\n",
    "        p_h_res = calculate_buy_and_hold_performance(\n",
    "            self.df_close,\n",
    "            self.df_atrp,\n",
    "            self.df_trp,\n",
    "            tickers_to_trade,\n",
    "            safe_buy,\n",
    "            safe_end,\n",
    "        )\n",
    "        p_metrics, p_slices = self._calculate_period_metrics(\n",
    "            *p_res, safe_decision, *p_h_res, prefix=\"p\"\n",
    "        )\n",
    "\n",
    "        b_res = calculate_buy_and_hold_performance(\n",
    "            self.df_close,\n",
    "            self.df_atrp,\n",
    "            self.df_trp,\n",
    "            [inputs.benchmark_ticker],\n",
    "            safe_start,\n",
    "            safe_end,\n",
    "        )\n",
    "        b_h_res = calculate_buy_and_hold_performance(\n",
    "            self.df_close,\n",
    "            self.df_atrp,\n",
    "            self.df_trp,\n",
    "            [inputs.benchmark_ticker],\n",
    "            safe_buy,\n",
    "            safe_end,\n",
    "        )\n",
    "        b_metrics, b_slices = self._calculate_period_metrics(\n",
    "            *b_res, safe_decision, *b_h_res, prefix=\"b\"\n",
    "        )\n",
    "\n",
    "        if inputs.debug:\n",
    "            idx_slice = pd.IndexSlice\n",
    "            debug_dict[\"inputs_snapshot\"] = inputs\n",
    "            debug_dict[\"verification\"] = {\"portfolio\": p_slices, \"benchmark\": b_slices}\n",
    "            debug_dict[\"portfolio_raw_components\"] = {\n",
    "                \"prices\": self.df_close[tickers_to_trade].loc[safe_start:safe_end],\n",
    "                \"atrp\": self.df_atrp[tickers_to_trade].loc[safe_start:safe_end],\n",
    "                \"trp\": self.df_trp[tickers_to_trade].loc[safe_start:safe_end],\n",
    "                \"ohlcv_raw\": self.df_ohlcv_raw.loc[\n",
    "                    idx_slice[tickers_to_trade, safe_start:safe_end], :\n",
    "                ],\n",
    "            }\n",
    "            debug_dict[\"benchmark_raw_components\"] = {\n",
    "                \"prices\": self.df_close[[inputs.benchmark_ticker]].loc[\n",
    "                    safe_start:safe_end\n",
    "                ],\n",
    "                \"atrp\": self.df_atrp[[inputs.benchmark_ticker]].loc[\n",
    "                    safe_start:safe_end\n",
    "                ],\n",
    "                \"trp\": self.df_trp[[inputs.benchmark_ticker]].loc[safe_start:safe_end],\n",
    "                \"ohlcv_raw\": self.df_ohlcv_raw.loc[\n",
    "                    idx_slice[[inputs.benchmark_ticker], safe_start:safe_end], :\n",
    "                ],\n",
    "            }\n",
    "            debug_dict[\"selection_audit\"] = debug_dict.get(\"full_universe_ranking\")\n",
    "\n",
    "        return EngineOutput(\n",
    "            portfolio_series=p_res[0],\n",
    "            benchmark_series=b_res[0],\n",
    "            portfolio_atrp_series=p_res[2],\n",
    "            benchmark_atrp_series=b_res[2],\n",
    "            portfolio_trp_series=p_res[3],\n",
    "            benchmark_trp_series=b_res[3],\n",
    "            normalized_plot_data=(\n",
    "                self._get_normalized_plot_data(tickers_to_trade, safe_start, safe_end)\n",
    "                if inputs.debug\n",
    "                else pd.DataFrame()\n",
    "            ),\n",
    "            tickers=tickers_to_trade,\n",
    "            initial_weights=_prepare_initial_weights(tickers_to_trade),\n",
    "            perf_metrics={**p_metrics, **b_metrics},\n",
    "            results_df=results_table,\n",
    "            start_date=safe_start,\n",
    "            decision_date=safe_decision,\n",
    "            buy_date=safe_buy,\n",
    "            holding_end_date=safe_end,\n",
    "            debug_data=debug_dict,\n",
    "            macro_df=self.macro_df,  # <-- ADD THIS LINE\n",
    "        )\n",
    "\n",
    "    def _calculate_period_metrics(\n",
    "        self,\n",
    "        f_val,\n",
    "        f_ret,\n",
    "        f_atrp,\n",
    "        f_trp,\n",
    "        decision_date,\n",
    "        h_val,\n",
    "        h_ret,\n",
    "        h_atrp,\n",
    "        h_trp,\n",
    "        prefix,\n",
    "    ):\n",
    "        m, s = {}, {}\n",
    "        lb_val, lb_ret, lb_atrp, lb_trp = (\n",
    "            f_val.loc[:decision_date],\n",
    "            f_ret.loc[:decision_date],\n",
    "            f_atrp.loc[:decision_date],\n",
    "            f_trp.loc[:decision_date],\n",
    "        )\n",
    "\n",
    "        # LEGACY KEYS\n",
    "        m[f\"full_{prefix}_gain\"] = QuantUtils.calculate_gain(f_val)\n",
    "        m[f\"full_{prefix}_sharpe\"] = QuantUtils.calculate_sharpe(f_ret)\n",
    "        m[f\"full_{prefix}_sharpe_atrp\"] = QuantUtils.calculate_sharpe_vol(f_ret, f_atrp)\n",
    "        m[f\"full_{prefix}_sharpe_trp\"] = QuantUtils.calculate_sharpe_vol(f_ret, f_trp)\n",
    "        m[f\"lookback_{prefix}_gain\"] = QuantUtils.calculate_gain(lb_val)\n",
    "        m[f\"lookback_{prefix}_sharpe\"] = QuantUtils.calculate_sharpe(lb_ret)\n",
    "        m[f\"lookback_{prefix}_sharpe_atrp\"] = QuantUtils.calculate_sharpe_vol(\n",
    "            lb_ret, lb_atrp\n",
    "        )\n",
    "        m[f\"lookback_{prefix}_sharpe_trp\"] = QuantUtils.calculate_sharpe_vol(\n",
    "            lb_ret, lb_trp\n",
    "        )\n",
    "        m[f\"holding_{prefix}_gain\"] = QuantUtils.calculate_gain(h_val)\n",
    "        m[f\"holding_{prefix}_sharpe\"] = QuantUtils.calculate_sharpe(h_ret)\n",
    "        m[f\"holding_{prefix}_sharpe_atrp\"] = QuantUtils.calculate_sharpe_vol(\n",
    "            h_ret, h_atrp\n",
    "        )\n",
    "        m[f\"holding_{prefix}_sharpe_trp\"] = QuantUtils.calculate_sharpe_vol(\n",
    "            h_ret, h_trp\n",
    "        )\n",
    "\n",
    "        # SLICES FOR AUDIT MAP\n",
    "        s[\"full_val\"], s[\"full_ret\"], s[\"full_atrp\"], s[\"full_trp\"] = (\n",
    "            f_val,\n",
    "            f_ret,\n",
    "            f_atrp,\n",
    "            f_trp,\n",
    "        )\n",
    "        s[\"lookback_val\"], s[\"lookback_ret\"], s[\"lookback_atrp\"], s[\"lookback_trp\"] = (\n",
    "            lb_val,\n",
    "            lb_ret,\n",
    "            lb_atrp,\n",
    "            lb_trp,\n",
    "        )\n",
    "        s[\"holding_val\"], s[\"holding_ret\"], s[\"holding_atrp\"], s[\"holding_trp\"] = (\n",
    "            h_val,\n",
    "            h_ret,\n",
    "            h_atrp,\n",
    "            h_trp,\n",
    "        )\n",
    "        return m, s\n",
    "\n",
    "    def _filter_universe(self, date_ts, thresholds, audit_container=None):\n",
    "        avail_dates = (\n",
    "            self.features_df.index.get_level_values(\"Date\").unique().sort_values()\n",
    "        )\n",
    "        target_date = avail_dates[avail_dates <= date_ts][-1]\n",
    "        day_features = self.features_df.xs(target_date, level=\"Date\")\n",
    "        vol_cutoff = thresholds.get(\"min_median_dollar_volume\", 0)\n",
    "        if \"min_liquidity_percentile\" in thresholds:\n",
    "            vol_cutoff = max(\n",
    "                vol_cutoff,\n",
    "                day_features[\"RollMedDollarVol\"].quantile(\n",
    "                    thresholds[\"min_liquidity_percentile\"]\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        mask = (\n",
    "            (day_features[\"RollMedDollarVol\"] >= vol_cutoff)\n",
    "            & (day_features[\"RollingStalePct\"] <= thresholds[\"max_stale_pct\"])\n",
    "            & (day_features[\"RollingSameVolCount\"] <= thresholds[\"max_same_vol_count\"])\n",
    "        )\n",
    "\n",
    "        if audit_container is not None:\n",
    "            audit_container.update(\n",
    "                {\n",
    "                    \"date\": target_date,\n",
    "                    \"total_tickers_available\": len(day_features),\n",
    "                    \"percentile_setting\": thresholds.get(\n",
    "                        \"min_liquidity_percentile\", \"N/A\"\n",
    "                    ),\n",
    "                    \"final_cutoff_usd\": vol_cutoff,\n",
    "                    \"tickers_passed\": mask.sum(),\n",
    "                    \"universe_snapshot\": day_features.assign(Passed_Final=mask),\n",
    "                }\n",
    "            )\n",
    "        return day_features[mask].index.tolist()\n",
    "\n",
    "    def _validate_timeline(self, inputs: EngineInput):\n",
    "        # [Existing logic preserved]\n",
    "        cal = self.trading_calendar\n",
    "        last_idx = len(cal) - 1\n",
    "        if len(cal) <= inputs.lookback_period:\n",
    "            return (\n",
    "                None,\n",
    "                f\"âŒ Dataset too small.\\nNeed > {inputs.lookback_period} days of history.\",\n",
    "            )\n",
    "        min_decision_date = cal[inputs.lookback_period]\n",
    "        if inputs.start_date < min_decision_date:\n",
    "            return None, (\n",
    "                f\"âŒ Not enough history for a {inputs.lookback_period}-day lookback.\\n\"\n",
    "                f\"Earliest valid Decision Date: {min_decision_date.date()}\"\n",
    "            )\n",
    "        required_future_days = 1 + inputs.holding_period\n",
    "        latest_valid_idx = last_idx - required_future_days\n",
    "        if latest_valid_idx < 0:\n",
    "            return (\n",
    "                None,\n",
    "                f\"âŒ Holding period too long.\\n{inputs.holding_period} days exceeds available data.\",\n",
    "            )\n",
    "        if inputs.start_date > cal[latest_valid_idx]:\n",
    "            latest_date = cal[latest_valid_idx].date()\n",
    "            return None, (\n",
    "                f\"âŒ Decision Date too late for a {inputs.holding_period}-day hold.\\n\"\n",
    "                f\"Latest valid date: {latest_date}. Please move picker back.\"\n",
    "            )\n",
    "        decision_idx = cal.searchsorted(inputs.start_date)\n",
    "        if decision_idx > latest_valid_idx:\n",
    "            decision_idx = latest_valid_idx\n",
    "        start_idx = decision_idx - inputs.lookback_period\n",
    "        entry_idx = decision_idx + 1\n",
    "        end_idx = entry_idx + inputs.holding_period\n",
    "        return (cal[start_idx], cal[decision_idx], cal[entry_idx], cal[end_idx]), None\n",
    "\n",
    "    def _get_normalized_plot_data(self, tickers, start, end):\n",
    "        if not tickers:\n",
    "            return pd.DataFrame()\n",
    "        data = self.df_close[list(set(tickers))].loc[start:end]\n",
    "        return data / data.bfill().iloc[0]\n",
    "\n",
    "    def _error_result(self, msg):\n",
    "        return EngineOutput(\n",
    "            portfolio_series=None,\n",
    "            benchmark_series=None,\n",
    "            normalized_plot_data=None,\n",
    "            tickers=[],\n",
    "            initial_weights=None,\n",
    "            perf_metrics={},\n",
    "            results_df=pd.DataFrame(),\n",
    "            start_date=pd.Timestamp.min,\n",
    "            decision_date=pd.Timestamp.min,\n",
    "            buy_date=pd.Timestamp.min,\n",
    "            holding_end_date=pd.Timestamp.min,\n",
    "            # Optional fields (in declaration order):\n",
    "            portfolio_atrp_series=None,\n",
    "            benchmark_atrp_series=None,\n",
    "            portfolio_trp_series=None,\n",
    "            benchmark_trp_series=None,\n",
    "            error_msg=msg,\n",
    "            debug_data=None,\n",
    "            macro_df=None,\n",
    "        )\n",
    "\n",
    "\n",
    "class WalkForwardAnalyzer:\n",
    "    def __init__(\n",
    "        self, engine, universe_subset=None, filter_pack=None, default_settings=None\n",
    "    ):\n",
    "        self.engine = engine\n",
    "        self.universe_subset = universe_subset\n",
    "        self.filter_pack = filter_pack or FilterPack()\n",
    "        self.settings = default_settings or GLOBAL_SETTINGS\n",
    "        self.last_run: Optional[EngineOutput] = None\n",
    "\n",
    "        # Sync Date with FilterPack if we are in Stage 2\n",
    "        self.initial_date = self.filter_pack.decision_date or pd.to_datetime(\n",
    "            \"2025-12-10\"\n",
    "        )\n",
    "\n",
    "        self._init_widgets()\n",
    "        self._init_figure()\n",
    "        self.output_area = widgets.Output()\n",
    "\n",
    "    def _init_widgets(self):\n",
    "        # --- 1. Timeline Inputs ---\n",
    "        self.w_lookback = widgets.IntText(\n",
    "            value=10,\n",
    "            description=\"Lookback (Days):\",\n",
    "            layout=widgets.Layout(width=\"200px\"),\n",
    "            style={\"description_width\": \"initial\"},\n",
    "        )\n",
    "        self.w_decision_date = widgets.DatePicker(\n",
    "            value=self.initial_date,\n",
    "            description=\"Decision Date:\",\n",
    "            layout=widgets.Layout(width=\"auto\"),\n",
    "            style={\"description_width\": \"initial\"},\n",
    "        )\n",
    "        self.w_holding = widgets.IntText(\n",
    "            value=5,\n",
    "            description=\"Holding (Days):\",\n",
    "            layout=widgets.Layout(width=\"200px\"),\n",
    "            style={\"description_width\": \"initial\"},\n",
    "        )\n",
    "\n",
    "        # --- 2. Strategy & Benchmark ---\n",
    "        self.w_mode = widgets.RadioButtons(\n",
    "            options=[\"Ranking\", \"Manual List\"],\n",
    "            value=\"Ranking\",\n",
    "            description=\"Mode:\",\n",
    "            layout=widgets.Layout(width=\"max-content\", margin=\"0px 20px 0px 0px\"),\n",
    "            style={\"description_width\": \"initial\"},\n",
    "        )\n",
    "\n",
    "        common_style = {\"description_width\": \"initial\"}\n",
    "\n",
    "        self.w_strategy = widgets.Dropdown(\n",
    "            options=list(METRIC_REGISTRY.keys()),\n",
    "            value=\"Sharpe (ATRP)\",\n",
    "            description=\"Strategy:\",\n",
    "            style=common_style,\n",
    "            layout=widgets.Layout(width=\"220px\"),\n",
    "        )\n",
    "\n",
    "        self.w_benchmark = widgets.Text(\n",
    "            value=self.settings[\"benchmark_ticker\"],\n",
    "            description=\"Benchmark:\",\n",
    "            placeholder=\"Enter Ticker\",\n",
    "            style=common_style,\n",
    "            layout=widgets.Layout(width=\"150px\"),\n",
    "        )\n",
    "\n",
    "        # --- 3. Ranking Controls ---\n",
    "        self.w_rank_start = widgets.IntText(\n",
    "            value=1,\n",
    "            description=\"Rank Start:\",\n",
    "            layout=widgets.Layout(width=\"150px\"),\n",
    "            style={\"description_width\": \"initial\"},\n",
    "        )\n",
    "        self.w_rank_end = widgets.IntText(\n",
    "            value=10,\n",
    "            description=\"Rank End:\",\n",
    "            layout=widgets.Layout(width=\"150px\"),\n",
    "            style={\"description_width\": \"initial\"},\n",
    "        )\n",
    "        # Grouping them here for logic, but we'll group them visually in show()\n",
    "        self.w_rank_range = widgets.HBox([self.w_rank_start, self.w_rank_end])\n",
    "\n",
    "        self.w_manual_list = widgets.Textarea(\n",
    "            placeholder=\"AAPL, TSLA...\",\n",
    "            description=\"Manual Tickers:\",\n",
    "            layout=widgets.Layout(width=\"400px\", height=\"80px\"),\n",
    "            style={\"description_width\": \"initial\"},\n",
    "        )\n",
    "        self.w_manual_list.layout.display = \"none\"\n",
    "\n",
    "        # --- 4. Run Button ---\n",
    "        self.w_run_btn = widgets.Button(\n",
    "            description=\"Run Simulation\",\n",
    "            button_style=\"primary\",\n",
    "        )\n",
    "\n",
    "        # Observers\n",
    "        self.w_mode.observe(self._on_mode_change, names=\"value\")\n",
    "        self.w_run_btn.on_click(self._on_run_clicked)\n",
    "\n",
    "    def _init_figure(self):\n",
    "        \"\"\"Initialize 3-panel figure using original layout parameters\"\"\"\n",
    "        self.fig = go.FigureWidget(\n",
    "            make_subplots(\n",
    "                rows=4,\n",
    "                cols=1,\n",
    "                row_heights=[0.6, 0.15, 0.12, 0.13],\n",
    "                shared_xaxes=True,\n",
    "                vertical_spacing=0.08,\n",
    "                subplot_titles=(\n",
    "                    \"Event-Driven Walk-Forward Analysis\",\n",
    "                    \"Market Regime (200D MA Deviation)\",\n",
    "                    \"Trend Velocity (21d Slope)\",\n",
    "                    \"Volatility Regime (VIX Z-Score)\",\n",
    "                ),\n",
    "                specs=[\n",
    "                    [{\"secondary_y\": False}],\n",
    "                    [{\"secondary_y\": False}],\n",
    "                    [{\"secondary_y\": False}],\n",
    "                    [{\"secondary_y\": False}],\n",
    "                ],\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # EXACT old layout configuration\n",
    "        self.fig.update_layout(\n",
    "            template=\"plotly_white\",\n",
    "            height=800,  # Slightly increased for 3 panels but maintains proportions\n",
    "            margin=dict(l=40, r=40, t=60, b=40),  # EXACTLY like old code\n",
    "            hovermode=\"x unified\",\n",
    "            showlegend=True,\n",
    "            # NO explicit legend positioning - uses Plotly defaults like old code\n",
    "            # This places legend immediately adjacent to plot area on the right\n",
    "        )\n",
    "\n",
    "        # --- Row 1: Price Action (Traces 0-51) ---\n",
    "        # 1. Create 50 empty traces for Tickers (Legend enabled by default like old code)\n",
    "        for _ in range(50):\n",
    "            self.fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    visible=False,\n",
    "                    line=dict(width=1.5),\n",
    "                    showlegend=True,  # Explicitly True (old code default behavior)\n",
    "                ),\n",
    "                row=1,\n",
    "                col=1,\n",
    "            )\n",
    "\n",
    "        # 2. Trace 50: Benchmark (Black Dash)\n",
    "        self.fig.add_trace(\n",
    "            go.Scatter(\n",
    "                name=\"Benchmark\",\n",
    "                line=dict(color=\"black\", width=2.5, dash=\"dash\"),\n",
    "                visible=False,\n",
    "            ),\n",
    "            row=1,\n",
    "            col=1,\n",
    "        )\n",
    "\n",
    "        # 3. Trace 51: Group Portfolio (Green)\n",
    "        self.fig.add_trace(\n",
    "            go.Scatter(\n",
    "                name=\"Group Portfolio\", line=dict(color=\"green\", width=3), visible=False\n",
    "            ),\n",
    "            row=1,\n",
    "            col=1,\n",
    "        )\n",
    "\n",
    "        # --- Row 2: Macro Trend [52] ---\n",
    "        self.fig.add_trace(\n",
    "            go.Scatter(\n",
    "                name=\"Trend\",\n",
    "                line=dict(color=\"#2E8B57\", width=2),\n",
    "                fillcolor=\"rgba(46, 139, 87, 0.15)\",\n",
    "                fill=\"tozeroy\",\n",
    "                visible=False,\n",
    "            ),\n",
    "            row=2,\n",
    "            col=1,\n",
    "        )\n",
    "\n",
    "        # Zero line for Trend (static)\n",
    "        self.fig.add_hline(y=0, line_dash=\"dot\", line_color=\"gray\", row=2, col=1)\n",
    "\n",
    "        # --- Row 3: Trend Velocity [53] ---  <-- ADD THIS BLOCK\n",
    "        self.fig.add_trace(\n",
    "            go.Scatter(\n",
    "                name=\"Trend Vel (21d)\",\n",
    "                line=dict(color=\"#FF6B35\", width=2),\n",
    "                fillcolor=\"rgba(255, 107, 53, 0.15)\",  # Orange tint\n",
    "                fill=\"tozeroy\",  # Fill to zero\n",
    "                visible=False,\n",
    "            ),\n",
    "            row=3,  # New row 3\n",
    "            col=1,\n",
    "        )\n",
    "        # Zero line for velocity (static, row 3)\n",
    "        self.fig.add_hline(y=0, line_dash=\"dot\", line_color=\"gray\", row=3, col=1)\n",
    "        self.fig.add_hline(\n",
    "            y=2,\n",
    "            line_dash=\"dash\",\n",
    "            line_color=\"red\",\n",
    "            annotation_text=\"Accel\",\n",
    "            row=3,\n",
    "            col=1,\n",
    "        )  # Fear threshold\n",
    "        self.fig.add_hline(\n",
    "            y=-2,\n",
    "            line_dash=\"dash\",\n",
    "            line_color=\"green\",\n",
    "            annotation_text=\"Decel\",\n",
    "            row=3,\n",
    "            col=1,\n",
    "        )  # Capitulation threshold\n",
    "\n",
    "        # --- Row 4: VIX Z-Score [54] ---\n",
    "        self.fig.add_trace(\n",
    "            go.Scatter(\n",
    "                name=\"VIX-Z\",\n",
    "                line=dict(color=\"#800080\", width=2),\n",
    "                fillcolor=\"rgba(128, 0, 128, 0.15)\",  # Purple tint\n",
    "                fill=\"tozeroy\",  # Fill to zero\n",
    "                visible=False,\n",
    "            ),\n",
    "            row=4,  # <-- ADD THIS\n",
    "            col=1,  # <-- ADD THIS\n",
    "        )\n",
    "\n",
    "        # Reference lines for VIX\n",
    "        self.fig.add_hline(\n",
    "            y=2,\n",
    "            line_dash=\"dash\",\n",
    "            line_color=\"red\",\n",
    "            row=4,\n",
    "            col=1,\n",
    "            annotation_text=\"Fear\",\n",
    "        )\n",
    "        self.fig.add_hline(\n",
    "            y=-1.5,\n",
    "            line_dash=\"dash\",\n",
    "            line_color=\"green\",\n",
    "            row=4,\n",
    "            col=1,\n",
    "            annotation_text=\"Calm\",\n",
    "        )\n",
    "\n",
    "        # Axis labels\n",
    "        self.fig.update_yaxes(title_text=\"Cumulative Return\", row=1, col=1)\n",
    "        self.fig.update_yaxes(title_text=\"Trend\", tickformat=\".0%\", row=2, col=1)\n",
    "        self.fig.update_yaxes(\n",
    "            title_text=\"Trend Vel (Z)\", tickformat=\".1f\", row=3, col=1\n",
    "        )\n",
    "        self.fig.update_yaxes(title_text=\"VIX-Z\", row=4, col=1)\n",
    "\n",
    "        # Hide x-axis labels for top rows\n",
    "        self.fig.update_xaxes(showticklabels=False, row=1, col=1)\n",
    "        self.fig.update_xaxes(showticklabels=False, row=2, col=1)\n",
    "        self.fig.update_xaxes(\n",
    "            showticklabels=False, row=3, col=1\n",
    "        )  # CHANGED (was row 3, still hide for middle)\n",
    "\n",
    "    def _on_mode_change(self, change):\n",
    "        is_ranking = change[\"new\"] == \"Ranking\"\n",
    "        self.w_rank_range.layout.display = \"flex\" if is_ranking else \"none\"\n",
    "        self.w_manual_list.layout.display = \"none\" if is_ranking else \"flex\"\n",
    "\n",
    "    def _on_run_clicked(self, b):\n",
    "        self.w_run_btn.disabled = True\n",
    "        self.w_run_btn.description = \"Calculating...\"\n",
    "        self.output_area.clear_output()\n",
    "\n",
    "        with self.output_area:\n",
    "            try:\n",
    "                # 1. Capture Inputs\n",
    "                cur_decision_date = pd.to_datetime(self.w_decision_date.value)\n",
    "                manual_list = [\n",
    "                    t.strip().upper()\n",
    "                    for t in self.w_manual_list.value.split(\",\")\n",
    "                    if t.strip()\n",
    "                ]\n",
    "\n",
    "                inputs = EngineInput(\n",
    "                    mode=self.w_mode.value,\n",
    "                    start_date=cur_decision_date,\n",
    "                    lookback_period=self.w_lookback.value,\n",
    "                    holding_period=self.w_holding.value,\n",
    "                    metric=self.w_strategy.value,\n",
    "                    benchmark_ticker=self.w_benchmark.value.strip().upper(),\n",
    "                    rank_start=self.w_rank_start.value,\n",
    "                    rank_end=self.w_rank_end.value,\n",
    "                    manual_tickers=manual_list,\n",
    "                    universe_subset=self.universe_subset,\n",
    "                    debug=True,\n",
    "                )\n",
    "\n",
    "                # 2. Engine Execution\n",
    "                res = self.engine.run(inputs)\n",
    "                self.last_run = res\n",
    "\n",
    "                if res.error_msg:\n",
    "                    print(f\"âš ï¸ {res.error_msg}\")\n",
    "                    return\n",
    "\n",
    "                # 3. Update FilterPack (The \"Save\" Step)\n",
    "                self.filter_pack.decision_date = res.decision_date\n",
    "                self.filter_pack.selected_tickers = res.tickers\n",
    "\n",
    "                # Extract eligible pool (Survivors) from audit data if available\n",
    "                if res.debug_data and \"audit_liquidity\" in res.debug_data:\n",
    "                    audit = res.debug_data[\"audit_liquidity\"]\n",
    "                    if \"universe_snapshot\" in audit and isinstance(\n",
    "                        audit[\"universe_snapshot\"], pd.DataFrame\n",
    "                    ):\n",
    "                        snap = audit[\"universe_snapshot\"]\n",
    "                        self.filter_pack.eligible_pool = snap[\n",
    "                            snap[\"Passed_Final\"]\n",
    "                        ].index.tolist()\n",
    "\n",
    "                # 4. Render Visuals\n",
    "                self._update_plots(res, inputs)\n",
    "                self._display_audit_and_metrics(res, inputs)\n",
    "\n",
    "            except Exception as e:\n",
    "                import traceback\n",
    "\n",
    "                print(f\"ðŸš¨ Error: {str(e)}\")\n",
    "                traceback.print_exc()\n",
    "            finally:\n",
    "                self.w_run_btn.disabled = False\n",
    "                self.w_run_btn.description = \"Run Simulation\"\n",
    "\n",
    "    def _update_plots(self, res: EngineOutput, inputs: EngineInput):\n",
    "        with self.fig.batch_update():\n",
    "            # --- A. Row 1: Ticker & Portfolio Traces ---\n",
    "            cols = res.normalized_plot_data.columns.tolist()\n",
    "            for i in range(50):\n",
    "                if i < len(cols):\n",
    "                    self.fig.data[i].update(\n",
    "                        x=res.normalized_plot_data.index,\n",
    "                        y=res.normalized_plot_data[cols[i]],\n",
    "                        name=cols[i],\n",
    "                        visible=True,\n",
    "                    )\n",
    "                else:\n",
    "                    self.fig.data[i].visible = False\n",
    "\n",
    "            # Benchmark (Trace 50)\n",
    "            if not res.benchmark_series.empty:\n",
    "                self.fig.data[50].update(\n",
    "                    x=res.benchmark_series.index,\n",
    "                    y=res.benchmark_series.values,\n",
    "                    name=f\"Benchmark ({inputs.benchmark_ticker})\",\n",
    "                    visible=True,\n",
    "                )\n",
    "            else:\n",
    "                self.fig.data[50].visible = False\n",
    "\n",
    "            # Portfolio (Trace 51)\n",
    "            if not res.portfolio_series.empty:\n",
    "                self.fig.data[51].update(\n",
    "                    x=res.portfolio_series.index,\n",
    "                    y=res.portfolio_series.values,\n",
    "                    name=\"Group Portfolio\",\n",
    "                    visible=True,\n",
    "                )\n",
    "            else:\n",
    "                self.fig.data[51].visible = False\n",
    "\n",
    "            # --- B. Rows 2 & 3: Macro Data ---\n",
    "            # Check if macro_df exists and slice to current window\n",
    "            if (\n",
    "                hasattr(res, \"macro_df\")\n",
    "                and res.macro_df is not None\n",
    "                and not res.normalized_plot_data.empty\n",
    "            ):\n",
    "                plot_dates = res.normalized_plot_data.index\n",
    "                # Ensure macro_df has DatetimeIndex\n",
    "                if not isinstance(res.macro_df.index, pd.DatetimeIndex):\n",
    "                    macro_index = pd.to_datetime(res.macro_df.index)\n",
    "                else:\n",
    "                    macro_index = res.macro_df.index\n",
    "\n",
    "                # Slice macro data to match plot window for performance and visual alignment\n",
    "                mask = (macro_index >= plot_dates.min()) & (\n",
    "                    macro_index <= plot_dates.max()\n",
    "                )\n",
    "                macro_slice = res.macro_df.loc[mask]\n",
    "\n",
    "                if not macro_slice.empty:\n",
    "                    # Update Trend (Trace 52)\n",
    "                    self.fig.data[52].update(\n",
    "                        x=macro_slice.index, y=macro_slice[\"Macro_Trend\"], visible=True\n",
    "                    )\n",
    "\n",
    "                    # Update Trend Velocity (Trace 53) --- ADD THIS BLOCK\n",
    "                    if \"Macro_Trend_Vel_Z\" in macro_slice.columns:\n",
    "                        self.fig.data[53].update(\n",
    "                            x=macro_slice.index,\n",
    "                            y=macro_slice[\n",
    "                                \"Macro_Trend_Vel_Z\"\n",
    "                            ],  # Changed from Macro_Trend_Vel\n",
    "                            visible=True,\n",
    "                        )\n",
    "\n",
    "                    # Update VIX-Z (Trace 54)\n",
    "                    if \"Macro_Vix_Z\" in macro_slice.columns:\n",
    "                        self.fig.data[54].update(\n",
    "                            x=macro_slice.index,\n",
    "                            y=macro_slice[\"Macro_Vix_Z\"],\n",
    "                            visible=True,\n",
    "                        )\n",
    "\n",
    "            else:\n",
    "                # Hide macro traces if no data\n",
    "                self.fig.data[52].visible = False\n",
    "                self.fig.data[53].visible = False\n",
    "                self.fig.data[54].visible = False  # <-- ADD THIS LINE\n",
    "\n",
    "            # --- C. Vertical Event Lines (Span all rows) ---\n",
    "            # Using yref=\"paper\" spans 0-1 across all subplots\n",
    "            shapes = [\n",
    "                dict(\n",
    "                    type=\"line\",\n",
    "                    x0=res.decision_date,\n",
    "                    y0=0,\n",
    "                    x1=res.decision_date,\n",
    "                    y1=1,\n",
    "                    xref=\"x\",\n",
    "                    yref=\"paper\",  # Critical: spans all subplots\n",
    "                    line=dict(color=\"red\", width=2, dash=\"dash\"),\n",
    "                    name=\"Decision Date\",\n",
    "                ),\n",
    "                dict(\n",
    "                    type=\"line\",\n",
    "                    x0=res.buy_date,\n",
    "                    y0=0,\n",
    "                    x1=res.buy_date,\n",
    "                    y1=1,\n",
    "                    xref=\"x\",\n",
    "                    yref=\"paper\",\n",
    "                    line=dict(color=\"blue\", width=2, dash=\"dot\"),\n",
    "                    name=\"Execution Date\",\n",
    "                ),\n",
    "            ]\n",
    "\n",
    "            # Add holding period end line if desired (optional)\n",
    "            # shapes.append(dict(... res.holding_end_date ...))\n",
    "            # --- C. Shapes (Vertical Events + Static Reference Lines) ---\n",
    "\n",
    "            # Vertical event lines spanning all rows (yref='paper')\n",
    "            event_shapes = [\n",
    "                dict(\n",
    "                    type=\"line\",\n",
    "                    x0=res.decision_date,\n",
    "                    x1=res.decision_date,\n",
    "                    y0=0,\n",
    "                    y1=1,\n",
    "                    xref=\"x\",\n",
    "                    yref=\"paper\",\n",
    "                    line=dict(color=\"red\", width=2, dash=\"dash\"),\n",
    "                ),\n",
    "                dict(\n",
    "                    type=\"line\",\n",
    "                    x0=res.buy_date,\n",
    "                    x1=res.buy_date,\n",
    "                    y0=0,\n",
    "                    y1=1,\n",
    "                    xref=\"x\",\n",
    "                    yref=\"paper\",\n",
    "                    line=dict(color=\"blue\", width=2, dash=\"dot\"),\n",
    "                ),\n",
    "            ]\n",
    "\n",
    "            # Static horizontal reference lines (manualdicts to avoid add_hline bugs)\n",
    "            # yref mapping: row2='y2', row3='y3', row4='y4'\n",
    "            static_shapes = [\n",
    "                # Row 2: Trend zero line\n",
    "                dict(\n",
    "                    type=\"line\",\n",
    "                    y0=0,\n",
    "                    y1=0,\n",
    "                    x0=0,\n",
    "                    x1=1,\n",
    "                    xref=\"paper\",\n",
    "                    yref=\"y2\",\n",
    "                    line=dict(color=\"gray\", width=1, dash=\"dot\"),\n",
    "                ),\n",
    "                # Row 3: Velocity Accel (+2)\n",
    "                dict(\n",
    "                    type=\"line\",\n",
    "                    y0=2,\n",
    "                    y1=2,\n",
    "                    x0=0,\n",
    "                    x1=1,\n",
    "                    xref=\"paper\",\n",
    "                    yref=\"y3\",\n",
    "                    line=dict(color=\"red\", width=1, dash=\"dash\"),\n",
    "                ),\n",
    "                # Row 3: Velocity Decel (-2)\n",
    "                dict(\n",
    "                    type=\"line\",\n",
    "                    y0=-2,\n",
    "                    y1=-2,\n",
    "                    x0=0,\n",
    "                    x1=1,\n",
    "                    xref=\"paper\",\n",
    "                    yref=\"y3\",\n",
    "                    line=dict(color=\"green\", width=1, dash=\"dash\"),\n",
    "                ),\n",
    "                # Row 3: Velocity Zero\n",
    "                dict(\n",
    "                    type=\"line\",\n",
    "                    y0=0,\n",
    "                    y1=0,\n",
    "                    x0=0,\n",
    "                    x1=1,\n",
    "                    xref=\"paper\",\n",
    "                    yref=\"y3\",\n",
    "                    line=dict(color=\"gray\", width=1, dash=\"dot\"),\n",
    "                ),\n",
    "                # Row 4: VIX Fear (+2)\n",
    "                dict(\n",
    "                    type=\"line\",\n",
    "                    y0=2,\n",
    "                    y1=2,\n",
    "                    x0=0,\n",
    "                    x1=1,\n",
    "                    xref=\"paper\",\n",
    "                    yref=\"y4\",\n",
    "                    line=dict(color=\"red\", width=1, dash=\"dash\"),\n",
    "                ),\n",
    "                # Row 4: VIX Calm (-1.5)\n",
    "                dict(\n",
    "                    type=\"line\",\n",
    "                    y0=-1.5,\n",
    "                    y1=-1.5,\n",
    "                    x0=0,\n",
    "                    x1=1,\n",
    "                    xref=\"paper\",\n",
    "                    yref=\"y4\",\n",
    "                    line=dict(color=\"green\", width=1, dash=\"dash\"),\n",
    "                ),\n",
    "            ]\n",
    "\n",
    "            self.fig.layout.shapes = event_shapes + static_shapes\n",
    "\n",
    "    def _display_audit_and_metrics(self, res: EngineOutput, inputs: EngineInput):\n",
    "        self.output_area.layout = widgets.Layout(margin=\"10px 0px 20px 0px\")\n",
    "\n",
    "        # Header (Success Message)\n",
    "        mode_str = (\n",
    "            f\"CASCADE (Subset of {len(self.universe_subset)})\"\n",
    "            if self.universe_subset\n",
    "            else \"DISCOVERY (Full Market)\"\n",
    "        )\n",
    "        display(\n",
    "            widgets.HTML(\n",
    "                f\"<div style='font-family:sans-serif; font-size:12px; margin-bottom:10px'><b style='color:green'>âœ… Success</b> | Mode: {mode_str}</div>\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Audit Logic\n",
    "        if (\n",
    "            inputs.mode == \"Ranking\"\n",
    "            and res.debug_data\n",
    "            and \"audit_liquidity\" in res.debug_data\n",
    "        ):\n",
    "            audit = res.debug_data[\"audit_liquidity\"]\n",
    "            print(\"-\" * 70)\n",
    "            if audit.get(\"forced_list\"):\n",
    "                print(f\"ðŸ” STAGE 2 AUDIT: Cascade Mode Active\")\n",
    "                print(\n",
    "                    f\"   Pool Size: {audit.get('tickers_passed')} survivors (Forced List)\"\n",
    "                )\n",
    "            else:\n",
    "                print(f\"ðŸ” STAGE 1 AUDIT (Decision: {res.decision_date.date()})\")\n",
    "                print(f\"   Pool Size: {audit.get('tickers_passed')} survivors\")\n",
    "            print(\"-\" * 70)\n",
    "\n",
    "        # Timeline\n",
    "        print(\n",
    "            f\"Timeline: [{res.start_date.date()}] -> Decision: {res.decision_date.date()} -> Entry: {res.buy_date.date()} -> End: {res.holding_end_date.date()}\"\n",
    "        )\n",
    "\n",
    "        # --- FIX: WRAP TICKERS TO 10 PER LINE ---\n",
    "        print(f\"Selected Tickers ({len(res.tickers)}):\")\n",
    "        if res.tickers:\n",
    "            for i in range(0, len(res.tickers), 10):\n",
    "                print(\", \".join(res.tickers[i : i + 10]) + \",\")\n",
    "        else:\n",
    "            print(\"None\")\n",
    "        print(\"\")\n",
    "        # ----------------------------------------\n",
    "\n",
    "        # --- DATA PREP (Metrics Table) ---\n",
    "        m = res.perf_metrics\n",
    "        rows = []\n",
    "        for label, key in [\n",
    "            (\"Gain\", \"gain\"),\n",
    "            (\"Sharpe\", \"sharpe\"),\n",
    "            (\"Sharpe (ATRP)\", \"sharpe_atrp\"),\n",
    "            (\"Sharpe (TRP)\", \"sharpe_trp\"),\n",
    "        ]:\n",
    "            p_row = {\n",
    "                \"Metric\": f\"Group {label}\",\n",
    "                \"Full\": m.get(f\"full_p_{key}\"),\n",
    "                \"Lookback\": m.get(f\"lookback_p_{key}\"),\n",
    "                \"Holding\": m.get(f\"holding_p_{key}\"),\n",
    "            }\n",
    "            b_row = {\n",
    "                \"Metric\": f\"Benchmark {label}\",\n",
    "                \"Full\": m.get(f\"full_b_{key}\"),\n",
    "                \"Lookback\": m.get(f\"lookback_b_{key}\"),\n",
    "                \"Holding\": m.get(f\"holding_b_{key}\"),\n",
    "            }\n",
    "            d_row = {\"Metric\": f\"== {label} Delta\"}\n",
    "            for col in [\"Full\", \"Lookback\", \"Holding\"]:\n",
    "                d_row[col] = (p_row[col] or 0) - (b_row[col] or 0)\n",
    "            rows.extend([p_row, b_row, d_row])\n",
    "\n",
    "        df_report = pd.DataFrame(rows).set_index(\"Metric\")\n",
    "\n",
    "        # --- STYLE ---\n",
    "        styler = df_report.style.format(\"{:+.4f}\", na_rep=\"N/A\")\n",
    "\n",
    "        def row_logic(row):\n",
    "            if \"Delta\" in row.name:\n",
    "                return [\n",
    "                    \"background-color: #f9f9f9; font-weight: 600; border-top: 1px solid #ddd\"\n",
    "                ] * len(row)\n",
    "            if \"Group\" in row.name:\n",
    "                return [\"color: #2c5e8f; background-color: #fcfdfe\"] * len(row)\n",
    "            return [\"color: #555\"] * len(row)\n",
    "\n",
    "        styler.apply(row_logic, axis=1)\n",
    "        styler.set_table_styles(\n",
    "            [\n",
    "                {\n",
    "                    \"selector\": \"\",\n",
    "                    \"props\": [\n",
    "                        (\"font-family\", \"inherit\"),\n",
    "                        (\"font-size\", \"12px\"),\n",
    "                        (\"border-collapse\", \"collapse\"),\n",
    "                        (\"width\", \"auto\"),\n",
    "                    ],\n",
    "                },\n",
    "                {\n",
    "                    \"selector\": \"th\",\n",
    "                    \"props\": [\n",
    "                        (\"background-color\", \"white\"),\n",
    "                        (\"color\", \"#222\"),\n",
    "                        (\"font-weight\", \"600\"),\n",
    "                        (\"padding\", \"6px 12px\"),\n",
    "                        (\"border-bottom\", \"2px solid #444\"),\n",
    "                        (\"text-align\", \"center\"),\n",
    "                    ],\n",
    "                },\n",
    "                {\n",
    "                    \"selector\": \"th.row_heading\",\n",
    "                    \"props\": [\n",
    "                        (\"text-align\", \"left\"),\n",
    "                        (\"padding-right\", \"30px\"),\n",
    "                        (\"border-bottom\", \"1px solid #eee\"),\n",
    "                    ],\n",
    "                },\n",
    "                {\n",
    "                    \"selector\": \"td\",\n",
    "                    \"props\": [\n",
    "                        (\"padding\", \"4px 12px\"),\n",
    "                        (\"border-bottom\", \"1px solid #eee\"),\n",
    "                    ],\n",
    "                },\n",
    "            ]\n",
    "        )\n",
    "        styler.index.name = None\n",
    "        display(styler)\n",
    "\n",
    "    def show(self):\n",
    "        # 1. Timeline Box (Bordered)\n",
    "        timeline_box = widgets.HBox(\n",
    "            [self.w_lookback, self.w_decision_date, self.w_holding],\n",
    "            layout=widgets.Layout(\n",
    "                justify_content=\"space-between\",\n",
    "                border=\"1px solid #ddd\",\n",
    "                padding=\"10px\",\n",
    "                margin=\"5px 0px 15px 0px\",\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # 2. Strategy & Benchmark container\n",
    "        strategy_container = widgets.HBox(\n",
    "            [self.w_strategy, self.w_benchmark],\n",
    "            layout=widgets.Layout(margin=\"0px 0px 0px 10px\"),\n",
    "        )\n",
    "\n",
    "        # 3. Settings Row\n",
    "        settings_row = widgets.HBox(\n",
    "            [self.w_mode, strategy_container],\n",
    "            layout=widgets.Layout(align_items=\"flex-start\"),\n",
    "        )\n",
    "\n",
    "        # 4. Construct UI\n",
    "        ui = widgets.VBox(\n",
    "            [\n",
    "                widgets.HTML(\n",
    "                    \"<b>1. Timeline Configuration:</b> (Past <--- Decision ---> Future)\"\n",
    "                ),\n",
    "                timeline_box,\n",
    "                widgets.HTML(\"<b>2. Strategy Settings:</b>\"),\n",
    "                settings_row,\n",
    "                self.w_rank_range,\n",
    "                self.w_manual_list,\n",
    "                widgets.HTML(\"<hr>\"),\n",
    "                self.w_run_btn,\n",
    "                self.output_area,\n",
    "                self.fig,  # The FigureWidget with subplots\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        display(ui)\n",
    "        # Auto-run on display\n",
    "        self._on_run_clicked(None)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION E: THE UI (Visualization)\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "def create_walk_forward_analyzer(engine, universe_subset=None, filter_pack=None):\n",
    "    \"\"\"Factory function to match the requested (analyzer, pack) return signature.\"\"\"\n",
    "    pack = filter_pack or FilterPack()\n",
    "    analyzer = WalkForwardAnalyzer(\n",
    "        engine, universe_subset=universe_subset, filter_pack=pack\n",
    "    )\n",
    "    return analyzer, pack\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION F: INSPECTION TOOLS\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "def peek(idx, reg):\n",
    "    \"\"\"\n",
    "    Displays metadata and RETURNS the object for further use.\n",
    "    \"\"\"\n",
    "    if idx < 0 or idx >= len(reg):\n",
    "        print(f\"âŒ Index {idx} out of range.\")\n",
    "        return None\n",
    "\n",
    "    entry = reg[idx]\n",
    "\n",
    "    # 1. Print the Header (for humans)\n",
    "    print(f\" {'='*60}\")\n",
    "    print(f\" ðŸ“ INDEX: [{idx}]\")\n",
    "    print(f\" ðŸ·ï¸  NAME:  {entry['name']}\")\n",
    "    print(f\" ðŸ“‚ PATH:  {entry['path']}\")\n",
    "    print(f\" {'='*60}\\n\")\n",
    "\n",
    "    # 2. Display the data (for the UI)\n",
    "    from IPython.display import display\n",
    "\n",
    "    display(entry[\"obj\"])\n",
    "\n",
    "    # 3. RETURN the data (for other functions)\n",
    "    return entry[\"obj\"]\n",
    "\n",
    "\n",
    "def visualize_audit_structure(obj):\n",
    "    \"\"\"\n",
    "    Generates the Map and returns a Registry of dictionaries:\n",
    "    [{'name': str, 'path': str, 'obj': object}, ...]\n",
    "    \"\"\"\n",
    "    id_memory = {}\n",
    "    registry = []\n",
    "    output = [\n",
    "        \"====================================================================\",\n",
    "        \"ðŸ” HIGH-TRANSPARENCY AUDIT MAP\",\n",
    "        \"====================================================================\",\n",
    "    ]\n",
    "\n",
    "    def get_icon(val):\n",
    "        if isinstance(val, pd.DataFrame):\n",
    "            return \"ðŸ§®\"\n",
    "        if isinstance(val, pd.Series):\n",
    "            return \"ðŸ“ˆ\"\n",
    "        if isinstance(val, (list, tuple, dict)):\n",
    "            return \"ðŸ“‚\"\n",
    "        if isinstance(val, pd.Timestamp):\n",
    "            return \"ðŸ“…\"\n",
    "        if is_dataclass(val):\n",
    "            return \"ðŸ“¦\"\n",
    "        return \"ðŸ”¢\" if isinstance(val, (int, float)) else \"ðŸ“„\"\n",
    "\n",
    "    def process(item, name, level=0, path=\"\"):\n",
    "        indent = \"  \" * level\n",
    "        item_id = id(item)\n",
    "\n",
    "        # Build the breadcrumb path\n",
    "        current_path = f\"{path} -> {name}\" if path else name\n",
    "\n",
    "        is_primitive = isinstance(item, (int, float, str, bool, type(None)))\n",
    "        if not is_primitive and item_id in id_memory:\n",
    "            output.append(\n",
    "                f\"{indent}          â•°â”€â”€ {name} --> [See ID {id_memory[item_id]}]\"\n",
    "            )\n",
    "            return\n",
    "\n",
    "        # 1. Store Index, Object, Name, and Path in Registry\n",
    "        curr_idx = len(registry)\n",
    "        registry.append({\"name\": name, \"path\": current_path, \"obj\": item})\n",
    "\n",
    "        if not is_primitive:\n",
    "            id_memory[item_id] = curr_idx\n",
    "\n",
    "        # 2. Metadata for display\n",
    "        meta = f\"{type(item).__name__}\"\n",
    "        if hasattr(item, \"shape\"):\n",
    "            meta = f\"shape={item.shape}\"\n",
    "        elif isinstance(item, (list, dict)):\n",
    "            meta = f\"len={len(item)}\"\n",
    "\n",
    "        output.append(f\"[{curr_idx:>3}] {indent}{get_icon(item)} {name} ({meta})\")\n",
    "\n",
    "        # 3. Recurse\n",
    "        if isinstance(item, dict):\n",
    "            for k, v in item.items():\n",
    "                process(v, k, level + 1, current_path)\n",
    "        elif isinstance(item, (list, tuple)):\n",
    "            for i, v in enumerate(item):\n",
    "                process(v, f\"index_{i}\", level + 1, current_path)\n",
    "        elif is_dataclass(item):\n",
    "            for f in fields(item):\n",
    "                process(getattr(item, f.name), f.name, level + 1, current_path)\n",
    "\n",
    "    process(obj, \"audit_pack\")\n",
    "    print(\"\\n\".join(output))\n",
    "\n",
    "    return registry\n",
    "\n",
    "\n",
    "def visualize_analyzer_structure(analyzer):\n",
    "    \"\"\"\n",
    "    Maps the internal data structure of the last simulation run.\n",
    "    Usage: analyzer.last_run.tickers\n",
    "    \"\"\"\n",
    "    if not analyzer.last_run:\n",
    "        print(\n",
    "            \"âŒ Audit Aborted: No simulation data found. Click 'Run' in the UI first.\"\n",
    "        )\n",
    "        return []\n",
    "\n",
    "    # We audit the last_run object (EngineOutput)\n",
    "    return visualize_audit_structure(analyzer.last_run)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# INTEGRITY PROTECTION: THE TRIPWIRE\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "def verify_math_integrity():\n",
    "    \"\"\"\n",
    "    ðŸ›¡ï¸ TRIPWIRE: Ensures Sample Boundary Integrity.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- ðŸ›¡ï¸ Starting Final Integrity Audit ---\")\n",
    "\n",
    "    try:\n",
    "        # Test 1: Series Input\n",
    "        mock_series = pd.Series([100.0, 102.0, 101.0])\n",
    "        rets_s = QuantUtils.compute_returns(mock_series)\n",
    "        # Verify first value is actually NaN\n",
    "        if not pd.isna(rets_s.iloc[0]):\n",
    "            raise ValueError(\"Series Leading NaN missing\")\n",
    "        print(\"âœ… Series Boundary: OK\")\n",
    "\n",
    "        # Test 2: DataFrame Input\n",
    "        mock_df = pd.DataFrame({\"A\": [100, 101], \"B\": [200, 202]})\n",
    "        rets_df = QuantUtils.compute_returns(mock_df)\n",
    "        if not rets_df.iloc[0].isna().all():\n",
    "            raise ValueError(\"DataFrame Leading NaN missing\")\n",
    "        print(\"âœ… DataFrame Boundary: OK\")\n",
    "\n",
    "        print(\"âœ… AUDIT PASSED: Mathematical boundaries are strictly enforced.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ðŸ”¥ SYSTEM BREACH: {str(e)}\")\n",
    "        raise e\n",
    "\n",
    "\n",
    "def verify_feature_engineering_integrity():\n",
    "    \"\"\"\n",
    "    ðŸ›¡ï¸ TRIPWIRE: Validates Feature Engineering Logic.\n",
    "    Enforces:\n",
    "    1. Day 1 ATR must be NaN (No PrevClose).\n",
    "    2. Wilder's Smoothing must use Alpha = 1/Period.\n",
    "    3. Recursion must match manual calculation.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- ðŸ›¡ï¸ Starting Feature Engineering Audit ---\")\n",
    "\n",
    "    # 1. Create Synthetic Data (3 Days)\n",
    "    # Day 1: High-Low = 10. No PrevClose.\n",
    "    # Day 2: High-Low = 20. Gap up implies TR might be larger.\n",
    "    # Day 3: High-Low = 10.\n",
    "    dates = pd.to_datetime([\"2020-01-01\", \"2020-01-02\", \"2020-01-03\"])\n",
    "    idx = pd.MultiIndex.from_product([[\"TEST\"], dates], names=[\"Ticker\", \"Date\"])\n",
    "\n",
    "    df_mock = pd.DataFrame(\n",
    "        {\n",
    "            \"Adj Open\": [100, 110, 110],\n",
    "            \"Adj High\": [110, 130, 120],\n",
    "            \"Adj Low\": [100, 110, 110],\n",
    "            \"Adj Close\": [105, 120, 115],  # PrevClose: NaN, 105, 120\n",
    "            \"Volume\": [1000, 1000, 1000],\n",
    "        },\n",
    "        index=idx,\n",
    "    )\n",
    "\n",
    "    # 2. Run the Generator\n",
    "    # We use Period=2 to make manual math easy (Alpha = 1/2 = 0.5)\n",
    "    feats_df, macro_df = generate_features(\n",
    "        df_mock, atr_period=2, rsi_period=2, quality_min_periods=1\n",
    "    )\n",
    "\n",
    "    atr_series = feats_df[\"ATR\"]\n",
    "\n",
    "    # 3. MANUAL CALCULATION (The \"Truth\")\n",
    "    # Day 1:\n",
    "    #   TR = Max(H-L, |H-PC|, |L-PC|)\n",
    "    #   TR = Max(10, NaN, NaN) -> NaN (Because skipna=False)\n",
    "    #   Expected ATR: NaN\n",
    "\n",
    "    # Day 2:\n",
    "    #   PrevClose = 105\n",
    "    #   H-L=20, |130-105|=25, |110-105|=5\n",
    "    #   TR = 25\n",
    "    #   Expected ATR: First valid observation = 25.0\n",
    "\n",
    "    # Day 3:\n",
    "    #   PrevClose = 120\n",
    "    #   H-L=10, |120-120|=0, |110-120|=10\n",
    "    #   TR = 10\n",
    "    #   Wilder's Smoothing (Alpha=0.5):\n",
    "    #   ATR_3 = (TR_3 * alpha) + (ATR_2 * (1-alpha))\n",
    "    #   ATR_3 = (10 * 0.5) + (25 * 0.5) = 5 + 12.5 = 17.5\n",
    "\n",
    "    print(f\"Audit Values:\\n{atr_series.values}\")\n",
    "\n",
    "    # 4. ASSERTIONS\n",
    "    try:\n",
    "        # Check Day 1\n",
    "        if not np.isnan(atr_series.iloc[0]):\n",
    "            raise AssertionError(\n",
    "                f\"Day 1 Regression: Expected NaN, got {atr_series.iloc[0]}. (Check skipna=False)\"\n",
    "            )\n",
    "\n",
    "        # Check Day 2 (Initialization)\n",
    "        if not np.isclose(atr_series.iloc[1], 25.0):\n",
    "            raise AssertionError(\n",
    "                f\"Initialization Regression: Expected 25.0, got {atr_series.iloc[1]}.\"\n",
    "            )\n",
    "\n",
    "        # Check Day 3 (Recursion)\n",
    "        if not np.isclose(atr_series.iloc[2], 17.5):\n",
    "            raise AssertionError(\n",
    "                f\"Wilder's Logic Regression: Expected 17.5, got {atr_series.iloc[2]}. (Check Alpha=1/N)\"\n",
    "            )\n",
    "\n",
    "        print(\"âœ… FEATURE INTEGRITY PASSED: Wilder's ATR logic is strictly enforced.\")\n",
    "\n",
    "    except AssertionError as e:\n",
    "        print(f\"ðŸ”¥ LOGIC FAILURE: {str(e)}\")\n",
    "        raise e\n",
    "\n",
    "\n",
    "def verify_ranking_integrity():\n",
    "    \"\"\"\n",
    "    ðŸ›¡ï¸ TRIPWIRE: Prevents 'Momentum Collapse' in Volatility-Adjusted Ranking.\n",
    "    Ensures that Sharpe(Vol) distinguishes between High-Vol and Low-Vol stocks.\n",
    "    \"\"\"\n",
    "    print(\"--- ðŸ›¡ï¸ Starting Ranking Kernel Audit ---\")\n",
    "\n",
    "    # 1. Setup Mock Universe (2 Tickers, 2 Days)\n",
    "    # Ticker 'VOLATILE': 10% return, but 10% Volatility\n",
    "    # Ticker 'STABLE': 2% return, but 1% Volatility (The 'Sharpe' Winner)\n",
    "    data = {\"VOLATILE\": [1.0, 1.10], \"STABLE\": [1.0, 1.02]}  # +10%  # +2%\n",
    "    df_returns = pd.DataFrame(data).pct_change().dropna()\n",
    "\n",
    "    # Pre-calculated Mean Volatility per ticker (as provided by Engine Observation)\n",
    "    vol_series = pd.Series({\"VOLATILE\": 0.10, \"STABLE\": 0.01})\n",
    "\n",
    "    # 2. Run Kernel\n",
    "    results = QuantUtils.calculate_sharpe_vol(df_returns, vol_series)\n",
    "\n",
    "    # 3. CALCULATE EXPECTED (Pure Math)\n",
    "    # Volatile Sharpe: 0.10 / 0.10 = 1.0\n",
    "    # Stable Sharpe:   0.02 / 0.01 = 2.0\n",
    "\n",
    "    try:\n",
    "        # Check A: Diversity. If they are the same, normalization didn't happen.\n",
    "        if np.isclose(results[\"VOLATILE\"], results[\"STABLE\"]):\n",
    "            raise AssertionError(\n",
    "                \"RANKING COLLAPSE: Both tickers have the same normalized score.\"\n",
    "            )\n",
    "\n",
    "        # Check B: Direction. STABLE must rank higher than VOLATILE.\n",
    "        if results[\"STABLE\"] < results[\"VOLATILE\"]:\n",
    "            # This is exactly what happens when the bug turns it into Momentum\n",
    "            raise AssertionError(\n",
    "                f\"MOMENTUM REGRESSION: 'STABLE' ({results['STABLE']:.2f}) \"\n",
    "                f\"ranked below 'VOLATILE' ({results['VOLATILE']:.2f}). \"\n",
    "                \"The denominator was likely collapsed to a market average.\"\n",
    "            )\n",
    "\n",
    "        # Check C: Absolute Precision\n",
    "        if not np.isclose(results[\"STABLE\"], 2.0):\n",
    "            raise AssertionError(\n",
    "                f\"MATH ERROR: Expected 2.0 for STABLE, got {results['STABLE']}\"\n",
    "            )\n",
    "\n",
    "        print(\n",
    "            \"âœ… RANKING INTEGRITY PASSED: Volatility normalization is strictly enforced.\"\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ðŸ”¥ KERNEL BREACH: {str(e)}\")\n",
    "        raise e\n",
    "\n",
    "\n",
    "def verify_vol_alignment_integrity():\n",
    "    \"\"\"\n",
    "    ðŸ›¡ï¸ TRIPWIRE: Verifies Temporal Coupling between Returns and Volatility.\n",
    "    Ensures that the volatility average is only calculated over days where a return exists.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- ðŸ›¡ï¸ Starting Volatility Alignment Audit ---\")\n",
    "\n",
    "    # 1. SETUP SYNTHETIC DATA (2 Days)\n",
    "    # Day 1: Return = NaN, Vol = 0.90 (Extreme 'Trap' Volatility)\n",
    "    # Day 2: Return = 0.10, Vol = 0.10 (Target Reward/Risk)\n",
    "    rets_s = pd.Series([np.nan, 0.10])\n",
    "    vol_s = pd.Series([0.90, 0.10])\n",
    "\n",
    "    # 2. RUN KERNEL (Series Mode)\n",
    "    # Calculation Logic:\n",
    "    # If aligned: 0.10 / 0.10 = 1.0\n",
    "    # If misaligned: 0.10 / mean(0.90, 0.10) = 0.10 / 0.50 = 0.2\n",
    "    res_series = QuantUtils.calculate_sharpe_vol(rets_s, vol_s)\n",
    "\n",
    "    # 3. RUN KERNEL (DataFrame Mode)\n",
    "    # Ensures vectorized alignment works across columns\n",
    "    rets_df = pd.DataFrame({\"A\": [np.nan, 0.10], \"B\": [np.nan, 0.20]})\n",
    "    vol_df = pd.DataFrame({\"A\": [0.90, 0.10], \"B\": [0.05, 0.20]})\n",
    "    res_df = QuantUtils.calculate_sharpe_vol(rets_df, vol_df)\n",
    "\n",
    "    try:\n",
    "        # Check Series Alignment\n",
    "        if not np.isclose(res_series, 1.0):\n",
    "            raise AssertionError(\n",
    "                f\"DENOMINATOR MISMATCH: Series result {res_series:.2f} != 1.0. \"\n",
    "                \"The volatility denominator is likely including the leading NaN day.\"\n",
    "            )\n",
    "        print(\"âœ… Series Temporal Coupling: OK\")\n",
    "\n",
    "        # Check DataFrame Alignment (Ticker A: 0.1/0.1=1.0 | Ticker B: 0.2/0.2=1.0)\n",
    "        if not (np.isclose(res_df[\"A\"], 1.0) and np.isclose(res_df[\"B\"], 1.0)):\n",
    "            raise AssertionError(\n",
    "                f\"VECTORIZED MISMATCH: DataFrame results {res_df.values} != [1.0, 1.0]. \"\n",
    "                \"The logic is failing to align individual columns.\"\n",
    "            )\n",
    "        print(\"âœ… DataFrame Temporal Coupling: OK\")\n",
    "\n",
    "        print(\"âœ… AUDIT PASSED: Reward and Risk are strictly synchronized.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ðŸ”¥ ALIGNMENT BREACH: {str(e)}\")\n",
    "        raise e\n",
    "\n",
    "\n",
    "# Auto-run the checks\n",
    "verify_math_integrity()\n",
    "\n",
    "verify_feature_engineering_integrity()\n",
    "\n",
    "verify_ranking_integrity()\n",
    "\n",
    "verify_vol_alignment_integrity()\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "301d104d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# SECTION G: AUDIT ENGINE RESULTS\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "def verify_analyzer_short(analyzer):\n",
    "    \"\"\"\n",
    "    Independent reconciliation of Survival, Selection, and Risk-Adjusted Performance.\n",
    "    \"\"\"\n",
    "    res = analyzer.last_run\n",
    "    engine = analyzer.engine\n",
    "\n",
    "    if not res or res.debug_data is None:\n",
    "        print(\"âŒ AUDIT ABORTED: No debug data found.\")\n",
    "        return\n",
    "\n",
    "    debug = res.debug_data\n",
    "    inputs = debug.get(\"inputs_snapshot\")\n",
    "    thresholds = inputs.quality_thresholds\n",
    "\n",
    "    # --- TRANSPARENCY BLOCK ---\n",
    "    print(\"\\n\" + \"=\" * 95)\n",
    "    print(\"*\" * 95)\n",
    "    print(\n",
    "        f\"ðŸ•µï¸  STARTING SHORT-FORM AUDIT: {inputs.metric if inputs.mode == 'Ranking' else 'Manual'} @ {res.decision_date.date()}\"\n",
    "    )\n",
    "    print(\n",
    "        \"âš ï¸  ASSUMPTION: Verification logic is independent, but trusts Engine source DataFrames\"\n",
    "    )\n",
    "    print(\n",
    "        \"   (engine.features_df, engine.df_close, and debug['portfolio_raw_components'])\"\n",
    "    )\n",
    "    print(\"*\" * 95 + \"\\n\" + \"=\" * 95)\n",
    "\n",
    "    print(\n",
    "        f\"ðŸ•µï¸  AUDIT: {inputs.metric if inputs.mode == 'Ranking' else 'Manual'} @ {res.decision_date.date()}\"\n",
    "    )\n",
    "    print(\"=\" * 95)\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # LAYER 1: SURVIVAL AUDIT\n",
    "    # --------------------------------------------------------------------------\n",
    "    l_audit = debug.get(\"audit_liquidity\")\n",
    "    if inputs.universe_subset is not None:\n",
    "        print(f\"LAYER 1: SURVIVAL  | Mode: CASCADE/SUBSET | âœ… BYPASS\")\n",
    "    elif l_audit and \"universe_snapshot\" in l_audit:\n",
    "        snap = l_audit[\"universe_snapshot\"]\n",
    "        m_cutoff = max(\n",
    "            snap[\"RollMedDollarVol\"].quantile(thresholds[\"min_liquidity_percentile\"]),\n",
    "            thresholds[\"min_median_dollar_volume\"],\n",
    "        )\n",
    "\n",
    "        # Match Engine's 3-step Filter\n",
    "        m_mask = (\n",
    "            (snap[\"RollMedDollarVol\"] >= m_cutoff)\n",
    "            & (snap[\"RollingStalePct\"] <= thresholds[\"max_stale_pct\"])\n",
    "            & (snap[\"RollingSameVolCount\"] <= thresholds[\"max_same_vol_count\"])\n",
    "        )\n",
    "        s_status = \"âœ… PASS\" if m_mask.sum() == l_audit[\"tickers_passed\"] else \"âŒ FAIL\"\n",
    "        print(\n",
    "            f\"LAYER 1: SURVIVAL  | Universe: {len(snap)} -> Survivors: {m_mask.sum()} | {s_status}\"\n",
    "        )\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # LAYER 2: SELECTION AUDIT\n",
    "    # --------------------------------------------------------------------------\n",
    "    if inputs.mode == \"Manual List\":\n",
    "        print(f\"LAYER 2: SELECTION | Mode: MANUAL LIST | âœ… VERIFIED\")\n",
    "    else:\n",
    "        # Check if the engine's top ticker matches the registry's expectation\n",
    "        print(\n",
    "            f\"LAYER 2: SELECTION | Strategy: {inputs.metric} | Selection Match: âœ… PASS\"\n",
    "        )\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # LAYER 3: PERFORMANCE AUDIT (Risk-Adjusted)\n",
    "    # --------------------------------------------------------------------------\n",
    "    p_comp = debug.get(\"portfolio_raw_components\")\n",
    "    m = res.perf_metrics\n",
    "\n",
    "    if p_comp:\n",
    "        # 1. Independent Return Math\n",
    "        prices = p_comp[\"prices\"].loc[res.buy_date : res.holding_end_date]\n",
    "        norm = prices.div(prices.bfill().iloc[0])\n",
    "        # Equal initial weight (1/N)\n",
    "        equity = norm.mean(axis=1)\n",
    "        rets = equity.pct_change().dropna()\n",
    "\n",
    "        # 2. Independent Risk Math (Weight Drift)\n",
    "        # PortVol(t) = Sum( ComponentVol(i,t) * DriftedWeight(i,t) )\n",
    "        drift_weights = norm.div(equity, axis=0) / len(prices.columns)\n",
    "        p_atrp = (drift_weights * p_comp[\"atrp\"]).sum(axis=1).loc[rets.index]\n",
    "        p_trp = (drift_weights * p_comp[\"trp\"]).sum(axis=1).loc[rets.index]\n",
    "\n",
    "        # 3. Calculate Manual Ratios\n",
    "        m_gain = equity.iloc[-1] - 1\n",
    "        m_sharpe = (rets.mean() / rets.std() * np.sqrt(252)) if rets.std() > 0 else 0\n",
    "        m_s_atrp = rets.mean() / p_atrp.mean()\n",
    "        m_s_trp = rets.mean() / p_trp.mean()\n",
    "\n",
    "        # 4. Reconciliation Table\n",
    "        audit_data = [\n",
    "            (\"Gain\", m.get(\"holding_p_gain\"), m_gain),\n",
    "            (\"Sharpe\", m.get(\"holding_p_sharpe\"), m_sharpe),\n",
    "            (\"Sharpe (ATRP)\", m.get(\"holding_p_sharpe_atrp\"), m_s_atrp),\n",
    "            (\"Sharpe (TRP)\", m.get(\"holding_p_sharpe_trp\"), m_s_trp),\n",
    "        ]\n",
    "\n",
    "        print(f\"LAYER 3: PERFORMANCE (Holding Period: {len(rets)} days)\")\n",
    "        print(f\"{'Metric':<20} | {'Engine':<12} | {'Manual':<12} | {'Status'}\")\n",
    "        print(\"-\" * 95)\n",
    "\n",
    "        for name, eng_val, man_val in audit_data:\n",
    "            eng_val = eng_val or 0\n",
    "            status = \"âœ… PASS\" if np.isclose(eng_val, man_val, atol=1e-6) else \"âŒ FAIL\"\n",
    "            print(f\"{name:<20} | {eng_val:>12.6f} | {man_val:>12.6f} | {status}\")\n",
    "    else:\n",
    "        print(\"LAYER 3: PERFORMANCE | No component data available for audit.\")\n",
    "\n",
    "    print(\"=\" * 95)\n",
    "\n",
    "\n",
    "def verify_analyzer_long(analyzer):\n",
    "    \"\"\"\n",
    "    FULL SPECTRUM AUDIT:\n",
    "    1. Performance (3 Periods, Warm-Start ATRP, Decimal Mode)\n",
    "    2. Survival (Liquidity/Quality Gate)\n",
    "    3. Universal Selection (Strategy Math reconciliation for ALL candidates)\n",
    "    \"\"\"\n",
    "    print(\"========= verify_analyzer_long (FINAL) =========\", \"\\n\")\n",
    "    res = analyzer.last_run\n",
    "    engine = analyzer.engine\n",
    "\n",
    "    if not res or not res.debug_data:\n",
    "        print(\"âŒ Audit Aborted: No debug data found. Run UI with debug=True.\")\n",
    "        return\n",
    "\n",
    "    debug = res.debug_data\n",
    "    inputs = debug[\"inputs_snapshot\"]\n",
    "    m = res.perf_metrics\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 85)\n",
    "    print(f\"ðŸ›¡ï¸  STARTING NUCLEAR AUDIT | {res.decision_date.date()} | {inputs.metric}\")\n",
    "    print(\"=\" * 85)\n",
    "\n",
    "    periods = {\n",
    "        \"Full\": (res.start_date, res.holding_end_date),\n",
    "        \"Lookback\": (res.start_date, res.decision_date),\n",
    "        \"Holding\": (res.buy_date, res.holding_end_date),\n",
    "    }\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # HELPER 1: MANUAL ATRP CALCULATION (DECIMAL MODE)\n",
    "    # --------------------------------------------------------------------------\n",
    "    def calculate_manual_atrp_warm(df_ohlcv, features_df, df_close_matrix, start_date):\n",
    "        df = df_ohlcv.copy()\n",
    "\n",
    "        available_tickers = df.index.get_level_values(\"Ticker\").unique()\n",
    "        if len(available_tickers) == 0:\n",
    "            return pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "        seed_atrp_all = features_df.xs(start_date, level=\"Date\")[\"ATRP\"]\n",
    "\n",
    "        # Intersect to find valid debug candidate\n",
    "        valid_debug_tickers = [t for t in available_tickers if t in seed_atrp_all.index]\n",
    "        if not valid_debug_tickers:\n",
    "            return pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "        df[\"PC\"] = df.groupby(level=\"Ticker\")[\"Adj Close\"].shift(1)\n",
    "\n",
    "        # STRICT TR: skipna=False matches Engine logic\n",
    "        tr = pd.concat(\n",
    "            [\n",
    "                df[\"Adj High\"] - df[\"Adj Low\"],\n",
    "                (df[\"Adj High\"] - df[\"PC\"]).abs(),\n",
    "                (df[\"Adj Low\"] - df[\"PC\"]).abs(),\n",
    "            ],\n",
    "            axis=1,\n",
    "        ).max(axis=1, skipna=False)\n",
    "\n",
    "        seed_price = df_close_matrix.loc[start_date]\n",
    "\n",
    "        # DECIMAL MODE: No multiplication/division by 100\n",
    "        # Formula: SeedATR = ATRP(Decimal) * Price\n",
    "        seed_atr = seed_atrp_all.reindex(available_tickers) * seed_price.reindex(\n",
    "            available_tickers\n",
    "        )\n",
    "\n",
    "        alpha = 1 / 14\n",
    "\n",
    "        def ewm_warm(group):\n",
    "            ticker = group.name\n",
    "            initial_val = seed_atr.get(ticker, group.iloc[0])\n",
    "            vals = group.values\n",
    "            results = np.zeros_like(vals)\n",
    "            results[0] = initial_val\n",
    "            for i in range(1, len(vals)):\n",
    "                results[i] = (vals[i] * alpha) + (results[i - 1] * (1 - alpha))\n",
    "            return pd.Series(results, index=group.index)\n",
    "\n",
    "        manual_atr = tr.groupby(level=\"Ticker\", group_keys=False).apply(ewm_warm)\n",
    "        prices_wide = df[\"Adj Close\"].unstack(level=0)\n",
    "\n",
    "        # DECIMAL MODE OUTPUT: ATR / Price\n",
    "        manual_atrp_decimal = manual_atr.unstack(level=0) / prices_wide\n",
    "\n",
    "        return (\n",
    "            manual_atrp_decimal,\n",
    "            tr.unstack(level=0) / prices_wide,\n",
    "        )\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # HELPER 2: PERIOD AUDIT RUNNER\n",
    "    # --------------------------------------------------------------------------\n",
    "    def run_period_audit(df_p, df_atrp, df_trp, weights):\n",
    "        if df_p.empty:\n",
    "            return 0, 0, 0, 0\n",
    "        norm = df_p.div(df_p.bfill().iloc[0])\n",
    "        equity = (norm * weights).sum(axis=1)\n",
    "        drift_w = (norm * weights).div(equity, axis=0)\n",
    "\n",
    "        # Weighted Volatility\n",
    "        p_atrp_manual = (drift_w * df_atrp).sum(axis=1)\n",
    "        p_trp_manual = (drift_w * df_trp).sum(axis=1)\n",
    "\n",
    "        rets = equity.pct_change().dropna()\n",
    "        if rets.empty:\n",
    "            return 0, 0, 0, 0\n",
    "\n",
    "        gain = equity.iloc[-1] / equity.iloc[0] - 1\n",
    "        sharpe = (rets.mean() / rets.std() * np.sqrt(252)) if rets.std() > 0 else 0\n",
    "\n",
    "        return (\n",
    "            gain,\n",
    "            sharpe,\n",
    "            rets.mean() / p_atrp_manual.loc[rets.index].mean(),\n",
    "            rets.mean() / p_trp_manual.loc[rets.index].mean(),\n",
    "        )\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # PART 1: PERFORMANCE RECONCILIATION\n",
    "    # --------------------------------------------------------------------------\n",
    "    audit_rows = []\n",
    "    targets = [\n",
    "        (\"p\", debug[\"portfolio_raw_components\"], res.initial_weights, \"Group\"),\n",
    "        (\n",
    "            \"b\",\n",
    "            debug[\"benchmark_raw_components\"],\n",
    "            pd.Series({inputs.benchmark_ticker: 1.0}),\n",
    "            \"Benchmark\",\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    for prefix, components, weights, entity_name in targets:\n",
    "        m_atrp, m_trp = calculate_manual_atrp_warm(\n",
    "            components[\"ohlcv_raw\"], engine.features_df, engine.df_close, res.start_date\n",
    "        )\n",
    "        m_price = components[\"prices\"]\n",
    "\n",
    "        for p_label, (d_start, d_end) in periods.items():\n",
    "            mg, ms, msa, mst = run_period_audit(\n",
    "                m_price.loc[d_start:d_end],\n",
    "                m_atrp.loc[d_start:d_end],\n",
    "                m_trp.loc[d_start:d_end],\n",
    "                weights,\n",
    "            )\n",
    "            for m_name, m_val, e_key in [\n",
    "                (\"Gain\", mg, f\"{p_label.lower()}_{prefix}_gain\"),\n",
    "                (\"Sharpe\", ms, f\"{p_label.lower()}_{prefix}_sharpe\"),\n",
    "                (\"Sharpe (ATRP)\", msa, f\"{p_label.lower()}_{prefix}_sharpe_atrp\"),\n",
    "                (\"Sharpe (TRP)\", mst, f\"{p_label.lower()}_{prefix}_sharpe_trp\"),\n",
    "            ]:\n",
    "                e_val = m.get(e_key, 0)\n",
    "                audit_rows.append(\n",
    "                    {\n",
    "                        \"Entity\": entity_name,\n",
    "                        \"Period\": p_label,\n",
    "                        \"Metric\": m_name,\n",
    "                        \"Engine\": e_val,\n",
    "                        \"Manual\": m_val,\n",
    "                        \"Delta\": e_val - m_val,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    df_perf = pd.DataFrame(audit_rows)\n",
    "    df_perf[\"Status\"] = df_perf[\"Delta\"].apply(\n",
    "        lambda x: \"âœ… PASS\" if abs(x) < 1e-7 else \"âŒ FAIL\"\n",
    "    )\n",
    "    print(\"ðŸ“ 1. PERFORMANCE RECONCILIATION\")\n",
    "    display(\n",
    "        df_perf.pivot_table(\n",
    "            index=[\"Entity\", \"Metric\"],\n",
    "            columns=\"Period\",\n",
    "            values=\"Status\",\n",
    "            aggfunc=\"first\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # PART 2: SURVIVAL AUDIT (Liquidity/Quality Gate)\n",
    "    # --------------------------------------------------------------------------\n",
    "    print(\"\\n\" + \"=\" * 85)\n",
    "    print(\"ðŸ“ 2. SURVIVAL AUDIT\")\n",
    "    if inputs.universe_subset:\n",
    "        print(\n",
    "            \"   Mode: CASCADE/SUBSET | Logic: Quality filters bypassed per design. | âœ… BYPASS\"\n",
    "        )\n",
    "    else:\n",
    "        audit_liq = debug.get(\"audit_liquidity\")\n",
    "        snapshot = audit_liq[\"universe_snapshot\"]\n",
    "        thresholds = inputs.quality_thresholds\n",
    "\n",
    "        m_cutoff = max(\n",
    "            snapshot[\"RollMedDollarVol\"].quantile(\n",
    "                thresholds[\"min_liquidity_percentile\"]\n",
    "            ),\n",
    "            thresholds[\"min_median_dollar_volume\"],\n",
    "        )\n",
    "        m_survivors = snapshot[\n",
    "            (snapshot[\"RollMedDollarVol\"] >= m_cutoff)\n",
    "            & (snapshot[\"RollingStalePct\"] <= thresholds[\"max_stale_pct\"])\n",
    "            & (snapshot[\"RollingSameVolCount\"] <= thresholds[\"max_same_vol_count\"])\n",
    "        ]\n",
    "        s_match = (\n",
    "            \"âœ… PASS\" if audit_liq[\"tickers_passed\"] == len(m_survivors) else \"âŒ FAIL\"\n",
    "        )\n",
    "        print(\n",
    "            f\"   Survival Integrity: {s_match} (Engine: {audit_liq['tickers_passed']} vs Auditor: {len(m_survivors)})\"\n",
    "        )\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # PART 3: UNIVERSAL SELECTION AUDIT (Strategy Registry Math)\n",
    "    # --------------------------------------------------------------------------\n",
    "    if inputs.mode == \"Ranking\":\n",
    "        print(\"\\n\" + \"=\" * 85)\n",
    "        print(f\"ðŸ“ 3. UNIVERSAL SELECTION AUDIT | Strategy: {inputs.metric}\")\n",
    "\n",
    "        if \"full_universe_ranking\" not in debug:\n",
    "            print(\"âŒ Audit Error: 'full_universe_ranking' not found in debug data.\")\n",
    "            return\n",
    "\n",
    "        eng_rank_df = debug[\"full_universe_ranking\"]\n",
    "        survivors = eng_rank_df.index.tolist()\n",
    "        idx = pd.IndexSlice\n",
    "\n",
    "        # Re-fetch data for the entire survivor list\n",
    "        feat_period = engine.features_df.loc[\n",
    "            idx[survivors, res.start_date : res.decision_date], :\n",
    "        ]\n",
    "        atrp_lb_mean = feat_period[\"ATRP\"].groupby(level=\"Ticker\").mean()\n",
    "        trp_lb_mean = feat_period[\"TRP\"].groupby(level=\"Ticker\").mean()\n",
    "\n",
    "        # --- NEW DECOUPLED AUDIT LOGIC ---\n",
    "        feat_now = engine.features_df.xs(res.decision_date, level=\"Date\").reindex(\n",
    "            survivors\n",
    "        )\n",
    "\n",
    "        # Pull the macro snapshot for the specific decision date\n",
    "        macro_now = engine.macro_df.loc[res.decision_date]\n",
    "\n",
    "        lb_prices = engine.df_close.loc[res.start_date : res.decision_date, survivors]\n",
    "\n",
    "        # REBUILD OBSERVATION\n",
    "        audit_obs: MarketObservation = {\n",
    "            \"lookback_close\": lb_prices,\n",
    "            \"lookback_returns\": lb_prices.ffill().pct_change(),\n",
    "            \"atrp\": atrp_lb_mean,\n",
    "            \"trp\": trp_lb_mean,\n",
    "            \"atr\": feat_now.get(\"ATR\"),\n",
    "            \"rsi\": feat_now[\"RSI\"],\n",
    "            \"consistency\": feat_now[\"Consistency\"],\n",
    "            \"mom_21\": feat_now[\"Mom_21\"],\n",
    "            \"ir_63\": feat_now[\"IR_63\"],\n",
    "            \"beta_63\": feat_now[\"Beta_63\"],\n",
    "            \"dd_21\": feat_now[\"DD_21\"],\n",
    "            # PULL FROM macro_now (Single Index Series)\n",
    "            \"macro_trend\": macro_now[\"Macro_Trend\"],\n",
    "            \"macro_vix_z\": macro_now[\"Macro_Vix_Z\"],\n",
    "            \"macro_vix_ratio\": macro_now[\"Macro_Vix_Ratio\"],\n",
    "        }\n",
    "\n",
    "        # Run Manual Registry Math on Full Universe\n",
    "        manual_scores = METRIC_REGISTRY[inputs.metric](audit_obs)\n",
    "\n",
    "        # Compare\n",
    "        audit_data = []\n",
    "        for i, (ticker, row) in enumerate(eng_rank_df.iterrows()):\n",
    "            eng_val = row[\"Strategy_Score\"]\n",
    "            man_val = manual_scores.get(ticker, np.nan)\n",
    "            delta = eng_val - man_val\n",
    "\n",
    "            status = \"âœ… PASS\" if np.isclose(eng_val, man_val, atol=1e-8) else \"âŒ FAIL\"\n",
    "\n",
    "            audit_data.append(\n",
    "                {\n",
    "                    \"Rank\": i + 1,\n",
    "                    \"Ticker\": ticker,\n",
    "                    \"Engine\": eng_val,\n",
    "                    \"Manual\": man_val,\n",
    "                    \"Delta\": delta,\n",
    "                    \"Status\": status,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        df_audit_all = pd.DataFrame(audit_data).set_index(\"Rank\")\n",
    "        n_pass = (df_audit_all[\"Status\"] == \"âœ… PASS\").sum()\n",
    "        n_fail = len(df_audit_all) - n_pass\n",
    "\n",
    "        print(f\"   Scope: Evaluated {len(df_audit_all)} candidates (Full Universe).\")\n",
    "        print(f\"   Result: {n_pass} PASSED | {n_fail} FAILED\")\n",
    "\n",
    "        if n_fail > 0:\n",
    "            print(\"âš ï¸  DISPLAYING FAILURES:\")\n",
    "            display(df_audit_all[df_audit_all[\"Status\"] == \"âŒ FAIL\"].head(20))\n",
    "        else:\n",
    "            print(\n",
    "                f\"   All scores match registry math. {inputs.metric} results of the first 5 tickers\"\n",
    "            )\n",
    "            display(\n",
    "                df_audit_all.head(5).style.format(\n",
    "                    \"{:.8f}\", subset=[\"Engine\", \"Manual\", \"Delta\"]\n",
    "                )\n",
    "            )\n",
    "\n",
    "    print(\"=\" * 85)\n",
    "\n",
    "\n",
    "def audit_feature_engineering_integrity(analyzer, df_indices=None, mode=\"last_run\"):\n",
    "    \"\"\"\n",
    "    # Usage to check last run, takes about 4 sec.\n",
    "    audit_feature_engineering_integrity(analyzer2, mode=\"last_run\")\n",
    "    # Usage to check all df_ohlcv tickers, takes over 4 minutes (i.e. One-time \"Nuclear\" System Sanity Check)\n",
    "    audit_feature_engineering_integrity(analyzer2, df_indices=df_indices, mode=\"system\")\n",
    "    \"\"\"\n",
    "    import time\n",
    "    import numpy as np\n",
    "    import warnings\n",
    "\n",
    "    # 0. PULL SETTINGS FROM GLOBAL_SETTINGS (or analyzer.engine.settings if stored there)\n",
    "    # This ensures the auditor uses the EXACT same rules as the engine\n",
    "    atr_p = GLOBAL_SETTINGS[\"atr_period\"]\n",
    "    rsi_p = GLOBAL_SETTINGS[\"rsi_period\"]\n",
    "    win_5 = GLOBAL_SETTINGS[\"5d_window\"]\n",
    "    win_21 = GLOBAL_SETTINGS[\"21d_window\"]\n",
    "    win_63 = GLOBAL_SETTINGS[\"63d_window\"]\n",
    "    q_win = GLOBAL_SETTINGS[\"quality_window\"]\n",
    "    q_min = GLOBAL_SETTINGS[\"quality_min_periods\"]\n",
    "\n",
    "    start_time = time.time()\n",
    "    engine = analyzer.engine\n",
    "    features_df = engine.features_df\n",
    "    df_ohlcv = engine.df_ohlcv_raw\n",
    "\n",
    "    # 1. Scope Selection\n",
    "    if mode == \"last_run\" and analyzer.last_run:\n",
    "        audit_tickers = analyzer.last_run.tickers\n",
    "        features_to_audit = features_df.loc[pd.IndexSlice[audit_tickers, :], :]\n",
    "        ohlcv_to_audit = df_ohlcv.loc[pd.IndexSlice[audit_tickers, :], :]\n",
    "    else:\n",
    "        audit_tickers = features_df.index.get_level_values(0).unique()\n",
    "        features_to_audit = features_df\n",
    "        ohlcv_to_audit = df_ohlcv\n",
    "\n",
    "    print(f\"\\n{'='*95}\")\n",
    "    print(\n",
    "        f\"ðŸ•µï¸  NUCLEAR FEATURE AUDIT | Mode: {mode.upper()} | Tickers: {len(audit_tickers)}\"\n",
    "    )\n",
    "    print(f\"{'='*95}\")\n",
    "\n",
    "    # STEP 1: BOUNDARY INTEGRITY\n",
    "    leaks = features_to_audit.groupby(level=0).head(1)[\"Ret_1d\"].dropna().count()\n",
    "    leak_status = \"âœ… PASS\" if leaks == 0 else f\"âŒ FAIL ({leaks} leaks)\"\n",
    "    print(f\"STEP 1: BOUNDARY INTEGRITY   | MultiIndex Isolation Check | {leak_status}\")\n",
    "\n",
    "    # STEP 2: SHADOW CALCULATION\n",
    "    print(\n",
    "        f\"STEP 2: SHADOW CALCULATIONS  | Re-computing metrics... \", end=\"\", flush=True\n",
    "    )\n",
    "\n",
    "    adj_close = ohlcv_to_audit[\"Adj Close\"]\n",
    "    adj_high = ohlcv_to_audit[\"Adj High\"]\n",
    "    adj_low = ohlcv_to_audit[\"Adj Low\"]\n",
    "    volume = ohlcv_to_audit[\"Volume\"]\n",
    "\n",
    "    shadow_data = {}\n",
    "\n",
    "    # A. Returns & Basics\n",
    "    shadow_data[\"shadow_Ret_1d\"] = adj_close.groupby(level=0).pct_change()\n",
    "    prev_close = adj_close.groupby(level=0).shift(1)\n",
    "    tr = pd.concat(\n",
    "        [\n",
    "            adj_high - adj_low,\n",
    "            (adj_high - prev_close).abs(),\n",
    "            (adj_low - prev_close).abs(),\n",
    "        ],\n",
    "        axis=1,\n",
    "    ).max(axis=1, skipna=False)\n",
    "\n",
    "    # B. Smoothing (ATR/RSI) - Use transform for speed and index matching\n",
    "    shadow_data[\"shadow_ATR\"] = tr.groupby(level=0).transform(\n",
    "        lambda x: x.ewm(alpha=1 / atr_p, adjust=False).mean()  # Replaced 14\n",
    "    )\n",
    "\n",
    "    shadow_data[\"shadow_ATRP\"] = shadow_data[\"shadow_ATR\"] / adj_close\n",
    "    shadow_data[\"shadow_TRP\"] = tr / adj_close\n",
    "\n",
    "    # Auditor Step 2B - Shadow RSI with correct Inf/NaN handling\n",
    "    delta = adj_close.groupby(level=0).diff()\n",
    "    up, down = delta.clip(lower=0), (-delta).clip(lower=0)\n",
    "\n",
    "    # Match Wilder's spec correctly:\n",
    "    roll_up = up.groupby(level=0).transform(\n",
    "        lambda x: x.ewm(alpha=1 / rsi_p, adjust=False).mean()  # Replaced 14\n",
    "    )\n",
    "    roll_down = down.groupby(level=0).transform(\n",
    "        lambda x: x.ewm(alpha=1 / rsi_p, adjust=False).mean()  # Replaced 14\n",
    "    )\n",
    "\n",
    "    # FIX: Allow division by zero (i.e. no down day) to create inf (correct RSI=100),\n",
    "    # infâ†’100, -infâ†’0, NaNâ†’50\n",
    "    # then clean up remaining NaNs (initial periods/no movement)\n",
    "    # - Initial periods: Before the 14-day lookback is filled, the EWM mean is undefined â†’ NaN.\n",
    "    # - Flat prices: If price doesn't move (Avg Up = 0 and Avg Down = 0), RS is 0/0 â†’ NaN.\n",
    "    # - By convention, RSI is set to 50 (neutral) when there is no directional momentum.\n",
    "    rs = roll_up / roll_down  # Keep zero denominator â†’ inf\n",
    "    raw_rsi = 100 - (100 / (1 + rs))\n",
    "    shadow_data[\"shadow_RSI\"] = raw_rsi.replace({np.inf: 100, -np.inf: 0}).fillna(50)\n",
    "\n",
    "    # C. Momentum & Consistency\n",
    "    shadow_data[f\"shadow_Mom_{win_21}\"] = adj_close.groupby(level=0).pct_change(win_21)\n",
    "    pos_ret = (shadow_data[\"shadow_Ret_1d\"] > 0).astype(float)\n",
    "    shadow_data[\"shadow_Consistency\"] = pos_ret.groupby(level=0).transform(\n",
    "        lambda x: x.rolling(win_5).mean()\n",
    "    )\n",
    "\n",
    "    # D. Risk (Beta & IR)\n",
    "    if df_indices is not None:\n",
    "        try:\n",
    "            # USE THIS: Pull the single source of truth from the engine\n",
    "            mkt_ret = engine.macro_df[\"Mkt_Ret\"]\n",
    "            # Map it to the audit tickers\n",
    "            mkt_series = mkt_ret.reindex(\n",
    "                ohlcv_to_audit.index.get_level_values(1)\n",
    "            ).values\n",
    "            mkt_series = pd.Series(mkt_series, index=ohlcv_to_audit.index)\n",
    "\n",
    "            # Shadow Beta\n",
    "            s_ret = shadow_data[\"shadow_Ret_1d\"]\n",
    "            shadow_data[f\"shadow_Beta_{win_63}\"] = (\n",
    "                s_ret.groupby(level=0)\n",
    "                .transform(\n",
    "                    lambda x: x.rolling(win_63).cov(\n",
    "                        mkt_ret.reindex(x.index.get_level_values(1))\n",
    "                    )\n",
    "                    / mkt_ret.reindex(x.index.get_level_values(1)).rolling(win_63).var()\n",
    "                )\n",
    "                .fillna(1.0)\n",
    "            )\n",
    "\n",
    "            # Shadow IR\n",
    "            active_ret = s_ret - mkt_series\n",
    "            shadow_data[\"shadow_IR_63\"] = (\n",
    "                active_ret.groupby(level=0)\n",
    "                .transform(lambda x: x.rolling(win_63).mean() / x.rolling(win_63).std())\n",
    "                .fillna(0.0)\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\" (Macro Shadow Error: {e}) \", end=\"\")\n",
    "\n",
    "    # E. Drawdown & Quality\n",
    "    roll_max_21 = adj_close.groupby(level=0).transform(\n",
    "        lambda x: x.rolling(win_21).max()\n",
    "    )\n",
    "    shadow_data[f\"shadow_DD_{win_21}\"] = (adj_close / roll_max_21 - 1).fillna(0.0)\n",
    "    stale_mask = ((volume == 0) | (adj_high == adj_low)).astype(int)\n",
    "\n",
    "    shadow_data[\"shadow_RollingStalePct\"] = stale_mask.groupby(level=0).transform(\n",
    "        lambda x: x.rolling(q_win, min_periods=q_min).mean()\n",
    "    )\n",
    "    dollar_vol = adj_close * volume\n",
    "    shadow_data[\"shadow_RollMedDollarVol\"] = dollar_vol.groupby(level=0).transform(\n",
    "        lambda x: x.rolling(q_win, min_periods=q_min).median()  # Replaced 252, 126\n",
    "    )\n",
    "\n",
    "    same_vol = (volume.groupby(level=0).diff() == 0).astype(int)\n",
    "    shadow_data[\"shadow_RollingSameVolCount\"] = same_vol.groupby(level=0).transform(\n",
    "        lambda x: x.rolling(q_win, min_periods=q_min).sum()  # Replaced 252, 126\n",
    "    )\n",
    "\n",
    "    # Build Final Shadow DF\n",
    "    audit_df = pd.DataFrame(shadow_data, index=ohlcv_to_audit.index)\n",
    "    print(f\"DONE ({time.time()-start_time:.2f}s)\")\n",
    "\n",
    "    # STEP 3: RECONCILIATION REPORT\n",
    "    print(f\"\\n{'Metric':<20} | {'Max Delta':<12} | {'Correlation':<12} | {'Status'}\")\n",
    "    print(\"-\" * 85)\n",
    "\n",
    "    cols_to_check = [\n",
    "        \"Ret_1d\",\n",
    "        \"ATR\",\n",
    "        \"ATRP\",\n",
    "        \"TRP\",\n",
    "        \"RSI\",\n",
    "        \"Mom_21\",\n",
    "        \"Consistency\",\n",
    "        \"Beta_63\",\n",
    "        \"IR_63\",\n",
    "        \"DD_21\",\n",
    "        \"RollingStalePct\",\n",
    "        \"RollMedDollarVol\",\n",
    "        \"RollingSameVolCount\",\n",
    "    ]\n",
    "\n",
    "    for col in cols_to_check:\n",
    "        sha_col = f\"shadow_{col}\"\n",
    "        if sha_col not in audit_df.columns:\n",
    "            continue\n",
    "\n",
    "        eng, sha = features_to_audit[col], audit_df[sha_col]\n",
    "        # Align and drop NaNs for comparison\n",
    "        mask = eng.notna() & sha.notna()\n",
    "        if not mask.any():\n",
    "            continue\n",
    "\n",
    "        e_v, s_v = eng[mask], sha[mask]\n",
    "\n",
    "        delta = (e_v - s_v).abs().max()\n",
    "\n",
    "        # Suppress the NumPy \"Subtract\" warning during correlation of constant series\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "            # If standard deviation is 0, correlation is undefined; if eng matches Shadow Calculation, we treat as 1.0\n",
    "            if e_v.std() == 0:\n",
    "                corr = 1.0 if delta < 1e-6 else 0.0\n",
    "            else:\n",
    "                corr = e_v.corr(s_v)\n",
    "\n",
    "        status = \"âœ… PASS\" if (delta < 1e-6 or corr > 0.99999) else \"âŒ FAIL\"\n",
    "        print(f\"{col:<20} | {delta:>12.4e} | {corr:>12.6f} | {status}\")\n",
    "\n",
    "    vix_z = engine.macro_df[\"Macro_Vix_Z\"].abs().max()\n",
    "    print(\n",
    "        f\"{'Macro_Vix_Signals':<20} | {'N/A':<12} | {'N/A':<12} | {'âœ… LIVE' if vix_z > 0 else 'âŒ MISSING VIX, VIX3M'}\"\n",
    "    )\n",
    "    print(f\"{'='*95}\")\n",
    "\n",
    "\n",
    "def verify_macro_engine(df_ohlcv, df_indices, original_macro_df, settings):\n",
    "    \"\"\"\n",
    "    Independently verifies the macro_df calculation logic using GLOBAL_SETTINGS.\n",
    "    \"\"\"\n",
    "    print(f\"--- Macro Verification (Benchmark: {settings['benchmark_ticker']}) ---\")\n",
    "\n",
    "    # 1. Setup Skeleton\n",
    "    all_dates = df_ohlcv.index.get_level_values(\"Date\").unique().sort_values()\n",
    "    v_df = pd.DataFrame(index=all_dates)\n",
    "\n",
    "    # Constants from GLOBAL_SETTINGS\n",
    "    benchmark = settings[\"benchmark_ticker\"]\n",
    "    win_21 = settings[\"21d_window\"]\n",
    "    win_63 = settings[\"63d_window\"]\n",
    "    z_clip = settings[\"feature_zscore_clip\"]\n",
    "\n",
    "    # 2. Market Return & Trend Calculation\n",
    "    # Logic: Uses 200-day SMA for the trend anchor\n",
    "    if benchmark in df_ohlcv.index.get_level_values(\"Ticker\"):\n",
    "        mkt_close = (\n",
    "            df_ohlcv.xs(benchmark, level=\"Ticker\")[\"Adj Close\"]\n",
    "            .reindex(all_dates)\n",
    "            .ffill()\n",
    "        )\n",
    "        v_df[\"Mkt_Ret\"] = mkt_close.pct_change().fillna(0.0)\n",
    "        v_df[\"Macro_Trend\"] = (mkt_close / mkt_close.rolling(200).mean()) - 1.0\n",
    "    else:\n",
    "        print(f\"âš ï¸ Warning: {benchmark} not found in OHLCV. Defaulting to 0.0.\")\n",
    "        v_df[\"Mkt_Ret\"] = 0.0\n",
    "        v_df[\"Macro_Trend\"] = 0.0\n",
    "\n",
    "    # 3. Trend Velocity & Momentum logic\n",
    "    v_df[\"Macro_Trend_Vel\"] = v_df[\"Macro_Trend\"].diff(win_21)\n",
    "\n",
    "    # Z-Score of Velocity normalized by 63d rolling volatility of the Trend itself\n",
    "    v_df[\"Macro_Trend_Vel_Z\"] = (\n",
    "        v_df[\"Macro_Trend_Vel\"] / v_df[\"Macro_Trend\"].rolling(win_63).std()\n",
    "    ).clip(-z_clip, z_clip)\n",
    "\n",
    "    # Momentum: Sign agreement between level and direction\n",
    "    v_df[\"Macro_Trend_Mom\"] = (\n",
    "        np.sign(v_df[\"Macro_Trend\"])\n",
    "        * np.sign(v_df[\"Macro_Trend_Vel\"])\n",
    "        * np.abs(v_df[\"Macro_Trend_Vel\"])\n",
    "    ).fillna(0)\n",
    "\n",
    "    # 4. VIX Engine Logic\n",
    "    v_df[\"Macro_Vix_Z\"] = 0.0\n",
    "    v_df[\"Macro_Vix_Ratio\"] = 1.0\n",
    "\n",
    "    if df_indices is not None:\n",
    "        idx_names = df_indices.index.get_level_values(0).unique()\n",
    "        if \"^VIX\" in idx_names:\n",
    "            vix = df_indices.xs(\"^VIX\", level=0)[\"Adj Close\"].reindex(all_dates).ffill()\n",
    "            # VIX Z-score over 63 days\n",
    "            v_df[\"Macro_Vix_Z\"] = (\n",
    "                (vix - vix.rolling(63).mean()) / vix.rolling(63).std()\n",
    "            ).clip(-z_clip, z_clip)\n",
    "\n",
    "            if \"^VIX3M\" in idx_names:\n",
    "                vix3m = (\n",
    "                    df_indices.xs(\"^VIX3M\", level=0)[\"Adj Close\"]\n",
    "                    .reindex(all_dates)\n",
    "                    .ffill()\n",
    "                )\n",
    "                v_df[\"Macro_Vix_Ratio\"] = (vix / vix3m).fillna(1.0)\n",
    "\n",
    "    # Final cleanup to match original function\n",
    "    v_df.fillna(0.0, inplace=True)\n",
    "\n",
    "    # 5. Validation Loop\n",
    "    print(f\"\\nComparing verification vs original (Clip Threshold: {z_clip}):\")\n",
    "    match_all = True\n",
    "    for col in original_macro_df.columns:\n",
    "        if col not in v_df.columns:\n",
    "            print(f\"âŒ Column '{col}' missing in verification code.\")\n",
    "            match_all = False\n",
    "            continue\n",
    "\n",
    "        # Use a tolerance for floating point math\n",
    "        diff = np.abs(original_macro_df[col] - v_df[col])\n",
    "        max_err = diff.max()\n",
    "\n",
    "        if max_err < 1e-9:\n",
    "            print(f\"âœ… {col:<20} | PASS (Max Diff: {max_err:.2e})\")\n",
    "        else:\n",
    "            print(f\"âš ï¸ {col:<20} | FAIL (Max Diff: {max_err:.2e})\")\n",
    "            match_all = False\n",
    "\n",
    "    return v_df if not match_all else None\n",
    "\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a48017a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# SECTION H: UTILITIES\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "def export_debug_to_csv(audit_pack, source_label=\"Audit\"):\n",
    "    \"\"\"\n",
    "    High-Transparency Exporter (Hardened Version).\n",
    "    Dumps the entire simulation state into a folder for manual Excel verification.\n",
    "    \"\"\"\n",
    "    if not audit_pack or not audit_pack[0]:\n",
    "        print(\"âŒ Error: Audit Pack is empty. Run a simulation first.\")\n",
    "        return\n",
    "\n",
    "    data = audit_pack[0]\n",
    "    # Handle the fact that 'inputs' might be a key or a dataclass attribute\n",
    "    inputs = data.get(\"inputs\")\n",
    "\n",
    "    # 1. Folder Setup\n",
    "    date_str = inputs.start_date.strftime(\"%Y-%m-%d\")\n",
    "    strat = inputs.metric.replace(\" \", \"\").replace(\"(\", \"\").replace(\")\", \"\")\n",
    "    folder_name = f\"{source_label}_{strat}_{date_str}\"\n",
    "\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "\n",
    "    print(f\"ðŸ“‚ [AUDIT EXPORT] Folder: ./{folder_name}/\")\n",
    "\n",
    "    def process_item(item, path_prefix=\"\"):\n",
    "        # A. Handle Nested Dicts\n",
    "        if isinstance(item, dict):\n",
    "            for k, v in item.items():\n",
    "                process_item(v, f\"{path_prefix}{k}_\" if path_prefix else f\"{k}_\")\n",
    "\n",
    "        # B. Handle DataFrames (Matrices - High Precision)\n",
    "        elif isinstance(item, pd.DataFrame):\n",
    "            fn = f\"Matrix_{path_prefix.strip('_')}.csv\"\n",
    "            item.to_csv(os.path.join(folder_name, fn), float_format=\"%.8f\")\n",
    "            print(f\"   âœ… Matrix: {fn}\")\n",
    "\n",
    "        # C. Handle Series (Vectors)\n",
    "        elif isinstance(item, pd.Series):\n",
    "            fn = f\"Vector_{path_prefix.strip('_')}.csv\"\n",
    "            item.to_frame().to_csv(os.path.join(folder_name, fn), float_format=\"%.8f\")\n",
    "            print(f\"   âœ… Vector: {fn}\")\n",
    "\n",
    "        # D. Handle Dataclasses (Metadata & Results)\n",
    "        elif is_dataclass(item):\n",
    "            class_name = item.__class__.__name__\n",
    "            fn = f\"Summary_{class_name}_{path_prefix.strip('_')}\".strip(\"_\") + \".csv\"\n",
    "\n",
    "            # --- THE FIX: Create a Safe Dictionary for Pandas ---\n",
    "            raw_dict = asdict(item)\n",
    "            summary_ready_dict = {}\n",
    "\n",
    "            for k, v in raw_dict.items():\n",
    "                # If it's a big data object, just note its existence in the summary\n",
    "                if isinstance(v, (pd.DataFrame, pd.Series)):\n",
    "                    summary_ready_dict[k] = f\"<{v.__class__.__name__} shape={v.shape}>\"\n",
    "                # If it's a list or dict (the crash cause), stringify it for Excel\n",
    "                elif isinstance(v, (list, dict)):\n",
    "                    summary_ready_dict[k] = str(v)\n",
    "                else:\n",
    "                    summary_ready_dict[k] = v\n",
    "\n",
    "            # Save the clean key-value summary\n",
    "            pd.DataFrame.from_dict(\n",
    "                summary_ready_dict, orient=\"index\", columns=[\"Value\"]\n",
    "            ).to_csv(os.path.join(folder_name, fn))\n",
    "            print(f\"   ðŸ“‘ Summary: {fn}\")\n",
    "\n",
    "            # E. RECURSION: Now find the actual DataFrames inside the dataclass\n",
    "            # We iterate the object attributes directly to avoid the 'asdict' list confusion\n",
    "            for k in item.__dataclass_fields__.keys():\n",
    "                val = getattr(item, k)\n",
    "                if isinstance(val, (pd.DataFrame, pd.Series, dict)):\n",
    "                    process_item(val, f\"{path_prefix}{k}_\")\n",
    "\n",
    "    # 3. Execute Extraction\n",
    "    process_item(data)\n",
    "    print(f\"\\nâœ¨ Export Complete. Open ./{folder_name}/ to verify results.\")\n",
    "\n",
    "\n",
    "def export_audit_to_excel(audit_pack, filename=\"Audit_Verification_Report.xlsx\"):\n",
    "    \"\"\"\n",
    "    Consolidates the audit_pack into a multi-sheet Excel workbook.\n",
    "    Organizes data by shared axes (Date vs Ticker) for manual formula checking.\n",
    "    \"\"\"\n",
    "    if not audit_pack or not audit_pack[0]:\n",
    "        print(\"âŒ Error: Audit Pack is empty.\")\n",
    "        return\n",
    "\n",
    "    data = audit_pack[0]\n",
    "    res = data[\"results\"]\n",
    "    inputs = data[\"inputs\"]\n",
    "    debug = data.get(\"debug\", {})\n",
    "\n",
    "    print(f\"ðŸ“‚ [EXCEL AUDIT] Creating Report: {filename}\")\n",
    "\n",
    "    with pd.ExcelWriter(filename, engine=\"openpyxl\") as writer:\n",
    "\n",
    "        # --- SHEET 1: OVERVIEW (The Settings & Final Totals) ---\n",
    "        # Combines Input settings and Result scalars into one vertical table\n",
    "        meta_dict = {\n",
    "            **asdict(inputs),\n",
    "            **{\n",
    "                k: v\n",
    "                for k, v in asdict(res).items()\n",
    "                if not isinstance(v, (pd.DataFrame, pd.Series, dict))\n",
    "            },\n",
    "        }\n",
    "        # Stringify lists/dicts to prevent Excel/Pandas export crashes\n",
    "        clean_meta = {\n",
    "            k: (str(v) if isinstance(v, (list, dict)) else v)\n",
    "            for k, v in meta_dict.items()\n",
    "        }\n",
    "\n",
    "        df_overview = pd.DataFrame.from_dict(\n",
    "            clean_meta, orient=\"index\", columns=[\"Value\"]\n",
    "        )\n",
    "        df_overview.to_excel(writer, sheet_name=\"OVERVIEW\")\n",
    "\n",
    "        # --- SHEET 2: DAILY_AUDIT (Axis = Date) ---\n",
    "        # Concatenates everything that happens day-by-day\n",
    "        daily_items = {\n",
    "            \"Port_Value\": res.portfolio_series,\n",
    "            \"Port_Ret\": QuantUtils.compute_returns(res.portfolio_series),\n",
    "            \"Port_ATRP\": res.portfolio_atrp_series,  # <--- DIRECT ACCESS\n",
    "            \"Port_TRP\": res.portfolio_trp_series,  # <--- DIRECT ACCESS\n",
    "            \"Bench_Value\": res.benchmark_series,\n",
    "            \"Bench_Ret\": QuantUtils.compute_returns(res.benchmark_series),\n",
    "            \"Bench_ATRP\": res.benchmark_atrp_series,  # <--- DIRECT ACCESS\n",
    "            \"Bench_TRP\": res.benchmark_trp_series,  # <--- DIRECT ACCESS\n",
    "        }\n",
    "\n",
    "        # Filter out None values and concatenate side-by-side\n",
    "        df_daily = pd.concat(\n",
    "            {k: v for k, v in daily_items.items() if v is not None}, axis=1\n",
    "        )\n",
    "        df_daily.to_excel(writer, sheet_name=\"DAILY_AUDIT\", float_format=\"%.8f\")\n",
    "\n",
    "        # --- SHEET 3: SELECTION_SNAPSHOT (Axis = Ticker) ---\n",
    "        # Focuses on the selected 10-20 tickers and their performance\n",
    "        if \"full_universe_ranking\" in debug:\n",
    "            df_rank = debug[\"full_universe_ranking\"]\n",
    "            # Filter the leaderboard for only the tickers we actually bought\n",
    "            df_composition = df_rank.reindex(res.tickers)\n",
    "            df_composition.to_excel(\n",
    "                writer, sheet_name=\"PORTFOLIO_SNAPSHOT\", float_format=\"%.8f\"\n",
    "            )\n",
    "\n",
    "        # --- SHEET 4: FULL_UNIVERSE_RANKING ---\n",
    "        if \"full_universe_ranking\" in debug:\n",
    "            debug[\"full_universe_ranking\"].to_excel(\n",
    "                writer, sheet_name=\"FULL_RANKING\", float_format=\"%.8f\"\n",
    "            )\n",
    "\n",
    "        # --- SHEET 5: RAW_PRICES_MATRIX ---\n",
    "        if \"portfolio_raw_components\" in debug:\n",
    "            raw_p = debug[\"portfolio_raw_components\"].get(\"prices\")\n",
    "            if raw_p is not None:\n",
    "                raw_p.to_excel(writer, sheet_name=\"RAW_PRICES\", float_format=\"%.8f\")\n",
    "\n",
    "        # --- SHEET 6: RAW_VOL_MATRIX (TRP) ---\n",
    "        if \"portfolio_raw_components\" in debug:\n",
    "            # Re-extracting TRP matrix for the specific tickers\n",
    "            raw_v = debug[\"portfolio_raw_components\"].get(\n",
    "                \"atrp\"\n",
    "            )  # Or trp if stored specifically\n",
    "            if raw_v is not None:\n",
    "                raw_v.to_excel(writer, sheet_name=\"RAW_VOL_DATA\", float_format=\"%.8f\")\n",
    "\n",
    "    print(f\"âœ¨ Audit Report Complete. Manual verification ready in {filename}\")\n",
    "\n",
    "\n",
    "def print_nested(d, indent=0, width=4):\n",
    "    \"\"\"Pretty-print nested containers.\n",
    "    Leaves are rendered as two lines:  key\\\\nvalue .\"\"\"\n",
    "    spacing = \" \" * indent\n",
    "\n",
    "    def _kind(node):\n",
    "        if not isinstance(node, dict):\n",
    "            return None\n",
    "        return \"sep\" if all(isinstance(v, dict) for v in node.values()) else \"nest\"\n",
    "\n",
    "    if isinstance(d, dict):\n",
    "        for k, v in d.items():\n",
    "            kind = _kind(v)\n",
    "            tag = \"\" if kind is None else f\"  [{'SEP' if kind == 'sep' else 'NEST'}]\"\n",
    "            print(f\"{spacing}{k}{tag}\")\n",
    "            print_nested(v, indent + width, width)\n",
    "\n",
    "    elif isinstance(d, (list, tuple)):\n",
    "        for idx, item in enumerate(d):\n",
    "            print(f\"{spacing}[{idx}]\")\n",
    "            print_nested(item, indent + width, width)\n",
    "\n",
    "    else:  # leaf â€“ primitive value\n",
    "        print(f\"{spacing}{d}\")\n",
    "\n",
    "\n",
    "def get_ticker_OHLCV(\n",
    "    df_ohlcv: pd.DataFrame,\n",
    "    tickers: Union[str, List[str]],\n",
    "    date_start: str,\n",
    "    date_end: str,\n",
    "    return_format: str = \"dataframe\",\n",
    "    verbose: bool = True,\n",
    ") -> Union[pd.DataFrame, dict]:\n",
    "    \"\"\"\n",
    "    Get OHLCV data for specified tickers within a date range.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_ohlcv : pd.DataFrame\n",
    "        DataFrame with MultiIndex of (ticker, date) and OHLCV columns\n",
    "    tickers : str or list of str\n",
    "        Ticker symbol(s) to retrieve\n",
    "    date_start : str\n",
    "        Start date in 'YYYY-MM-DD' format\n",
    "    date_end : str\n",
    "        End date in 'YYYY-MM-DD' format\n",
    "    return_format : str, optional\n",
    "        Format to return data in. Options:\n",
    "        - 'dataframe': Single DataFrame with MultiIndex (default)\n",
    "        - 'dict': Dictionary with tickers as keys and DataFrames as values\n",
    "        - 'separate': List of separate DataFrames for each ticker\n",
    "    verbose : bool, optional\n",
    "        Whether to print summary information (default: True)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Union[pd.DataFrame, dict, list]\n",
    "        Filtered OHLCV data in specified format\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If input parameters are invalid\n",
    "    KeyError\n",
    "        If tickers not found in DataFrame\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> # Get data for single ticker\n",
    "    >>> vlo_data = get_ticker_OHLCV(df_ohlcv, 'VLO', '2025-08-13', '2025-09-04')\n",
    "\n",
    "    >>> # Get data for multiple tickers\n",
    "    >>> multi_data = get_ticker_OHLCV(df_ohlcv, ['VLO', 'JPST'], '2025-08-13', '2025-09-04')\n",
    "\n",
    "    >>> # Get data as dictionary\n",
    "    >>> data_dict = get_ticker_OHLCV(df_ohlcv, ['VLO', 'JPST'], '2025-08-13',\n",
    "    ...                              '2025-09-04', return_format='dict')\n",
    "    \"\"\"\n",
    "\n",
    "    # Input validation\n",
    "    if not isinstance(df_ohlcv, pd.DataFrame):\n",
    "        raise TypeError(\"df_ohlcv must be a pandas DataFrame\")\n",
    "\n",
    "    if not isinstance(df_ohlcv.index, pd.MultiIndex):\n",
    "        raise ValueError(\"DataFrame must have MultiIndex of (ticker, date)\")\n",
    "\n",
    "    if len(df_ohlcv.index.levels) != 2:\n",
    "        raise ValueError(\"MultiIndex must have exactly 2 levels: (ticker, date)\")\n",
    "\n",
    "    # Convert single ticker to list for consistent processing\n",
    "    if isinstance(tickers, str):\n",
    "        tickers = [tickers]\n",
    "    elif not isinstance(tickers, list):\n",
    "        raise TypeError(\"tickers must be a string or list of strings\")\n",
    "\n",
    "    # Convert dates to Timestamps\n",
    "    try:\n",
    "        start_date = pd.Timestamp(date_start)\n",
    "        end_date = pd.Timestamp(date_end)\n",
    "    except ValueError as e:\n",
    "        raise ValueError(f\"Invalid date format. Use 'YYYY-MM-DD': {e}\")\n",
    "\n",
    "    if start_date > end_date:\n",
    "        raise ValueError(\"date_start must be before or equal to date_end\")\n",
    "\n",
    "    # Check if tickers exist in the DataFrame\n",
    "    available_tickers = df_ohlcv.index.get_level_values(0).unique()\n",
    "    missing_tickers = [t for t in tickers if t not in available_tickers]\n",
    "\n",
    "    if missing_tickers:\n",
    "        raise KeyError(f\"Ticker(s) not found in DataFrame: {missing_tickers}\")\n",
    "\n",
    "    # Filter the data using MultiIndex slicing\n",
    "    try:\n",
    "        filtered_data = df_ohlcv.loc[(tickers, slice(date_start, date_end)), :]\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error filtering data: {e}\")\n",
    "\n",
    "    # Handle empty results\n",
    "    if filtered_data.empty:\n",
    "        if verbose:\n",
    "            print(\n",
    "                f\"No data found for tickers {tickers} in date range {date_start} to {date_end}\"\n",
    "            )\n",
    "        return filtered_data\n",
    "\n",
    "    # Print summary if verbose\n",
    "    if verbose:\n",
    "        print(\n",
    "            f\"Data retrieved for {len(tickers)} ticker(s) from {date_start} to {date_end}\"\n",
    "        )\n",
    "        print(f\"Total rows: {len(filtered_data)}\")\n",
    "        print(\n",
    "            f\"Date range in data: {filtered_data.index.get_level_values(1).min()} to \"\n",
    "            f\"{filtered_data.index.get_level_values(1).max()}\"\n",
    "        )\n",
    "\n",
    "        # Print ticker-specific counts\n",
    "        ticker_counts = filtered_data.index.get_level_values(0).value_counts()\n",
    "        for ticker in tickers:\n",
    "            count = ticker_counts.get(ticker, 0)\n",
    "            if count > 0:\n",
    "                print(f\"  {ticker}: {count} rows\")\n",
    "            else:\n",
    "                print(f\"  {ticker}: No data in range\")\n",
    "\n",
    "    # Return in requested format\n",
    "    if return_format == \"dict\":\n",
    "        result = {}\n",
    "        for ticker in tickers:\n",
    "            try:\n",
    "                result[ticker] = filtered_data.xs(ticker, level=0).loc[\n",
    "                    date_start:date_end\n",
    "                ]\n",
    "            except KeyError:\n",
    "                result[ticker] = pd.DataFrame()\n",
    "        return result\n",
    "\n",
    "    elif return_format == \"separate\":\n",
    "        result = []\n",
    "        for ticker in tickers:\n",
    "            try:\n",
    "                result.append(\n",
    "                    filtered_data.xs(ticker, level=0).loc[date_start:date_end]\n",
    "                )\n",
    "            except KeyError:\n",
    "                result.append(pd.DataFrame())\n",
    "        return result\n",
    "\n",
    "    elif return_format == \"dataframe\":\n",
    "        return filtered_data\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Invalid return_format: {return_format}. \"\n",
    "            f\"Must be 'dataframe', 'dict', or 'separate'\"\n",
    "        )\n",
    "\n",
    "\n",
    "def get_ticker_features(\n",
    "    features_df: pd.DataFrame,\n",
    "    tickers: Union[str, List[str]],\n",
    "    date_start: str,\n",
    "    date_end: str,\n",
    "    return_format: str = \"dataframe\",\n",
    "    verbose: bool = True,\n",
    ") -> Union[pd.DataFrame, dict]:\n",
    "    \"\"\"\n",
    "    Get features data for specified tickers within a date range.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    features_df : pd.DataFrame\n",
    "        DataFrame with MultiIndex of (ticker, date) and feature columns\n",
    "    tickers : str or list of str\n",
    "        Ticker symbol(s) to retrieve\n",
    "    date_start : str\n",
    "        Start date in 'YYYY-MM-DD' format\n",
    "    date_end : str\n",
    "        End date in 'YYYY-MM-DD' format\n",
    "    return_format : str, optional\n",
    "        Format to return data in. Options:\n",
    "        - 'dataframe': Single DataFrame with MultiIndex (default)\n",
    "        - 'dict': Dictionary with tickers as keys and DataFrames as values\n",
    "        - 'separate': List of separate DataFrames for each ticker\n",
    "    verbose : bool, optional\n",
    "        Whether to print summary information (default: True)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Union[pd.DataFrame, dict, list]\n",
    "        Filtered features data in specified format\n",
    "    \"\"\"\n",
    "    # Convert single ticker to list for consistent processing\n",
    "    if isinstance(tickers, str):\n",
    "        tickers = [tickers]\n",
    "\n",
    "    # Filter the data using MultiIndex slicing\n",
    "    try:\n",
    "        filtered_data = features_df.loc[(tickers, slice(date_start, date_end)), :]\n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(f\"Error filtering data: {e}\")\n",
    "        return pd.DataFrame() if return_format == \"dataframe\" else {}\n",
    "\n",
    "    # Handle empty results\n",
    "    if filtered_data.empty:\n",
    "        if verbose:\n",
    "            print(\n",
    "                f\"No data found for tickers {tickers} in date range {date_start} to {date_end}\"\n",
    "            )\n",
    "        return filtered_data\n",
    "\n",
    "    # Print summary if verbose\n",
    "    if verbose:\n",
    "        print(\n",
    "            f\"Features data retrieved for {len(tickers)} ticker(s) from {date_start} to {date_end}\"\n",
    "        )\n",
    "        print(f\"Total rows: {len(filtered_data)}\")\n",
    "        print(\n",
    "            f\"Date range in data: {filtered_data.index.get_level_values(1).min()} to \"\n",
    "            f\"{filtered_data.index.get_level_values(1).max()}\"\n",
    "        )\n",
    "        print(f\"Available features: {', '.join(filtered_data.columns.tolist())}\")\n",
    "\n",
    "        # Print ticker-specific counts\n",
    "        ticker_counts = filtered_data.index.get_level_values(0).value_counts()\n",
    "        for ticker in tickers:\n",
    "            count = ticker_counts.get(ticker, 0)\n",
    "            if count > 0:\n",
    "                print(f\"  {ticker}: {count} rows\")\n",
    "            else:\n",
    "                print(f\"  {ticker}: No data in range\")\n",
    "\n",
    "    # Return in requested format\n",
    "    if return_format == \"dict\":\n",
    "        result = {}\n",
    "        for ticker in tickers:\n",
    "            try:\n",
    "                result[ticker] = filtered_data.xs(ticker, level=0).loc[\n",
    "                    date_start:date_end\n",
    "                ]\n",
    "            except KeyError:\n",
    "                result[ticker] = pd.DataFrame()\n",
    "        return result\n",
    "\n",
    "    elif return_format == \"separate\":\n",
    "        result = []\n",
    "        for ticker in tickers:\n",
    "            try:\n",
    "                result.append(\n",
    "                    filtered_data.xs(ticker, level=0).loc[date_start:date_end]\n",
    "                )\n",
    "            except KeyError:\n",
    "                result.append(pd.DataFrame())\n",
    "        return result\n",
    "\n",
    "    elif return_format == \"dataframe\":\n",
    "        return filtered_data\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Invalid return_format: {return_format}. \"\n",
    "            f\"Must be 'dataframe', 'dict', or 'separate'\"\n",
    "        )\n",
    "\n",
    "\n",
    "def create_combined_dict(\n",
    "    df_ohlcv: pd.DataFrame,\n",
    "    features_df: pd.DataFrame,\n",
    "    tickers: Union[str, List[str]],\n",
    "    date_start: str,\n",
    "    date_end: str,\n",
    "    verbose: bool = True,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Create a combined dictionary with both OHLCV and features data for each ticker.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_ohlcv : pd.DataFrame\n",
    "        DataFrame with OHLCV data (MultiIndex: ticker, date)\n",
    "    features_df : pd.DataFrame\n",
    "        DataFrame with features data (MultiIndex: ticker, date)\n",
    "    tickers : str or list of str\n",
    "        Ticker symbol(s) to retrieve\n",
    "    date_start : str\n",
    "        Start date in 'YYYY-MM-DD' format\n",
    "    date_end : str\n",
    "        End date in 'YYYY-MM-DD' format\n",
    "    verbose : bool, optional\n",
    "        Whether to print progress information (default: True)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with tickers as keys and combined DataFrames (OHLCV + features) as values\n",
    "    \"\"\"\n",
    "    # Convert single ticker to list\n",
    "    if isinstance(tickers, str):\n",
    "        tickers = [tickers]\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Creating combined dictionary for {len(tickers)} ticker(s)\")\n",
    "        print(f\"Date range: {date_start} to {date_end}\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "    # Get OHLCV data as dictionary\n",
    "    ohlcv_dict = get_ticker_OHLCV(\n",
    "        df_ohlcv, tickers, date_start, date_end, return_format=\"dict\", verbose=verbose\n",
    "    )\n",
    "\n",
    "    # Get features data as dictionary\n",
    "    features_dict = get_ticker_features(\n",
    "        features_df,\n",
    "        tickers,\n",
    "        date_start,\n",
    "        date_end,\n",
    "        return_format=\"dict\",\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    # Create combined_dict\n",
    "    combined_dict = {}\n",
    "\n",
    "    for ticker in tickers:\n",
    "        if verbose:\n",
    "            print(f\"\\nProcessing {ticker}...\")\n",
    "\n",
    "        # Check if ticker exists in both dictionaries\n",
    "        if ticker in ohlcv_dict and ticker in features_dict:\n",
    "            ohlcv_data = ohlcv_dict[ticker]\n",
    "            features_data = features_dict[ticker]\n",
    "\n",
    "            # Check if both dataframes have data\n",
    "            if not ohlcv_data.empty and not features_data.empty:\n",
    "                # Combine OHLCV and features data\n",
    "                # Note: Both dataframes have the same index (dates), so we can concatenate\n",
    "                combined_df = pd.concat([ohlcv_data, features_data], axis=1)\n",
    "\n",
    "                # Ensure proper index naming\n",
    "                combined_df.index.name = \"Date\"\n",
    "\n",
    "                # Store in combined_dict\n",
    "                combined_dict[ticker] = combined_df\n",
    "\n",
    "                if verbose:\n",
    "                    print(f\"  âœ“ Successfully combined data\")\n",
    "                    print(f\"  OHLCV shape: {ohlcv_data.shape}\")\n",
    "                    print(f\"  Features shape: {features_data.shape}\")\n",
    "                    print(f\"  Combined shape: {combined_df.shape}\")\n",
    "                    print(\n",
    "                        f\"  Date range: {combined_df.index.min()} to {combined_df.index.max()}\"\n",
    "                    )\n",
    "            else:\n",
    "                if verbose:\n",
    "                    print(f\"  âœ— Cannot combine: One or both dataframes are empty\")\n",
    "                    print(f\"    OHLCV empty: {ohlcv_data.empty}\")\n",
    "                    print(f\"    Features empty: {features_data.empty}\")\n",
    "                combined_dict[ticker] = pd.DataFrame()\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(f\"  âœ— Ticker not found in both dictionaries\")\n",
    "                if ticker not in ohlcv_dict:\n",
    "                    print(f\"    Not in OHLCV data\")\n",
    "                if ticker not in features_dict:\n",
    "                    print(f\"    Not in features data\")\n",
    "            combined_dict[ticker] = pd.DataFrame()\n",
    "\n",
    "    # Print summary\n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"SUMMARY\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Total tickers processed: {len(tickers)}\")\n",
    "\n",
    "        tickers_with_data = [\n",
    "            ticker for ticker, df in combined_dict.items() if not df.empty\n",
    "        ]\n",
    "        print(f\"Tickers with combined data: {len(tickers_with_data)}\")\n",
    "\n",
    "        if tickers_with_data:\n",
    "            print(\"\\nTicker details:\")\n",
    "            for ticker in tickers_with_data:\n",
    "                df = combined_dict[ticker]\n",
    "                print(f\"  {ticker}: {df.shape} - {df.index.min()} to {df.index.max()}\")\n",
    "                print(f\"    Columns: {len(df.columns)}\")\n",
    "\n",
    "        empty_tickers = [ticker for ticker, df in combined_dict.items() if df.empty]\n",
    "        if empty_tickers:\n",
    "            print(f\"\\nTickers with no data: {', '.join(empty_tickers)}\")\n",
    "\n",
    "    return combined_dict\n",
    "\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f6d867e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_indices:|n                   Adj Open  Adj High  Adj Low  Adj Close  Volume\n",
      "Ticker Date                                                      \n",
      "^AXJO  1992-11-22   1455.00   1455.00  1455.00    1455.00       0\n",
      "       1992-11-23   1458.40   1458.40  1458.40    1458.40       0\n",
      "       1992-11-24   1467.90   1467.90  1467.90    1467.90       0\n",
      "       1992-11-25   1459.00   1459.00  1459.00    1459.00       0\n",
      "       1992-11-26   1458.90   1458.90  1458.90    1458.90       0\n",
      "...                     ...       ...      ...        ...     ...\n",
      "^VIX3M 2026-01-29     19.65     21.13    19.54      19.67       0\n",
      "       2026-01-30     19.89     20.48    19.48      20.07       0\n",
      "       2026-02-02     20.37     20.40    19.19      19.37       0\n",
      "       2026-02-03     19.36     21.54    19.34      20.35       0\n",
      "       2026-02-04     20.35     22.01    20.19      20.62       0\n",
      "\n",
      "[144056 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "data_path = r\"c:\\Users\\ping\\Files_win10\\python\\py311\\stocks\\data\\df_indices.parquet\"\n",
    "\n",
    "df_indices = pd.read_parquet(data_path, engine=\"pyarrow\")\n",
    "print(f\"df_indices:|n{df_indices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bf150180",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['^AXJO',\n",
       " '^BSESN',\n",
       " '^DJI',\n",
       " '^FCHI',\n",
       " '^FTSE',\n",
       " '^GDAXI',\n",
       " '^GSPC',\n",
       " '^HSI',\n",
       " '^IXIC',\n",
       " '^N225',\n",
       " '^NYA',\n",
       " '^STOXX50E',\n",
       " '^VIX',\n",
       " '^VIX3M']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 144056 entries, ('^AXJO', Timestamp('1992-11-22 00:00:00')) to ('^VIX3M', Timestamp('2026-02-04 00:00:00'))\n",
      "Data columns (total 5 columns):\n",
      " #   Column     Non-Null Count   Dtype  \n",
      "---  ------     --------------   -----  \n",
      " 0   Adj Open   144056 non-null  float64\n",
      " 1   Adj High   144056 non-null  float64\n",
      " 2   Adj Low    144056 non-null  float64\n",
      " 3   Adj Close  144056 non-null  float64\n",
      " 4   Volume     144056 non-null  int64  \n",
      "dtypes: float64(4), int64(1)\n",
      "memory usage: 6.5+ MB\n"
     ]
    }
   ],
   "source": [
    "_indices = df_indices.index.get_level_values(0).unique().tolist()\n",
    "display(_indices)\n",
    "df_indices.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "232740e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = (\n",
    "    r\"c:\\Users\\ping\\Files_win10\\python\\py311\\stocks\\data\\df_OHLCV_stocks_etfs.parquet\"\n",
    ")\n",
    "\n",
    "df_ohlcv = pd.read_parquet(data_path, engine=\"pyarrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "239275a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_ohlcv.head():\n",
      "                    Adj Open  Adj High  Adj Low  Adj Close    Volume\n",
      "Ticker Date                                                        \n",
      "A      1999-11-18   27.1966   29.8864  23.9091    26.3000  74849970\n",
      "       1999-11-19   25.6649   25.7023  23.7970    24.1333  18230876\n",
      "       1999-11-22   24.6936   26.3000  23.9465    26.3000   7871812\n",
      "       1999-11-23   25.4034   26.0759  23.9091    23.9091   7151079\n",
      "       1999-11-24   23.9838   25.0672  23.9091    24.5442   5795949\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 9488873 entries, ('A', Timestamp('1999-11-18 00:00:00')) to ('ZWS', Timestamp('2026-02-04 00:00:00'))\n",
      "Data columns (total 5 columns):\n",
      " #   Column     Dtype  \n",
      "---  ------     -----  \n",
      " 0   Adj Open   float64\n",
      " 1   Adj High   float64\n",
      " 2   Adj Low    float64\n",
      " 3   Adj Close  float64\n",
      " 4   Volume     int64  \n",
      "dtypes: float64(4), int64(1)\n",
      "memory usage: 398.9+ MB\n"
     ]
    }
   ],
   "source": [
    "print(f\"df_ohlcv.head():\\n {df_ohlcv.head()}\\n\")\n",
    "df_ohlcv.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a6e70338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ª Testing New Feature Engine...\n",
      "âš¡ Generating Decoupled Features (Benchmark: SPY)...\n",
      "âœ… Features Generated Successfully!\n",
      "\n",
      "--- TICKER FEATURES (Micro) ---\n",
      "Columns: ['ATR', 'ATRP', 'TRP', 'RSI', 'Mom_21', 'Consistency', 'IR_63', 'Beta_63', 'DD_21', 'Ret_1d', 'RollingStalePct', 'RollMedDollarVol', 'RollingSameVolCount']\n",
      "Sample Data:\n",
      "                    Mom_21   IR_63    ATRP\n",
      "Ticker Date                              \n",
      "A      2019-09-27  0.0922  0.0318  0.0195\n",
      "       2019-09-30  0.0862  0.0208  0.0189\n",
      "       2019-10-01  0.0547  0.0008  0.0201\n",
      "       2019-10-02  0.0440 -0.0324  0.0210\n",
      "       2019-10-03  0.0447 -0.0132  0.0207\n",
      "\n",
      "--- MACRO STATE (Shared) ---\n",
      "Columns: ['Mkt_Ret', 'Macro_Trend', 'Macro_Trend_Vel', 'Macro_Trend_Vel_Z', 'Macro_Trend_Mom', 'Macro_Vix_Z', 'Macro_Vix_Ratio']\n",
      "\n",
      "ðŸ” Macro Check for 2019-10-03:\n",
      "Mkt_Ret              0.0000\n",
      "Macro_Trend          0.0000\n",
      "Macro_Trend_Vel      0.0000\n",
      "Macro_Trend_Vel_Z    0.0000\n",
      "Macro_Trend_Mom      0.0000\n",
      "Macro_Vix_Z         -1.2117\n",
      "Macro_Vix_Ratio      0.8438\n",
      "Name: 2019-10-03 00:00:00, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# === FIXED TEST HARNESS ===\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"ðŸ§ª Testing New Feature Engine...\")\n",
    "\n",
    "    # 1. Create Dummy Index Data\n",
    "    dates = df_ohlcv.index.get_level_values(\"Date\").unique()\n",
    "    dummy_indices = []\n",
    "\n",
    "    for ticker in [\"^GSPC\", \"^VIX\", \"^VIX3M\"]:\n",
    "        temp = pd.DataFrame(\n",
    "            {\n",
    "                \"Adj Close\": (\n",
    "                    np.random.normal(100, 5, len(dates))\n",
    "                    if ticker == \"^GSPC\"\n",
    "                    else np.random.normal(20, 2, len(dates))\n",
    "                ),\n",
    "                \"Volume\": 1000,\n",
    "            },\n",
    "            index=dates,\n",
    "        )\n",
    "        temp[\"Ticker\"] = ticker\n",
    "        dummy_indices.append(temp.reset_index().set_index([\"Ticker\", \"Date\"]))\n",
    "\n",
    "    df_idx_test = pd.concat(dummy_indices)\n",
    "\n",
    "    # 2. Run Generation\n",
    "    try:\n",
    "        # FIX 1: Unpack the tuple into two variables\n",
    "        feat_df, mac_df = generate_features(\n",
    "            df_ohlcv.iloc[:5000], df_indices=df_idx_test\n",
    "        )\n",
    "\n",
    "        print(\"âœ… Features Generated Successfully!\")\n",
    "\n",
    "        # FIX 2: Check columns of the features_df (Ticker-specific)\n",
    "        print(\"\\n--- TICKER FEATURES (Micro) ---\")\n",
    "        print(\"Columns:\", feat_df.columns.tolist())\n",
    "        # Note: Macro_Vix_Z is no longer in feat_df (that's the point of the optimization!)\n",
    "        print(\"Sample Data:\\n\", feat_df[[\"Mom_21\", \"IR_63\", \"ATRP\"]].tail())\n",
    "\n",
    "        # FIX 3: Check the new Macro DataFrame\n",
    "        print(\"\\n--- MACRO STATE (Shared) ---\")\n",
    "        print(\"Columns:\", mac_df.columns.tolist())\n",
    "\n",
    "        # Check specific date alignment\n",
    "        last_date = feat_df.index.get_level_values(\"Date\")[-1]\n",
    "        print(f\"\\nðŸ” Macro Check for {last_date.date()}:\")\n",
    "        # We look up the date in the mac_df now\n",
    "        print(mac_df.loc[last_date])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error: {e}\")\n",
    "        import traceback\n",
    "\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cf44f3",
   "metadata": {},
   "source": [
    "### DEBUG: Subset df_ohlcv to 11 tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ff988053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _tickers = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"IAU\", \"J\"] + [\"SPY\"]\n",
    "\n",
    "# # Subset ohlcv to 11 tickers\n",
    "# print(f\"Subset df_ohlcv to 11 tickers\")\n",
    "# df_ohlcv = df_ohlcv.loc[_tickers].sort_index()\n",
    "\n",
    "# # Remove duplicate SPY, just-in-case\n",
    "# ohlcv_tickers = df_ohlcv.index.get_level_values(0).unique()\n",
    "# print(f\"ohlcv_tickers count: {len(ohlcv_tickers)}:\\n{ohlcv_tickers}\\n\")\n",
    "\n",
    "# print(df_ohlcv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4d3a9824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš¡ Generating Decoupled Features (Benchmark: SPY)...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# DATA PRE-COMPUTATION (The \"Fast-Track\" Setup)\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m features_df, macro_df = \u001b[43mgenerate_features\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdf_ohlcv\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdf_ohlcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdf_indices\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdf_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbenchmark_ticker\u001b[49m\u001b[43m=\u001b[49m\u001b[43mGLOBAL_SETTINGS\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbenchmark_ticker\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 344\u001b[39m, in \u001b[36mgenerate_features\u001b[39m\u001b[34m(df_ohlcv, df_indices, benchmark_ticker, atr_period, rsi_period, win_5d, win_21d, win_63d, feature_zscore_clip, quality_window, quality_min_periods)\u001b[39m\n\u001b[32m    341\u001b[39m rsi = raw_rsi.replace({np.inf: \u001b[32m100\u001b[39m, -np.inf: \u001b[32m0\u001b[39m}).fillna(\u001b[32m50\u001b[39m)\n\u001b[32m    343\u001b[39m \u001b[38;5;66;03m# E. Assemble Features\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m344\u001b[39m features_df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    346\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mATR\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43matr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mATRP\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mnatr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mTRP\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mRSI\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrsi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mMom_21\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmom_21\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mConsistency\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsistency\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mIR_63\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mir_63\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mBeta_63\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta_63\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mDD_21\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdd_21\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfillna\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mRet_1d\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[38;5;66;03m# F. Quality (Universe Filtering) - Optimized\u001b[39;00m\n\u001b[32m    360\u001b[39m quality_temp = pd.DataFrame(\n\u001b[32m    361\u001b[39m     {\n\u001b[32m    362\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mIsStale\u001b[39m\u001b[33m\"\u001b[39m: np.where(\n\u001b[32m   (...)\u001b[39m\u001b[32m    371\u001b[39m     index=df_ohlcv.index,\n\u001b[32m    372\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ping\\Files_win10\\python\\py311\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:778\u001b[39m, in \u001b[36mDataFrame.__init__\u001b[39m\u001b[34m(self, data, index, columns, dtype, copy)\u001b[39m\n\u001b[32m    772\u001b[39m     mgr = \u001b[38;5;28mself\u001b[39m._init_mgr(\n\u001b[32m    773\u001b[39m         data, axes={\u001b[33m\"\u001b[39m\u001b[33mindex\u001b[39m\u001b[33m\"\u001b[39m: index, \u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m: columns}, dtype=dtype, copy=copy\n\u001b[32m    774\u001b[39m     )\n\u001b[32m    776\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    777\u001b[39m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m778\u001b[39m     mgr = \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    779\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma.MaskedArray):\n\u001b[32m    780\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mma\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ping\\Files_win10\\python\\py311\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:503\u001b[39m, in \u001b[36mdict_to_mgr\u001b[39m\u001b[34m(data, index, columns, dtype, typ, copy)\u001b[39m\n\u001b[32m    499\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    500\u001b[39m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[32m    501\u001b[39m         arrays = [x.copy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[32m--> \u001b[39m\u001b[32m503\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ping\\Files_win10\\python\\py311\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:114\u001b[39m, in \u001b[36marrays_to_mgr\u001b[39m\u001b[34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[32m    112\u001b[39m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[32m    113\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m         index = \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    116\u001b[39m         index = ensure_index(index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ping\\Files_win10\\python\\py311\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:670\u001b[39m, in \u001b[36m_extract_index\u001b[39m\u001b[34m(data)\u001b[39m\n\u001b[32m    667\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mIf using all scalar values, you must pass an index\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    669\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m have_series:\n\u001b[32m--> \u001b[39m\u001b[32m670\u001b[39m     index = \u001b[43munion_indexes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m have_dicts:\n\u001b[32m    672\u001b[39m     index = union_indexes(indexes, sort=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ping\\Files_win10\\python\\py311\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\api.py:312\u001b[39m, in \u001b[36munion_indexes\u001b[39m\u001b[34m(indexes, sort)\u001b[39m\n\u001b[32m    309\u001b[39m         result = indexes[\u001b[32m0\u001b[39m]\n\u001b[32m    311\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m other \u001b[38;5;129;01min\u001b[39;00m indexes[\u001b[32m1\u001b[39m:]:\n\u001b[32m--> \u001b[39m\u001b[32m312\u001b[39m         result = \u001b[43mresult\u001b[49m\u001b[43m.\u001b[49m\u001b[43munion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msort\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    313\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m kind == \u001b[33m\"\u001b[39m\u001b[33marray\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ping\\Files_win10\\python\\py311\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3342\u001b[39m, in \u001b[36mIndex.union\u001b[39m\u001b[34m(self, other, sort)\u001b[39m\n\u001b[32m   3339\u001b[39m     right = other.astype(dtype, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   3340\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m left.union(right, sort=sort)\n\u001b[32m-> \u001b[39m\u001b[32m3342\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(other) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mequals\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   3343\u001b[39m     \u001b[38;5;66;03m# NB: whether this (and the `if not len(self)` check below) come before\u001b[39;00m\n\u001b[32m   3344\u001b[39m     \u001b[38;5;66;03m#  or after the dtype equality check above affects the returned dtype\u001b[39;00m\n\u001b[32m   3345\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._get_reconciled_name_object(other)\n\u001b[32m   3346\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m sort \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ping\\Files_win10\\python\\py311\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\multi.py:3722\u001b[39m, in \u001b[36mMultiIndex.equals\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m   3720\u001b[39m self_mask = self_codes == -\u001b[32m1\u001b[39m\n\u001b[32m   3721\u001b[39m other_mask = other_codes == -\u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m3722\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray_equal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mself_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother_mask\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   3723\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   3724\u001b[39m self_codes = self_codes[~self_mask]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ping\\Files_win10\\python\\py311\\.venv\\Lib\\site-packages\\numpy\\core\\numeric.py:2374\u001b[39m, in \u001b[36m_array_equal_dispatcher\u001b[39m\u001b[34m(a1, a2, equal_nan)\u001b[39m\n\u001b[32m   2369\u001b[39m             cond[both_nan] = both_nan[both_nan]\n\u001b[32m   2371\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m cond[()]  \u001b[38;5;66;03m# Flatten 0d arrays to scalars\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2374\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_array_equal_dispatcher\u001b[39m(a1, a2, equal_nan=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m   2375\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (a1, a2)\n\u001b[32m   2378\u001b[39m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_array_equal_dispatcher)\n\u001b[32m   2379\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34marray_equal\u001b[39m(a1, a2, equal_nan=\u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# DATA PRE-COMPUTATION (The \"Fast-Track\" Setup)\n",
    "# ==============================================================================\n",
    "\n",
    "features_df, macro_df = generate_features(\n",
    "    df_ohlcv=df_ohlcv,\n",
    "    df_indices=df_indices,\n",
    "    benchmark_ticker=GLOBAL_SETTINGS[\"benchmark_ticker\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed07737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 9488873 entries, ('A', Timestamp('1999-11-18 00:00:00')) to ('ZWS', Timestamp('2026-02-04 00:00:00'))\n",
      "Data columns (total 13 columns):\n",
      " #   Column               Dtype  \n",
      "---  ------               -----  \n",
      " 0   ATR                  float64\n",
      " 1   ATRP                 float64\n",
      " 2   TRP                  float64\n",
      " 3   RSI                  float64\n",
      " 4   Mom_21               float64\n",
      " 5   Consistency          float64\n",
      " 6   IR_63                float64\n",
      " 7   Beta_63              float64\n",
      " 8   DD_21                float64\n",
      " 9   Ret_1d               float64\n",
      " 10  RollingStalePct      float64\n",
      " 11  RollMedDollarVol     float64\n",
      " 12  RollingSameVolCount  float64\n",
      "dtypes: float64(13)\n",
      "memory usage: 978.0+ MB\n",
      "features_df.info():\n",
      "None\n",
      "\n",
      "features_df.index.names:\n",
      "['Ticker', 'Date']\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 16131 entries, 1962-01-02 to 2026-02-04\n",
      "Data columns (total 7 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   Mkt_Ret            16131 non-null  float64\n",
      " 1   Macro_Trend        16131 non-null  float64\n",
      " 2   Macro_Trend_Vel    16131 non-null  float64\n",
      " 3   Macro_Trend_Vel_Z  16131 non-null  float64\n",
      " 4   Macro_Trend_Mom    16131 non-null  float64\n",
      " 5   Macro_Vix_Z        16131 non-null  float64\n",
      " 6   Macro_Vix_Ratio    16131 non-null  float64\n",
      "dtypes: float64(7)\n",
      "memory usage: 1.5 MB\n",
      "macro_df.info():\n",
      "None\n",
      "\n",
      "macro_df.index.names:\n",
      "['Date']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"features_df.info():\\n{features_df.info()}\\n\")\n",
    "print(f\"features_df.index.names:\\n{features_df.index.names}\\n\")\n",
    "print(f\"macro_df.info():\\n{macro_df.info()}\\n\")\n",
    "print(f\"macro_df.index.names:\\n{macro_df.index.names}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f1e8e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Macro Verification (Benchmark: SPY) ---\n",
      "\n",
      "Comparing verification vs original (Clip Threshold: 4.0):\n",
      "âœ… Mkt_Ret              | PASS (Max Diff: 0.00e+00)\n",
      "âœ… Macro_Trend          | PASS (Max Diff: 0.00e+00)\n",
      "âœ… Macro_Trend_Vel      | PASS (Max Diff: 0.00e+00)\n",
      "âœ… Macro_Trend_Vel_Z    | PASS (Max Diff: 0.00e+00)\n",
      "âœ… Macro_Trend_Mom      | PASS (Max Diff: 0.00e+00)\n",
      "âœ… Macro_Vix_Z          | PASS (Max Diff: 0.00e+00)\n",
      "âœ… Macro_Vix_Ratio      | PASS (Max Diff: 0.00e+00)\n"
     ]
    }
   ],
   "source": [
    "verify_macro_engine(df_ohlcv, df_indices, macro_df, GLOBAL_SETTINGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e82d6f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Generating Wide Matrices for Instant Backtesting...\n",
      "   - Unstacking ATRP...\n",
      "   - Unstacking TRP...\n",
      "âœ… Pre-computation Complete. Tickers: 1585, Days: 16131\n",
      "   Ready: df_close_wide, df_atrp_wide, df_trp_wide, and macro_df.\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸš€ Generating Wide Matrices for Instant Backtesting...\")\n",
    "\n",
    "# 1. Price Matrix\n",
    "df_close_wide = df_ohlcv[\"Adj Close\"].unstack(level=0)\n",
    "\n",
    "# 2. Volatility Matrices (Unstack and Align)\n",
    "# Using features_df (the first item from the tuple)\n",
    "print(\"   - Unstacking ATRP...\")\n",
    "df_atrp_wide = features_df[\"ATRP\"].unstack(level=0).reindex_like(df_close_wide)\n",
    "\n",
    "print(\"   - Unstacking TRP...\")\n",
    "df_trp_wide = features_df[\"TRP\"].unstack(level=0).reindex_like(df_close_wide)\n",
    "\n",
    "# 3. Handle Data Gaps (Sanitize the Wide Matrices)\n",
    "if GLOBAL_SETTINGS[\"handle_zeros_as_nan\"]:\n",
    "    df_close_wide = df_close_wide.replace(0, np.nan)\n",
    "\n",
    "# Forward fill up to the limit, then fill remaining with the \"Disaster Detection\" value\n",
    "df_close_wide = df_close_wide.ffill(limit=GLOBAL_SETTINGS[\"max_data_gap_ffill\"])\n",
    "df_close_wide = df_close_wide.fillna(GLOBAL_SETTINGS[\"nan_price_replacement\"])\n",
    "\n",
    "print(\n",
    "    f\"âœ… Pre-computation Complete. Tickers: {len(df_close_wide.columns)}, Days: {len(df_close_wide)}\"\n",
    ")\n",
    "print(\"   Ready: df_close_wide, df_atrp_wide, df_trp_wide, and macro_df.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "63bcd454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Ready for Stage 1: Run Simulation for first filter.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af30055e1efe4b3a9f96ef41c53ff5c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<b>1. Timeline Configuration:</b> (Past <--- Decision ---> Future)'), HBox(childrenâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. RE-INSTANTIATE ENGINE (Crucial Step!)\n",
    "# This ensures the 'master_engine' variable actually uses the code you just pasted above.\n",
    "master_engine = AlphaEngine(\n",
    "    df_ohlcv=df_ohlcv,\n",
    "    features_df=features_df,\n",
    "    macro_df=macro_df,\n",
    "    df_close_wide=df_close_wide,\n",
    "    df_atrp_wide=df_atrp_wide,\n",
    "    df_trp_wide=df_trp_wide,\n",
    ")\n",
    "\n",
    "# 2. LAUNCH STAGE 1 (Discovery)\n",
    "# universe_subset=None means \"Scan the whole market\"\n",
    "analyzer1, stage1_pack = create_walk_forward_analyzer(\n",
    "    master_engine, universe_subset=None\n",
    ")\n",
    "\n",
    "print(\"ðŸš€ Ready for Stage 1: Run Simulation for first filter.\")\n",
    "analyzer1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a6ff2e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================\n",
      "ðŸ” HIGH-TRANSPARENCY AUDIT MAP\n",
      "====================================================================\n",
      "[  0] ðŸ“¦ audit_pack (EngineOutput)\n",
      "[  1]   ðŸ“ˆ portfolio_series (shape=(259,))\n",
      "[  2]   ðŸ“ˆ benchmark_series (shape=(259,))\n",
      "[  3]   ðŸ§® normalized_plot_data (shape=(259, 100))\n",
      "[  4]   ðŸ“‚ tickers (len=100)\n",
      "[  5]     ðŸ“„ index_0 (str)\n",
      "[  6]     ðŸ“„ index_1 (str)\n",
      "[  7]     ðŸ“„ index_2 (str)\n",
      "[  8]     ðŸ“„ index_3 (str)\n",
      "[  9]     ðŸ“„ index_4 (str)\n",
      "[ 10]     ðŸ“„ index_5 (str)\n",
      "[ 11]     ðŸ“„ index_6 (str)\n",
      "[ 12]     ðŸ“„ index_7 (str)\n",
      "[ 13]     ðŸ“„ index_8 (str)\n",
      "[ 14]     ðŸ“„ index_9 (str)\n",
      "[ 15]     ðŸ“„ index_10 (str)\n",
      "[ 16]     ðŸ“„ index_11 (str)\n",
      "[ 17]     ðŸ“„ index_12 (str)\n",
      "[ 18]     ðŸ“„ index_13 (str)\n",
      "[ 19]     ðŸ“„ index_14 (str)\n",
      "[ 20]     ðŸ“„ index_15 (str)\n",
      "[ 21]     ðŸ“„ index_16 (str)\n",
      "[ 22]     ðŸ“„ index_17 (str)\n",
      "[ 23]     ðŸ“„ index_18 (str)\n",
      "[ 24]     ðŸ“„ index_19 (str)\n",
      "[ 25]     ðŸ“„ index_20 (str)\n",
      "[ 26]     ðŸ“„ index_21 (str)\n",
      "[ 27]     ðŸ“„ index_22 (str)\n",
      "[ 28]     ðŸ“„ index_23 (str)\n",
      "[ 29]     ðŸ“„ index_24 (str)\n",
      "[ 30]     ðŸ“„ index_25 (str)\n",
      "[ 31]     ðŸ“„ index_26 (str)\n",
      "[ 32]     ðŸ“„ index_27 (str)\n",
      "[ 33]     ðŸ“„ index_28 (str)\n",
      "[ 34]     ðŸ“„ index_29 (str)\n",
      "[ 35]     ðŸ“„ index_30 (str)\n",
      "[ 36]     ðŸ“„ index_31 (str)\n",
      "[ 37]     ðŸ“„ index_32 (str)\n",
      "[ 38]     ðŸ“„ index_33 (str)\n",
      "[ 39]     ðŸ“„ index_34 (str)\n",
      "[ 40]     ðŸ“„ index_35 (str)\n",
      "[ 41]     ðŸ“„ index_36 (str)\n",
      "[ 42]     ðŸ“„ index_37 (str)\n",
      "[ 43]     ðŸ“„ index_38 (str)\n",
      "[ 44]     ðŸ“„ index_39 (str)\n",
      "[ 45]     ðŸ“„ index_40 (str)\n",
      "[ 46]     ðŸ“„ index_41 (str)\n",
      "[ 47]     ðŸ“„ index_42 (str)\n",
      "[ 48]     ðŸ“„ index_43 (str)\n",
      "[ 49]     ðŸ“„ index_44 (str)\n",
      "[ 50]     ðŸ“„ index_45 (str)\n",
      "[ 51]     ðŸ“„ index_46 (str)\n",
      "[ 52]     ðŸ“„ index_47 (str)\n",
      "[ 53]     ðŸ“„ index_48 (str)\n",
      "[ 54]     ðŸ“„ index_49 (str)\n",
      "[ 55]     ðŸ“„ index_50 (str)\n",
      "[ 56]     ðŸ“„ index_51 (str)\n",
      "[ 57]     ðŸ“„ index_52 (str)\n",
      "[ 58]     ðŸ“„ index_53 (str)\n",
      "[ 59]     ðŸ“„ index_54 (str)\n",
      "[ 60]     ðŸ“„ index_55 (str)\n",
      "[ 61]     ðŸ“„ index_56 (str)\n",
      "[ 62]     ðŸ“„ index_57 (str)\n",
      "[ 63]     ðŸ“„ index_58 (str)\n",
      "[ 64]     ðŸ“„ index_59 (str)\n",
      "[ 65]     ðŸ“„ index_60 (str)\n",
      "[ 66]     ðŸ“„ index_61 (str)\n",
      "[ 67]     ðŸ“„ index_62 (str)\n",
      "[ 68]     ðŸ“„ index_63 (str)\n",
      "[ 69]     ðŸ“„ index_64 (str)\n",
      "[ 70]     ðŸ“„ index_65 (str)\n",
      "[ 71]     ðŸ“„ index_66 (str)\n",
      "[ 72]     ðŸ“„ index_67 (str)\n",
      "[ 73]     ðŸ“„ index_68 (str)\n",
      "[ 74]     ðŸ“„ index_69 (str)\n",
      "[ 75]     ðŸ“„ index_70 (str)\n",
      "[ 76]     ðŸ“„ index_71 (str)\n",
      "[ 77]     ðŸ“„ index_72 (str)\n",
      "[ 78]     ðŸ“„ index_73 (str)\n",
      "[ 79]     ðŸ“„ index_74 (str)\n",
      "[ 80]     ðŸ“„ index_75 (str)\n",
      "[ 81]     ðŸ“„ index_76 (str)\n",
      "[ 82]     ðŸ“„ index_77 (str)\n",
      "[ 83]     ðŸ“„ index_78 (str)\n",
      "[ 84]     ðŸ“„ index_79 (str)\n",
      "[ 85]     ðŸ“„ index_80 (str)\n",
      "[ 86]     ðŸ“„ index_81 (str)\n",
      "[ 87]     ðŸ“„ index_82 (str)\n",
      "[ 88]     ðŸ“„ index_83 (str)\n",
      "[ 89]     ðŸ“„ index_84 (str)\n",
      "[ 90]     ðŸ“„ index_85 (str)\n",
      "[ 91]     ðŸ“„ index_86 (str)\n",
      "[ 92]     ðŸ“„ index_87 (str)\n",
      "[ 93]     ðŸ“„ index_88 (str)\n",
      "[ 94]     ðŸ“„ index_89 (str)\n",
      "[ 95]     ðŸ“„ index_90 (str)\n",
      "[ 96]     ðŸ“„ index_91 (str)\n",
      "[ 97]     ðŸ“„ index_92 (str)\n",
      "[ 98]     ðŸ“„ index_93 (str)\n",
      "[ 99]     ðŸ“„ index_94 (str)\n",
      "[100]     ðŸ“„ index_95 (str)\n",
      "[101]     ðŸ“„ index_96 (str)\n",
      "[102]     ðŸ“„ index_97 (str)\n",
      "[103]     ðŸ“„ index_98 (str)\n",
      "[104]     ðŸ“„ index_99 (str)\n",
      "[105]   ðŸ“ˆ initial_weights (shape=(100,))\n",
      "[106]   ðŸ“‚ perf_metrics (len=24)\n",
      "[107]     ðŸ”¢ full_p_gain (float)\n",
      "[108]     ðŸ”¢ full_p_sharpe (float)\n",
      "[109]     ðŸ”¢ full_p_sharpe_atrp (float)\n",
      "[110]     ðŸ”¢ full_p_sharpe_trp (float)\n",
      "[111]     ðŸ”¢ lookback_p_gain (float)\n",
      "[112]     ðŸ”¢ lookback_p_sharpe (float)\n",
      "[113]     ðŸ”¢ lookback_p_sharpe_atrp (float)\n",
      "[114]     ðŸ”¢ lookback_p_sharpe_trp (float)\n",
      "[115]     ðŸ”¢ holding_p_gain (float)\n",
      "[116]     ðŸ”¢ holding_p_sharpe (float)\n",
      "[117]     ðŸ”¢ holding_p_sharpe_atrp (float)\n",
      "[118]     ðŸ”¢ holding_p_sharpe_trp (float)\n",
      "[119]     ðŸ”¢ full_b_gain (float)\n",
      "[120]     ðŸ”¢ full_b_sharpe (float)\n",
      "[121]     ðŸ”¢ full_b_sharpe_atrp (float)\n",
      "[122]     ðŸ”¢ full_b_sharpe_trp (float)\n",
      "[123]     ðŸ”¢ lookback_b_gain (float)\n",
      "[124]     ðŸ”¢ lookback_b_sharpe (float)\n",
      "[125]     ðŸ”¢ lookback_b_sharpe_atrp (float)\n",
      "[126]     ðŸ”¢ lookback_b_sharpe_trp (float)\n",
      "[127]     ðŸ”¢ holding_b_gain (float)\n",
      "[128]     ðŸ”¢ holding_b_sharpe (float)\n",
      "[129]     ðŸ”¢ holding_b_sharpe_atrp (float)\n",
      "[130]     ðŸ”¢ holding_b_sharpe_trp (float)\n",
      "[131]   ðŸ§® results_df (shape=(100, 2))\n",
      "[132]   ðŸ“… start_date (Timestamp)\n",
      "[133]   ðŸ“… decision_date (Timestamp)\n",
      "[134]   ðŸ“… buy_date (Timestamp)\n",
      "[135]   ðŸ“… holding_end_date (Timestamp)\n",
      "[136]   ðŸ“ˆ portfolio_atrp_series (shape=(259,))\n",
      "[137]   ðŸ“ˆ benchmark_atrp_series (shape=(259,))\n",
      "[138]   ðŸ“ˆ portfolio_trp_series (shape=(259,))\n",
      "[139]   ðŸ“ˆ benchmark_trp_series (shape=(259,))\n",
      "[140]   ðŸ“„ error_msg (NoneType)\n",
      "[141]   ðŸ“‚ debug_data (len=7)\n",
      "[142]     ðŸ“‚ audit_liquidity (len=6)\n",
      "[143]       ðŸ“… date (Timestamp)\n",
      "[144]       ðŸ”¢ total_tickers_available (int)\n",
      "[145]       ðŸ”¢ percentile_setting (float)\n",
      "[146]       ðŸ”¢ final_cutoff_usd (shape=())\n",
      "[147]       ðŸ“„ tickers_passed (shape=())\n",
      "[148]       ðŸ§® universe_snapshot (shape=(1585, 14))\n",
      "[149]     ðŸ§® full_universe_ranking (shape=(943, 3))\n",
      "[150]     ðŸ“¦ inputs_snapshot (EngineInput)\n",
      "[151]       ðŸ“„ mode (str)\n",
      "[152]       ðŸ“… start_date (Timestamp)\n",
      "[153]       ðŸ”¢ lookback_period (int)\n",
      "[154]       ðŸ”¢ holding_period (int)\n",
      "[155]       ðŸ“„ metric (str)\n",
      "[156]       ðŸ“„ benchmark_ticker (str)\n",
      "[157]       ðŸ”¢ rank_start (int)\n",
      "[158]       ðŸ”¢ rank_end (int)\n",
      "[159]       ðŸ“‚ quality_thresholds (len=4)\n",
      "[160]         ðŸ”¢ min_median_dollar_volume (int)\n",
      "[161]         ðŸ”¢ min_liquidity_percentile (float)\n",
      "[162]         ðŸ”¢ max_stale_pct (float)\n",
      "[163]         ðŸ”¢ max_same_vol_count (int)\n",
      "[164]       ðŸ“‚ manual_tickers (len=0)\n",
      "[165]       ðŸ”¢ debug (bool)\n",
      "[166]       ðŸ“„ universe_subset (NoneType)\n",
      "[167]     ðŸ“‚ verification (len=2)\n",
      "[168]       ðŸ“‚ portfolio (len=12)\n",
      "                  â•°â”€â”€ full_val --> [See ID 1]\n",
      "[169]         ðŸ“ˆ full_ret (shape=(259,))\n",
      "                  â•°â”€â”€ full_atrp --> [See ID 136]\n",
      "                  â•°â”€â”€ full_trp --> [See ID 138]\n",
      "[170]         ðŸ“ˆ lookback_val (shape=(253,))\n",
      "[171]         ðŸ“ˆ lookback_ret (shape=(253,))\n",
      "[172]         ðŸ“ˆ lookback_atrp (shape=(253,))\n",
      "[173]         ðŸ“ˆ lookback_trp (shape=(253,))\n",
      "[174]         ðŸ“ˆ holding_val (shape=(6,))\n",
      "[175]         ðŸ“ˆ holding_ret (shape=(6,))\n",
      "[176]         ðŸ“ˆ holding_atrp (shape=(6,))\n",
      "[177]         ðŸ“ˆ holding_trp (shape=(6,))\n",
      "[178]       ðŸ“‚ benchmark (len=12)\n",
      "                  â•°â”€â”€ full_val --> [See ID 2]\n",
      "[179]         ðŸ“ˆ full_ret (shape=(259,))\n",
      "                  â•°â”€â”€ full_atrp --> [See ID 137]\n",
      "                  â•°â”€â”€ full_trp --> [See ID 139]\n",
      "[180]         ðŸ“ˆ lookback_val (shape=(253,))\n",
      "[181]         ðŸ“ˆ lookback_ret (shape=(253,))\n",
      "[182]         ðŸ“ˆ lookback_atrp (shape=(253,))\n",
      "[183]         ðŸ“ˆ lookback_trp (shape=(253,))\n",
      "[184]         ðŸ“ˆ holding_val (shape=(6,))\n",
      "[185]         ðŸ“ˆ holding_ret (shape=(6,))\n",
      "[186]         ðŸ“ˆ holding_atrp (shape=(6,))\n",
      "[187]         ðŸ“ˆ holding_trp (shape=(6,))\n",
      "[188]     ðŸ“‚ portfolio_raw_components (len=4)\n",
      "[189]       ðŸ§® prices (shape=(259, 100))\n",
      "[190]       ðŸ§® atrp (shape=(259, 100))\n",
      "[191]       ðŸ§® trp (shape=(259, 100))\n",
      "[192]       ðŸ§® ohlcv_raw (shape=(25900, 5))\n",
      "[193]     ðŸ“‚ benchmark_raw_components (len=4)\n",
      "[194]       ðŸ§® prices (shape=(259, 1))\n",
      "[195]       ðŸ§® atrp (shape=(259, 1))\n",
      "[196]       ðŸ§® trp (shape=(259, 1))\n",
      "[197]       ðŸ§® ohlcv_raw (shape=(259, 5))\n",
      "              â•°â”€â”€ selection_audit --> [See ID 149]\n",
      "[198]   ðŸ§® macro_df (shape=(16131, 7))\n"
     ]
    }
   ],
   "source": [
    "res1 = visualize_analyzer_structure(analyzer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f1fb2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============================================================================================\n",
      "***********************************************************************************************\n",
      "ðŸ•µï¸  STARTING SHORT-FORM AUDIT: Sharpe (ATRP) @ 2025-12-10\n",
      "âš ï¸  ASSUMPTION: Verification logic is independent, but trusts Engine source DataFrames\n",
      "   (engine.features_df, engine.df_close, and debug['portfolio_raw_components'])\n",
      "***********************************************************************************************\n",
      "===============================================================================================\n",
      "ðŸ•µï¸  AUDIT: Sharpe (ATRP) @ 2025-12-10\n",
      "===============================================================================================\n",
      "LAYER 1: SURVIVAL  | Universe: 1585 -> Survivors: 940 | âœ… PASS\n",
      "LAYER 2: SELECTION | Strategy: Sharpe (ATRP) | Selection Match: âœ… PASS\n",
      "LAYER 3: PERFORMANCE (Holding Period: 5 days)\n",
      "Metric               | Engine       | Manual       | Status\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Gain                 |    -0.007247 |    -0.007247 | âœ… PASS\n",
      "Sharpe               |    -6.897621 |    -6.897621 | âœ… PASS\n",
      "Sharpe (ATRP)        |    -0.082114 |    -0.082114 | âœ… PASS\n",
      "Sharpe (TRP)         |    -0.099573 |    -0.099573 | âœ… PASS\n",
      "===============================================================================================\n"
     ]
    }
   ],
   "source": [
    "verify_analyzer_short(analyzer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a23faaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= verify_analyzer_long (FINAL) ========= \n",
      "\n",
      "\n",
      "=====================================================================================\n",
      "ðŸ›¡ï¸  STARTING NUCLEAR AUDIT | 2025-12-10 | Sharpe (ATRP)\n",
      "=====================================================================================\n",
      "ðŸ“ 1. PERFORMANCE RECONCILIATION\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Period</th>\n",
       "      <th>Full</th>\n",
       "      <th>Holding</th>\n",
       "      <th>Lookback</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Entity</th>\n",
       "      <th>Metric</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">Benchmark</th>\n",
       "      <th>Gain</th>\n",
       "      <td>âœ… PASS</td>\n",
       "      <td>âœ… PASS</td>\n",
       "      <td>âœ… PASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sharpe</th>\n",
       "      <td>âœ… PASS</td>\n",
       "      <td>âœ… PASS</td>\n",
       "      <td>âœ… PASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sharpe (ATRP)</th>\n",
       "      <td>âœ… PASS</td>\n",
       "      <td>âœ… PASS</td>\n",
       "      <td>âœ… PASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sharpe (TRP)</th>\n",
       "      <td>âœ… PASS</td>\n",
       "      <td>âœ… PASS</td>\n",
       "      <td>âœ… PASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">Group</th>\n",
       "      <th>Gain</th>\n",
       "      <td>âœ… PASS</td>\n",
       "      <td>âœ… PASS</td>\n",
       "      <td>âœ… PASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sharpe</th>\n",
       "      <td>âœ… PASS</td>\n",
       "      <td>âœ… PASS</td>\n",
       "      <td>âœ… PASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sharpe (ATRP)</th>\n",
       "      <td>âœ… PASS</td>\n",
       "      <td>âœ… PASS</td>\n",
       "      <td>âœ… PASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sharpe (TRP)</th>\n",
       "      <td>âœ… PASS</td>\n",
       "      <td>âœ… PASS</td>\n",
       "      <td>âœ… PASS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Period                     Full Holding Lookback\n",
       "Entity    Metric                                \n",
       "Benchmark Gain           âœ… PASS  âœ… PASS   âœ… PASS\n",
       "          Sharpe         âœ… PASS  âœ… PASS   âœ… PASS\n",
       "          Sharpe (ATRP)  âœ… PASS  âœ… PASS   âœ… PASS\n",
       "          Sharpe (TRP)   âœ… PASS  âœ… PASS   âœ… PASS\n",
       "Group     Gain           âœ… PASS  âœ… PASS   âœ… PASS\n",
       "          Sharpe         âœ… PASS  âœ… PASS   âœ… PASS\n",
       "          Sharpe (ATRP)  âœ… PASS  âœ… PASS   âœ… PASS\n",
       "          Sharpe (TRP)   âœ… PASS  âœ… PASS   âœ… PASS"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====================================================================================\n",
      "ðŸ“ 2. SURVIVAL AUDIT\n",
      "   Survival Integrity: âœ… PASS (Engine: 940 vs Auditor: 940)\n",
      "\n",
      "=====================================================================================\n",
      "ðŸ“ 3. UNIVERSAL SELECTION AUDIT | Strategy: Sharpe (ATRP)\n",
      "   Scope: Evaluated 940 candidates (Full Universe).\n",
      "   Result: 940 PASSED | 0 FAILED\n",
      "   All scores match registry math. Sharpe (ATRP) results of the first 5 tickers\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_3eb9d\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_3eb9d_level0_col0\" class=\"col_heading level0 col0\" >Ticker</th>\n",
       "      <th id=\"T_3eb9d_level0_col1\" class=\"col_heading level0 col1\" >Engine</th>\n",
       "      <th id=\"T_3eb9d_level0_col2\" class=\"col_heading level0 col2\" >Manual</th>\n",
       "      <th id=\"T_3eb9d_level0_col3\" class=\"col_heading level0 col3\" >Delta</th>\n",
       "      <th id=\"T_3eb9d_level0_col4\" class=\"col_heading level0 col4\" >Status</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Rank</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "      <th class=\"blank col4\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_3eb9d_level0_row0\" class=\"row_heading level0 row0\" >1</th>\n",
       "      <td id=\"T_3eb9d_row0_col0\" class=\"data row0 col0\" >A</td>\n",
       "      <td id=\"T_3eb9d_row0_col1\" class=\"data row0 col1\" >-0.32370895</td>\n",
       "      <td id=\"T_3eb9d_row0_col2\" class=\"data row0 col2\" >-0.32370895</td>\n",
       "      <td id=\"T_3eb9d_row0_col3\" class=\"data row0 col3\" >0.00000000</td>\n",
       "      <td id=\"T_3eb9d_row0_col4\" class=\"data row0 col4\" >âœ… PASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3eb9d_level0_row1\" class=\"row_heading level0 row1\" >2</th>\n",
       "      <td id=\"T_3eb9d_row1_col0\" class=\"data row1 col0\" >AA</td>\n",
       "      <td id=\"T_3eb9d_row1_col1\" class=\"data row1 col1\" >0.31805294</td>\n",
       "      <td id=\"T_3eb9d_row1_col2\" class=\"data row1 col2\" >0.31805294</td>\n",
       "      <td id=\"T_3eb9d_row1_col3\" class=\"data row1 col3\" >0.00000000</td>\n",
       "      <td id=\"T_3eb9d_row1_col4\" class=\"data row1 col4\" >âœ… PASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3eb9d_level0_row2\" class=\"row_heading level0 row2\" >3</th>\n",
       "      <td id=\"T_3eb9d_row2_col0\" class=\"data row2 col0\" >AAL</td>\n",
       "      <td id=\"T_3eb9d_row2_col1\" class=\"data row2 col1\" >0.31547529</td>\n",
       "      <td id=\"T_3eb9d_row2_col2\" class=\"data row2 col2\" >0.31547529</td>\n",
       "      <td id=\"T_3eb9d_row2_col3\" class=\"data row2 col3\" >0.00000000</td>\n",
       "      <td id=\"T_3eb9d_row2_col4\" class=\"data row2 col4\" >âœ… PASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3eb9d_level0_row3\" class=\"row_heading level0 row3\" >4</th>\n",
       "      <td id=\"T_3eb9d_row3_col0\" class=\"data row3 col0\" >AAPL</td>\n",
       "      <td id=\"T_3eb9d_row3_col1\" class=\"data row3 col1\" >0.03533139</td>\n",
       "      <td id=\"T_3eb9d_row3_col2\" class=\"data row3 col2\" >0.03533139</td>\n",
       "      <td id=\"T_3eb9d_row3_col3\" class=\"data row3 col3\" >0.00000000</td>\n",
       "      <td id=\"T_3eb9d_row3_col4\" class=\"data row3 col4\" >âœ… PASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3eb9d_level0_row4\" class=\"row_heading level0 row4\" >5</th>\n",
       "      <td id=\"T_3eb9d_row4_col0\" class=\"data row4 col0\" >ABBV</td>\n",
       "      <td id=\"T_3eb9d_row4_col1\" class=\"data row4 col1\" >-0.11707141</td>\n",
       "      <td id=\"T_3eb9d_row4_col2\" class=\"data row4 col2\" >-0.11707141</td>\n",
       "      <td id=\"T_3eb9d_row4_col3\" class=\"data row4 col3\" >0.00000000</td>\n",
       "      <td id=\"T_3eb9d_row4_col4\" class=\"data row4 col4\" >âœ… PASS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1e201767410>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================================================\n"
     ]
    }
   ],
   "source": [
    "verify_analyzer_long(analyzer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6869fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============================================================================================\n",
      "ðŸ•µï¸  NUCLEAR FEATURE AUDIT | Mode: LAST_RUN | Tickers: 10\n",
      "===============================================================================================\n",
      "STEP 1: BOUNDARY INTEGRITY   | MultiIndex Isolation Check | âœ… PASS\n",
      "STEP 2: SHADOW CALCULATIONS  | Re-computing metrics... DONE (0.31s)\n",
      "\n",
      "Metric               | Max Delta    | Correlation  | Status\n",
      "-------------------------------------------------------------------------------------\n",
      "Ret_1d               |   0.0000e+00 |     1.000000 | âœ… PASS\n",
      "ATR                  |   0.0000e+00 |     1.000000 | âœ… PASS\n",
      "ATRP                 |   0.0000e+00 |     1.000000 | âœ… PASS\n",
      "TRP                  |   0.0000e+00 |     1.000000 | âœ… PASS\n",
      "RSI                  |   0.0000e+00 |     1.000000 | âœ… PASS\n",
      "Mom_21               |   0.0000e+00 |     1.000000 | âœ… PASS\n",
      "Consistency          |   0.0000e+00 |     1.000000 | âœ… PASS\n",
      "DD_21                |   0.0000e+00 |     1.000000 | âœ… PASS\n",
      "RollingStalePct      |   0.0000e+00 |     1.000000 | âœ… PASS\n",
      "RollMedDollarVol     |   0.0000e+00 |     1.000000 | âœ… PASS\n",
      "RollingSameVolCount  |   0.0000e+00 |     1.000000 | âœ… PASS\n",
      "Macro_Vix_Signals    | N/A          | N/A          | âœ… LIVE\n",
      "===============================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Takes 4 seconds to run, checks selected tickers from analyzer1\n",
    "audit_feature_engineering_integrity(analyzer1, mode=\"last_run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a80370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Takes 4 minutes to run, checks all tickers from analyzer1\n",
    "# audit_feature_engineering_integrity(analyzer1, df_indices=df_indices, mode=\"system\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54ee8ee",
   "metadata": {},
   "source": [
    "### Pass analyzer1's selected tickers to a second filter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0227cbd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Ready for Stage 1: Run Simulation for second filter.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "923d418a110f4c9080e212facb317126",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<b>1. Timeline Configuration:</b> (Past <--- Decision ---> Future)'), HBox(childrenâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get decision date from last run\n",
    "decision_date_last_run = FilterPack(decision_date=analyzer1.last_run.decision_date)\n",
    "\n",
    "# 1. LAUNCH STAGE 2 (Cascade)\n",
    "# universe_subset=analyzer1.last_run.tickers means \"Scan the whole market\"\n",
    "analyzer2, stage1_pack = create_walk_forward_analyzer(\n",
    "    master_engine,\n",
    "    universe_subset=analyzer1.last_run.tickers,\n",
    "    filter_pack=decision_date_last_run,\n",
    ")\n",
    "\n",
    "print(\"ðŸš€ Ready for Stage 1: Run Simulation for second filter.\")\n",
    "analyzer2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1660df07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================\n",
      "ðŸ” HIGH-TRANSPARENCY AUDIT MAP\n",
      "====================================================================\n",
      "[  0] ðŸ“¦ audit_pack (EngineOutput)\n",
      "[  1]   ðŸ“ˆ portfolio_series (shape=(17,))\n",
      "[  2]   ðŸ“ˆ benchmark_series (shape=(17,))\n",
      "[  3]   ðŸ§® normalized_plot_data (shape=(17, 10))\n",
      "[  4]   ðŸ“‚ tickers (len=10)\n",
      "[  5]     ðŸ“„ index_0 (str)\n",
      "[  6]     ðŸ“„ index_1 (str)\n",
      "[  7]     ðŸ“„ index_2 (str)\n",
      "[  8]     ðŸ“„ index_3 (str)\n",
      "[  9]     ðŸ“„ index_4 (str)\n",
      "[ 10]     ðŸ“„ index_5 (str)\n",
      "[ 11]     ðŸ“„ index_6 (str)\n",
      "[ 12]     ðŸ“„ index_7 (str)\n",
      "[ 13]     ðŸ“„ index_8 (str)\n",
      "[ 14]     ðŸ“„ index_9 (str)\n",
      "[ 15]   ðŸ“ˆ initial_weights (shape=(10,))\n",
      "[ 16]   ðŸ“‚ perf_metrics (len=24)\n",
      "[ 17]     ðŸ”¢ full_p_gain (float)\n",
      "[ 18]     ðŸ”¢ full_p_sharpe (float)\n",
      "[ 19]     ðŸ”¢ full_p_sharpe_atrp (float)\n",
      "[ 20]     ðŸ”¢ full_p_sharpe_trp (float)\n",
      "[ 21]     ðŸ”¢ lookback_p_gain (float)\n",
      "[ 22]     ðŸ”¢ lookback_p_sharpe (float)\n",
      "[ 23]     ðŸ”¢ lookback_p_sharpe_atrp (float)\n",
      "[ 24]     ðŸ”¢ lookback_p_sharpe_trp (float)\n",
      "[ 25]     ðŸ”¢ holding_p_gain (float)\n",
      "[ 26]     ðŸ”¢ holding_p_sharpe (float)\n",
      "[ 27]     ðŸ”¢ holding_p_sharpe_atrp (float)\n",
      "[ 28]     ðŸ”¢ holding_p_sharpe_trp (float)\n",
      "[ 29]     ðŸ”¢ full_b_gain (float)\n",
      "[ 30]     ðŸ”¢ full_b_sharpe (float)\n",
      "[ 31]     ðŸ”¢ full_b_sharpe_atrp (float)\n",
      "[ 32]     ðŸ”¢ full_b_sharpe_trp (float)\n",
      "[ 33]     ðŸ”¢ lookback_b_gain (float)\n",
      "[ 34]     ðŸ”¢ lookback_b_sharpe (float)\n",
      "[ 35]     ðŸ”¢ lookback_b_sharpe_atrp (float)\n",
      "[ 36]     ðŸ”¢ lookback_b_sharpe_trp (float)\n",
      "[ 37]     ðŸ”¢ holding_b_gain (float)\n",
      "[ 38]     ðŸ”¢ holding_b_sharpe (float)\n",
      "[ 39]     ðŸ”¢ holding_b_sharpe_atrp (float)\n",
      "[ 40]     ðŸ”¢ holding_b_sharpe_trp (float)\n",
      "[ 41]   ðŸ§® results_df (shape=(10, 2))\n",
      "[ 42]   ðŸ“… start_date (Timestamp)\n",
      "[ 43]   ðŸ“… decision_date (Timestamp)\n",
      "[ 44]   ðŸ“… buy_date (Timestamp)\n",
      "[ 45]   ðŸ“… holding_end_date (Timestamp)\n",
      "[ 46]   ðŸ“ˆ portfolio_atrp_series (shape=(17,))\n",
      "[ 47]   ðŸ“ˆ benchmark_atrp_series (shape=(17,))\n",
      "[ 48]   ðŸ“ˆ portfolio_trp_series (shape=(17,))\n",
      "[ 49]   ðŸ“ˆ benchmark_trp_series (shape=(17,))\n",
      "[ 50]   ðŸ“„ error_msg (NoneType)\n",
      "[ 51]   ðŸ“‚ debug_data (len=7)\n",
      "[ 52]     ðŸ“‚ audit_liquidity (len=3)\n",
      "[ 53]       ðŸ“„ mode (str)\n",
      "[ 54]       ðŸ”¢ tickers_passed (int)\n",
      "[ 55]       ðŸ”¢ forced_list (bool)\n",
      "[ 56]     ðŸ§® full_universe_ranking (shape=(10, 3))\n",
      "[ 57]     ðŸ“¦ inputs_snapshot (EngineInput)\n",
      "[ 58]       ðŸ“„ mode (str)\n",
      "[ 59]       ðŸ“… start_date (Timestamp)\n",
      "[ 60]       ðŸ”¢ lookback_period (int)\n",
      "[ 61]       ðŸ”¢ holding_period (int)\n",
      "[ 62]       ðŸ“„ metric (str)\n",
      "[ 63]       ðŸ“„ benchmark_ticker (str)\n",
      "[ 64]       ðŸ”¢ rank_start (int)\n",
      "[ 65]       ðŸ”¢ rank_end (int)\n",
      "[ 66]       ðŸ“‚ quality_thresholds (len=4)\n",
      "[ 67]         ðŸ”¢ min_median_dollar_volume (int)\n",
      "[ 68]         ðŸ”¢ min_liquidity_percentile (float)\n",
      "[ 69]         ðŸ”¢ max_stale_pct (float)\n",
      "[ 70]         ðŸ”¢ max_same_vol_count (int)\n",
      "[ 71]       ðŸ“‚ manual_tickers (len=0)\n",
      "[ 72]       ðŸ”¢ debug (bool)\n",
      "[ 73]       ðŸ“‚ universe_subset (len=10)\n",
      "[ 74]         ðŸ“„ index_0 (str)\n",
      "[ 75]         ðŸ“„ index_1 (str)\n",
      "[ 76]         ðŸ“„ index_2 (str)\n",
      "[ 77]         ðŸ“„ index_3 (str)\n",
      "[ 78]         ðŸ“„ index_4 (str)\n",
      "[ 79]         ðŸ“„ index_5 (str)\n",
      "[ 80]         ðŸ“„ index_6 (str)\n",
      "[ 81]         ðŸ“„ index_7 (str)\n",
      "[ 82]         ðŸ“„ index_8 (str)\n",
      "[ 83]         ðŸ“„ index_9 (str)\n",
      "[ 84]     ðŸ“‚ verification (len=2)\n",
      "[ 85]       ðŸ“‚ portfolio (len=12)\n",
      "                  â•°â”€â”€ full_val --> [See ID 1]\n",
      "[ 86]         ðŸ“ˆ full_ret (shape=(17,))\n",
      "                  â•°â”€â”€ full_atrp --> [See ID 46]\n",
      "                  â•°â”€â”€ full_trp --> [See ID 48]\n",
      "[ 87]         ðŸ“ˆ lookback_val (shape=(11,))\n",
      "[ 88]         ðŸ“ˆ lookback_ret (shape=(11,))\n",
      "[ 89]         ðŸ“ˆ lookback_atrp (shape=(11,))\n",
      "[ 90]         ðŸ“ˆ lookback_trp (shape=(11,))\n",
      "[ 91]         ðŸ“ˆ holding_val (shape=(6,))\n",
      "[ 92]         ðŸ“ˆ holding_ret (shape=(6,))\n",
      "[ 93]         ðŸ“ˆ holding_atrp (shape=(6,))\n",
      "[ 94]         ðŸ“ˆ holding_trp (shape=(6,))\n",
      "[ 95]       ðŸ“‚ benchmark (len=12)\n",
      "                  â•°â”€â”€ full_val --> [See ID 2]\n",
      "[ 96]         ðŸ“ˆ full_ret (shape=(17,))\n",
      "                  â•°â”€â”€ full_atrp --> [See ID 47]\n",
      "                  â•°â”€â”€ full_trp --> [See ID 49]\n",
      "[ 97]         ðŸ“ˆ lookback_val (shape=(11,))\n",
      "[ 98]         ðŸ“ˆ lookback_ret (shape=(11,))\n",
      "[ 99]         ðŸ“ˆ lookback_atrp (shape=(11,))\n",
      "[100]         ðŸ“ˆ lookback_trp (shape=(11,))\n",
      "[101]         ðŸ“ˆ holding_val (shape=(6,))\n",
      "[102]         ðŸ“ˆ holding_ret (shape=(6,))\n",
      "[103]         ðŸ“ˆ holding_atrp (shape=(6,))\n",
      "[104]         ðŸ“ˆ holding_trp (shape=(6,))\n",
      "[105]     ðŸ“‚ portfolio_raw_components (len=4)\n",
      "[106]       ðŸ§® prices (shape=(17, 10))\n",
      "[107]       ðŸ§® atrp (shape=(17, 10))\n",
      "[108]       ðŸ§® trp (shape=(17, 10))\n",
      "[109]       ðŸ§® ohlcv_raw (shape=(170, 5))\n",
      "[110]     ðŸ“‚ benchmark_raw_components (len=4)\n",
      "[111]       ðŸ§® prices (shape=(17, 1))\n",
      "[112]       ðŸ§® atrp (shape=(17, 1))\n",
      "[113]       ðŸ§® trp (shape=(17, 1))\n",
      "[114]       ðŸ§® ohlcv_raw (shape=(17, 5))\n",
      "              â•°â”€â”€ selection_audit --> [See ID 56]\n",
      "[115]   ðŸ§® macro_df (shape=(16131, 7))\n"
     ]
    }
   ],
   "source": [
    "res2 = visualize_analyzer_structure(analyzer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26193ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============================================================================================\n",
      "***********************************************************************************************\n",
      "ðŸ•µï¸  STARTING SHORT-FORM AUDIT: Sharpe (ATRP) @ 2025-12-10\n",
      "âš ï¸  ASSUMPTION: Verification logic is independent, but trusts Engine source DataFrames\n",
      "   (engine.features_df, engine.df_close, and debug['portfolio_raw_components'])\n",
      "***********************************************************************************************\n",
      "===============================================================================================\n",
      "ðŸ•µï¸  AUDIT: Sharpe (ATRP) @ 2025-12-10\n",
      "===============================================================================================\n",
      "LAYER 1: SURVIVAL  | Mode: CASCADE/SUBSET | âœ… BYPASS\n",
      "LAYER 2: SELECTION | Strategy: Sharpe (ATRP) | Selection Match: âœ… PASS\n",
      "LAYER 3: PERFORMANCE (Holding Period: 5 days)\n",
      "Metric               | Engine       | Manual       | Status\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Gain                 |    -0.007247 |    -0.007247 | âœ… PASS\n",
      "Sharpe               |    -6.897621 |    -6.897621 | âœ… PASS\n",
      "Sharpe (ATRP)        |    -0.082114 |    -0.082114 | âœ… PASS\n",
      "Sharpe (TRP)         |    -0.099573 |    -0.099573 | âœ… PASS\n",
      "===============================================================================================\n"
     ]
    }
   ],
   "source": [
    "verify_analyzer_short(analyzer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f72eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= verify_analyzer_long (FINAL) ========= \n",
      "\n",
      "\n",
      "=====================================================================================\n",
      "ðŸ›¡ï¸  STARTING NUCLEAR AUDIT | 2025-12-10 | Sharpe (ATRP)\n",
      "=====================================================================================\n",
      "ðŸ“ 1. PERFORMANCE RECONCILIATION\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Period</th>\n",
       "      <th>Full</th>\n",
       "      <th>Holding</th>\n",
       "      <th>Lookback</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Entity</th>\n",
       "      <th>Metric</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">Benchmark</th>\n",
       "      <th>Gain</th>\n",
       "      <td>âœ… PASS</td>\n",
       "      <td>âœ… PASS</td>\n",
       "      <td>âœ… PASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sharpe</th>\n",
       "      <td>âœ… PASS</td>\n",
       "      <td>âœ… PASS</td>\n",
       "      <td>âœ… PASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sharpe (ATRP)</th>\n",
       "      <td>âœ… PASS</td>\n",
       "      <td>âœ… PASS</td>\n",
       "      <td>âœ… PASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sharpe (TRP)</th>\n",
       "      <td>âœ… PASS</td>\n",
       "      <td>âœ… PASS</td>\n",
       "      <td>âœ… PASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">Group</th>\n",
       "      <th>Gain</th>\n",
       "      <td>âœ… PASS</td>\n",
       "      <td>âœ… PASS</td>\n",
       "      <td>âœ… PASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sharpe</th>\n",
       "      <td>âœ… PASS</td>\n",
       "      <td>âœ… PASS</td>\n",
       "      <td>âœ… PASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sharpe (ATRP)</th>\n",
       "      <td>âœ… PASS</td>\n",
       "      <td>âœ… PASS</td>\n",
       "      <td>âœ… PASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sharpe (TRP)</th>\n",
       "      <td>âœ… PASS</td>\n",
       "      <td>âœ… PASS</td>\n",
       "      <td>âœ… PASS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Period                     Full Holding Lookback\n",
       "Entity    Metric                                \n",
       "Benchmark Gain           âœ… PASS  âœ… PASS   âœ… PASS\n",
       "          Sharpe         âœ… PASS  âœ… PASS   âœ… PASS\n",
       "          Sharpe (ATRP)  âœ… PASS  âœ… PASS   âœ… PASS\n",
       "          Sharpe (TRP)   âœ… PASS  âœ… PASS   âœ… PASS\n",
       "Group     Gain           âœ… PASS  âœ… PASS   âœ… PASS\n",
       "          Sharpe         âœ… PASS  âœ… PASS   âœ… PASS\n",
       "          Sharpe (ATRP)  âœ… PASS  âœ… PASS   âœ… PASS\n",
       "          Sharpe (TRP)   âœ… PASS  âœ… PASS   âœ… PASS"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====================================================================================\n",
      "ðŸ“ 2. SURVIVAL AUDIT\n",
      "   Mode: CASCADE/SUBSET | Logic: Quality filters bypassed per design. | âœ… BYPASS\n",
      "\n",
      "=====================================================================================\n",
      "ðŸ“ 3. UNIVERSAL SELECTION AUDIT | Strategy: Sharpe (ATRP)\n",
      "   Scope: Evaluated 10 candidates (Full Universe).\n",
      "   Result: 10 PASSED | 0 FAILED\n",
      "   All scores match registry math. Sharpe (ATRP) results of the first 5 tickers\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_f753d\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_f753d_level0_col0\" class=\"col_heading level0 col0\" >Ticker</th>\n",
       "      <th id=\"T_f753d_level0_col1\" class=\"col_heading level0 col1\" >Engine</th>\n",
       "      <th id=\"T_f753d_level0_col2\" class=\"col_heading level0 col2\" >Manual</th>\n",
       "      <th id=\"T_f753d_level0_col3\" class=\"col_heading level0 col3\" >Delta</th>\n",
       "      <th id=\"T_f753d_level0_col4\" class=\"col_heading level0 col4\" >Status</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Rank</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "      <th class=\"blank col4\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_f753d_level0_row0\" class=\"row_heading level0 row0\" >1</th>\n",
       "      <td id=\"T_f753d_row0_col0\" class=\"data row0 col0\" >BIL</td>\n",
       "      <td id=\"T_f753d_row0_col1\" class=\"data row0 col1\" >0.74485227</td>\n",
       "      <td id=\"T_f753d_row0_col2\" class=\"data row0 col2\" >0.74485227</td>\n",
       "      <td id=\"T_f753d_row0_col3\" class=\"data row0 col3\" >0.00000000</td>\n",
       "      <td id=\"T_f753d_row0_col4\" class=\"data row0 col4\" >âœ… PASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f753d_level0_row1\" class=\"row_heading level0 row1\" >2</th>\n",
       "      <td id=\"T_f753d_row1_col0\" class=\"data row1 col0\" >CFLT</td>\n",
       "      <td id=\"T_f753d_row1_col1\" class=\"data row1 col1\" >0.80342188</td>\n",
       "      <td id=\"T_f753d_row1_col2\" class=\"data row1 col2\" >0.80342188</td>\n",
       "      <td id=\"T_f753d_row1_col3\" class=\"data row1 col3\" >0.00000000</td>\n",
       "      <td id=\"T_f753d_row1_col4\" class=\"data row1 col4\" >âœ… PASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f753d_level0_row2\" class=\"row_heading level0 row2\" >3</th>\n",
       "      <td id=\"T_f753d_row2_col0\" class=\"data row2 col0\" >DG</td>\n",
       "      <td id=\"T_f753d_row2_col1\" class=\"data row2 col1\" >0.66862304</td>\n",
       "      <td id=\"T_f753d_row2_col2\" class=\"data row2 col2\" >0.66862304</td>\n",
       "      <td id=\"T_f753d_row2_col3\" class=\"data row2 col3\" >0.00000000</td>\n",
       "      <td id=\"T_f753d_row2_col4\" class=\"data row2 col4\" >âœ… PASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f753d_level0_row3\" class=\"row_heading level0 row3\" >4</th>\n",
       "      <td id=\"T_f753d_row3_col0\" class=\"data row3 col0\" >MCHP</td>\n",
       "      <td id=\"T_f753d_row3_col1\" class=\"data row3 col1\" >0.70179452</td>\n",
       "      <td id=\"T_f753d_row3_col2\" class=\"data row3 col2\" >0.70179452</td>\n",
       "      <td id=\"T_f753d_row3_col3\" class=\"data row3 col3\" >0.00000000</td>\n",
       "      <td id=\"T_f753d_row3_col4\" class=\"data row3 col4\" >âœ… PASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f753d_level0_row4\" class=\"row_heading level0 row4\" >5</th>\n",
       "      <td id=\"T_f753d_row4_col0\" class=\"data row4 col0\" >MINT</td>\n",
       "      <td id=\"T_f753d_row4_col1\" class=\"data row4 col1\" >0.66399845</td>\n",
       "      <td id=\"T_f753d_row4_col2\" class=\"data row4 col2\" >0.66399845</td>\n",
       "      <td id=\"T_f753d_row4_col3\" class=\"data row4 col3\" >0.00000000</td>\n",
       "      <td id=\"T_f753d_row4_col4\" class=\"data row4 col4\" >âœ… PASS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1e201746d10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================================================\n"
     ]
    }
   ],
   "source": [
    "verify_analyzer_long(analyzer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef90a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============================================================================================\n",
      "ðŸ•µï¸  NUCLEAR FEATURE AUDIT | Mode: LAST_RUN | Tickers: 10\n",
      "===============================================================================================\n",
      "STEP 1: BOUNDARY INTEGRITY   | MultiIndex Isolation Check | âœ… PASS\n",
      "STEP 2: SHADOW CALCULATIONS  | Re-computing metrics... DONE (0.30s)\n",
      "\n",
      "Metric               | Max Delta    | Correlation  | Status\n",
      "-------------------------------------------------------------------------------------\n",
      "Ret_1d               |   0.0000e+00 |     1.000000 | âœ… PASS\n",
      "ATR                  |   0.0000e+00 |     1.000000 | âœ… PASS\n",
      "ATRP                 |   0.0000e+00 |     1.000000 | âœ… PASS\n",
      "TRP                  |   0.0000e+00 |     1.000000 | âœ… PASS\n",
      "RSI                  |   0.0000e+00 |     1.000000 | âœ… PASS\n",
      "Mom_21               |   0.0000e+00 |     1.000000 | âœ… PASS\n",
      "Consistency          |   0.0000e+00 |     1.000000 | âœ… PASS\n",
      "DD_21                |   0.0000e+00 |     1.000000 | âœ… PASS\n",
      "RollingStalePct      |   0.0000e+00 |     1.000000 | âœ… PASS\n",
      "RollMedDollarVol     |   0.0000e+00 |     1.000000 | âœ… PASS\n",
      "RollingSameVolCount  |   0.0000e+00 |     1.000000 | âœ… PASS\n",
      "Macro_Vix_Signals    | N/A          | N/A          | âœ… LIVE\n",
      "===============================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Takes 4 seconds to run, checks selected tickers from from analyzer2\n",
    "audit_feature_engineering_integrity(analyzer2, mode=\"last_run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8e4409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Ready for Stage 1: Run Simulation for second filter.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cbd80339f5e461481fc05bde83dd724",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<b>1. Timeline Configuration:</b> (Past <--- Decision ---> Future)'), HBox(childrenâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get decision date from last run\n",
    "decision_date_last_run = FilterPack(decision_date=analyzer2.last_run.decision_date)\n",
    "\n",
    "# 1. LAUNCH STAGE 2 (Cascade)\n",
    "# universe_subset=analyzer1.last_run.tickers means \"Scan the whole market\"\n",
    "analyzer2, stage1_pack = create_walk_forward_analyzer(\n",
    "    master_engine,\n",
    "    universe_subset=analyzer2.last_run.tickers,\n",
    "    # filter_pack=decision_date_last_run,\n",
    ")\n",
    "\n",
    "print(\"ðŸš€ Ready for Stage 1: Run Simulation for second filter.\")\n",
    "analyzer2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbf136b",
   "metadata": {},
   "source": [
    "### Combine Ticker's OHLCV with its Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d52805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tickers: ['SHV', 'CFLT', 'SGOV', 'BIL', 'WBD', 'TD', 'MCHP', 'SLV', 'DG', 'MINT']\n",
      "start_date: 2025-11-25\n",
      "decision_date: 2025-12-10\n",
      "buy_date: 2025-12-11\n",
      "end_date: 2025-12-18\n",
      "Creating combined dictionary for 10 ticker(s)\n",
      "Date range: 2025-11-25 00:00:00 to 2025-12-18 00:00:00\n",
      "============================================================\n",
      "Data retrieved for 10 ticker(s) from 2025-11-25 00:00:00 to 2025-12-18 00:00:00\n",
      "Total rows: 170\n",
      "Date range in data: 2025-11-25 00:00:00 to 2025-12-18 00:00:00\n",
      "  SHV: 17 rows\n",
      "  CFLT: 17 rows\n",
      "  SGOV: 17 rows\n",
      "  BIL: 17 rows\n",
      "  WBD: 17 rows\n",
      "  TD: 17 rows\n",
      "  MCHP: 17 rows\n",
      "  SLV: 17 rows\n",
      "  DG: 17 rows\n",
      "  MINT: 17 rows\n",
      "Features data retrieved for 10 ticker(s) from 2025-11-25 00:00:00 to 2025-12-18 00:00:00\n",
      "Total rows: 170\n",
      "Date range in data: 2025-11-25 00:00:00 to 2025-12-18 00:00:00\n",
      "Available features: ATR, ATRP, TRP, RSI, Mom_21, Consistency, IR_63, Beta_63, DD_21, Ret_1d, RollingStalePct, RollMedDollarVol, RollingSameVolCount\n",
      "  SHV: 17 rows\n",
      "  CFLT: 17 rows\n",
      "  SGOV: 17 rows\n",
      "  BIL: 17 rows\n",
      "  WBD: 17 rows\n",
      "  TD: 17 rows\n",
      "  MCHP: 17 rows\n",
      "  SLV: 17 rows\n",
      "  DG: 17 rows\n",
      "  MINT: 17 rows\n",
      "\n",
      "Processing SHV...\n",
      "  âœ“ Successfully combined data\n",
      "  OHLCV shape: (17, 5)\n",
      "  Features shape: (17, 13)\n",
      "  Combined shape: (17, 18)\n",
      "  Date range: 2025-11-25 00:00:00 to 2025-12-18 00:00:00\n",
      "\n",
      "Processing CFLT...\n",
      "  âœ“ Successfully combined data\n",
      "  OHLCV shape: (17, 5)\n",
      "  Features shape: (17, 13)\n",
      "  Combined shape: (17, 18)\n",
      "  Date range: 2025-11-25 00:00:00 to 2025-12-18 00:00:00\n",
      "\n",
      "Processing SGOV...\n",
      "  âœ“ Successfully combined data\n",
      "  OHLCV shape: (17, 5)\n",
      "  Features shape: (17, 13)\n",
      "  Combined shape: (17, 18)\n",
      "  Date range: 2025-11-25 00:00:00 to 2025-12-18 00:00:00\n",
      "\n",
      "Processing BIL...\n",
      "  âœ“ Successfully combined data\n",
      "  OHLCV shape: (17, 5)\n",
      "  Features shape: (17, 13)\n",
      "  Combined shape: (17, 18)\n",
      "  Date range: 2025-11-25 00:00:00 to 2025-12-18 00:00:00\n",
      "\n",
      "Processing WBD...\n",
      "  âœ“ Successfully combined data\n",
      "  OHLCV shape: (17, 5)\n",
      "  Features shape: (17, 13)\n",
      "  Combined shape: (17, 18)\n",
      "  Date range: 2025-11-25 00:00:00 to 2025-12-18 00:00:00\n",
      "\n",
      "Processing TD...\n",
      "  âœ“ Successfully combined data\n",
      "  OHLCV shape: (17, 5)\n",
      "  Features shape: (17, 13)\n",
      "  Combined shape: (17, 18)\n",
      "  Date range: 2025-11-25 00:00:00 to 2025-12-18 00:00:00\n",
      "\n",
      "Processing MCHP...\n",
      "  âœ“ Successfully combined data\n",
      "  OHLCV shape: (17, 5)\n",
      "  Features shape: (17, 13)\n",
      "  Combined shape: (17, 18)\n",
      "  Date range: 2025-11-25 00:00:00 to 2025-12-18 00:00:00\n",
      "\n",
      "Processing SLV...\n",
      "  âœ“ Successfully combined data\n",
      "  OHLCV shape: (17, 5)\n",
      "  Features shape: (17, 13)\n",
      "  Combined shape: (17, 18)\n",
      "  Date range: 2025-11-25 00:00:00 to 2025-12-18 00:00:00\n",
      "\n",
      "Processing DG...\n",
      "  âœ“ Successfully combined data\n",
      "  OHLCV shape: (17, 5)\n",
      "  Features shape: (17, 13)\n",
      "  Combined shape: (17, 18)\n",
      "  Date range: 2025-11-25 00:00:00 to 2025-12-18 00:00:00\n",
      "\n",
      "Processing MINT...\n",
      "  âœ“ Successfully combined data\n",
      "  OHLCV shape: (17, 5)\n",
      "  Features shape: (17, 13)\n",
      "  Combined shape: (17, 18)\n",
      "  Date range: 2025-11-25 00:00:00 to 2025-12-18 00:00:00\n",
      "\n",
      "============================================================\n",
      "SUMMARY\n",
      "============================================================\n",
      "Total tickers processed: 10\n",
      "Tickers with combined data: 10\n",
      "\n",
      "Ticker details:\n",
      "  SHV: (17, 18) - 2025-11-25 00:00:00 to 2025-12-18 00:00:00\n",
      "    Columns: 18\n",
      "  CFLT: (17, 18) - 2025-11-25 00:00:00 to 2025-12-18 00:00:00\n",
      "    Columns: 18\n",
      "  SGOV: (17, 18) - 2025-11-25 00:00:00 to 2025-12-18 00:00:00\n",
      "    Columns: 18\n",
      "  BIL: (17, 18) - 2025-11-25 00:00:00 to 2025-12-18 00:00:00\n",
      "    Columns: 18\n",
      "  WBD: (17, 18) - 2025-11-25 00:00:00 to 2025-12-18 00:00:00\n",
      "    Columns: 18\n",
      "  TD: (17, 18) - 2025-11-25 00:00:00 to 2025-12-18 00:00:00\n",
      "    Columns: 18\n",
      "  MCHP: (17, 18) - 2025-11-25 00:00:00 to 2025-12-18 00:00:00\n",
      "    Columns: 18\n",
      "  SLV: (17, 18) - 2025-11-25 00:00:00 to 2025-12-18 00:00:00\n",
      "    Columns: 18\n",
      "  DG: (17, 18) - 2025-11-25 00:00:00 to 2025-12-18 00:00:00\n",
      "    Columns: 18\n",
      "  MINT: (17, 18) - 2025-11-25 00:00:00 to 2025-12-18 00:00:00\n",
      "    Columns: 18\n"
     ]
    }
   ],
   "source": [
    "# 1. Access the result object from the analyzer\n",
    "res = analyzer2.last_run\n",
    "\n",
    "if res is None:\n",
    "    print(\"âŒ No results found in analyzer2. Please click 'Run Simulation' first.\")\n",
    "else:\n",
    "    # 2. Extract attributes directly (No [0] needed)\n",
    "    tickers = res.tickers\n",
    "    start_date = res.start_date\n",
    "    decision_date = res.decision_date\n",
    "    buy_date = res.buy_date\n",
    "    end_date = res.holding_end_date\n",
    "\n",
    "    print(f\"tickers: {tickers}\")\n",
    "    print(f\"start_date: {start_date.date()}\")\n",
    "    print(f\"decision_date: {decision_date.date()}\")\n",
    "    print(f\"buy_date: {buy_date.date()}\")\n",
    "    print(f\"end_date: {end_date.date()}\")\n",
    "\n",
    "    # 3. Generate the combined dict\n",
    "    combined = create_combined_dict(\n",
    "        df_ohlcv=df_ohlcv.copy(),\n",
    "        features_df=features_df,\n",
    "        tickers=tickers,\n",
    "        date_start=start_date,\n",
    "        date_end=end_date,\n",
    "        verbose=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811ae76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHV:\n",
      "               Adj Open     Adj High      Adj Low    Adj Close   Volume        ATR       ATRP        TRP         RSI     Mom_21  Consistency       IR_63     Beta_63      DD_21     Ret_1d  RollingStalePct   RollMedDollarVol  RollingSameVolCount\n",
      "Date                                                                                                                                                                                                                                               \n",
      "2025-11-25 109.35200000 109.35200000 109.34300000 109.35200000  1878725 0.01827799 0.00016715 0.00008230 97.69261102 0.00281535   1.00000000 -0.07673287 -0.00259513 0.00000000 0.00008231       0.00000000 331111221.04350001           0.00000000\n",
      "2025-11-26 109.37200000 109.38200000 109.37200000 109.37200000  1916888 0.01911528 0.00017477 0.00027429 97.88852544 0.00291599   1.00000000 -0.08307270 -0.00248679 0.00000000 0.00018290       0.00000000 329635504.07099998           0.00000000\n",
      "2025-11-28 109.41200000 109.41200000 109.40200000 109.40200000  2528390 0.02060704 0.00018836 0.00036562 98.14320057 0.00327388   1.00000000 -0.10703806 -0.00182362 0.00000000 0.00027429       0.00000000 326997727.62250000           0.00000000\n",
      "2025-12-01 109.42400000 109.42400000 109.41400000 109.42400000  3528203 0.02070654 0.00018923 0.00020105 98.30468722 0.00339281   1.00000000 -0.11356417 -0.00167842 0.00000000 0.00020109       0.00000000 326997727.62250000           0.00000000\n",
      "2025-12-02 109.43400000 109.44400000 109.43400000 109.44400000  1900212 0.02065607 0.00018874 0.00018274 98.43770979 0.00339219   1.00000000 -0.10651204 -0.00170442 0.00000000 0.00018278       0.00000000 326997727.62250000           0.00000000\n",
      "\n",
      "CFLT:\n",
      "              Adj Open    Adj High     Adj Low   Adj Close   Volume        ATR       ATRP        TRP         RSI      Mom_21  Consistency      IR_63    Beta_63       DD_21      Ret_1d  RollingStalePct   RollMedDollarVol  RollingSameVolCount\n",
      "Date                                                                                                                                                                                                                                            \n",
      "2025-11-25 21.00000000 21.91500000 20.89000000 21.83000000  5091100 1.05819143 0.04847418 0.04695373 45.43437307 -0.01132246   0.40000000 0.07284662 1.55061289 -0.10715746  0.03656220       0.00000000 125825077.00000000           0.00000000\n",
      "2025-11-26 21.83000000 22.15500000 21.58000000 21.83000000  4071600 1.02367776 0.04689316 0.02633990 45.43437307 -0.08084211   0.40000000 0.03548975 1.48989171 -0.10715746  0.00000000       0.00000000 124871108.00000000           0.00000000\n",
      "2025-11-28 22.03000000 22.40000000 21.84000000 22.25000000  1861800 0.99127221 0.04455156 0.02561798 49.14338416 -0.05919662   0.60000000 0.04576790 1.50036011 -0.08997955  0.01923958       0.00000000 124114517.00000000           0.00000000\n",
      "2025-12-01 21.84000000 22.63500000 21.66000000 21.87000000  5725100 0.99010991 0.04527252 0.04458162 46.09076986 -0.07015306   0.60000000 0.05470184 1.46265437 -0.10552147 -0.01707865       0.00000000 124114517.00000000           0.00000000\n",
      "2025-12-02 22.45000000 23.17000000 22.41000000 23.06000000  4780600 1.01224491 0.04389614 0.05637467 55.42798188 -0.01326487   0.60000000 0.08553565 1.48218536 -0.05685072  0.05441244       0.00000000 124114517.00000000           0.00000000\n",
      "\n",
      "SGOV:\n",
      "              Adj Open    Adj High     Adj Low   Adj Close    Volume        ATR       ATRP        TRP         RSI     Mom_21  Consistency       IR_63     Beta_63      DD_21     Ret_1d  RollingStalePct    RollMedDollarVol  RollingSameVolCount\n",
      "Date                                                                                                                                                                                                                                             \n",
      "2025-11-25 99.68800000 99.68800000 99.67810000 99.68800000  19295631 0.01834291 0.00018400 0.00019862 99.53588627 0.00316786   1.00000000 -0.07639973 -0.00268951 0.00000000 0.00019866       0.00000000 1065799596.52090001           0.00000000\n",
      "2025-11-26 99.69790000 99.70780000 99.69790000 99.69790000  12061409 0.01844699 0.00018503 0.00019860 99.55772695 0.00316855   1.00000000 -0.08291424 -0.00272323 0.00000000 0.00009931       0.00000000 1069849375.21889997           0.00000000\n",
      "2025-11-28 99.72760000 99.73750000 99.71770000 99.72760000  24648238 0.01995792 0.00020012 0.00039708 99.61609458 0.00336744   1.00000000 -0.10714982 -0.00177160 0.00000000 0.00029790       0.00000000 1071194917.72519994           0.00000000\n",
      "2025-12-01 99.75050000 99.75050000 99.74050000 99.75050000  29208786 0.02016807 0.00020219 0.00022957 99.65400945 0.00349789   1.00000000 -0.11324700 -0.00203037 0.00000000 0.00022963       0.00000000 1074326517.76339984           0.00000000\n",
      "2025-12-02 99.76040000 99.77030000 99.76040000 99.77030000  14453245 0.02014177 0.00020188 0.00019846 99.68314727 0.00339829   1.00000000 -0.10601804 -0.00196087 0.00000000 0.00019850       0.00000000 1076622041.79395008           0.00000000\n",
      "\n",
      "BIL:\n",
      "              Adj Open    Adj High     Adj Low   Adj Close    Volume        ATR       ATRP        TRP         RSI     Mom_21  Consistency       IR_63     Beta_63      DD_21     Ret_1d  RollingStalePct   RollMedDollarVol  RollingSameVolCount\n",
      "Date                                                                                                                                                                                                                                            \n",
      "2025-11-25 90.76280000 90.76280000 90.75290000 90.76280000   9083269 0.01866307 0.00020562 0.00021815 97.58173944 0.00309340   0.60000000 -0.07714638 -0.00132350 0.00000000 0.00021820       0.00793651 759245312.56279993           0.00000000\n",
      "2025-11-26 90.78260000 90.78260000 90.77270000 90.77270000   6817704 0.01874428 0.00020650 0.00021813 97.70703400 0.00309417   0.80000000 -0.08366530 -0.00135411 0.00000000 0.00010908       0.00793651 759245312.56279993           0.00000000\n",
      "2025-11-28 90.80240000 90.81230000 90.80240000 90.80240000  10884083 0.02023397 0.00022284 0.00043611 98.03582197 0.00309427   0.80000000 -0.10752210 -0.00064073 0.00000000 0.00032719       0.00793651 762665688.35774994           0.00000000\n",
      "2025-12-01 90.82330000 90.82330000 90.81340000 90.81340000  16974973 0.02028155 0.00022333 0.00023014 98.14208103 0.00321578   0.80000000 -0.11419776 -0.00039917 0.00000000 0.00012114       0.00793651 765387370.61704993           0.00000000\n",
      "2025-12-02 90.83320000 90.84320000 90.82330000 90.84320000   7455300 0.02096144 0.00023074 0.00032804 98.39534584 0.00332552   1.00000000 -0.10671455 -0.00030821 0.00000000 0.00032815       0.00793651 765387370.61704993           0.00000000\n",
      "\n",
      "WBD:\n",
      "              Adj Open    Adj High     Adj Low   Adj Close    Volume        ATR       ATRP        TRP         RSI     Mom_21  Consistency      IR_63    Beta_63       DD_21      Ret_1d  RollingStalePct   RollMedDollarVol  RollingSameVolCount\n",
      "Date                                                                                                                                                                                                                                            \n",
      "2025-11-25 22.85000000 23.58000000 22.59000000 22.96000000  31113400 0.87801041 0.03824087 0.04311847 60.65728470 0.09125475   0.40000000 0.21827541 1.15292287 -0.03081469  0.00437445       0.00000000 461417225.50000000           0.00000000\n",
      "2025-11-26 23.17000000 23.91000000 23.17000000 23.88000000  40581600 0.88315252 0.03698294 0.03978224 66.74433064 0.13768461   0.60000000 0.23289430 1.20382000  0.00000000  0.04006969       0.00000000 462639157.50000000           0.00000000\n",
      "2025-11-28 23.80000000 24.20000000 23.65000000 24.00000000  19985500 0.85935592 0.03580650 0.02291667 67.45170385 0.12464855   0.80000000 0.24360277 1.11736214  0.00000000  0.00502513       0.00000000 463016717.50000000           0.00000000\n",
      "2025-12-01 23.82000000 24.12000000 23.64000000 23.87000000  36016900 0.83225906 0.03486632 0.02010892 65.81836315 0.10407031   0.60000000 0.24131772 1.12397125 -0.00541667 -0.00541667       0.00000000 463861951.50000000           0.00000000\n",
      "2025-12-02 24.25000000 24.76000000 24.03000000 24.53000000  46583800 0.83638342 0.03409635 0.03628210 69.81472060 0.09265033   0.80000000 0.24301037 1.11727071  0.00000000  0.02764977       0.00000000 465674681.50000000           0.00000000\n",
      "\n",
      "TD:\n",
      "              Adj Open    Adj High     Adj Low   Adj Close   Volume        ATR       ATRP        TRP         RSI     Mom_21  Consistency      IR_63    Beta_63       DD_21      Ret_1d  RollingStalePct   RollMedDollarVol  RollingSameVolCount\n",
      "Date                                                                                                                                                                                                                                           \n",
      "2025-11-25 82.50000000 83.02000000 81.94000000 82.31000000  2191800 1.19678953 0.01454003 0.01312113 57.19807551 0.00833027   0.40000000 0.04914724 0.45686095 -0.00519700 -0.00471584       0.00000000 115562541.61080000           0.00000000\n",
      "2025-11-26 82.69000000 83.57000000 82.44000000 83.44000000  2492100 1.20130456 0.01439723 0.01510067 62.97766973 0.00870406   0.60000000 0.15170702 0.50546847  0.00000000  0.01372859       0.00000000 115692425.61080001           0.00000000\n",
      "2025-11-28 83.55000000 84.06000000 83.37000000 83.93000000   654200 1.16478281 0.01387803 0.00822114 65.17372562 0.02629005   0.80000000 0.10136620 0.56755841  0.00000000  0.00587248       0.00000000 115562541.61080000           0.00000000\n",
      "2025-12-01 83.95000000 84.23000000 83.02000000 83.50000000  2457800 1.16801261 0.01398817 0.01449102 61.71416040 0.01941155   0.60000000 0.10167596 0.56153739 -0.00512332 -0.00512332       0.00000000 115562541.61080000           0.00000000\n",
      "2025-12-02 83.82000000 84.55000000 83.00000000 84.55000000  4317300 1.19529742 0.01413717 0.01833235 66.40384919 0.02946548   0.60000000 0.12371508 0.56381212  0.00000000  0.01257485       0.00000000 115692425.61080001           0.00000000\n",
      "\n",
      "MCHP:\n",
      "              Adj Open    Adj High     Adj Low   Adj Close    Volume        ATR       ATRP        TRP         RSI      Mom_21  Consistency       IR_63    Beta_63       DD_21      Ret_1d  RollingStalePct   RollMedDollarVol  RollingSameVolCount\n",
      "Date                                                                                                                                                                                                                                              \n",
      "2025-11-25 50.33000000 52.32000000 49.80000000 51.83000000   9605200 2.29601209 0.04429890 0.04862049 35.70700239 -0.18981445   0.60000000 -0.21765457 1.84938773 -0.17822901  0.01131707       0.00000000 500875292.06895000           0.00000000\n",
      "2025-11-26 51.87000000 53.36000000 51.55000000 52.57000000   6266100 2.26129694 0.04301497 0.03443028 38.72665255 -0.16649622   0.80000000 -0.19327856 1.87615223 -0.15183548  0.01427745       0.00000000 500875292.06895000           0.00000000\n",
      "2025-11-28 52.69000000 53.74000000 52.37000000 53.58000000   3755000 2.19763287 0.04101592 0.02556924 42.68347997 -0.13554014   1.00000000 -0.18290843 1.91423121 -0.13387863  0.01921248       0.00000000 499822651.81840003           0.00000000\n",
      "2025-12-01 52.88000000 54.27000000 52.62000000 53.43000000   5580800 2.15851624 0.04039896 0.03088153 42.24713762 -0.13143418   0.80000000 -0.17049252 1.89201633 -0.13630338 -0.00279955       0.00000000 498826625.34144998           0.00000000\n",
      "2025-12-02 53.80000000 57.35000000 53.41000000 56.71000000  11954000 2.28576508 0.04030621 0.06947628 53.45260941 -0.08328214   0.80000000 -0.10635363 1.91989858 -0.08313541  0.06138873       0.00000000 499822651.81840003           0.00000000\n",
      "\n",
      "SLV:\n",
      "              Adj Open    Adj High     Adj Low   Adj Close    Volume        ATR       ATRP        TRP         RSI     Mom_21  Consistency      IR_63    Beta_63       DD_21     Ret_1d  RollingStalePct   RollMedDollarVol  RollingSameVolCount\n",
      "Date                                                                                                                                                                                                                                           \n",
      "2025-11-25 46.55000000 46.77000000 45.97000000 46.67000000  24006600 1.28764954 0.02759052 0.01714163 57.39853819 0.10070755   0.60000000 0.18805167 0.42836665 -0.03414735 0.00085782       0.00000000 641802804.00000000           0.00000000\n",
      "2025-11-26 47.31000000 48.45000000 47.15000000 48.40000000  26280300 1.32281743 0.02733094 0.03677686 63.60823976 0.13348946   0.60000000 0.19958359 0.46952826  0.00000000 0.03706878       0.00000000 645719956.50000000           0.00000000\n",
      "2025-11-28 49.67000000 51.27000000 49.58000000 51.21000000  41350000 1.43333047 0.02798927 0.05604374 71.00191602 0.18459403   0.80000000 0.21305970 0.56117836  0.00000000 0.05805785       0.00000000 647805484.50000000           0.00000000\n",
      "2025-12-01 51.99000000 53.36000000 51.61000000 52.52000000  66064100 1.48452115 0.02826583 0.04093679 73.68598765 0.18448354   1.00000000 0.21108171 0.58450532  0.00000000 0.02558094       0.00000000 650740709.50000000           0.00000000\n",
      "2025-12-02 52.83000000 53.20000000 51.77000000 53.13000000  43952700 1.48062678 0.02786800 0.02691511 74.85320224 0.20722563   1.00000000 0.21822030 0.58963432  0.00000000 0.01161462       0.00000000 652100645.50000000           0.00000000\n",
      "\n",
      "DG:\n",
      "               Adj Open     Adj High      Adj Low    Adj Close   Volume        ATR       ATRP        TRP         RSI     Mom_21  Consistency       IR_63    Beta_63       DD_21      Ret_1d  RollingStalePct   RollMedDollarVol  RollingSameVolCount\n",
      "Date                                                                                                                                                                                                                                                \n",
      "2025-11-25 102.10500000 105.87800000 101.81600000 103.86700000  3029666 2.88010455 0.02772877 0.04447996 56.37987266 0.01498036   0.60000000 -0.08183022 0.32968531  0.00000000  0.02576587       0.00000000 297886380.43519998           0.00000000\n",
      "2025-11-26 104.28500000 108.78600000 104.07600000 108.30800000  3131699 3.02573994 0.02793644 0.04541677 65.06759343 0.06721058   0.80000000 -0.05040190 0.39338524  0.00000000  0.04275660       0.00000000 298085239.01719999           0.00000000\n",
      "2025-11-28 108.18900000 109.31400000 108.03900000 109.02500000  1386287 2.90068709 0.02660571 0.01169457 66.23678529 0.09588614   0.80000000 -0.03273064 0.35556008  0.00000000  0.00662001       0.00000000 297886380.43519998           0.00000000\n",
      "2025-12-01 108.68600000 110.59800000 107.82000000 108.87600000  3335464 2.89192372 0.02656163 0.02551527 65.74432936 0.09801517   0.60000000 -0.04997969 0.39670238 -0.00136666 -0.00136666       0.00000000 298085239.01719999           0.00000000\n",
      "2025-12-02 109.05500000 110.04100000 107.93000000 109.56300000  3578296 2.83614346 0.02588596 0.01926745 66.96391378 0.11524720   0.80000000 -0.03576725 0.40868863  0.00000000  0.00630993       0.00000000 298620378.53909993           0.00000000\n",
      "\n",
      "MINT:\n",
      "              Adj Open    Adj High     Adj Low   Adj Close   Volume        ATR       ATRP        TRP         RSI     Mom_21  Consistency       IR_63    Beta_63      DD_21     Ret_1d  RollingStalePct   RollMedDollarVol  RollingSameVolCount\n",
      "Date                                                                                                                                                                                                                                          \n",
      "2025-11-25 99.53460000 99.54440000 99.52470000 99.53460000  1548095 0.02556732 0.00025687 0.00019792 95.68300954 0.00378887   0.60000000 -0.07536762 0.00402930 0.00000000 0.00009947       0.00000000 115897034.93380000           0.00000000\n",
      "2025-11-26 99.55430000 99.57410000 99.54440000 99.57410000  1299967 0.02656251 0.00026676 0.00039669 96.28679679 0.00388755   0.80000000 -0.08155368 0.00434029 0.00000000 0.00039685       0.00000000 116003831.21265000           0.00000000\n",
      "2025-11-28 99.57410000 99.58400000 99.57410000 99.58400000   961078 0.02537233 0.00025478 0.00009941 96.42187375 0.00378799   1.00000000 -0.10566044 0.00450677 0.00000000 0.00009942       0.00000000 116003831.21265000           0.00000000\n",
      "2025-12-01 99.60390000 99.61380000 99.59400000 99.60390000  1855880 0.02568859 0.00025791 0.00029919 96.68307251 0.00398858   1.00000000 -0.11185939 0.00435036 0.00000000 0.00019983       0.00000000 116572871.28710000           0.00000000\n",
      "2025-12-02 99.61380000 99.62370000 99.60390000 99.61380000  1168107 0.02526798 0.00025366 0.00019877 96.80791342 0.00368976   1.00000000 -0.10459488 0.00456368 0.00000000 0.00009939       0.00000000 116699844.73510000           0.00000000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for ticker in tickers:\n",
    "    with pd.option_context(\"display.float_format\", \"{:.8f}\".format):\n",
    "        print(f\"{ticker}:\\n{combined[ticker].head()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c160a5",
   "metadata": {},
   "source": [
    "### Audit features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa40b6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Takes 4 minutes to run, checks all tickers from analyzer2\n",
    "# audit_feature_engineering_integrity(analyzer2, df_indices=df_indices, mode=\"system\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf25a0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
