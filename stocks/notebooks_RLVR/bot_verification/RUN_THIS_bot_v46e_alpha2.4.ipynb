{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "192dddfd",
   "metadata": {},
   "source": [
    "### Use bot_verification_v2.xlsx to Verify Calculation  \n",
    "### C:\\Users\\ping\\Files_win10\\python\\py311\\stocks\\notebooks_RLVR\\bot_verification\\bot_verification_v2.xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1110f2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from dataclasses import dataclass, field, asdict, is_dataclass\n",
    "from typing import List, Dict, Optional, Any, Union, TypedDict\n",
    "from collections import Counter\n",
    "from datetime import datetime, date\n",
    "from pandas.testing import assert_series_equal\n",
    "\n",
    "\n",
    "# pd.set_option('display.max_rows', None)  display all rows\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", 1000)\n",
    "pd.set_option(\"display.max_colwidth\", 50)\n",
    "pd.set_option(\"display.precision\", 4)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# GLOBAL SETTINGS: The \"Control Panel\" for the Strategy\n",
    "# ==============================================================================\n",
    "\n",
    "GLOBAL_SETTINGS = {\n",
    "    # ENVIRONMENT (The \"Where\")\n",
    "    \"benchmark_ticker\": \"SPY\",\n",
    "    \"calendar_ticker\": \"SPY\",  # Used as the \"Master Clock\" for trading days\n",
    "    # DATA SANITIZER (The \"Glitches & Gaps\" Protector)\n",
    "    \"handle_zeros_as_nan\": True,  # Convert 0.0 prices to NaN to prevent math errors\n",
    "    \"max_data_gap_ffill\": 1,  # Max consecutive days to \"Forward Fill\" missing data\n",
    "    # IMPLICATION OF nan_price_replacement:\n",
    "    # - This defines what happens if the \"Forward Fill\" limit is exceeded.\n",
    "    # - If set to 0.0: A permanent data gap will look like a \"total loss\" (-100%).\n",
    "    #   The equity curve will plummet. Good for \"disaster detection.\"\n",
    "    #   Sharpe and Sharpe(ATR) drop because: return (gets smaller) / std (gets larger)\n",
    "    # - If set to np.nan: A permanent gap will cause portfolio calculations to return NaN.\n",
    "    #   The chart may break or show gaps. Good for \"math integrity.\"\n",
    "    \"nan_price_replacement\": 0.0,\n",
    "    # STRATEGY PARAMETERS (The \"How\")\n",
    "    \"atr_period\": 14,  # Used for volatility normalization\n",
    "    \"quality_window\": 252,  # 1 year lookback for liquidity/quality stats\n",
    "    \"quality_min_periods\": 126,  # Min history required to judge a stock\n",
    "    # QUALITY THRESHOLDS (The \"Rules\")\n",
    "    \"thresholds\": {\n",
    "        # HARD LIQUIDITY FLOOR\n",
    "        # Logic: Calculates (Adj Close * Volume) daily, then takes the ROLLING MEDIAN\n",
    "        # over the quality_window (252 days). Filters out stocks where the\n",
    "        # typical daily dollar turnover is below this absolute value.\n",
    "        \"min_median_dollar_volume\": 1_000_000,\n",
    "        # DYNAMIC LIQUIDITY CUTOFF (Relative to Universe)\n",
    "        # Logic: On the decision date, the engine calculates the X-quantile\n",
    "        # of 'RollMedDollarVol' across ALL available stocks.\n",
    "        # Setting this to 0.40 calculates the 60th percentile and requires\n",
    "        # stocks to be above it‚Äîeffectively keeping only the TOP 60% of the market.\n",
    "        \"min_liquidity_percentile\": 0.40,\n",
    "        # PRICE/VOLUME STALENESS\n",
    "        # Logic: Creates a binary flag (1 if Volume is 0 OR High equals Low).\n",
    "        # It then calculates the ROLLING MEAN of this flag.\n",
    "        # A value of 0.05 means the stock is rejected if it was \"stale\"\n",
    "        # for more than 5% of the trading days in the rolling window.\n",
    "        \"max_stale_pct\": 0.05,\n",
    "        # DATA INTEGRITY (FROZEN VOLUME)\n",
    "        # Logic: Checks if Volume is identical to the previous day (Volume.diff() == 0).\n",
    "        # It calculates the ROLLING SUM of these occurrences over the window.\n",
    "        # If the exact same volume is reported more than 10 times, the stock\n",
    "        # is rejected as having \"frozen\" or low-quality data.\n",
    "        \"max_same_vol_count\": 10,\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION A: CORE HELPER FUNCTIONS & FEATURE GENERATION\n",
    "# (Unchanged from previous version)\n",
    "# ==============================================================================\n",
    "# ... (Keep generate_features, calculate_gain, calculate_sharpe,\n",
    "#      calculate_sharpe_atr, calculate_buy_and_hold_performance as is) ...\n",
    "\n",
    "\n",
    "def generate_features(\n",
    "    df_ohlcv: pd.DataFrame,\n",
    "    atr_period: int = 14,\n",
    "    quality_window: int = 252,\n",
    "    quality_min_periods: int = 126,\n",
    ") -> pd.DataFrame:\n",
    "    # 1. Sort and Group\n",
    "    if not df_ohlcv.index.is_monotonic_increasing:\n",
    "        df_ohlcv = df_ohlcv.sort_index()\n",
    "    grouped = df_ohlcv.groupby(level=\"Ticker\")\n",
    "\n",
    "    # 2. ATR Calculation (Existing)\n",
    "    prev_close = grouped[\"Adj Close\"].shift(1)\n",
    "    tr = pd.concat(\n",
    "        [\n",
    "            df_ohlcv[\"Adj High\"] - df_ohlcv[\"Adj Low\"],\n",
    "            abs(df_ohlcv[\"Adj High\"] - prev_close),\n",
    "            abs(df_ohlcv[\"Adj Low\"] - prev_close),\n",
    "        ],\n",
    "        axis=1,\n",
    "    ).max(axis=1, skipna=False)\n",
    "\n",
    "    atr = tr.groupby(level=\"Ticker\").transform(\n",
    "        lambda x: x.ewm(alpha=1 / atr_period, adjust=False).mean()\n",
    "    )\n",
    "    atrp = (atr / df_ohlcv[\"Adj Close\"]).replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    # 3. --- NEW: MOMENTUM / RETURN FEATURES ---\n",
    "    # We calculate percentage change for specific windows useful for Pullbacks (3D, 5D) and Trends (21D)\n",
    "    # Note: We use grouped.pct_change to respect Ticker boundaries\n",
    "    roc_1 = grouped[\"Adj Close\"].pct_change(1)\n",
    "    roc_3 = grouped[\"Adj Close\"].pct_change(3)\n",
    "    roc_5 = grouped[\"Adj Close\"].pct_change(5)\n",
    "    roc_10 = grouped[\"Adj Close\"].pct_change(10)\n",
    "    roc_21 = grouped[\"Adj Close\"].pct_change(21)\n",
    "\n",
    "    indicator_df = pd.DataFrame(\n",
    "        {\n",
    "            \"ATR\": atr,\n",
    "            \"ATRP\": atrp,\n",
    "            \"ROC_1\": roc_1,\n",
    "            \"ROC_3\": roc_3,\n",
    "            \"ROC_5\": roc_5,\n",
    "            \"ROC_10\": roc_10,\n",
    "            \"ROC_21\": roc_21,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # 4. Quality/Liquidity Features (Existing)\n",
    "    quality_temp_df = pd.DataFrame(\n",
    "        {\n",
    "            \"IsStale\": np.where(\n",
    "                (df_ohlcv[\"Volume\"] == 0)\n",
    "                | (df_ohlcv[\"Adj High\"] == df_ohlcv[\"Adj Low\"]),\n",
    "                1,\n",
    "                0,\n",
    "            ),\n",
    "            \"DollarVolume\": df_ohlcv[\"Adj Close\"] * df_ohlcv[\"Volume\"],\n",
    "            \"HasSameVolume\": (grouped[\"Volume\"].diff() == 0).astype(int),\n",
    "        },\n",
    "        index=df_ohlcv.index,\n",
    "    )\n",
    "\n",
    "    rolling_result = (\n",
    "        quality_temp_df.groupby(level=\"Ticker\")\n",
    "        .rolling(window=quality_window, min_periods=quality_min_periods)\n",
    "        .agg({\"IsStale\": \"mean\", \"DollarVolume\": \"median\", \"HasSameVolume\": \"sum\"})\n",
    "        .rename(\n",
    "            columns={\n",
    "                \"IsStale\": \"RollingStalePct\",\n",
    "                \"DollarVolume\": \"RollMedDollarVol\",\n",
    "                \"HasSameVolume\": \"RollingSameVolCount\",\n",
    "            }\n",
    "        )\n",
    "        .reset_index(level=0, drop=True)\n",
    "    )\n",
    "\n",
    "    # 5. Merge\n",
    "    return pd.concat([indicator_df, rolling_result], axis=1)\n",
    "\n",
    "\n",
    "def calculate_buy_and_hold_performance(\n",
    "    df_close, features_df, tickers, start_date, end_date\n",
    "):\n",
    "    if not tickers:\n",
    "        return pd.Series(dtype=float), pd.Series(dtype=float), pd.Series(dtype=float)\n",
    "    ticker_counts = Counter(tickers)\n",
    "    initial_weights = pd.Series({t: c / len(tickers) for t, c in ticker_counts.items()})\n",
    "    prices_raw = df_close[initial_weights.index.tolist()].loc[start_date:end_date]\n",
    "    if prices_raw.dropna(how=\"all\").empty:\n",
    "        return pd.Series(dtype=float), pd.Series(dtype=float), pd.Series(dtype=float)\n",
    "    prices_norm = prices_raw.div(prices_raw.bfill().iloc[0])\n",
    "    weighted_growth = prices_norm.mul(initial_weights, axis=\"columns\")\n",
    "    value_series = weighted_growth.sum(axis=1)\n",
    "    return_series = value_series.ffill().pct_change()\n",
    "    full_idx = pd.MultiIndex.from_product(\n",
    "        [initial_weights.index.tolist(), return_series.index], names=[\"Ticker\", \"Date\"]\n",
    "    )\n",
    "    feat_subset = features_df.reindex(full_idx)[\"ATRP\"].unstack(level=\"Ticker\")\n",
    "    atrp_series = (\n",
    "        weighted_growth.div(value_series, axis=\"index\").align(\n",
    "            feat_subset, join=\"inner\", axis=1\n",
    "        )[0]\n",
    "        * weighted_growth.div(value_series, axis=\"index\").align(\n",
    "            feat_subset, join=\"inner\", axis=1\n",
    "        )[1]\n",
    "    ).sum(axis=1)\n",
    "    return value_series, return_series, atrp_series\n",
    "\n",
    "\n",
    "def calculate_summary_gain(price_series: pd.Series) -> float:\n",
    "    \"\"\"REPORTING: Returns the total return of a single series.\"\"\"\n",
    "    if price_series.dropna().shape[0] < 2:\n",
    "        return 0.0\n",
    "    # (Final Price / Starting Price) - 1\n",
    "    res = (price_series.ffill().iloc[-1] / price_series.bfill().iloc[0]) - 1\n",
    "    return float(res) if np.isfinite(res) else 0.0\n",
    "\n",
    "\n",
    "def calculate_cross_sectional_gain(price_df: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"RANKING: Returns the total return for every ticker in the universe.\"\"\"\n",
    "    if price_df.empty:\n",
    "        return pd.Series(dtype=float)\n",
    "    # Vectorized calculation across all columns (tickers)\n",
    "    res = (price_df.ffill().iloc[-1] / price_df.bfill().iloc[0]) - 1\n",
    "    return res.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "\n",
    "\n",
    "def calculate_summary_sharpe(return_series: pd.Series) -> float:\n",
    "    \"\"\"REPORTING: Returns a single Reward value.\"\"\"\n",
    "    if return_series.dropna().shape[0] < 2:\n",
    "        return 0.0\n",
    "    mu, std = return_series.mean(), return_series.std()\n",
    "\n",
    "    # SENIOR FIX: Volatility floor to prevent 'Infinity' or 'Exploding' rewards\n",
    "    if std < 1e-6:\n",
    "        return 0.0\n",
    "\n",
    "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "        res = (mu / std) * np.sqrt(252)\n",
    "    return float(res) if np.isfinite(res) else 0.0\n",
    "\n",
    "\n",
    "def calculate_cross_sectional_sharpe(return_df: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"RANKING: Returns a Series of values for the whole universe.\"\"\"\n",
    "    if return_df.empty:\n",
    "        return pd.Series(dtype=float)\n",
    "    mu, std = return_df.mean(), return_df.std()\n",
    "\n",
    "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "        res = (mu / std) * np.sqrt(252)\n",
    "\n",
    "    # SENIOR FIX: Convert 'Broken' data (std=0) into 0.0 reward\n",
    "    return res.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "\n",
    "\n",
    "def calculate_summary_sharpe_atr(\n",
    "    return_series: pd.Series, atrp_input: Union[pd.Series, float]\n",
    ") -> float:\n",
    "    \"\"\"REPORTING: Returns a single Reward value normalized by Volatility.\"\"\"\n",
    "    if return_series.dropna().shape[0] < 2:\n",
    "        return 0.0\n",
    "    avg_atrp = atrp_input.mean() if hasattr(atrp_input, \"mean\") else atrp_input\n",
    "\n",
    "    if avg_atrp < 1e-6:\n",
    "        return 0.0  # Safety floor\n",
    "\n",
    "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "        res = return_series.mean() / avg_atrp\n",
    "    return float(res) if np.isfinite(res) else 0.0\n",
    "\n",
    "\n",
    "def calculate_cross_sectional_sharpe_atr(\n",
    "    return_df: pd.DataFrame, atrp_series: pd.Series\n",
    ") -> pd.Series:\n",
    "    \"\"\"RANKING: Returns a Series of Volatility-normalized values.\"\"\"\n",
    "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "        res = return_df.mean() / atrp_series\n",
    "    return res.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION B: METRIC REGISTRY (UPDATED VARIABLES)\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "class MarketObservation(TypedDict):\n",
    "    \"\"\"\n",
    "    The 'STATE' (Observation) in Reinforcement Learning.\n",
    "    This defines the context given to the agent to make a decision.\n",
    "    \"\"\"\n",
    "\n",
    "    lookback_returns: pd.DataFrame  # (Time x Tickers)\n",
    "    lookback_close: pd.DataFrame  # (Time x Tickers)\n",
    "    atrp: pd.Series  # (Tickers,) - The mean ATR% over lookback\n",
    "    roc_1: pd.Series  # (Tickers,) - Current 1D Momentum\n",
    "    roc_3: pd.Series  # ... etc\n",
    "    roc_5: pd.Series\n",
    "    roc_10: pd.Series\n",
    "    roc_21: pd.Series\n",
    "\n",
    "\n",
    "# Use the centralized helper functions for calculations\n",
    "\n",
    "\n",
    "def metric_price(obs: MarketObservation) -> pd.Series:\n",
    "    return calculate_cross_sectional_gain(obs[\"lookback_close\"])\n",
    "\n",
    "\n",
    "def metric_sharpe(obs: MarketObservation) -> pd.Series:\n",
    "    return calculate_cross_sectional_sharpe(obs[\"lookback_returns\"])\n",
    "\n",
    "\n",
    "def metric_sharpe_atr(obs: MarketObservation) -> pd.Series:\n",
    "    return calculate_cross_sectional_sharpe_atr(obs[\"lookback_returns\"], obs[\"atrp\"])\n",
    "\n",
    "\n",
    "METRIC_REGISTRY = {\n",
    "    \"Price\": metric_price,\n",
    "    \"Sharpe\": metric_sharpe,\n",
    "    \"Sharpe (ATR)\": metric_sharpe_atr,\n",
    "    \"Momentum 1D\": lambda obs: obs[\"roc_1\"],\n",
    "    \"Momentum 3D\": lambda obs: obs[\"roc_3\"],\n",
    "    \"Momentum 5D\": lambda obs: obs[\"roc_5\"],\n",
    "    \"Momentum 10D\": lambda obs: obs[\"roc_10\"],\n",
    "    \"Momentum 1M\": lambda obs: obs[\"roc_21\"],\n",
    "    \"Pullback 1D\": lambda obs: -obs[\"roc_1\"],\n",
    "    \"Pullback 3D\": lambda obs: -obs[\"roc_3\"],\n",
    "    \"Pullback 5D\": lambda obs: -obs[\"roc_5\"],\n",
    "    \"Pullback 10D\": lambda obs: -obs[\"roc_10\"],\n",
    "    \"Pullback 1M\": lambda obs: -obs[\"roc_21\"],\n",
    "}\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION C: DATA CONTRACTS (UPDATED v2.2 - Verification Ready)\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EngineInput:\n",
    "    mode: str\n",
    "    start_date: pd.Timestamp\n",
    "    lookback_period: int\n",
    "    holding_period: int\n",
    "    metric: str\n",
    "    benchmark_ticker: str\n",
    "    rank_start: int = 1\n",
    "    rank_end: int = 10\n",
    "    # Default factory pulls from Global thresholds\n",
    "    quality_thresholds: Dict[str, float] = field(\n",
    "        default_factory=lambda: GLOBAL_SETTINGS[\"thresholds\"].copy()\n",
    "    )\n",
    "    manual_tickers: List[str] = field(default_factory=list)\n",
    "    debug: bool = False\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EngineOutput:\n",
    "    portfolio_series: pd.Series\n",
    "    benchmark_series: pd.Series\n",
    "    normalized_plot_data: pd.DataFrame\n",
    "    tickers: List[str]\n",
    "    initial_weights: pd.Series\n",
    "    perf_metrics: Dict[str, float]\n",
    "    results_df: pd.DataFrame\n",
    "\n",
    "    # Dates\n",
    "    start_date: pd.Timestamp\n",
    "    decision_date: pd.Timestamp\n",
    "    buy_date: pd.Timestamp\n",
    "    holding_end_date: pd.Timestamp\n",
    "\n",
    "    error_msg: Optional[str] = None\n",
    "    debug_data: Optional[Dict[str, Any]] = None\n",
    "\n",
    "\n",
    "class AlphaEngine:\n",
    "    def __init__(\n",
    "        self,\n",
    "        df_ohlcv: pd.DataFrame,\n",
    "        features_df: pd.DataFrame = None,\n",
    "        df_close_wide: pd.DataFrame = None,\n",
    "        master_ticker: str = GLOBAL_SETTINGS[\"calendar_ticker\"],\n",
    "    ):\n",
    "        print(\"--- ‚öôÔ∏è Initializing AlphaEngine v2.2 (Sanitized) ---\")\n",
    "\n",
    "        # 1. SETUP PRICES (CLEAN-AT-ENTRY)\n",
    "        if df_close_wide is not None:\n",
    "            self.df_close = df_close_wide\n",
    "        else:\n",
    "            print(\"üê¢ Pivoting and Sanitizing Price Data...\")\n",
    "            self.df_close = df_ohlcv[\"Adj Close\"].unstack(level=0)\n",
    "\n",
    "        # APPLY DATA SANITIZER LOGIC\n",
    "        if GLOBAL_SETTINGS[\"handle_zeros_as_nan\"]:\n",
    "            # Replace 0.0 with NaN so math functions (mean/std) ignore them\n",
    "            self.df_close = self.df_close.replace(0, np.nan)\n",
    "\n",
    "        # Smooth over 1-2 day glitches (The \"FNV\" Fix)\n",
    "        self.df_close = self.df_close.ffill(limit=GLOBAL_SETTINGS[\"max_data_gap_ffill\"])\n",
    "\n",
    "        # Handle the remaining \"unfillable\" gaps\n",
    "        self.df_close = self.df_close.fillna(GLOBAL_SETTINGS[\"nan_price_replacement\"])\n",
    "\n",
    "        # 2. SETUP FEATURES\n",
    "        if features_df is not None:\n",
    "            self.features_df = features_df\n",
    "        else:\n",
    "            # We pass the cleaned price data if needed, or calculate from raw\n",
    "            self.features_df = generate_features(\n",
    "                df_ohlcv,\n",
    "                atr_period=GLOBAL_SETTINGS[\"atr_period\"],\n",
    "                quality_window=GLOBAL_SETTINGS[\"quality_window\"],\n",
    "                quality_min_periods=GLOBAL_SETTINGS[\"quality_min_periods\"],\n",
    "            )\n",
    "\n",
    "        # 3. Setup Calendar\n",
    "        if master_ticker not in self.df_close.columns:\n",
    "            master_ticker = self.df_close.columns[0]\n",
    "        self.trading_calendar = (\n",
    "            self.df_close[master_ticker].dropna().index.unique().sort_values()\n",
    "        )\n",
    "\n",
    "    def run(self, inputs: EngineInput) -> EngineOutput:\n",
    "        dates, error = self._validate_timeline(inputs)\n",
    "        if error:\n",
    "            return self._error_result(error)\n",
    "        (safe_start, safe_decision, safe_buy, safe_end) = dates\n",
    "\n",
    "        tickers_to_trade, results_table, debug_dict, error = self._select_tickers(\n",
    "            inputs, safe_start, safe_decision\n",
    "        )\n",
    "        if error:\n",
    "            return self._error_result(error)\n",
    "\n",
    "        # GENERATE TRACKS\n",
    "        p_f_val, p_f_ret, p_f_atrp = calculate_buy_and_hold_performance(\n",
    "            self.df_close, self.features_df, tickers_to_trade, safe_start, safe_end\n",
    "        )\n",
    "        b_f_val, b_f_ret, b_f_atrp = calculate_buy_and_hold_performance(\n",
    "            self.df_close,\n",
    "            self.features_df,\n",
    "            [inputs.benchmark_ticker],\n",
    "            safe_start,\n",
    "            safe_end,\n",
    "        )\n",
    "\n",
    "        p_h_val, p_h_ret, p_h_atrp = calculate_buy_and_hold_performance(\n",
    "            self.df_close, self.features_df, tickers_to_trade, safe_buy, safe_end\n",
    "        )\n",
    "        b_h_val, b_h_ret, b_h_atrp = calculate_buy_and_hold_performance(\n",
    "            self.df_close,\n",
    "            self.features_df,\n",
    "            [inputs.benchmark_ticker],\n",
    "            safe_buy,\n",
    "            safe_end,\n",
    "        )\n",
    "\n",
    "        # CALCULATE METRICS\n",
    "        p_metrics, p_slices = self._calculate_period_metrics(\n",
    "            p_f_val,\n",
    "            p_f_ret,\n",
    "            p_f_atrp,\n",
    "            safe_decision,\n",
    "            p_h_val,\n",
    "            p_h_ret,\n",
    "            p_h_atrp,\n",
    "            prefix=\"p\",\n",
    "        )\n",
    "        b_metrics, b_slices = self._calculate_period_metrics(\n",
    "            b_f_val,\n",
    "            b_f_ret,\n",
    "            b_f_atrp,\n",
    "            safe_decision,\n",
    "            b_h_val,\n",
    "            b_h_ret,\n",
    "            b_h_atrp,\n",
    "            prefix=\"b\",\n",
    "        )\n",
    "\n",
    "        # CONSOLIDATE DEBUG DATA\n",
    "        debug_dict[\"verification\"] = {\"portfolio\": p_slices, \"benchmark\": b_slices}\n",
    "\n",
    "        # ADD RAW COMPONENT EXPORTS\n",
    "        debug_dict[\"portfolio_raw_components\"] = {\n",
    "            \"prices\": self.df_close[tickers_to_trade].loc[safe_start:safe_end],\n",
    "            \"atrp\": self.features_df.loc[\n",
    "                (tickers_to_trade, slice(safe_start, safe_end)), \"ATRP\"\n",
    "            ].unstack(level=0),\n",
    "        }\n",
    "        debug_dict[\"benchmark_raw_components\"] = {\n",
    "            \"prices\": self.df_close[[inputs.benchmark_ticker]].loc[safe_start:safe_end],\n",
    "            \"atrp\": self.features_df.loc[\n",
    "                ([inputs.benchmark_ticker], slice(safe_start, safe_end)), \"ATRP\"\n",
    "            ].unstack(level=0),\n",
    "        }\n",
    "\n",
    "        # FINAL OUTPUT\n",
    "        results_table[\"Holding Gain\"] = (p_h_val.iloc[-1] / p_h_val.iloc[0]) - 1\n",
    "        return EngineOutput(\n",
    "            portfolio_series=p_f_val,\n",
    "            benchmark_series=b_f_val,\n",
    "            normalized_plot_data=self._get_normalized_plot_data(\n",
    "                tickers_to_trade, safe_start, safe_end\n",
    "            ),\n",
    "            tickers=tickers_to_trade,\n",
    "            initial_weights=pd.Series(\n",
    "                {t: 1 / len(tickers_to_trade) for t in tickers_to_trade}\n",
    "            ),\n",
    "            perf_metrics={**p_metrics, **b_metrics},\n",
    "            results_df=results_table,\n",
    "            start_date=safe_start,\n",
    "            decision_date=safe_decision,\n",
    "            buy_date=safe_buy,\n",
    "            holding_end_date=safe_end,\n",
    "            debug_data=debug_dict,\n",
    "        )\n",
    "\n",
    "    # ==============================================================================\n",
    "    # INTERNAL LOGIC MODULES\n",
    "    # ==============================================================================\n",
    "\n",
    "    def _validate_timeline(self, inputs: EngineInput):\n",
    "        cal = self.trading_calendar\n",
    "        last_idx = len(cal) - 1\n",
    "\n",
    "        if len(cal) <= inputs.lookback_period:\n",
    "            return (\n",
    "                None,\n",
    "                f\"‚ùå Dataset too small.\\nNeed > {inputs.lookback_period} days of history.\",\n",
    "            )\n",
    "\n",
    "        # 2. Check \"Past\" Constraints (Lookback)\n",
    "        min_decision_date = cal[inputs.lookback_period]\n",
    "        if inputs.start_date < min_decision_date:\n",
    "            # Added \\n here\n",
    "            return None, (\n",
    "                f\"‚ùå Not enough history for a {inputs.lookback_period}-day lookback.\\n\"\n",
    "                f\"Earliest valid Decision Date: {min_decision_date.date()}\"\n",
    "            )\n",
    "\n",
    "        # 3. Check \"Future\" Constraints (Entry T+1 and Holding Period)\n",
    "        required_future_days = 1 + inputs.holding_period\n",
    "        latest_valid_idx = last_idx - required_future_days\n",
    "\n",
    "        if latest_valid_idx < 0:\n",
    "            return (\n",
    "                None,\n",
    "                f\"‚ùå Holding period too long.\\n{inputs.holding_period} days exceeds available data.\",\n",
    "            )\n",
    "\n",
    "        # If user picked a date beyond the available \"future\" runway\n",
    "        if inputs.start_date > cal[latest_valid_idx]:\n",
    "            latest_date = cal[latest_valid_idx].date()\n",
    "            # Added \\n here and shortened the text slightly to fit better\n",
    "            return None, (\n",
    "                f\"‚ùå Decision Date too late for a {inputs.holding_period}-day hold.\\n\"\n",
    "                f\"Latest valid date: {latest_date}. Please move picker back.\"\n",
    "            )\n",
    "\n",
    "        # 4. Map the safe indices\n",
    "        decision_idx = cal.searchsorted(inputs.start_date)\n",
    "        if decision_idx > latest_valid_idx:\n",
    "            decision_idx = latest_valid_idx\n",
    "\n",
    "        start_idx = decision_idx - inputs.lookback_period\n",
    "        entry_idx = decision_idx + 1\n",
    "        end_idx = entry_idx + inputs.holding_period\n",
    "\n",
    "        return (cal[start_idx], cal[decision_idx], cal[entry_idx], cal[end_idx]), None\n",
    "\n",
    "    def _select_tickers(self, inputs: EngineInput, start_date, decision_date):\n",
    "        debug_dict = {}\n",
    "        if inputs.mode == \"Manual List\":\n",
    "            validation_errors = []\n",
    "            valid_tickers = []\n",
    "            for t in inputs.manual_tickers:\n",
    "                if t not in self.df_close.columns:\n",
    "                    validation_errors.append(f\"‚ùå {t}: Not found.\")\n",
    "                    continue\n",
    "                if pd.isna(self.df_close.at[start_date, t]):\n",
    "                    validation_errors.append(f\"‚ö†Ô∏è {t}: No data on start date.\")\n",
    "                    continue\n",
    "                valid_tickers.append(t)\n",
    "\n",
    "            if validation_errors:\n",
    "                return [], pd.DataFrame(), {}, \"\\n\".join(validation_errors)\n",
    "            if not valid_tickers:\n",
    "                return [], pd.DataFrame(), {}, \"No valid tickers found.\"\n",
    "            return valid_tickers, pd.DataFrame(index=valid_tickers), {}, None\n",
    "\n",
    "        else:  # Ranking\n",
    "            audit_info = {}\n",
    "            eligible_tickers = self._filter_universe(\n",
    "                decision_date, inputs.quality_thresholds, audit_info\n",
    "            )\n",
    "            debug_dict[\"audit_liquidity\"] = audit_info\n",
    "\n",
    "            if not eligible_tickers:\n",
    "                return (\n",
    "                    [],\n",
    "                    pd.DataFrame(),\n",
    "                    debug_dict,\n",
    "                    \"No tickers passed quality filters.\",\n",
    "                )\n",
    "\n",
    "            lookback_close = self.df_close.loc[\n",
    "                start_date:decision_date, eligible_tickers\n",
    "            ]\n",
    "            idx_product = pd.MultiIndex.from_product(\n",
    "                [eligible_tickers, lookback_close.index], names=[\"Ticker\", \"Date\"]\n",
    "            )\n",
    "\n",
    "            feat_slice_current = self.features_df.xs(\n",
    "                decision_date, level=\"Date\"\n",
    "            ).reindex(eligible_tickers)\n",
    "            feat_slice_period = self.features_df.loc[\n",
    "                (slice(None), lookback_close.index), :\n",
    "            ].reindex(idx_product)\n",
    "            atrp_mean = feat_slice_period[\"ATRP\"].groupby(level=\"Ticker\").mean()\n",
    "\n",
    "            # 1. Package the Observation (The 'State')\n",
    "            observation: MarketObservation = {\n",
    "                \"lookback_close\": lookback_close,\n",
    "                \"lookback_returns\": lookback_close.ffill().pct_change(),\n",
    "                \"atrp\": atrp_mean,\n",
    "                \"roc_1\": feat_slice_current[\"ROC_1\"],\n",
    "                \"roc_3\": feat_slice_current[\"ROC_3\"],\n",
    "                \"roc_5\": feat_slice_current[\"ROC_5\"],\n",
    "                \"roc_10\": feat_slice_current[\"ROC_10\"],\n",
    "                \"roc_21\": feat_slice_current[\"ROC_21\"],\n",
    "            }\n",
    "\n",
    "            # 2. Run the Strategy (The 'Agent')\n",
    "            if inputs.metric not in METRIC_REGISTRY:\n",
    "                return [], pd.DataFrame(), {}, f\"Strategy '{inputs.metric}' not found.\"\n",
    "\n",
    "            metric_vals = METRIC_REGISTRY[inputs.metric](observation)\n",
    "            sorted_tickers = metric_vals.sort_values(ascending=False)\n",
    "            start_r = max(0, inputs.rank_start - 1)\n",
    "            end_r = inputs.rank_end\n",
    "            selected_tickers = sorted_tickers.iloc[start_r:end_r].index.tolist()\n",
    "\n",
    "            # --- VERIFICATION ADDITION: Ranking Audit (Bot Version) ---\n",
    "            debug_dict[\"full_universe_ranking\"] = pd.DataFrame(\n",
    "                {\n",
    "                    \"Strategy_Score\": metric_vals,\n",
    "                    \"Lookback_Return_Ann\": observation[\"lookback_returns\"].mean() * 252,\n",
    "                    \"Lookback_ATRP\": observation[\"atrp\"],\n",
    "                }\n",
    "            )\n",
    "\n",
    "            if not selected_tickers:\n",
    "                return (\n",
    "                    [],\n",
    "                    pd.DataFrame(),\n",
    "                    debug_dict,\n",
    "                    \"No tickers generated from ranking.\",\n",
    "                )\n",
    "\n",
    "            results_table = pd.DataFrame(\n",
    "                {\n",
    "                    \"Rank\": range(\n",
    "                        inputs.rank_start, inputs.rank_start + len(selected_tickers)\n",
    "                    ),\n",
    "                    \"Ticker\": selected_tickers,\n",
    "                    \"Strategy Value\": sorted_tickers.loc[selected_tickers].values,\n",
    "                }\n",
    "            ).set_index(\"Ticker\")\n",
    "\n",
    "            return selected_tickers, results_table, debug_dict, None\n",
    "\n",
    "    def _filter_universe(self, date_ts, thresholds, audit_container=None):\n",
    "        avail_dates = (\n",
    "            self.features_df.index.get_level_values(\"Date\").unique().sort_values()\n",
    "        )\n",
    "        valid_dates = avail_dates[avail_dates <= date_ts]\n",
    "        if valid_dates.empty:\n",
    "            return []\n",
    "        target_date = valid_dates[-1]\n",
    "        day_features = self.features_df.xs(target_date, level=\"Date\")\n",
    "\n",
    "        vol_cutoff = thresholds.get(\"min_median_dollar_volume\", 0)\n",
    "        percentile_used = \"N/A\"\n",
    "        if \"min_liquidity_percentile\" in thresholds:\n",
    "            percentile_used = thresholds[\"min_liquidity_percentile\"]\n",
    "            dynamic_val = day_features[\"RollMedDollarVol\"].quantile(percentile_used)\n",
    "            vol_cutoff = max(vol_cutoff, dynamic_val)\n",
    "\n",
    "        mask = (\n",
    "            (day_features[\"RollMedDollarVol\"] >= vol_cutoff)\n",
    "            & (day_features[\"RollingStalePct\"] <= thresholds[\"max_stale_pct\"])\n",
    "            & (day_features[\"RollingSameVolCount\"] <= thresholds[\"max_same_vol_count\"])\n",
    "        )\n",
    "\n",
    "        if audit_container is not None:\n",
    "            audit_container[\"date\"] = target_date\n",
    "            audit_container[\"total_tickers_available\"] = len(day_features)\n",
    "            audit_container[\"percentile_setting\"] = percentile_used\n",
    "            audit_container[\"final_cutoff_usd\"] = vol_cutoff\n",
    "            audit_container[\"tickers_passed\"] = mask.sum()\n",
    "            snapshot = day_features.copy()\n",
    "            snapshot[\"Calculated_Cutoff\"] = vol_cutoff\n",
    "            snapshot[\"Passed_Vol_Check\"] = snapshot[\"RollMedDollarVol\"] >= vol_cutoff\n",
    "            snapshot[\"Passed_Final\"] = mask\n",
    "            snapshot = snapshot.sort_values(\"RollMedDollarVol\", ascending=False)\n",
    "            audit_container[\"universe_snapshot\"] = snapshot\n",
    "\n",
    "        return day_features[mask].index.tolist()\n",
    "\n",
    "    def _calculate_period_metrics(\n",
    "        self, f_val, f_ret, f_atrp, decision_date, h_val, h_ret, h_atrp, prefix\n",
    "    ):\n",
    "        metrics = {}\n",
    "        slices = {}\n",
    "\n",
    "        # Slices for Lookback (Derived from 'Full' track)\n",
    "        lb_val = f_val.loc[:decision_date]\n",
    "        lb_ret = f_ret.loc[:decision_date]\n",
    "        lb_atrp = f_atrp.loc[:decision_date]\n",
    "\n",
    "        # 1. GAIN\n",
    "        metrics[f\"full_{prefix}_gain\"] = calculate_summary_gain(f_val)\n",
    "        metrics[f\"lookback_{prefix}_gain\"] = calculate_summary_gain(lb_val)\n",
    "        metrics[f\"holding_{prefix}_gain\"] = calculate_summary_gain(h_val)\n",
    "\n",
    "        # 2. SHARPE\n",
    "        metrics[f\"full_{prefix}_sharpe\"] = calculate_summary_sharpe(f_ret)\n",
    "        metrics[f\"lookback_{prefix}_sharpe\"] = calculate_summary_sharpe(lb_ret)\n",
    "        metrics[f\"holding_{prefix}_sharpe\"] = calculate_summary_sharpe(h_ret)\n",
    "\n",
    "        # 3. SHARPE (ATR)\n",
    "        metrics[f\"full_{prefix}_sharpe_atr\"] = calculate_summary_sharpe_atr(\n",
    "            f_ret, f_atrp\n",
    "        )\n",
    "        metrics[f\"lookback_{prefix}_sharpe_atr\"] = calculate_summary_sharpe_atr(\n",
    "            lb_ret, lb_atrp\n",
    "        )\n",
    "        metrics[f\"holding_{prefix}_sharpe_atr\"] = calculate_summary_sharpe_atr(\n",
    "            h_ret, h_atrp\n",
    "        )\n",
    "\n",
    "        # 4. CAPTURE ALL SLICES FOR EXPORT (This was what was missing)\n",
    "        slices[f\"full_val\"] = f_val\n",
    "        slices[f\"full_ret\"] = f_ret\n",
    "        slices[f\"full_atrp\"] = f_atrp\n",
    "\n",
    "        slices[f\"lookback_val\"] = lb_val\n",
    "        slices[f\"lookback_ret\"] = lb_ret\n",
    "        slices[f\"lookback_atrp\"] = lb_atrp\n",
    "\n",
    "        slices[f\"holding_val\"] = h_val\n",
    "        slices[f\"holding_ret\"] = h_ret\n",
    "        slices[f\"holding_atrp\"] = h_atrp\n",
    "\n",
    "        return metrics, slices\n",
    "\n",
    "    def _get_normalized_plot_data(self, tickers, start_date, end_date):\n",
    "        if not tickers:\n",
    "            return pd.DataFrame()\n",
    "        data = self.df_close[list(set(tickers))].loc[start_date:end_date]\n",
    "        if data.empty:\n",
    "            return pd.DataFrame()\n",
    "        return data / data.bfill().iloc[0]\n",
    "\n",
    "    def _error_result(self, msg):\n",
    "        return EngineOutput(\n",
    "            portfolio_series=pd.Series(dtype=float),\n",
    "            benchmark_series=pd.Series(dtype=float),\n",
    "            normalized_plot_data=pd.DataFrame(),\n",
    "            tickers=[],\n",
    "            initial_weights=pd.Series(dtype=float),\n",
    "            perf_metrics={},\n",
    "            results_df=pd.DataFrame(),\n",
    "            start_date=pd.Timestamp.min,\n",
    "            decision_date=pd.Timestamp.min,\n",
    "            buy_date=pd.Timestamp.min,\n",
    "            holding_end_date=pd.Timestamp.min,\n",
    "            error_msg=msg,\n",
    "        )\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION E: THE UI (Visualization) - UPDATED v2.4 (Complete Timeline)\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "def plot_walk_forward_analyzer(\n",
    "    df_ohlcv,\n",
    "    precomputed_features=None,\n",
    "    precomputed_close=None,\n",
    "    default_start_date=\"2025-01-17\",\n",
    "    default_lookback=10,\n",
    "    default_holding=5,\n",
    "    default_strategy=\"Sharpe (ATR)\",\n",
    "    default_rank_start=1,\n",
    "    default_rank_end=10,\n",
    "    default_benchmark_ticker=GLOBAL_SETTINGS[\"benchmark_ticker\"],\n",
    "    master_calendar_ticker=GLOBAL_SETTINGS[\"calendar_ticker\"],\n",
    "    quality_thresholds=GLOBAL_SETTINGS[\"thresholds\"],\n",
    "    debug=False,\n",
    "):\n",
    "\n",
    "    engine = AlphaEngine(\n",
    "        df_ohlcv,\n",
    "        features_df=precomputed_features,\n",
    "        df_close_wide=precomputed_close,\n",
    "        master_ticker=master_calendar_ticker,\n",
    "    )\n",
    "\n",
    "    # Initialize containers\n",
    "    results_container = [None]\n",
    "    debug_container = [{}]\n",
    "\n",
    "    # If no thresholds passed, use the global Source of Truth\n",
    "    if quality_thresholds is None:\n",
    "        quality_thresholds = GLOBAL_SETTINGS[\"thresholds\"]\n",
    "\n",
    "    # --- Widgets ---\n",
    "    mode_selector = widgets.RadioButtons(\n",
    "        options=[\"Ranking\", \"Manual List\"],\n",
    "        value=\"Ranking\",\n",
    "        description=\"Mode:\",\n",
    "        layout={\"width\": \"max-content\"},\n",
    "        style={\"description_width\": \"initial\"},\n",
    "    )\n",
    "    lookback_input = widgets.IntText(\n",
    "        value=default_lookback,\n",
    "        description=\"Lookback (Days):\",\n",
    "        layout={\"width\": \"200px\"},\n",
    "        style={\"description_width\": \"initial\"},\n",
    "    )\n",
    "    decision_date_picker = widgets.DatePicker(\n",
    "        description=\"Decision Date:\",\n",
    "        value=pd.to_datetime(default_start_date),\n",
    "        layout={\"width\": \"auto\"},\n",
    "        style={\"description_width\": \"initial\"},\n",
    "    )\n",
    "    holding_input = widgets.IntText(\n",
    "        value=default_holding,\n",
    "        description=\"Holding (Days):\",\n",
    "        layout={\"width\": \"200px\"},\n",
    "        style={\"description_width\": \"initial\"},\n",
    "    )\n",
    "    strategy_dropdown = widgets.Dropdown(\n",
    "        options=list(METRIC_REGISTRY.keys()),\n",
    "        value=default_strategy,\n",
    "        description=\"Strategy:\",\n",
    "        layout={\"width\": \"220px\"},\n",
    "        style={\"description_width\": \"initial\"},\n",
    "    )\n",
    "    benchmark_input = widgets.Text(\n",
    "        value=default_benchmark_ticker,\n",
    "        description=\"Benchmark:\",\n",
    "        placeholder=\"Enter Ticker\",\n",
    "        layout={\"width\": \"180px\"},\n",
    "        style={\"description_width\": \"initial\"},\n",
    "    )\n",
    "    rank_start_input = widgets.IntText(\n",
    "        value=default_rank_start,\n",
    "        description=\"Rank Start:\",\n",
    "        layout={\"width\": \"150px\"},\n",
    "        style={\"description_width\": \"initial\"},\n",
    "    )\n",
    "    rank_end_input = widgets.IntText(\n",
    "        value=default_rank_end,\n",
    "        description=\"Rank End:\",\n",
    "        layout={\"width\": \"150px\"},\n",
    "        style={\"description_width\": \"initial\"},\n",
    "    )\n",
    "    manual_tickers_input = widgets.Textarea(\n",
    "        value=\"\",\n",
    "        placeholder=\"Enter tickers...\",\n",
    "        description=\"Manual Tickers:\",\n",
    "        layout={\"width\": \"400px\", \"height\": \"80px\"},\n",
    "        style={\"description_width\": \"initial\"},\n",
    "    )\n",
    "    update_button = widgets.Button(description=\"Run Simulation\", button_style=\"primary\")\n",
    "    ticker_list_output = widgets.Output()\n",
    "\n",
    "    # --- Layouts ---\n",
    "    timeline_box = widgets.HBox(\n",
    "        [lookback_input, decision_date_picker, holding_input],\n",
    "        layout=widgets.Layout(\n",
    "            justify_content=\"space-between\",\n",
    "            border=\"1px solid #ddd\",\n",
    "            padding=\"10px\",\n",
    "            margin=\"5px\",\n",
    "        ),\n",
    "    )\n",
    "    strategy_box = widgets.HBox([strategy_dropdown, benchmark_input])\n",
    "    ranking_box = widgets.HBox([rank_start_input, rank_end_input])\n",
    "\n",
    "    def on_mode_change(c):\n",
    "        ranking_box.layout.display = \"flex\" if c[\"new\"] == \"Ranking\" else \"none\"\n",
    "        manual_tickers_input.layout.display = (\n",
    "            \"none\" if c[\"new\"] == \"Ranking\" else \"flex\"\n",
    "        )\n",
    "        strategy_dropdown.disabled = c[\"new\"] == \"Manual List\"\n",
    "\n",
    "    mode_selector.observe(on_mode_change, names=\"value\")\n",
    "    on_mode_change({\"new\": mode_selector.value})\n",
    "\n",
    "    ui = widgets.VBox(\n",
    "        [\n",
    "            widgets.HTML(\n",
    "                \"<b>1. Timeline Configuration:</b> (Past <--- Decision ---> Future)\"\n",
    "            ),\n",
    "            timeline_box,\n",
    "            widgets.HTML(\"<b>2. Strategy Settings:</b>\"),\n",
    "            widgets.HBox([mode_selector, strategy_box]),\n",
    "            ranking_box,\n",
    "            manual_tickers_input,\n",
    "            widgets.HTML(\"<hr>\"),\n",
    "            update_button,\n",
    "            ticker_list_output,\n",
    "        ],\n",
    "        layout=widgets.Layout(margin=\"10px 0 20px 0\"),\n",
    "    )\n",
    "\n",
    "    fig = go.FigureWidget()\n",
    "    fig.update_layout(\n",
    "        title=\"Event-Driven Walk-Forward Analysis\",\n",
    "        height=600,\n",
    "        template=\"plotly_white\",\n",
    "        hovermode=\"x unified\",\n",
    "    )\n",
    "    for i in range(50):\n",
    "        fig.add_trace(go.Scatter(visible=False, line=dict(width=2)))\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            name=\"Benchmark\",\n",
    "            visible=True,\n",
    "            line=dict(color=\"black\", width=3, dash=\"dash\"),\n",
    "        )\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            name=\"Group Portfolio\", visible=True, line=dict(color=\"green\", width=3)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # --- Update Logic ---\n",
    "    def update_plot(b):\n",
    "        ticker_list_output.clear_output()\n",
    "        manual_list = [\n",
    "            t.strip().upper()\n",
    "            for t in manual_tickers_input.value.split(\",\")\n",
    "            if t.strip()\n",
    "        ]\n",
    "        decision_date_raw = pd.to_datetime(decision_date_picker.value)\n",
    "\n",
    "        inputs = EngineInput(\n",
    "            mode=mode_selector.value,\n",
    "            start_date=decision_date_raw,\n",
    "            lookback_period=lookback_input.value,\n",
    "            holding_period=holding_input.value,\n",
    "            metric=strategy_dropdown.value,\n",
    "            benchmark_ticker=benchmark_input.value.strip().upper(),\n",
    "            rank_start=rank_start_input.value,\n",
    "            rank_end=rank_end_input.value,\n",
    "            quality_thresholds=quality_thresholds,\n",
    "            manual_tickers=manual_list,\n",
    "            debug=debug,\n",
    "        )\n",
    "\n",
    "        # --- CAPTURE INPUTS FOR AUDIT ---\n",
    "        debug_container[0][\"inputs\"] = inputs\n",
    "\n",
    "        with ticker_list_output:\n",
    "            res = engine.run(inputs)\n",
    "            results_container[0] = res\n",
    "\n",
    "            # --- MERGE ENGINE DEBUG DATA ---\n",
    "            if res.debug_data:\n",
    "                debug_container[0].update(res.debug_data)\n",
    "\n",
    "            if res.error_msg:\n",
    "                print(f\"‚ö†Ô∏è Simulation Stopped: {res.error_msg}\")\n",
    "                return\n",
    "\n",
    "            # Plotting\n",
    "            with fig.batch_update():\n",
    "                cols = res.normalized_plot_data.columns.tolist()\n",
    "                for i in range(50):\n",
    "                    if i < len(cols):\n",
    "                        fig.data[i].update(\n",
    "                            x=res.normalized_plot_data.index,\n",
    "                            y=res.normalized_plot_data[cols[i]],\n",
    "                            name=cols[i],\n",
    "                            visible=True,\n",
    "                        )\n",
    "                    else:\n",
    "                        fig.data[i].visible = False\n",
    "\n",
    "                fig.data[50].update(\n",
    "                    x=res.benchmark_series.index,\n",
    "                    y=res.benchmark_series.values,\n",
    "                    name=f\"Benchmark ({inputs.benchmark_ticker})\",\n",
    "                    visible=not res.benchmark_series.empty,\n",
    "                )\n",
    "                fig.data[51].update(\n",
    "                    x=res.portfolio_series.index,\n",
    "                    y=res.portfolio_series.values,\n",
    "                    visible=True,\n",
    "                )\n",
    "\n",
    "                # Visual Lines\n",
    "                fig.layout.shapes = [\n",
    "                    dict(\n",
    "                        type=\"line\",\n",
    "                        x0=res.decision_date,\n",
    "                        y0=0,\n",
    "                        x1=res.decision_date,\n",
    "                        y1=1,\n",
    "                        xref=\"x\",\n",
    "                        yref=\"paper\",\n",
    "                        line=dict(color=\"red\", width=2, dash=\"dash\"),\n",
    "                    ),\n",
    "                    dict(\n",
    "                        type=\"line\",\n",
    "                        x0=res.buy_date,\n",
    "                        y0=0,\n",
    "                        x1=res.buy_date,\n",
    "                        y1=1,\n",
    "                        xref=\"x\",\n",
    "                        yref=\"paper\",\n",
    "                        line=dict(color=\"blue\", width=2, dash=\"dot\"),\n",
    "                    ),\n",
    "                ]\n",
    "\n",
    "                fig.layout.annotations = [\n",
    "                    dict(\n",
    "                        x=res.decision_date,\n",
    "                        y=0.05,\n",
    "                        xref=\"x\",\n",
    "                        yref=\"paper\",\n",
    "                        text=\"DECISION\",\n",
    "                        showarrow=False,\n",
    "                        bgcolor=\"red\",\n",
    "                        font=dict(color=\"white\"),\n",
    "                    ),\n",
    "                    dict(\n",
    "                        x=res.buy_date,\n",
    "                        y=1.0,\n",
    "                        xref=\"x\",\n",
    "                        yref=\"paper\",\n",
    "                        text=\"ENTRY (T+1)\",\n",
    "                        showarrow=False,\n",
    "                        bgcolor=\"blue\",\n",
    "                        font=dict(color=\"white\"),\n",
    "                    ),\n",
    "                ]\n",
    "\n",
    "            start_date = res.start_date.date()\n",
    "            act_date = res.decision_date.date()\n",
    "            entry_date = res.buy_date.date()\n",
    "\n",
    "            # Liquidity Audit Print\n",
    "            if (\n",
    "                inputs.mode == \"Ranking\"\n",
    "                and res.debug_data\n",
    "                and \"audit_liquidity\" in res.debug_data\n",
    "            ):\n",
    "                audit = res.debug_data[\"audit_liquidity\"]\n",
    "                if audit:\n",
    "                    raw_percentile = audit.get(\"percentile_setting\", 0)\n",
    "                    keep_pct = (\n",
    "                        1 - raw_percentile\n",
    "                    ) * 100  # Calculates the actual portion kept\n",
    "                    cut_val = audit.get(\"final_cutoff_usd\", 0)\n",
    "\n",
    "                    print(\"-\" * 60)\n",
    "                    print(f\"üîç LIQUIDITY CHECK (On Decision Date: {act_date})\")\n",
    "                    print(\n",
    "                        f\"   Universe Size: {audit.get('total_tickers_available')} tickers\"\n",
    "                    )\n",
    "                    print(\n",
    "                        f\"   Liquidity Threshold: {raw_percentile*100:.0f}th Percentile\"\n",
    "                    )\n",
    "                    print(f\"   Action: Keeping the Top {keep_pct:.0f}% of Market\")\n",
    "                    print(f\"   Calculated Cutoff: ${cut_val:,.0f} / day\")\n",
    "                    print(f\"   Tickers Remaining: {audit.get('tickers_passed')}\")\n",
    "                    print(\"-\" * 60)\n",
    "\n",
    "            # --- UPDATED TIMELINE PRINT ---\n",
    "            print(\n",
    "                f\"Timeline: Start [ {start_date} ] --> Decision [ {act_date} ] --> Cash (1d) --> Entry [ {entry_date} ] --> End [ {res.holding_end_date.date()} ]\"\n",
    "            )\n",
    "\n",
    "            if inputs.mode == \"Ranking\":\n",
    "                print(f\"Ranked Tickers ({len(res.tickers)}):\")\n",
    "                for i in range(0, len(res.tickers), 10):\n",
    "                    print(\", \".join(res.tickers[i : i + 10]))\n",
    "            else:\n",
    "                print(\"Manual Portfolio Tickers:\")\n",
    "                for i in range(0, len(res.tickers), 10):\n",
    "                    print(\", \".join(res.tickers[i : i + 10]))\n",
    "\n",
    "            m = res.perf_metrics\n",
    "\n",
    "            # --- DRY UI GENERATION ---\n",
    "            # 1. Define the metrics we want to display\n",
    "            metrics_to_show = [\n",
    "                (\"Gain\", \"gain\"),\n",
    "                (\"Sharpe\", \"sharpe\"),\n",
    "                (\"Sharpe (ATR)\", \"sharpe_atr\"),\n",
    "            ]\n",
    "\n",
    "            rows = []\n",
    "            for label, key in metrics_to_show:\n",
    "                p_row = {\n",
    "                    \"Metric\": f\"Group {label}\",\n",
    "                    \"Full\": m.get(f\"full_p_{key}\"),\n",
    "                    \"Lookback\": m.get(f\"lookback_p_{key}\"),\n",
    "                    \"Holding\": m.get(f\"holding_p_{key}\"),\n",
    "                }\n",
    "                b_row = {\n",
    "                    \"Metric\": f\"Benchmark {label}\",\n",
    "                    \"Full\": m.get(f\"full_b_{key}\"),\n",
    "                    \"Lookback\": m.get(f\"lookback_b_{key}\"),\n",
    "                    \"Holding\": m.get(f\"holding_b_{key}\"),\n",
    "                }\n",
    "\n",
    "                # Delta calculation\n",
    "                d_row = {\"Metric\": f\"== {label} Delta\"}\n",
    "                for col in [\"Full\", \"Lookback\", \"Holding\"]:\n",
    "                    d_row[col] = (p_row[col] or 0) - (b_row[col] or 0)\n",
    "\n",
    "                rows.extend([p_row, b_row, d_row])\n",
    "\n",
    "            df_report = pd.DataFrame(rows).set_index(\"Metric\")\n",
    "\n",
    "            # --- 2. STYLING (The \"Senior\" Design) ---\n",
    "            # --- 1. PREP DATA (Flattening the Index) ---\n",
    "            # We convert the index to a column so \"Metric\" sits on the same row as other headers\n",
    "            df_report = pd.DataFrame(rows)\n",
    "            df_report = df_report.set_index(\"Metric\")\n",
    "\n",
    "            # --- 2. THE STYLING (Sleek & Proportional) ---\n",
    "            def apply_sleek_style(styler):\n",
    "                # Match notebook font size (usually 13px)\n",
    "                styler.format(\"{:+.4f}\", na_rep=\"N/A\")\n",
    "\n",
    "                # Dynamic Row Highlighting\n",
    "                def row_logic(row):\n",
    "                    if \"Delta\" in row.name:\n",
    "                        return [\n",
    "                            \"background-color: #f9f9f9; font-weight: 600; border-top: 1px solid #ddd\"\n",
    "                        ] * len(row)\n",
    "                    if \"Group\" in row.name:\n",
    "                        return [\"color: #2c5e8f; background-color: #fcfdfe\"] * len(row)\n",
    "                    return [\"color: #555\"] * len(\n",
    "                        row\n",
    "                    )  # Benchmark rows are slightly muted\n",
    "\n",
    "                styler.apply(row_logic, axis=1)\n",
    "\n",
    "                styler.set_table_styles(\n",
    "                    [\n",
    "                        # Base Table Font - Scaling down to match standard text\n",
    "                        {\n",
    "                            \"selector\": \"\",\n",
    "                            \"props\": [\n",
    "                                (\"font-family\", \"inherit\"),\n",
    "                                (\"font-size\", \"12px\"),\n",
    "                                (\"border-collapse\", \"collapse\"),\n",
    "                                (\"width\", \"auto\"),\n",
    "                                (\"margin-left\", \"0\"),\n",
    "                            ],\n",
    "                        },\n",
    "                        # Header Row - Flattened and Muted\n",
    "                        {\n",
    "                            \"selector\": \"th\",\n",
    "                            \"props\": [\n",
    "                                (\"background-color\", \"white\"),\n",
    "                                (\"color\", \"#222\"),\n",
    "                                (\"font-weight\", \"600\"),\n",
    "                                (\"padding\", \"6px 12px\"),\n",
    "                                (\"border-bottom\", \"2px solid #444\"),\n",
    "                                (\"text-align\", \"center\"),\n",
    "                                (\n",
    "                                    \"vertical-align\",\n",
    "                                    \"bottom\",\n",
    "                                ),  # Aligns 'Metric' with others\n",
    "                            ],\n",
    "                        },\n",
    "                        # Index Column (The \"Metric\" labels)\n",
    "                        {\n",
    "                            \"selector\": \"th.row_heading\",\n",
    "                            \"props\": [\n",
    "                                (\"text-align\", \"left\"),\n",
    "                                (\"padding-right\", \"30px\"),\n",
    "                                (\"border-bottom\", \"1px solid #eee\"),\n",
    "                            ],\n",
    "                        },\n",
    "                        # Cell Data - Tighter padding\n",
    "                        {\n",
    "                            \"selector\": \"td\",\n",
    "                            \"props\": [\n",
    "                                (\"padding\", \"4px 12px\"),\n",
    "                                (\"border-bottom\", \"1px solid #eee\"),\n",
    "                            ],\n",
    "                        },\n",
    "                        # Remove the extra \"Index Name\" row completely\n",
    "                        {\n",
    "                            \"selector\": \"thead tr:nth-child(1) th\",\n",
    "                            \"props\": [(\"display\", \"table-cell\")],\n",
    "                        },\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "                # Hack to fix the 'Metric' alignment:\n",
    "                # We remove the index name and set it as the horizontal label for the index\n",
    "                styler.index.name = None\n",
    "\n",
    "                return styler\n",
    "\n",
    "            display(apply_sleek_style(df_report.style))\n",
    "\n",
    "    update_button.on_click(update_plot)\n",
    "    update_plot(None)\n",
    "    display(ui, fig)\n",
    "    return results_container, debug_container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a48017a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# SECTION F: UTILITIES\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "def export_debug_to_csv(container, source_label=\"Simulation\"):\n",
    "    \"\"\"\n",
    "    Flattens the debug_container and saves all components to high-precision CSVs.\n",
    "    \"\"\"\n",
    "    if not container or not container[0]:\n",
    "        print(\"‚ùå Error: Debug container is empty.\")\n",
    "        return\n",
    "\n",
    "    data = container[0]\n",
    "    inputs = data.get(\"inputs\")\n",
    "\n",
    "    # 1. Create a clean folder name\n",
    "    # e.g., Audit_Golden_20251211_SharpeATR\n",
    "    date_str = inputs.start_date.strftime(\"%Y-%m-%d\")\n",
    "    strategy_str = inputs.metric.replace(\" \", \"\").replace(\"(\", \"\").replace(\")\", \"\")\n",
    "    folder_name = f\"Audit_{source_label}_{date_str}_{strategy_str}\"\n",
    "\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "\n",
    "    print(f\"üìÇ Exporting audit data to: ./{folder_name}/\")\n",
    "\n",
    "    # 2. Recursive function to traverse the dictionary and save files\n",
    "    def process_item(item, path_prefix=\"\"):\n",
    "        # Handle Nested Dictionaries (like 'verification' or 'raw_components')\n",
    "        if isinstance(item, dict):\n",
    "            for key, value in item.items():\n",
    "                new_prefix = f\"{path_prefix}{key}_\" if path_prefix else f\"{key}_\"\n",
    "                process_item(value, new_prefix)\n",
    "\n",
    "        # Handle DataFrames\n",
    "        elif isinstance(item, pd.DataFrame):\n",
    "            filename = f\"{path_prefix.strip('_')}.csv\"\n",
    "            item.to_csv(os.path.join(folder_name, filename), float_format=\"%.8f\")\n",
    "            print(f\"   ‚úÖ Saved DataFrame: {filename}\")\n",
    "\n",
    "        # Handle Series (Convert to DataFrame for CSV preservation of Index)\n",
    "        elif isinstance(item, pd.Series):\n",
    "            filename = f\"{path_prefix.strip('_')}.csv\"\n",
    "            item.to_frame().to_csv(\n",
    "                os.path.join(folder_name, filename), float_format=\"%.8f\"\n",
    "            )\n",
    "            print(f\"   ‚úÖ Saved Series:    {filename}\")\n",
    "\n",
    "        # Handle Dataclasses (like 'inputs')\n",
    "        elif is_dataclass(item):\n",
    "            filename = f\"{path_prefix}Metadata.csv\"\n",
    "            # Convert to a vertical 2-column table for easy Excel reading\n",
    "            meta_df = pd.DataFrame.from_dict(\n",
    "                asdict(item), orient=\"index\", columns=[\"Value\"]\n",
    "            )\n",
    "            meta_df.to_csv(os.path.join(folder_name, filename))\n",
    "            print(f\"   ‚úÖ Saved Metadata:  {filename}\")\n",
    "\n",
    "    # 3. Start the extraction\n",
    "    process_item(data)\n",
    "    print(f\"\\n‚ú® Export Complete. All numbers saved with 8 decimal places.\")\n",
    "\n",
    "\n",
    "def print_nested(d, indent=0, width=4):\n",
    "    \"\"\"Pretty-print nested containers.\n",
    "    Leaves are rendered as two lines:  key\\\\nvalue .\"\"\"\n",
    "    spacing = \" \" * indent\n",
    "\n",
    "    def _kind(node):\n",
    "        if not isinstance(node, dict):\n",
    "            return None\n",
    "        return \"sep\" if all(isinstance(v, dict) for v in node.values()) else \"nest\"\n",
    "\n",
    "    if isinstance(d, dict):\n",
    "        for k, v in d.items():\n",
    "            kind = _kind(v)\n",
    "            tag = \"\" if kind is None else f\"  [{'SEP' if kind == 'sep' else 'NEST'}]\"\n",
    "            print(f\"{spacing}{k}{tag}\")\n",
    "            print_nested(v, indent + width, width)\n",
    "\n",
    "    elif isinstance(d, (list, tuple)):\n",
    "        for idx, item in enumerate(d):\n",
    "            print(f\"{spacing}[{idx}]\")\n",
    "            print_nested(item, indent + width, width)\n",
    "\n",
    "    else:  # leaf ‚Äì primitive value\n",
    "        print(f\"{spacing}{d}\")\n",
    "\n",
    "\n",
    "def get_ticker_OHLCV(\n",
    "    df_ohlcv: pd.DataFrame,\n",
    "    tickers: Union[str, List[str]],\n",
    "    date_start: str,\n",
    "    date_end: str,\n",
    "    return_format: str = \"dataframe\",\n",
    "    verbose: bool = True,\n",
    ") -> Union[pd.DataFrame, dict]:\n",
    "    \"\"\"\n",
    "    Get OHLCV data for specified tickers within a date range.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_ohlcv : pd.DataFrame\n",
    "        DataFrame with MultiIndex of (ticker, date) and OHLCV columns\n",
    "    tickers : str or list of str\n",
    "        Ticker symbol(s) to retrieve\n",
    "    date_start : str\n",
    "        Start date in 'YYYY-MM-DD' format\n",
    "    date_end : str\n",
    "        End date in 'YYYY-MM-DD' format\n",
    "    return_format : str, optional\n",
    "        Format to return data in. Options:\n",
    "        - 'dataframe': Single DataFrame with MultiIndex (default)\n",
    "        - 'dict': Dictionary with tickers as keys and DataFrames as values\n",
    "        - 'separate': List of separate DataFrames for each ticker\n",
    "    verbose : bool, optional\n",
    "        Whether to print summary information (default: True)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Union[pd.DataFrame, dict, list]\n",
    "        Filtered OHLCV data in specified format\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If input parameters are invalid\n",
    "    KeyError\n",
    "        If tickers not found in DataFrame\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> # Get data for single ticker\n",
    "    >>> vlo_data = get_ticker_OHLCV(df_ohlcv, 'VLO', '2025-08-13', '2025-09-04')\n",
    "\n",
    "    >>> # Get data for multiple tickers\n",
    "    >>> multi_data = get_ticker_OHLCV(df_ohlcv, ['VLO', 'JPST'], '2025-08-13', '2025-09-04')\n",
    "\n",
    "    >>> # Get data as dictionary\n",
    "    >>> data_dict = get_ticker_OHLCV(df_ohlcv, ['VLO', 'JPST'], '2025-08-13',\n",
    "    ...                              '2025-09-04', return_format='dict')\n",
    "    \"\"\"\n",
    "\n",
    "    # Input validation\n",
    "    if not isinstance(df_ohlcv, pd.DataFrame):\n",
    "        raise TypeError(\"df_ohlcv must be a pandas DataFrame\")\n",
    "\n",
    "    if not isinstance(df_ohlcv.index, pd.MultiIndex):\n",
    "        raise ValueError(\"DataFrame must have MultiIndex of (ticker, date)\")\n",
    "\n",
    "    if len(df_ohlcv.index.levels) != 2:\n",
    "        raise ValueError(\"MultiIndex must have exactly 2 levels: (ticker, date)\")\n",
    "\n",
    "    # Convert single ticker to list for consistent processing\n",
    "    if isinstance(tickers, str):\n",
    "        tickers = [tickers]\n",
    "    elif not isinstance(tickers, list):\n",
    "        raise TypeError(\"tickers must be a string or list of strings\")\n",
    "\n",
    "    # Convert dates to Timestamps\n",
    "    try:\n",
    "        start_date = pd.Timestamp(date_start)\n",
    "        end_date = pd.Timestamp(date_end)\n",
    "    except ValueError as e:\n",
    "        raise ValueError(f\"Invalid date format. Use 'YYYY-MM-DD': {e}\")\n",
    "\n",
    "    if start_date > end_date:\n",
    "        raise ValueError(\"date_start must be before or equal to date_end\")\n",
    "\n",
    "    # Check if tickers exist in the DataFrame\n",
    "    available_tickers = df_ohlcv.index.get_level_values(0).unique()\n",
    "    missing_tickers = [t for t in tickers if t not in available_tickers]\n",
    "\n",
    "    if missing_tickers:\n",
    "        raise KeyError(f\"Ticker(s) not found in DataFrame: {missing_tickers}\")\n",
    "\n",
    "    # Filter the data using MultiIndex slicing\n",
    "    try:\n",
    "        filtered_data = df_ohlcv.loc[(tickers, slice(date_start, date_end)), :]\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error filtering data: {e}\")\n",
    "\n",
    "    # Handle empty results\n",
    "    if filtered_data.empty:\n",
    "        if verbose:\n",
    "            print(\n",
    "                f\"No data found for tickers {tickers} in date range {date_start} to {date_end}\"\n",
    "            )\n",
    "        return filtered_data\n",
    "\n",
    "    # Print summary if verbose\n",
    "    if verbose:\n",
    "        print(\n",
    "            f\"Data retrieved for {len(tickers)} ticker(s) from {date_start} to {date_end}\"\n",
    "        )\n",
    "        print(f\"Total rows: {len(filtered_data)}\")\n",
    "        print(\n",
    "            f\"Date range in data: {filtered_data.index.get_level_values(1).min()} to \"\n",
    "            f\"{filtered_data.index.get_level_values(1).max()}\"\n",
    "        )\n",
    "\n",
    "        # Print ticker-specific counts\n",
    "        ticker_counts = filtered_data.index.get_level_values(0).value_counts()\n",
    "        for ticker in tickers:\n",
    "            count = ticker_counts.get(ticker, 0)\n",
    "            if count > 0:\n",
    "                print(f\"  {ticker}: {count} rows\")\n",
    "            else:\n",
    "                print(f\"  {ticker}: No data in range\")\n",
    "\n",
    "    # Return in requested format\n",
    "    if return_format == \"dict\":\n",
    "        result = {}\n",
    "        for ticker in tickers:\n",
    "            try:\n",
    "                result[ticker] = filtered_data.xs(ticker, level=0).loc[\n",
    "                    date_start:date_end\n",
    "                ]\n",
    "            except KeyError:\n",
    "                result[ticker] = pd.DataFrame()\n",
    "        return result\n",
    "\n",
    "    elif return_format == \"separate\":\n",
    "        result = []\n",
    "        for ticker in tickers:\n",
    "            try:\n",
    "                result.append(\n",
    "                    filtered_data.xs(ticker, level=0).loc[date_start:date_end]\n",
    "                )\n",
    "            except KeyError:\n",
    "                result.append(pd.DataFrame())\n",
    "        return result\n",
    "\n",
    "    elif return_format == \"dataframe\":\n",
    "        return filtered_data\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Invalid return_format: {return_format}. \"\n",
    "            f\"Must be 'dataframe', 'dict', or 'separate'\"\n",
    "        )\n",
    "\n",
    "\n",
    "def get_ticker_features(\n",
    "    features_df: pd.DataFrame,\n",
    "    tickers: Union[str, List[str]],\n",
    "    date_start: str,\n",
    "    date_end: str,\n",
    "    return_format: str = \"dataframe\",\n",
    "    verbose: bool = True,\n",
    ") -> Union[pd.DataFrame, dict]:\n",
    "    \"\"\"\n",
    "    Get features data for specified tickers within a date range.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    features_df : pd.DataFrame\n",
    "        DataFrame with MultiIndex of (ticker, date) and feature columns\n",
    "    tickers : str or list of str\n",
    "        Ticker symbol(s) to retrieve\n",
    "    date_start : str\n",
    "        Start date in 'YYYY-MM-DD' format\n",
    "    date_end : str\n",
    "        End date in 'YYYY-MM-DD' format\n",
    "    return_format : str, optional\n",
    "        Format to return data in. Options:\n",
    "        - 'dataframe': Single DataFrame with MultiIndex (default)\n",
    "        - 'dict': Dictionary with tickers as keys and DataFrames as values\n",
    "        - 'separate': List of separate DataFrames for each ticker\n",
    "    verbose : bool, optional\n",
    "        Whether to print summary information (default: True)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Union[pd.DataFrame, dict, list]\n",
    "        Filtered features data in specified format\n",
    "    \"\"\"\n",
    "    # Convert single ticker to list for consistent processing\n",
    "    if isinstance(tickers, str):\n",
    "        tickers = [tickers]\n",
    "\n",
    "    # Filter the data using MultiIndex slicing\n",
    "    try:\n",
    "        filtered_data = features_df.loc[(tickers, slice(date_start, date_end)), :]\n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(f\"Error filtering data: {e}\")\n",
    "        return pd.DataFrame() if return_format == \"dataframe\" else {}\n",
    "\n",
    "    # Handle empty results\n",
    "    if filtered_data.empty:\n",
    "        if verbose:\n",
    "            print(\n",
    "                f\"No data found for tickers {tickers} in date range {date_start} to {date_end}\"\n",
    "            )\n",
    "        return filtered_data\n",
    "\n",
    "    # Print summary if verbose\n",
    "    if verbose:\n",
    "        print(\n",
    "            f\"Features data retrieved for {len(tickers)} ticker(s) from {date_start} to {date_end}\"\n",
    "        )\n",
    "        print(f\"Total rows: {len(filtered_data)}\")\n",
    "        print(\n",
    "            f\"Date range in data: {filtered_data.index.get_level_values(1).min()} to \"\n",
    "            f\"{filtered_data.index.get_level_values(1).max()}\"\n",
    "        )\n",
    "        print(f\"Available features: {', '.join(filtered_data.columns.tolist())}\")\n",
    "\n",
    "        # Print ticker-specific counts\n",
    "        ticker_counts = filtered_data.index.get_level_values(0).value_counts()\n",
    "        for ticker in tickers:\n",
    "            count = ticker_counts.get(ticker, 0)\n",
    "            if count > 0:\n",
    "                print(f\"  {ticker}: {count} rows\")\n",
    "            else:\n",
    "                print(f\"  {ticker}: No data in range\")\n",
    "\n",
    "    # Return in requested format\n",
    "    if return_format == \"dict\":\n",
    "        result = {}\n",
    "        for ticker in tickers:\n",
    "            try:\n",
    "                result[ticker] = filtered_data.xs(ticker, level=0).loc[\n",
    "                    date_start:date_end\n",
    "                ]\n",
    "            except KeyError:\n",
    "                result[ticker] = pd.DataFrame()\n",
    "        return result\n",
    "\n",
    "    elif return_format == \"separate\":\n",
    "        result = []\n",
    "        for ticker in tickers:\n",
    "            try:\n",
    "                result.append(\n",
    "                    filtered_data.xs(ticker, level=0).loc[date_start:date_end]\n",
    "                )\n",
    "            except KeyError:\n",
    "                result.append(pd.DataFrame())\n",
    "        return result\n",
    "\n",
    "    elif return_format == \"dataframe\":\n",
    "        return filtered_data\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Invalid return_format: {return_format}. \"\n",
    "            f\"Must be 'dataframe', 'dict', or 'separate'\"\n",
    "        )\n",
    "\n",
    "\n",
    "def create_combined_dict(\n",
    "    df_ohlcv: pd.DataFrame,\n",
    "    features_df: pd.DataFrame,\n",
    "    tickers: Union[str, List[str]],\n",
    "    date_start: str,\n",
    "    date_end: str,\n",
    "    verbose: bool = True,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Create a combined dictionary with both OHLCV and features data for each ticker.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_ohlcv : pd.DataFrame\n",
    "        DataFrame with OHLCV data (MultiIndex: ticker, date)\n",
    "    features_df : pd.DataFrame\n",
    "        DataFrame with features data (MultiIndex: ticker, date)\n",
    "    tickers : str or list of str\n",
    "        Ticker symbol(s) to retrieve\n",
    "    date_start : str\n",
    "        Start date in 'YYYY-MM-DD' format\n",
    "    date_end : str\n",
    "        End date in 'YYYY-MM-DD' format\n",
    "    verbose : bool, optional\n",
    "        Whether to print progress information (default: True)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with tickers as keys and combined DataFrames (OHLCV + features) as values\n",
    "    \"\"\"\n",
    "    # Convert single ticker to list\n",
    "    if isinstance(tickers, str):\n",
    "        tickers = [tickers]\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Creating combined dictionary for {len(tickers)} ticker(s)\")\n",
    "        print(f\"Date range: {date_start} to {date_end}\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "    # Get OHLCV data as dictionary\n",
    "    ohlcv_dict = get_ticker_OHLCV(\n",
    "        df_ohlcv, tickers, date_start, date_end, return_format=\"dict\", verbose=verbose\n",
    "    )\n",
    "\n",
    "    # Get features data as dictionary\n",
    "    features_dict = get_ticker_features(\n",
    "        features_df,\n",
    "        tickers,\n",
    "        date_start,\n",
    "        date_end,\n",
    "        return_format=\"dict\",\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    # Create combined_dict\n",
    "    combined_dict = {}\n",
    "\n",
    "    for ticker in tickers:\n",
    "        if verbose:\n",
    "            print(f\"\\nProcessing {ticker}...\")\n",
    "\n",
    "        # Check if ticker exists in both dictionaries\n",
    "        if ticker in ohlcv_dict and ticker in features_dict:\n",
    "            ohlcv_data = ohlcv_dict[ticker]\n",
    "            features_data = features_dict[ticker]\n",
    "\n",
    "            # Check if both dataframes have data\n",
    "            if not ohlcv_data.empty and not features_data.empty:\n",
    "                # Combine OHLCV and features data\n",
    "                # Note: Both dataframes have the same index (dates), so we can concatenate\n",
    "                combined_df = pd.concat([ohlcv_data, features_data], axis=1)\n",
    "\n",
    "                # Ensure proper index naming\n",
    "                combined_df.index.name = \"Date\"\n",
    "\n",
    "                # Store in combined_dict\n",
    "                combined_dict[ticker] = combined_df\n",
    "\n",
    "                if verbose:\n",
    "                    print(f\"  ‚úì Successfully combined data\")\n",
    "                    print(f\"  OHLCV shape: {ohlcv_data.shape}\")\n",
    "                    print(f\"  Features shape: {features_data.shape}\")\n",
    "                    print(f\"  Combined shape: {combined_df.shape}\")\n",
    "                    print(\n",
    "                        f\"  Date range: {combined_df.index.min()} to {combined_df.index.max()}\"\n",
    "                    )\n",
    "            else:\n",
    "                if verbose:\n",
    "                    print(f\"  ‚úó Cannot combine: One or both dataframes are empty\")\n",
    "                    print(f\"    OHLCV empty: {ohlcv_data.empty}\")\n",
    "                    print(f\"    Features empty: {features_data.empty}\")\n",
    "                combined_dict[ticker] = pd.DataFrame()\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(f\"  ‚úó Ticker not found in both dictionaries\")\n",
    "                if ticker not in ohlcv_dict:\n",
    "                    print(f\"    Not in OHLCV data\")\n",
    "                if ticker not in features_dict:\n",
    "                    print(f\"    Not in features data\")\n",
    "            combined_dict[ticker] = pd.DataFrame()\n",
    "\n",
    "    # Print summary\n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"SUMMARY\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Total tickers processed: {len(tickers)}\")\n",
    "\n",
    "        tickers_with_data = [\n",
    "            ticker for ticker, df in combined_dict.items() if not df.empty\n",
    "        ]\n",
    "        print(f\"Tickers with combined data: {len(tickers_with_data)}\")\n",
    "\n",
    "        if tickers_with_data:\n",
    "            print(\"\\nTicker details:\")\n",
    "            for ticker in tickers_with_data:\n",
    "                df = combined_dict[ticker]\n",
    "                print(f\"  {ticker}: {df.shape} - {df.index.min()} to {df.index.max()}\")\n",
    "                print(f\"    Columns: {len(df.columns)}\")\n",
    "\n",
    "        empty_tickers = [ticker for ticker, df in combined_dict.items() if df.empty]\n",
    "        if empty_tickers:\n",
    "            print(f\"\\nTickers with no data: {', '.join(empty_tickers)}\")\n",
    "\n",
    "    return combined_dict\n",
    "\n",
    "\n",
    "def export_debug_to_csv(container, source_label=\"Simulation\"):\n",
    "    \"\"\"\n",
    "    Flattens the debug_container and saves all components to high-precision CSVs.\n",
    "    \"\"\"\n",
    "    if not container or not container[0]:\n",
    "        print(\"‚ùå Error: Debug container is empty.\")\n",
    "        return\n",
    "\n",
    "    data = container[0]\n",
    "    inputs = data.get(\"inputs\")\n",
    "\n",
    "    # 1. Create a clean folder name\n",
    "    # e.g., Audit_Golden_20251211_SharpeATR\n",
    "    date_str = inputs.start_date.strftime(\"%Y-%m-%d\")\n",
    "    strategy_str = inputs.metric.replace(\" \", \"\").replace(\"(\", \"\").replace(\")\", \"\")\n",
    "    folder_name = f\"Audit_{source_label}_{date_str}_{strategy_str}\"\n",
    "\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "\n",
    "    print(f\"üìÇ Exporting audit data to: ./{folder_name}/\")\n",
    "\n",
    "    # 2. Recursive function to traverse the dictionary and save files\n",
    "    def process_item(item, path_prefix=\"\"):\n",
    "        # Handle Nested Dictionaries (like 'verification' or 'raw_components')\n",
    "        if isinstance(item, dict):\n",
    "            for key, value in item.items():\n",
    "                new_prefix = f\"{path_prefix}{key}_\" if path_prefix else f\"{key}_\"\n",
    "                process_item(value, new_prefix)\n",
    "\n",
    "        # Handle DataFrames\n",
    "        elif isinstance(item, pd.DataFrame):\n",
    "            filename = f\"{path_prefix.strip('_')}.csv\"\n",
    "            item.to_csv(os.path.join(folder_name, filename), float_format=\"%.8f\")\n",
    "            print(f\"   ‚úÖ Saved DataFrame: {filename}\")\n",
    "\n",
    "        # Handle Series (Convert to DataFrame for CSV preservation of Index)\n",
    "        elif isinstance(item, pd.Series):\n",
    "            filename = f\"{path_prefix.strip('_')}.csv\"\n",
    "            item.to_frame().to_csv(\n",
    "                os.path.join(folder_name, filename), float_format=\"%.8f\"\n",
    "            )\n",
    "            print(f\"   ‚úÖ Saved Series:    {filename}\")\n",
    "\n",
    "        # Handle Dataclasses (like 'inputs')\n",
    "        elif is_dataclass(item):\n",
    "            filename = f\"{path_prefix}Metadata.csv\"\n",
    "            # Convert to a vertical 2-column table for easy Excel reading\n",
    "            meta_df = pd.DataFrame.from_dict(\n",
    "                asdict(item), orient=\"index\", columns=[\"Value\"]\n",
    "            )\n",
    "            meta_df.to_csv(os.path.join(folder_name, filename))\n",
    "            print(f\"   ‚úÖ Saved Metadata:  {filename}\")\n",
    "\n",
    "    # 3. Start the extraction\n",
    "    process_item(data)\n",
    "    print(f\"\\n‚ú® Export Complete. All numbers saved with 8 decimal places.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba143293",
   "metadata": {},
   "source": [
    "#####  ========== VERIFICATION TEST START, DO NOT DELETE =========  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c79f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 9476424 entries, ('A', Timestamp('1999-11-18 00:00:00')) to ('ZWS', Timestamp('2025-12-22 00:00:00'))\n",
      "Data columns (total 5 columns):\n",
      " #   Column     Dtype  \n",
      "---  ------     -----  \n",
      " 0   Adj Open   float64\n",
      " 1   Adj High   float64\n",
      " 2   Adj Low    float64\n",
      " 3   Adj Close  float64\n",
      " 4   Volume     int64  \n",
      "dtypes: float64(4), int64(1)\n",
      "memory usage: 398.3+ MB\n",
      "df_ohlcv.info():\n",
      "None\n",
      "Calculating features... this might take few minutes...\n",
      "1. Calculating Features...\n",
      "2. Pivoting Price Matrix...\n",
      "‚úÖ Optimization Complete. Ready for UI.\n"
     ]
    }
   ],
   "source": [
    "# ========== DO NOT DELETE =========\n",
    "# Use to verify bot calculation.\n",
    "# For inputs in image \"C:\\Users\\ping\\Files_win10\\python\\py311\\stocks\\notebooks_RLVR\\bot_verification\\bot_calc_verification.PNG\"\n",
    "# plot_walk_forward_analyzer plot should match results in image\n",
    "#\n",
    "data_path_test = r\"C:\\Users\\ping\\Files_win10\\python\\py311\\stocks\\notebooks_RLVR\\bot_verification\\df_OHLCV_stocks_etfs.parquet\"\n",
    "df_ohlcv = pd.read_parquet(data_path_test, engine=\"pyarrow\")\n",
    "\n",
    "print(f\"df_ohlcv.info():\\n{df_ohlcv.info()}\")\n",
    "df_ohlcv\n",
    "\n",
    "# Calculate features ONCE and store them in a variable\n",
    "print(\"Calculating features... this might take few minutes...\")\n",
    "print(\"1. Calculating Features...\")\n",
    "my_features = generate_features(\n",
    "    df_ohlcv=df_ohlcv,\n",
    "    atr_period=GLOBAL_SETTINGS[\"atr_period\"],\n",
    "    quality_window=GLOBAL_SETTINGS[\"quality_window\"],\n",
    "    quality_min_periods=GLOBAL_SETTINGS[\"quality_min_periods\"],\n",
    ")\n",
    "\n",
    "print(\"2. Pivoting Price Matrix...\")\n",
    "# This is the line that takes 12 seconds, but now we only run it ONCE.\n",
    "my_close_matrix = df_ohlcv[\"Adj Close\"].unstack(level=0)\n",
    "\n",
    "print(\"‚úÖ Optimization Complete. Ready for UI.\")\n",
    "# ========== DO NOT DELETE ========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d34bb7f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ‚öôÔ∏è Initializing AlphaEngine v2.2 (Sanitized) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dec11e4e3bf457bb513180fdbf21f5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<b>1. Timeline Configuration:</b> (Past <--- Decision ---> Future)'), HBox(children‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f5b89a92db8408383eb66102a5862f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'line': {'width': 2},\n",
       "              'name': 'RPRX',\n",
       "              'type': 'scatter',\n",
       "              'uid': '381addaa-5d32-4e9b-8660-36d4cb0c781d',\n",
       "              'visible': True,\n",
       "              'x': array([datetime.datetime(2025, 1, 2, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 3, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 6, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 7, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 8, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 10, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 13, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 14, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 15, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 16, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 17, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 21, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 22, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 23, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 24, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 27, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 28, 0, 0)], dtype=object),\n",
       "              'y': array([1.        , 1.00193989, 0.99844968, 1.00969546, 1.01589674, 1.14346818,\n",
       "                          1.16285513, 1.16440545, 1.19193754, 1.20007871, 1.19232313, 1.17487607,\n",
       "                          1.16634534, 1.18728261, 1.20434408, 1.22993628, 1.23846701])},\n",
       "             {'line': {'width': 2},\n",
       "              'name': 'SHV',\n",
       "              'type': 'scatter',\n",
       "              'uid': 'd18e3b5a-b40c-42ca-be8e-09057cc99242',\n",
       "              'visible': True,\n",
       "              'x': array([datetime.datetime(2025, 1, 2, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 3, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 6, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 7, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 8, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 10, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 13, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 14, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 15, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 16, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 17, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 21, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 22, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 23, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 24, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 27, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 28, 0, 0)], dtype=object),\n",
       "              'y': array([1.        , 1.00027431, 1.00045403, 1.00063374, 1.00063374, 1.00090805,\n",
       "                          1.00108777, 1.00118236, 1.00136208, 1.00145666, 1.00191069, 1.00209041,\n",
       "                          1.00209041, 1.00227013, 1.00263902, 1.00281874, 1.00290387])},\n",
       "             {'line': {'width': 2},\n",
       "              'name': 'SGOV',\n",
       "              'type': 'scatter',\n",
       "              'uid': '1821c993-0713-4c17-b2ab-d20d47fea1e3',\n",
       "              'visible': True,\n",
       "              'x': array([datetime.datetime(2025, 1, 2, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 3, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 6, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 7, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 8, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 10, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 13, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 14, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 15, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 16, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 17, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 21, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 22, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 23, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 24, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 27, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 28, 0, 0)], dtype=object),\n",
       "              'y': array([1.        , 1.00029902, 1.00029902, 1.00049836, 1.00069771, 1.0010964 ,\n",
       "                          1.00129575, 1.00139542, 1.00149509, 1.00149509, 1.00199346, 1.00209313,\n",
       "                          1.00229248, 1.00239215, 1.00269117, 1.00269117, 1.00299019])},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '9a2273f8-aeb8-4f2a-b49c-2c567976fb05', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '8c58a561-e495-4b8b-b2f6-6315b927b04a', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': 'cf5a5f42-9cba-4728-a92b-f704ef9ac66f', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': 'ef3aa670-1e25-4bba-bae8-0b6658753c83', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': 'cc5c6e84-a884-437b-ac1d-3eb4fd966f00', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '8115949c-b864-4dcb-9983-d1f59bab676b', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '58941763-a301-418e-8795-e86fef0089f9', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '29c12b81-be22-4e2d-a242-3578fab83c7b', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '49497fcc-fecb-4066-8428-491695ddb11a', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '73ce2112-5bea-4005-8f7e-c219134c7367', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': 'a2669f0c-e39e-4f64-bf3e-3bbcdcb326d9', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '6471f49a-20ec-4ddc-b463-6ee1775ebf76', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '4f061afc-30dd-4ad3-b5a2-cfda1242c1db', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '8b689728-5858-418f-a3b2-20073b914791', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '7128a796-1959-4a76-80c4-89fdc1e75313', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '61cbd8cb-ce5d-4206-898e-e60a8a5d95ff', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '436f0224-63ef-4212-bf3e-8b2c1e130839', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '065f8a7c-9c8e-4f5c-b533-a8099dc35daa', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': 'a7eb0944-3d6c-4e6e-8bc1-dc9eeee7c217', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '72b06b58-81ee-4279-af2f-1bba9145b229', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '6b2c4609-5cae-4784-9d80-46e5c1c0b7ef', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': 'd743762a-3fd2-4257-8b12-a6dd349f4925', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '8247e634-61d0-4d94-9041-c2c3c5655d9b', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '01acfffa-cd55-41f3-8c65-155f293cb6f0', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': 'b0e230d3-fc0d-4f81-861d-fd7a2cfb796c', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '6c876ace-434f-4f4d-8264-bf20a6755f7d', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': 'd6b5a1fa-e5a4-4634-a671-4c3f69ba7862', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': 'c4665fe7-e7cd-4a4c-b4de-0c6232bb3160', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': 'acd2a4e9-22b1-4afa-a955-d13d35a10f02', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '7b05e780-acad-4a82-8c35-5435a2b9dd05', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '53e6baa0-5a31-4338-8eb4-83586d94fbd3', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '1f7e2816-2cb7-413d-ade8-742a5fbb00e9', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': 'd3a305e4-6436-484d-a076-e0af73b3eb56', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': 'dc01557b-ba86-405b-ae69-101461c2e641', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '914d48c6-5bbd-4a90-a3f1-ec559d03efbf', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '7331e838-0b74-4426-a1d6-bf213155246f', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '98f463a8-b8b4-450d-b6f0-bd81baa775bd', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '08981e75-b90d-43a2-9fc1-fe76ac0703e8', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '63595a41-f84e-4708-b2e6-d128741d7499', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': 'ab173f3a-8328-4ace-a5db-dd86f8b78abe', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '7bcb1226-d811-4e9d-9962-b4578b0d8ed7', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '271a945e-08eb-45cf-b0d3-3179b8695428', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '87f9eb46-061a-43e9-9e15-df930bf15f1a', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '0fd82c5d-1963-4ea8-86ec-354256229a71', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '9a964dba-20f2-451e-9b29-95702a223d8c', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '005b507f-8eed-4289-8946-1c9013dcad72', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '4fd9adc6-0860-4f1b-86a7-965b7230b3e9', 'visible': False},\n",
       "             {'line': {'color': 'black', 'dash': 'dash', 'width': 3},\n",
       "              'name': 'Benchmark (SPY)',\n",
       "              'type': 'scatter',\n",
       "              'uid': 'ccebe05a-0974-4ee8-987b-58edb265c6c8',\n",
       "              'visible': True,\n",
       "              'x': array([datetime.datetime(2025, 1, 2, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 3, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 6, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 7, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 8, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 10, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 13, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 14, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 15, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 16, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 17, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 21, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 22, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 23, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 24, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 27, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 28, 0, 0)], dtype=object),\n",
       "              'y': array([1.        , 1.01250316, 1.01833681, 1.00682525, 1.00829621, 0.99290132,\n",
       "                          0.9944415 , 0.99581036, 1.01392393, 1.01197361, 1.02213362, 1.03148892,\n",
       "                          1.03728797, 1.0429503 , 1.03990454, 1.02519495, 1.0340034 ])},\n",
       "             {'line': {'color': 'green', 'width': 3},\n",
       "              'name': 'Group Portfolio',\n",
       "              'type': 'scatter',\n",
       "              'uid': '9696f660-3cf9-434e-8df1-366ecdca2b81',\n",
       "              'visible': True,\n",
       "              'x': array([datetime.datetime(2025, 1, 2, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 3, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 6, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 7, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 8, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 10, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 13, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 14, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 15, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 16, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 17, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 21, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 22, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 23, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 24, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 27, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 28, 0, 0)], dtype=object),\n",
       "              'y': array([1.        , 1.00083774, 0.99973424, 1.00360919, 1.00574273, 1.04849088,\n",
       "                          1.05507955, 1.05566108, 1.06493157, 1.06767682, 1.06540909, 1.05968654,\n",
       "                          1.05690941, 1.06398163, 1.06989142, 1.07848206, 1.08145369])}],\n",
       "    'layout': {'annotations': [{'bgcolor': 'red',\n",
       "                                'font': {'color': 'white'},\n",
       "                                'showarrow': False,\n",
       "                                'text': 'DECISION',\n",
       "                                'x': Timestamp('2025-01-17 00:00:00'),\n",
       "                                'xref': 'x',\n",
       "                                'y': 0.05,\n",
       "                                'yref': 'paper'},\n",
       "                               {'bgcolor': 'blue',\n",
       "                                'font': {'color': 'white'},\n",
       "                                'showarrow': False,\n",
       "                                'text': 'ENTRY (T+1)',\n",
       "                                'x': Timestamp('2025-01-21 00:00:00'),\n",
       "                                'xref': 'x',\n",
       "                                'y': 1.0,\n",
       "                                'yref': 'paper'}],\n",
       "               'height': 600,\n",
       "               'hovermode': 'x unified',\n",
       "               'shapes': [{'line': {'color': 'red', 'dash': 'dash', 'width': 2},\n",
       "                           'type': 'line',\n",
       "                           'x0': Timestamp('2025-01-17 00:00:00'),\n",
       "                           'x1': Timestamp('2025-01-17 00:00:00'),\n",
       "                           'xref': 'x',\n",
       "                           'y0': 0,\n",
       "                           'y1': 1,\n",
       "                           'yref': 'paper'},\n",
       "                          {'line': {'color': 'blue', 'dash': 'dot', 'width': 2},\n",
       "                           'type': 'line',\n",
       "                           'x0': Timestamp('2025-01-21 00:00:00'),\n",
       "                           'x1': Timestamp('2025-01-21 00:00:00'),\n",
       "                           'xref': 'x',\n",
       "                           'y0': 0,\n",
       "                           'y1': 1,\n",
       "                           'yref': 'paper'}],\n",
       "               'template': '...',\n",
       "               'title': {'text': 'Event-Driven Walk-Forward Analysis'}}\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ========== DO NOT DELETE =========\n",
    "results_container, debug_container = plot_walk_forward_analyzer(\n",
    "    df_ohlcv=df_ohlcv,\n",
    "    precomputed_features=my_features,\n",
    "    precomputed_close=my_close_matrix,\n",
    "    default_start_date=\"2025-01-17\",\n",
    "    default_lookback=10,\n",
    "    default_holding=5,\n",
    "    default_strategy=\"Sharpe (ATR)\",\n",
    "    default_rank_start=1,\n",
    "    default_rank_end=3,\n",
    "    default_benchmark_ticker=GLOBAL_SETTINGS[\"benchmark_ticker\"],\n",
    "    master_calendar_ticker=GLOBAL_SETTINGS[\"calendar_ticker\"],\n",
    "    quality_thresholds=GLOBAL_SETTINGS[\"thresholds\"],\n",
    "    debug=True,\n",
    ")\n",
    "# ========== DO NOT DELETE ========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "538fabbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== DO NOT DELETE =========\n",
    "import subprocess, os\n",
    "from pathlib import Path\n",
    "\n",
    "root = Path(\n",
    "    r\"C:/Users/ping/Files_win10/python/py311/stocks/notebooks_RLVR/bot_verification\"\n",
    ")\n",
    "csv = root / \"inputs_Metadata.csv\"\n",
    "png = root / \"bot_calc_verification.PNG\"\n",
    "\n",
    "code_exe = r\"C:\\Users\\ping\\AppData\\Local\\Programs\\Microsoft VS Code\\bin\\code.cmd\"\n",
    "\n",
    "for f in (csv, png):\n",
    "    if not f.exists():\n",
    "        raise FileNotFoundError(f)\n",
    "    subprocess.run([code_exe, str(f)])  # opens in current VS Code window\n",
    "# ========== DO NOT DELETE ========="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53ec71d",
   "metadata": {},
   "source": [
    "#####  ================= VERIFICATION TEST END ================  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1a94c3",
   "metadata": {},
   "source": [
    "#####  Output Debug Data to csv Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dad10a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Exporting audit data to: ./Audit_bot_verfication_2025-01-17_SharpeATR/\n",
      "   ‚úÖ Saved Metadata:  inputs_Metadata.csv\n",
      "   ‚úÖ Saved DataFrame: audit_liquidity_universe_snapshot.csv\n",
      "   ‚úÖ Saved DataFrame: full_universe_ranking.csv\n",
      "   ‚úÖ Saved Series:    verification_portfolio_full_val.csv\n",
      "   ‚úÖ Saved Series:    verification_portfolio_full_ret.csv\n",
      "   ‚úÖ Saved Series:    verification_portfolio_full_atrp.csv\n",
      "   ‚úÖ Saved Series:    verification_portfolio_lookback_val.csv\n",
      "   ‚úÖ Saved Series:    verification_portfolio_lookback_ret.csv\n",
      "   ‚úÖ Saved Series:    verification_portfolio_lookback_atrp.csv\n",
      "   ‚úÖ Saved Series:    verification_portfolio_holding_val.csv\n",
      "   ‚úÖ Saved Series:    verification_portfolio_holding_ret.csv\n",
      "   ‚úÖ Saved Series:    verification_portfolio_holding_atrp.csv\n",
      "   ‚úÖ Saved Series:    verification_benchmark_full_val.csv\n",
      "   ‚úÖ Saved Series:    verification_benchmark_full_ret.csv\n",
      "   ‚úÖ Saved Series:    verification_benchmark_full_atrp.csv\n",
      "   ‚úÖ Saved Series:    verification_benchmark_lookback_val.csv\n",
      "   ‚úÖ Saved Series:    verification_benchmark_lookback_ret.csv\n",
      "   ‚úÖ Saved Series:    verification_benchmark_lookback_atrp.csv\n",
      "   ‚úÖ Saved Series:    verification_benchmark_holding_val.csv\n",
      "   ‚úÖ Saved Series:    verification_benchmark_holding_ret.csv\n",
      "   ‚úÖ Saved Series:    verification_benchmark_holding_atrp.csv\n",
      "   ‚úÖ Saved DataFrame: portfolio_raw_components_prices.csv\n",
      "   ‚úÖ Saved DataFrame: portfolio_raw_components_atrp.csv\n",
      "   ‚úÖ Saved DataFrame: benchmark_raw_components_prices.csv\n",
      "   ‚úÖ Saved DataFrame: benchmark_raw_components_atrp.csv\n",
      "\n",
      "‚ú® Export Complete. All numbers saved with 8 decimal places.\n"
     ]
    }
   ],
   "source": [
    "export_debug_to_csv(debug_container, source_label=\"bot_verfication\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631da036",
   "metadata": {},
   "source": [
    "#####  Get Subset Data, Copy Cell Output and Paste into Excel with 'Import Wizard'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c34f99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date,Adj Open,Adj High,Adj Low,Adj Close,Volume\n",
      "2025-01-02,582.549,584.269,573.762,577.854,50793558\n",
      "2025-01-03,580.711,585.722,579.623,585.079,38333433\n",
      "2025-01-06,589.349,592.739,586.71,588.45,48239308\n",
      "2025-01-07,590.486,590.812,579.969,581.798,61102313\n",
      "2025-01-08,581.867,583.725,578.408,582.648,47860209\n",
      "2025-01-10,579.08,579.149,571.835,573.752,73963484\n",
      "2025-01-13,569.087,574.998,568.672,574.642,48472715\n",
      "2025-01-14,577.577,578.21,571.637,575.433,48989215\n",
      "2025-01-15,583.478,587.046,582.361,585.9,57568387\n",
      "2025-01-16,587.273,587.451,584.071,584.773,43828413\n",
      "2025-01-17,590.031,592.403,588.697,590.644,58752533\n",
      "2025-01-21,593.698,596.06,591.721,596.05,43032370\n",
      "2025-01-22,598.887,600.765,598.334,599.401,48761969\n",
      "2025-01-23,598.769,602.673,598.492,602.673,41635354\n",
      "2025-01-24,602.732,603.691,599.757,600.913,35011069\n",
      "2025-01-27,587.906,592.73,587.738,592.413,71187359\n",
      "2025-01-28,593.649,598.344,590.318,597.503,44955089\n",
      "\n",
      "Date,ATR,ATRP,ROC_1,ROC_3,ROC_5,ROC_10,ROC_21,RollingStalePct,RollMedDollarVol,RollingSameVolCount\n",
      "2025-01-02,6.971378751878971,0.012064256285980492,-0.002456510443190396,-0.017428890128276642,-0.02770724911285316,-0.029262204064353647,-0.028199427535476218,0.0,28448974215.475,0.0\n",
      "2025-01-03,7.035423126744755,0.01202474046538118,0.012503158237201717,0.006339945613380138,-0.015615115418265102,0.013069428307744602,-0.016505350497060145,0.0,28400995665.875,0.0\n",
      "2025-01-06,7.080035760548707,0.012031669233662515,0.005761615098132111,0.015835256707930734,0.000588331315549695,0.019218711570371028,-0.01694309312520037,0.0,28378186103.284,0.0\n",
      "2025-01-07,7.348818920509511,0.012631220665092542,-0.011304273940011988,0.006825253437719558,0.0006966028142754155,-0.00426330120881957,-0.02645384006546103,0.0,28378186103.284,0.0\n",
      "2025-01-08,7.203688997615975,0.012363706727931745,0.0014609881780274225,-0.004154994453740346,0.005819323052701941,-0.008742963012111526,-0.02687644052510274,0.0,28368298998.3385,0.0\n",
      "2025-01-10,7.4614969263576905,0.013004742338776494,-0.01526822369595382,-0.024977483218625363,-0.007098678905052336,-0.03460924315311098,-0.03677451956411171,0.0,28368298998.3385,0.0\n",
      "2025-01-13,7.380390003046428,0.012843457323074936,0.0015511928498725958,-0.012299801649369613,-0.01783861666544162,-0.033175180025573625,-0.0322715369523664,0.0,28336585077.9485,0.0\n",
      "2025-01-14,7.3227192885431185,0.012725581064247477,0.0013765092005109114,-0.01238311982534912,-0.022120825898547136,-0.021545508957599435,-0.03837264683027097,0.0,28303272760.1825,0.0\n",
      "2025-01-15,7.629167910790042,0.013021279929663837,0.018189780565243785,0.02117291094410123,0.007050557066198282,0.007752071318368081,-0.015810197005939775,0.0,28336585077.9485,0.0\n",
      "2025-01-16,7.325655917162182,0.012527349787288712,-0.0019235364396653631,0.017630107092763803,0.0036471420137029753,0.00948768896400165,-0.017508577035386663,0.0,28303272760.1825,0.0\n",
      "2025-01-17,7.347394780222026,0.012439633315875597,0.010039793218907134,0.026434007086837186,0.02944129170791565,0.022133618526478882,-0.011863116598492707,0.0,28303272760.1825,0.0\n",
      "2025-01-21,7.209438010206163,0.012095357789121991,0.009152721436262778,0.01732377538829155,0.03725449932305658,0.018751313925128077,0.0013052834581779305,0.0,28283334591.9635,0.0\n",
      "2025-01-22,7.0312638666200105,0.011730484044270881,0.0056220115762100065,0.02501483481624489,0.04165211240926392,0.018609907383804858,0.0378680971237908,0.0,28283334591.9635,0.0\n",
      "2025-01-23,6.82767359043287,0.01132898535430137,0.005458783018380098,0.020365905689383013,0.02862775217613933,0.03588015084273244,0.04385351101750401,0.0,28228329095.9225,0.0\n",
      "2025-01-24,6.620982619687671,0.011018204997541525,-0.0029203232930626877,0.008158711517490147,0.027600453509310396,0.03134825829660448,0.02845167374364399,0.0,28228329095.9225,0.0\n",
      "2025-01-27,7.089126718281405,0.011966527942974588,-0.014145142474867423,-0.011658305541698999,0.0029950359268866578,0.03252450536120155,0.007870197877802632,0.0,28228329095.9225,0.0\n",
      "2025-01-28,7.1560462384041665,0.01197658629061974,0.008591978906607345,-0.008578449673371735,0.0024377149567991196,0.03978303013006346,0.005354009028790907,0.0,28161285525.052498,0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_ticker = \"SPY\"\n",
    "_start_date = \"2025-01-02\"\n",
    "_end_date = \"2025-01-28\"\n",
    "\n",
    "_df = df_ohlcv.loc[_ticker][_start_date:_end_date]\n",
    "print(_df.to_csv())\n",
    "\n",
    "_df = my_features.loc[_ticker][_start_date:_end_date]\n",
    "print(_df.to_csv())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
