{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b6c8539",
   "metadata": {},
   "source": [
    "v52  \n",
    "\n",
    "1.  **The Temporal Alignment Fix:** We synchronized the \"Reward\" (Returns) and \"Risk\" (Volatility) by implementing the $N-1$ denominator logic. This ensures that Day 1's volatility no longer dilutes your Sharpe scores.\n",
    "2.  **The Event-Driven Re-normalization:** We verified that the Engine correctly resets capital and weights at the start of the Holding period, giving you an accurate \"Fresh Start\" performance metric.\n",
    "3.  **The Double-Blind Verification:** We proved that the Engine's True Range (TRP) math is flawless by recreating it from raw High/Low/Close data and achieving an 8-decimal match.\n",
    "4.  **Mathematical Fortification:** We centralized all logic into a polymorphic `QuantUtils` kernel that handles both single-portfolio reports and whole-universe rankings with built-in numerical safety.\n",
    "5.  **Volatility Evolution:** We successfully added `TRP` (True Range Percent) and the `Sharpe (TRP)` metric, giving you a raw, high-frequency alternative to the smoothed ATR.\n",
    "6.  **Data Integrity:** We implemented the \"Momentum Collapse\" tripwire (`verify_ranking_integrity`) to ensure that your risk-adjusted rankings never accidentally devolve into simple price momentum.\n",
    "7.  **The \"Audit Pack\" Architecture:** We collapsed fragmented results into a single, atomic container, ensuring that your inputs, results, and debug data are always perfectly synchronized.\n",
    "8.  **Total Transparency:** We replaced scattered CSV files with a unified **Excel Audit Report**, allowing for 1-to-1 manual verification of every calculation in the system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9817eb05",
   "metadata": {},
   "source": [
    "v51\n",
    "\n",
    "UNDO v50, Calculate Sharpe(ATR) using mean over lookback period.  \n",
    "\n",
    "Comment out ``# --- PINPOINT START: ATRP SWITCH ---`` in function ``_select_tickers`` can switch between ``Averaged ATRP over lookback period`` and ``Current ATRP``  \n",
    "    # --- PINPOINT START: ATRP SWITCH ---  \n",
    "    # To switch between Old (Averaged ATRP) and New (Current ATRP):  \n",
    "    # 1. Comment out the logic you DON'T want.  \n",
    "    # 2. Uncomment the logic you DO want.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15eb349d",
   "metadata": {},
   "source": [
    "v50\n",
    "\n",
    "Ticker selection based on atrp_value_for_obs based on decision day, was based on average over lookback period. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31dde13",
   "metadata": {},
   "source": [
    "v48  \n",
    "### Summary of what you just accomplished:\n",
    "1.  **Strict Math:** `QuantUtils` now contains an `assert` that prevents any dev (or AI) from filling the first day with 0.0.\n",
    "2.  **Semantic Protection:** Variables are now named `returns_WITH_BOUNDARY_NAN`, signaling to the AI that the Null value is part of its identity.\n",
    "3.  **Complete SOLID Separation:** The Engine CONDUCTS the simulation, while `QuantUtils` CALCULATES the results. They no longer share logic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61923b0e",
   "metadata": {},
   "source": [
    "**1. Data Flow of `plot_walk_forward_analyzer`**\n",
    "The function acts as a **UI wrapper** around the `AlphaEngine` class. The flow is:\n",
    "1.  **Input:** User selects parameters (Dates, Lookback, Strategy).\n",
    "2.  **State Construction:** `AlphaEngine` slices the historical data (`df_ohlcv`, `df_atrp`) up to the `decision_date`.\n",
    "3.  **Policy Execution (Hardcoded):** The engine applies the logic (e.g., `METRIC_REGISTRY['Sharpe']`) to rank stocks based *only* on the Lookback window.\n",
    "4.  **Environment Step:** It simulates a \"Buy\" at `decision_date + 1` and calculates the returns over the `holding_period`.\n",
    "5.  **Reward Generation:** It outputs performance metrics (`holding_p_gain`, `holding_p_sharpe`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1110f2e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- üõ°Ô∏è Starting Final Integrity Audit ---\n",
      "‚úÖ Series Boundary: OK\n",
      "‚úÖ DataFrame Boundary: OK\n",
      "‚úÖ AUDIT PASSED: Mathematical boundaries are strictly enforced.\n",
      "\n",
      "--- üõ°Ô∏è Starting Feature Engineering Audit ---\n",
      "‚ö° Generating SOTA Quant Features (Benchmark: SPY)...\n",
      "Audit Values:\n",
      "[ nan 25.  17.5]\n",
      "‚úÖ FEATURE INTEGRITY PASSED: Wilder's ATR logic is strictly enforced.\n",
      "--- üõ°Ô∏è Starting Ranking Kernel Audit ---\n",
      "‚úÖ RANKING INTEGRITY PASSED: Volatility normalization is strictly enforced.\n",
      "\n",
      "--- üõ°Ô∏è Starting Volatility Alignment Audit ---\n",
      "‚úÖ Series Temporal Coupling: OK\n",
      "‚úÖ DataFrame Temporal Coupling: OK\n",
      "‚úÖ AUDIT PASSED: Reward and Risk are strictly synchronized.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from dataclasses import dataclass, field, asdict, is_dataclass\n",
    "from typing import List, Dict, Optional, Any, Union, TypedDict, Tuple\n",
    "from collections import Counter\n",
    "from datetime import datetime, date\n",
    "from pandas.testing import assert_series_equal\n",
    "\n",
    "\n",
    "# pd.set_option('display.max_rows', None)  display all rows\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", 1000)\n",
    "pd.set_option(\"display.max_colwidth\", 50)\n",
    "pd.set_option(\"display.precision\", 4)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# GLOBAL SETTINGS: The \"Control Panel\" for the Strategy\n",
    "# ==============================================================================\n",
    "\n",
    "GLOBAL_SETTINGS = {\n",
    "    # ENVIRONMENT (The \"Where\")\n",
    "    \"benchmark_ticker\": \"SPY\",\n",
    "    \"calendar_ticker\": \"SPY\",  # Used as the \"Master Clock\" for trading days\n",
    "    # DATA SANITIZER (The \"Glitches & Gaps\" Protector)\n",
    "    \"handle_zeros_as_nan\": True,  # Convert 0.0 prices to NaN to prevent math errors\n",
    "    \"max_data_gap_ffill\": 1,  # Max consecutive days to \"Forward Fill\" missing data\n",
    "    # IMPLICATION OF nan_price_replacement:\n",
    "    # - This defines what happens if the \"Forward Fill\" limit is exceeded.\n",
    "    # - If set to 0.0: A permanent data gap will look like a \"total loss\" (-100%).\n",
    "    #   The equity curve will plummet. Good for \"disaster detection.\"\n",
    "    #   Sharpe and Sharpe(ATR) drop because: return (gets smaller) / std (gets larger)\n",
    "    # - If set to np.nan: A permanent gap will cause portfolio calculations to return NaN.\n",
    "    #   The chart may break or show gaps. Good for \"math integrity.\"\n",
    "    \"nan_price_replacement\": 0.0,\n",
    "    # STRATEGY PARAMETERS (The \"How\")\n",
    "    \"atr_period\": 14,  # Used for volatility normalization\n",
    "    \"quality_window\": 252,  # 1 year lookback for liquidity/quality stats\n",
    "    \"quality_min_periods\": 126,  # Min history required to judge a stock\n",
    "    # QUALITY THRESHOLDS (The \"Rules\")\n",
    "    \"thresholds\": {\n",
    "        # HARD LIQUIDITY FLOOR\n",
    "        # Logic: Calculates (Adj Close * Volume) daily, then takes the ROLLING MEDIAN\n",
    "        # over the quality_window (252 days). Filters out stocks where the\n",
    "        # typical daily dollar turnover is below this absolute value.\n",
    "        \"min_median_dollar_volume\": 1_000_000,\n",
    "        # DYNAMIC LIQUIDITY CUTOFF (Relative to Universe)\n",
    "        # Logic: On the decision date, the engine calculates the X-quantile\n",
    "        # of 'RollMedDollarVol' across ALL available stocks.\n",
    "        # Setting this to 0.40 calculates the 60th percentile and requires\n",
    "        # stocks to be above it‚Äîeffectively keeping only the TOP 60% of the market.\n",
    "        \"min_liquidity_percentile\": 0.40,\n",
    "        # PRICE/VOLUME STALENESS\n",
    "        # Logic: Creates a binary flag (1 if Volume is 0 OR High equals Low).\n",
    "        # It then calculates the ROLLING MEAN of this flag.\n",
    "        # A value of 0.05 means the stock is rejected if it was \"stale\"\n",
    "        # for more than 5% of the trading days in the rolling window.\n",
    "        \"max_stale_pct\": 0.05,\n",
    "        # DATA INTEGRITY (FROZEN VOLUME)\n",
    "        # Logic: Checks if Volume is identical to the previous day (Volume.diff() == 0).\n",
    "        # It calculates the ROLLING SUM of these occurrences over the window.\n",
    "        # If the exact same volume is reported more than 10 times, the stock\n",
    "        # is rejected as having \"frozen\" or low-quality data.\n",
    "        \"max_same_vol_count\": 10,\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION A: CORE KERNELS & QUANT UTILITIES (THE SAFE ROOM)\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "class QuantUtils:\n",
    "    \"\"\"\n",
    "    MATHEMATICAL KERNEL REGISTRY: THE SINGLE SOURCE OF TRUTH.\n",
    "    Handles both pd.Series (Report) and pd.DataFrame (Ranking) robustly.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_returns(\n",
    "        data: Union[pd.Series, pd.DataFrame],\n",
    "    ) -> Union[pd.Series, pd.DataFrame]:\n",
    "        return data.pct_change().replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_gain(data: Union[pd.Series, pd.DataFrame]) -> Union[float, pd.Series]:\n",
    "        if data.empty:\n",
    "            return 0.0\n",
    "        res = (data.ffill().iloc[-1] / data.bfill().iloc[0]) - 1\n",
    "\n",
    "        if isinstance(res, (pd.Series, pd.DataFrame)):\n",
    "            return res.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "        return float(res) if np.isfinite(res) else 0.0\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_sharpe(\n",
    "        data: Union[pd.Series, pd.DataFrame], periods: int = 252\n",
    "    ) -> Union[float, pd.Series]:\n",
    "        mu, std = data.mean(), data.std()\n",
    "        # Use np.maximum for universal floor (works on scalars and Series)\n",
    "        res = (mu / np.maximum(std, 1e-8)) * np.sqrt(periods)\n",
    "\n",
    "        if isinstance(res, (pd.Series, pd.DataFrame)):\n",
    "            return res.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "        return float(res) if np.isfinite(res) else 0.0\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_sharpe_vol(\n",
    "        returns: Union[pd.Series, pd.DataFrame],\n",
    "        vol_data: Union[pd.Series, pd.DataFrame],\n",
    "    ) -> Union[float, pd.Series]:\n",
    "        \"\"\"\n",
    "        Aligned Reward / Risk.\n",
    "        Filters out volatility observations where no return exists (e.g. Day 1 NaN).\n",
    "        \"\"\"\n",
    "        # 1. Identify valid timestamps (Pandas .mean() skips NaNs in returns)\n",
    "        # but we must manually force the volatility denominator to skip those same rows.\n",
    "        mask = returns.notna()\n",
    "        avg_ret = returns.mean()\n",
    "\n",
    "        # 2. Handle Logic Branches\n",
    "        if isinstance(returns, pd.DataFrame) and isinstance(vol_data, pd.Series):\n",
    "            # RANKING MODE: vol_data is usually a pre-calculated snapshot Series\n",
    "            avg_vol = vol_data\n",
    "        else:\n",
    "            # REPORT MODE (Series) or Cross-Sectional DataFrame\n",
    "            # Filter vol_data to only include rows where returns exist\n",
    "            avg_vol = vol_data.where(mask).mean()\n",
    "\n",
    "        # 3. Final Division\n",
    "        res = avg_ret / np.maximum(avg_vol, 1e-8)\n",
    "\n",
    "        if isinstance(res, (pd.Series, pd.DataFrame)):\n",
    "            return res.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "        return float(res) if np.isfinite(res) else 0.0\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_portfolio_stats(\n",
    "        prices: pd.DataFrame,\n",
    "        atrp_matrix: pd.DataFrame,\n",
    "        trp_matrix: pd.DataFrame,\n",
    "        weights: pd.Series,\n",
    "    ) -> Tuple[pd.Series, pd.Series, pd.Series, pd.Series]:\n",
    "        \"\"\"\n",
    "        MATRIX KERNEL: Calculates equity curve and weighted volatility.\n",
    "        \"\"\"\n",
    "        # 1. Equity Curve Logic (Price-Weighted Drift)\n",
    "        norm_prices = prices.div(prices.bfill().iloc[0])\n",
    "        weighted_components = norm_prices.mul(weights, axis=1)\n",
    "        equity_curve = weighted_components.sum(axis=1)\n",
    "\n",
    "        # MANDATORY: Use internal compute_returns to preserve boundary NaN\n",
    "        returns_WITH_BOUNDARY_NAN = QuantUtils.compute_returns(equity_curve)\n",
    "\n",
    "        # 2. Portfolio Volatility Logic (Weighted Average)\n",
    "        # We calculate current_weights (rebalanced daily by price drift)\n",
    "        current_weights = weighted_components.div(equity_curve, axis=0)\n",
    "\n",
    "        # Weighted average of ATRP and TRP\n",
    "        portfolio_atrp = (current_weights * atrp_matrix).sum(axis=1, min_count=1)\n",
    "        portfolio_trp = (current_weights * trp_matrix).sum(axis=1, min_count=1)\n",
    "\n",
    "        return equity_curve, returns_WITH_BOUNDARY_NAN, portfolio_atrp, portfolio_trp\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION B: STRATEGY HELPERS & FEATURES\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "def generate_features(\n",
    "    df_ohlcv: pd.DataFrame,\n",
    "    df_indices: pd.DataFrame = None,\n",
    "    benchmark_ticker: str = \"SPY\",\n",
    "    atr_period: int = 14,\n",
    "    rsi_period: int = 14,\n",
    "    quality_window: int = 252,\n",
    "    quality_min_periods: int = 126,\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    print(f\"‚ö° Generating SOTA Quant Features (Benchmark: {benchmark_ticker})...\")\n",
    "\n",
    "    # 1. Sort and Group\n",
    "    if not df_ohlcv.index.is_monotonic_increasing:\n",
    "        df_ohlcv = df_ohlcv.sort_index()\n",
    "    grouped = df_ohlcv.groupby(level=\"Ticker\")\n",
    "\n",
    "    # 2. VECTORIZED ATR (Wilder's)\n",
    "    prev_close = grouped[\"Adj Close\"].shift(1)\n",
    "    high_low = df_ohlcv[\"Adj High\"] - df_ohlcv[\"Adj Low\"]\n",
    "    high_prev = abs(df_ohlcv[\"Adj High\"] - prev_close)\n",
    "    low_prev = abs(df_ohlcv[\"Adj Low\"] - prev_close)\n",
    "    tr = pd.concat([high_low, high_prev, low_prev], axis=1).max(axis=1, skipna=False)\n",
    "\n",
    "    atr = (\n",
    "        tr.groupby(level=\"Ticker\")\n",
    "        .ewm(alpha=1 / atr_period, adjust=False)\n",
    "        .mean()\n",
    "        .reset_index(level=0, drop=True)\n",
    "    )\n",
    "    atrp = (atr / df_ohlcv[\"Adj Close\"]).replace([np.inf, -np.inf], np.nan)\n",
    "    trp = (tr / df_ohlcv[\"Adj Close\"]).replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    # 3. VECTORIZED RSI\n",
    "    delta = grouped[\"Adj Close\"].diff()\n",
    "    up = delta.clip(lower=0)\n",
    "    down = -1 * delta.clip(upper=0)\n",
    "\n",
    "    ma_up = (\n",
    "        up.groupby(level=\"Ticker\")\n",
    "        .ewm(alpha=1 / rsi_period, adjust=False)\n",
    "        .mean()\n",
    "        .reset_index(level=0, drop=True)\n",
    "    )\n",
    "    ma_down = (\n",
    "        down.groupby(level=\"Ticker\")\n",
    "        .ewm(alpha=1 / rsi_period, adjust=False)\n",
    "        .mean()\n",
    "        .reset_index(level=0, drop=True)\n",
    "    )\n",
    "\n",
    "    rsi = 100 - (100 / (1 + (ma_up / ma_down)))\n",
    "    rsi = rsi.replace([np.inf, -np.inf], 50).fillna(50)\n",
    "\n",
    "    # 4. OBV (Ticker Specific)\n",
    "    direction = np.sign(delta).fillna(0)\n",
    "    obv_raw = (direction * df_ohlcv[\"Volume\"]).groupby(level=\"Ticker\").cumsum()\n",
    "    obv_roll_mean = (\n",
    "        obv_raw.groupby(level=\"Ticker\")\n",
    "        .rolling(21)\n",
    "        .mean()\n",
    "        .reset_index(level=0, drop=True)\n",
    "    )\n",
    "    obv_roll_std = (\n",
    "        obv_raw.groupby(level=\"Ticker\")\n",
    "        .rolling(21)\n",
    "        .std()\n",
    "        .reset_index(level=0, drop=True)\n",
    "    )\n",
    "    obv_score = (\n",
    "        ((obv_raw - obv_roll_mean) / obv_roll_std)\n",
    "        .fillna(0.0)\n",
    "        .clip(lower=-5.0, upper=5.0)\n",
    "    )\n",
    "\n",
    "    # Dollar Volume\n",
    "    dollar_vol_series = df_ohlcv[\"Adj Close\"] * df_ohlcv[\"Volume\"]\n",
    "\n",
    "    # 5. BENCHMARK FEATURES (Price & Volume)\n",
    "    bench_close_series = None\n",
    "    bench_vol_series = None\n",
    "\n",
    "    found_bench = False\n",
    "    if (\n",
    "        df_indices is not None\n",
    "        and benchmark_ticker in df_indices.index.get_level_values(0)\n",
    "    ):\n",
    "        try:\n",
    "            bench_close_series = df_indices.xs(benchmark_ticker, level=0)[\"Adj Close\"]\n",
    "            bench_vol_series = df_indices.xs(benchmark_ticker, level=0)[\"Volume\"]\n",
    "            found_bench = True\n",
    "        except Exception:\n",
    "            pass\n",
    "    if not found_bench and benchmark_ticker in df_ohlcv.index.get_level_values(0):\n",
    "        try:\n",
    "            bench_close_series = df_ohlcv.xs(benchmark_ticker, level=0)[\"Adj Close\"]\n",
    "            bench_vol_series = df_ohlcv.xs(benchmark_ticker, level=0)[\"Volume\"]\n",
    "            found_bench = True\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Initialize Containers\n",
    "    rel_strength_21 = pd.Series(0.0, index=df_ohlcv.index)\n",
    "    spy_rvol = pd.Series(1.0, index=df_ohlcv.index)\n",
    "    spy_obv_score = pd.Series(0.0, index=df_ohlcv.index)  # <--- NEW CONTAINER\n",
    "\n",
    "    if found_bench:\n",
    "        try:\n",
    "            # A. Relative Strength\n",
    "            bench_close_aligned = bench_close_series.reindex(\n",
    "                df_ohlcv.index.get_level_values(\"Date\")\n",
    "            ).values\n",
    "            rel_ratio = df_ohlcv[\"Adj Close\"] / bench_close_aligned\n",
    "            rel_strength_21 = (\n",
    "                rel_ratio.groupby(level=\"Ticker\")\n",
    "                .pct_change(21, fill_method=None)\n",
    "                .fillna(0.0)\n",
    "            )\n",
    "\n",
    "            # B. Spy RVol (Magnitude)\n",
    "            bench_dvol = bench_close_series * bench_vol_series\n",
    "            bench_dvol_avg = bench_dvol.rolling(21).mean()\n",
    "            bench_rvol_raw = (bench_dvol / bench_dvol_avg).fillna(1.0)\n",
    "\n",
    "            # C. SPY OBV Score (Direction) <--- NEW\n",
    "            # Calculate OBV for SPY specifically\n",
    "            spy_delta = bench_close_series.diff()\n",
    "            spy_direction = np.sign(spy_delta).fillna(0)\n",
    "            spy_obv_raw = (spy_direction * bench_vol_series).cumsum()\n",
    "\n",
    "            # Normalize SPY OBV (Z-Score)\n",
    "            spy_obv_mean = spy_obv_raw.rolling(21).mean()\n",
    "            spy_obv_std = spy_obv_raw.rolling(21).std()\n",
    "            spy_obv_z = (\n",
    "                ((spy_obv_raw - spy_obv_mean) / spy_obv_std)\n",
    "                .fillna(0.0)\n",
    "                .clip(lower=-5.0, upper=5.0)\n",
    "            )\n",
    "\n",
    "            # D. BROADCAST TO ALL TICKERS\n",
    "            # Reindex creates a Series aligned to the full DataFrame (Date-matched)\n",
    "            spy_rvol_values = (\n",
    "                bench_rvol_raw.reindex(df_ohlcv.index.get_level_values(\"Date\"))\n",
    "                .fillna(1.0)\n",
    "                .values\n",
    "            )\n",
    "            spy_obv_values = (\n",
    "                spy_obv_z.reindex(df_ohlcv.index.get_level_values(\"Date\"))\n",
    "                .fillna(0.0)\n",
    "                .values\n",
    "            )\n",
    "\n",
    "            spy_rvol = pd.Series(spy_rvol_values, index=df_ohlcv.index).clip(upper=10.0)\n",
    "            spy_obv_score = pd.Series(spy_obv_values, index=df_ohlcv.index)  # <--- NEW\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Benchmark Math Error: {e}\")\n",
    "\n",
    "    # 6. TICKER RELATIVE VOLUME (RVol)\n",
    "    dvol_grouped = dollar_vol_series.groupby(level=\"Ticker\")\n",
    "    dvol_avg = dvol_grouped.rolling(21).mean().reset_index(level=0, drop=True)\n",
    "    ticker_rvol = (\n",
    "        (dollar_vol_series / dvol_avg)\n",
    "        .replace([np.inf, -np.inf], 1.0)\n",
    "        .fillna(1.0)\n",
    "        .clip(upper=10.0)\n",
    "    )\n",
    "\n",
    "    # 7. MOMENTUM / RETURN FEATURES\n",
    "    daily_returns = grouped[\"Adj Close\"].pct_change(1, fill_method=None)\n",
    "    roc_1 = daily_returns\n",
    "    roc_3 = grouped[\"Adj Close\"].pct_change(3, fill_method=None)\n",
    "    roc_5 = grouped[\"Adj Close\"].pct_change(5, fill_method=None)\n",
    "    roc_10 = grouped[\"Adj Close\"].pct_change(10, fill_method=None)\n",
    "    roc_21 = grouped[\"Adj Close\"].pct_change(21, fill_method=None)\n",
    "\n",
    "    # 8. VOLATILITY REGIME\n",
    "    returns_grouped = daily_returns.groupby(level=\"Ticker\")\n",
    "    std_5 = returns_grouped.rolling(5).std().reset_index(level=0, drop=True)\n",
    "    std_21 = returns_grouped.rolling(21).std().reset_index(level=0, drop=True)\n",
    "\n",
    "    if std_5.index.nlevels > df_ohlcv.index.nlevels:\n",
    "        std_5 = std_5.reset_index(level=0, drop=True)\n",
    "        std_21 = std_21.reset_index(level=0, drop=True)\n",
    "\n",
    "    vol_regime = (std_5 / std_21).replace([np.inf, -np.inf], 1.0)\n",
    "\n",
    "    # 9. MERGE\n",
    "    indicator_df = pd.DataFrame(\n",
    "        {\n",
    "            \"ATR\": atr,\n",
    "            \"ATRP\": atrp,\n",
    "            \"TRP\": trp,  # <--- PINPOINT CHANGE: Add to output\n",
    "            \"RSI\": rsi,\n",
    "            \"RelStrength\": rel_strength_21,\n",
    "            \"VolRegime\": vol_regime,\n",
    "            \"RVol\": ticker_rvol,\n",
    "            \"Spy_RVol\": spy_rvol,\n",
    "            \"OBV_Score\": obv_score,\n",
    "            \"Spy_OBV_Score\": spy_obv_score,  # <--- NEW\n",
    "            \"ROC_1\": roc_1,\n",
    "            \"ROC_3\": roc_3,\n",
    "            \"ROC_5\": roc_5,\n",
    "            \"ROC_10\": roc_10,\n",
    "            \"ROC_21\": roc_21,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # 10. Quality/Liquidity Features\n",
    "    quality_temp_df = pd.DataFrame(\n",
    "        {\n",
    "            \"IsStale\": np.where(\n",
    "                (df_ohlcv[\"Volume\"] == 0)\n",
    "                | (df_ohlcv[\"Adj High\"] == df_ohlcv[\"Adj Low\"]),\n",
    "                1,\n",
    "                0,\n",
    "            ),\n",
    "            \"DollarVolume\": dollar_vol_series,\n",
    "            \"HasSameVolume\": (grouped[\"Volume\"].diff() == 0).astype(int),\n",
    "        },\n",
    "        index=df_ohlcv.index,\n",
    "    )\n",
    "\n",
    "    rolling_result = (\n",
    "        quality_temp_df.groupby(level=\"Ticker\")\n",
    "        .rolling(window=quality_window, min_periods=quality_min_periods)\n",
    "        .agg({\"IsStale\": \"mean\", \"DollarVolume\": \"median\", \"HasSameVolume\": \"sum\"})\n",
    "        .rename(\n",
    "            columns={\n",
    "                \"IsStale\": \"RollingStalePct\",\n",
    "                \"DollarVolume\": \"RollMedDollarVol\",\n",
    "                \"HasSameVolume\": \"RollingSameVolCount\",\n",
    "            }\n",
    "        )\n",
    "        .reset_index(level=0, drop=True)\n",
    "    )\n",
    "\n",
    "    return pd.concat([indicator_df, rolling_result], axis=1)\n",
    "\n",
    "\n",
    "def _prepare_initial_weights(tickers: List[str]) -> pd.Series:\n",
    "    \"\"\"\n",
    "    METADATA: Converts a list of tickers into a weight map.\n",
    "    Example: ['AAPL', 'AAPL', 'TSLA'] -> {'AAPL': 0.66, 'TSLA': 0.33}\n",
    "    \"\"\"\n",
    "    ticker_counts = Counter(tickers)\n",
    "    total = len(tickers)\n",
    "    return pd.Series({t: c / total for t, c in ticker_counts.items()})\n",
    "\n",
    "\n",
    "def calculate_buy_and_hold_performance(\n",
    "    df_close_wide: pd.DataFrame,  # Use the WIDE version\n",
    "    df_atrp_wide: pd.DataFrame,  # Use the WIDE version\n",
    "    df_trp_wide: pd.DataFrame,  # <--- Added\n",
    "    tickers: List[str],\n",
    "    start_date: pd.Timestamp,\n",
    "    end_date: pd.Timestamp,\n",
    "):\n",
    "    if not tickers:\n",
    "        return pd.Series(), pd.Series(), pd.Series()\n",
    "\n",
    "    initial_weights = _prepare_initial_weights(tickers)\n",
    "\n",
    "    # SLICE (Fix Part B)\n",
    "    ticker_list = initial_weights.index.tolist()\n",
    "    p_slice = df_close_wide.reindex(columns=ticker_list).loc[start_date:end_date]\n",
    "    a_slice = df_atrp_wide.reindex(columns=ticker_list).loc[start_date:end_date]\n",
    "    t_slice = df_trp_wide.reindex(columns=ticker_list).loc[start_date:end_date]\n",
    "    # KERNEL - Pure Math\n",
    "    return QuantUtils.compute_portfolio_stats(\n",
    "        p_slice, a_slice, t_slice, initial_weights\n",
    "    )\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION C: METRIC REGISTRY\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "class MarketObservation(TypedDict):\n",
    "    \"\"\"\n",
    "    The 'STATE' (Observation) in Reinforcement Learning.\n",
    "    This defines the context given to the agent to make a decision.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- The Movie (Time Series) ---\n",
    "    lookback_returns: pd.DataFrame  # (Time x Tickers)\n",
    "    lookback_close: pd.DataFrame  # (Time x Tickers)\n",
    "\n",
    "    # --- The Snapshot (Scalar values at Decision Time) ---\n",
    "    atrp: pd.Series  # Volatility (Mean over lookback)\n",
    "    trp: pd.Series  # Volatility (Snapshot)\n",
    "\n",
    "    # NEW SENSORS\n",
    "    rsi: pd.Series  # Internal Momentum (0-100)\n",
    "    rel_strength: pd.Series  # Performance vs SPY\n",
    "    vol_regime: pd.Series  # Volatility Expansion/Compression\n",
    "    rvol: pd.Series  # Ticker Conviction\n",
    "    spy_rvol: pd.Series  # Market Participation\n",
    "    obv_score: pd.Series  # Ticker Accumulation/Distribution\n",
    "    spy_obv_score: pd.Series  # Market Tide\n",
    "\n",
    "    # MOMENTUM VECTORS\n",
    "    roc_1: pd.Series\n",
    "    roc_3: pd.Series\n",
    "    roc_5: pd.Series\n",
    "    roc_10: pd.Series\n",
    "    roc_21: pd.Series\n",
    "\n",
    "\n",
    "METRIC_REGISTRY = {\n",
    "    # --- CLASSIC METRICS ---\n",
    "    \"Price\": lambda obs: QuantUtils.calculate_gain(obs[\"lookback_close\"]),\n",
    "    \"Sharpe\": lambda obs: QuantUtils.calculate_sharpe(obs[\"lookback_returns\"]),\n",
    "    \"Sharpe (ATRP)\": lambda obs: QuantUtils.calculate_sharpe_vol(\n",
    "        obs[\"lookback_returns\"], obs[\"atrp\"]\n",
    "    ),\n",
    "    \"Sharpe (TRP)\": lambda obs: QuantUtils.calculate_sharpe_vol(\n",
    "        obs[\"lookback_returns\"], obs[\"trp\"]\n",
    "    ),  # <--- New Strategy\n",
    "    # --- MOMENTUM VECTORS ---\n",
    "    \"Momentum 1D\": lambda obs: obs[\"roc_1\"],\n",
    "    \"Momentum 3D\": lambda obs: obs[\"roc_3\"],\n",
    "    \"Momentum 5D\": lambda obs: obs[\"roc_5\"],\n",
    "    \"Momentum 10D\": lambda obs: obs[\"roc_10\"],\n",
    "    \"Momentum 1M\": lambda obs: obs[\"roc_21\"],\n",
    "    # --- PULLBACK VECTORS ---\n",
    "    \"Pullback 1D\": lambda obs: -obs[\"roc_1\"],\n",
    "    \"Pullback 3D\": lambda obs: -obs[\"roc_3\"],\n",
    "    \"Pullback 5D\": lambda obs: -obs[\"roc_5\"],\n",
    "    \"Pullback 10D\": lambda obs: -obs[\"roc_10\"],\n",
    "    \"Pullback 1M\": lambda obs: -obs[\"roc_21\"],\n",
    "    # --- NEW SOTA SENSORS ---\n",
    "    \"RSI (Reversal)\": lambda obs: -obs[\"rsi\"],  # Rank Low RSI (Oversold) higher\n",
    "    \"RSI (Trend)\": lambda obs: obs[\"rsi\"],  # Rank High RSI (Strong Trend) higher\n",
    "    \"Alpha (RelStrength)\": lambda obs: obs[\"rel_strength\"],  # Rank stocks beating SPY\n",
    "    \"OBV_Score (Accumulation)\": lambda obs: obs[\"obv_score\"],  # Rank High OBV Score\n",
    "    \"Cur$Vol / Avg$Vol21 (V Inc)\": lambda obs: obs[\"rvol\"],  # Rank High Relative Volume\n",
    "    \"5dStDev / 21dStDev (Vol Inc)\": lambda obs: obs[\n",
    "        \"vol_regime\"\n",
    "    ],  # Rank High Volatility Expansion\n",
    "}\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION D: DATA CONTRACTS\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EngineInput:\n",
    "    mode: str\n",
    "    start_date: pd.Timestamp\n",
    "    lookback_period: int\n",
    "    holding_period: int\n",
    "    metric: str\n",
    "    benchmark_ticker: str\n",
    "    rank_start: int = 1\n",
    "    rank_end: int = 10\n",
    "    # Default factory pulls from Global thresholds\n",
    "    quality_thresholds: Dict[str, float] = field(\n",
    "        default_factory=lambda: GLOBAL_SETTINGS[\"thresholds\"].copy()\n",
    "    )\n",
    "    manual_tickers: List[str] = field(default_factory=list)\n",
    "    debug: bool = False\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EngineOutput:\n",
    "    # 1. CORE DATA (Required - No Defaults)\n",
    "    portfolio_series: pd.Series\n",
    "    benchmark_series: pd.Series\n",
    "    normalized_plot_data: pd.DataFrame\n",
    "    tickers: List[str]\n",
    "    initial_weights: pd.Series\n",
    "    perf_metrics: Dict[str, float]\n",
    "    results_df: pd.DataFrame\n",
    "\n",
    "    # 2. TIMELINE (Required - No Defaults)\n",
    "    start_date: pd.Timestamp\n",
    "    decision_date: pd.Timestamp\n",
    "    buy_date: pd.Timestamp\n",
    "    holding_end_date: pd.Timestamp\n",
    "\n",
    "    # 3. OPTIONAL / AUDIT DATA (Must be at the bottom because they have defaults)\n",
    "    portfolio_atrp_series: Optional[pd.Series] = None\n",
    "    benchmark_atrp_series: Optional[pd.Series] = None\n",
    "    portfolio_trp_series: Optional[pd.Series] = None\n",
    "    benchmark_trp_series: Optional[pd.Series] = None\n",
    "    error_msg: Optional[str] = None\n",
    "    debug_data: Optional[Dict[str, Any]] = None\n",
    "\n",
    "\n",
    "class AlphaEngine:\n",
    "    def __init__(\n",
    "        self,\n",
    "        df_ohlcv: pd.DataFrame,\n",
    "        features_df: pd.DataFrame = None,\n",
    "        df_close_wide: pd.DataFrame = None,\n",
    "        df_atrp_wide: pd.DataFrame = None,\n",
    "        df_trp_wide: pd.DataFrame = None,\n",
    "        master_ticker: str = GLOBAL_SETTINGS[\"calendar_ticker\"],\n",
    "    ):\n",
    "        print(\"--- ‚öôÔ∏è Initializing AlphaEngine v2.2 (Sanitized) ---\")\n",
    "\n",
    "        # 1. SETUP PRICES (CLEAN-AT-ENTRY)\n",
    "        if df_close_wide is not None:\n",
    "            self.df_close = df_close_wide\n",
    "        else:\n",
    "            print(\"üê¢ Pivoting and Sanitizing Price Data...\")\n",
    "            self.df_close = df_ohlcv[\"Adj Close\"].unstack(level=0)\n",
    "\n",
    "        # STORE RAW SOURCE (The \"Transparency\" Line)\n",
    "        self.df_ohlcv_raw = df_ohlcv\n",
    "\n",
    "        # 3. DATA SANITIZER STEP 1: Handle Zeros\n",
    "        if GLOBAL_SETTINGS[\"handle_zeros_as_nan\"]:\n",
    "            # Replace 0.0 with NaN so math functions (mean/std) ignore them\n",
    "            self.df_close = self.df_close.replace(0, np.nan)\n",
    "\n",
    "        # Smooth over 1-2 day glitches (The \"FNV\" Fix)\n",
    "        self.df_close = self.df_close.ffill(limit=GLOBAL_SETTINGS[\"max_data_gap_ffill\"])\n",
    "\n",
    "        # Handle the remaining \"unfillable\" gaps\n",
    "        self.df_close = self.df_close.fillna(GLOBAL_SETTINGS[\"nan_price_replacement\"])\n",
    "\n",
    "        # 2. SETUP FEATURES\n",
    "        if features_df is not None:\n",
    "            self.features_df = features_df\n",
    "        else:\n",
    "            # We pass the cleaned price data if needed, or calculate from raw\n",
    "            self.features_df = generate_features(\n",
    "                df_ohlcv,\n",
    "                atr_period=GLOBAL_SETTINGS[\"atr_period\"],\n",
    "                quality_window=GLOBAL_SETTINGS[\"quality_window\"],\n",
    "                quality_min_periods=GLOBAL_SETTINGS[\"quality_min_periods\"],\n",
    "            )\n",
    "\n",
    "        # 1. SETUP ATRP\n",
    "        if df_atrp_wide is not None:\n",
    "            # INSTANT: Use the matrix precomputed outside the UI\n",
    "            self.df_atrp = df_atrp_wide\n",
    "        else:\n",
    "            # SLOW FALLBACK: Only runs if you forget to precompute\n",
    "            print(\"üöÄ Pre-aligning Volatility (ATRP) Matrix (Slow Fallback)...\")\n",
    "            self.df_atrp = self.features_df[\"ATRP\"].unstack(level=0)\n",
    "\n",
    "        # 2. SETUP TRP\n",
    "        if df_trp_wide is not None:\n",
    "            self.df_trp = df_trp_wide\n",
    "        else:\n",
    "            print(\"üöÄ Pre-aligning Volatility (TRP) Matrix (Slow Fallback)...\")\n",
    "            self.df_trp = self.features_df[\"TRP\"].unstack(level=0)\n",
    "\n",
    "        # 3. FINAL ALIGNMENT (The \"Safety Seal\")\n",
    "        # Ensures all matrices have the exact same Dimensions, Tickers, and Dates\n",
    "        common_idx = self.df_close.index\n",
    "        common_cols = self.df_close.columns\n",
    "\n",
    "        self.df_atrp = self.df_atrp.reindex(index=common_idx, columns=common_cols)\n",
    "        self.df_trp = self.df_trp.reindex(index=common_idx, columns=common_cols)\n",
    "\n",
    "        # 3. Setup Calendar\n",
    "        if master_ticker not in self.df_close.columns:\n",
    "            master_ticker = self.df_close.columns[0]\n",
    "        self.trading_calendar = (\n",
    "            self.df_close[master_ticker].dropna().index.unique().sort_values()\n",
    "        )\n",
    "\n",
    "    def run(self, inputs: EngineInput) -> EngineOutput:\n",
    "        dates, error = self._validate_timeline(inputs)\n",
    "        if error:\n",
    "            return self._error_result(error)\n",
    "        (safe_start, safe_decision, safe_buy, safe_end) = dates\n",
    "\n",
    "        tickers_to_trade, results_table, debug_dict, error = self._select_tickers(\n",
    "            inputs, safe_start, safe_decision\n",
    "        )\n",
    "        if error:\n",
    "            return self._error_result(error)\n",
    "\n",
    "        # GENERATE TRACKS (Fix Part A)\n",
    "        p_f_val, p_f_ret, p_f_atrp, p_f_trp = calculate_buy_and_hold_performance(\n",
    "            self.df_close,\n",
    "            self.df_atrp,\n",
    "            self.df_trp,\n",
    "            tickers_to_trade,\n",
    "            safe_start,\n",
    "            safe_end,\n",
    "        )\n",
    "        b_f_val, b_f_ret, b_f_atrp, b_f_trp = calculate_buy_and_hold_performance(\n",
    "            self.df_close,\n",
    "            self.df_atrp,\n",
    "            self.df_trp,\n",
    "            [inputs.benchmark_ticker],\n",
    "            safe_start,\n",
    "            safe_end,\n",
    "        )\n",
    "\n",
    "        p_h_val, p_h_ret, p_h_atrp, p_h_trp = calculate_buy_and_hold_performance(\n",
    "            self.df_close,\n",
    "            self.df_atrp,\n",
    "            self.df_trp,\n",
    "            tickers_to_trade,\n",
    "            safe_buy,\n",
    "            safe_end,\n",
    "        )\n",
    "        b_h_val, b_h_ret, b_h_atrp, b_h_trp = calculate_buy_and_hold_performance(\n",
    "            self.df_close,\n",
    "            self.df_atrp,\n",
    "            self.df_trp,\n",
    "            [inputs.benchmark_ticker],\n",
    "            safe_buy,\n",
    "            safe_end,\n",
    "        )\n",
    "\n",
    "        # CALCULATE METRICS\n",
    "        p_metrics, p_slices = self._calculate_period_metrics(\n",
    "            p_f_val,\n",
    "            p_f_ret,\n",
    "            p_f_atrp,\n",
    "            p_f_trp,\n",
    "            safe_decision,\n",
    "            p_h_val,\n",
    "            p_h_ret,\n",
    "            p_h_atrp,\n",
    "            p_h_trp,\n",
    "            prefix=\"p\",\n",
    "        )\n",
    "        b_metrics, b_slices = self._calculate_period_metrics(\n",
    "            b_f_val,\n",
    "            b_f_ret,\n",
    "            b_f_atrp,\n",
    "            b_f_trp,\n",
    "            safe_decision,\n",
    "            b_h_val,\n",
    "            b_h_ret,\n",
    "            b_h_atrp,\n",
    "            b_h_trp,\n",
    "            prefix=\"b\",\n",
    "        )\n",
    "\n",
    "        # 1. Prepare the Index Slice for the selected tickers/dates\n",
    "        # This works even if Ticker/Date order is swapped or mixed\n",
    "        idx_slice = pd.IndexSlice\n",
    "\n",
    "        # CONSOLIDATE DEBUG DATA\n",
    "        debug_dict[\"verification\"] = {\"portfolio\": p_slices, \"benchmark\": b_slices}\n",
    "\n",
    "        # 2. ADD RAW COMPONENT EXPORTS\n",
    "        debug_dict[\"portfolio_raw_components\"] = {\n",
    "            \"prices\": self.df_close[tickers_to_trade].loc[safe_start:safe_end],\n",
    "            \"atrp\": self.df_atrp[tickers_to_trade].loc[safe_start:safe_end],\n",
    "            \"trp\": self.df_trp[tickers_to_trade].loc[safe_start:safe_end],\n",
    "            # --- PINPOINT CHANGE: Add Raw OHLCV Slice ---\n",
    "            \"ohlcv_raw\": self.df_ohlcv_raw.loc[\n",
    "                idx_slice[tickers_to_trade, safe_start:safe_end], :\n",
    "            ],\n",
    "        }\n",
    "\n",
    "        debug_dict[\"benchmark_raw_components\"] = {\n",
    "            \"prices\": self.df_close[[inputs.benchmark_ticker]].loc[safe_start:safe_end],\n",
    "            \"atrp\": self.df_atrp[[inputs.benchmark_ticker]].loc[safe_start:safe_end],\n",
    "            \"trp\": self.df_trp[[inputs.benchmark_ticker]].loc[safe_start:safe_end],\n",
    "            # --- PINPOINT CHANGE: Add Raw OHLCV Slice ---\n",
    "            \"ohlcv_raw\": self.df_ohlcv_raw.loc[\n",
    "                idx_slice[[inputs.benchmark_ticker], safe_start:safe_end], :\n",
    "            ],\n",
    "        }\n",
    "\n",
    "        # FINAL OUTPUT\n",
    "        results_table[\"Holding Gain\"] = (p_h_val.iloc[-1] / p_h_val.iloc[0]) - 1\n",
    "\n",
    "        # 1. FINAL CALCULATION / PRE-PACKING\n",
    "        # Merge existing rankings or audits into the dict before sealing the result\n",
    "        debug_dict[\"selection_audit\"] = debug_dict.get(\"full_universe_ranking\")\n",
    "\n",
    "        # 2. CREATE THE OUTPUT OBJECT (The \"Seal\")\n",
    "        res_output = EngineOutput(\n",
    "            portfolio_series=p_f_val,\n",
    "            benchmark_series=b_f_val,\n",
    "            portfolio_atrp_series=p_f_atrp,\n",
    "            benchmark_atrp_series=b_f_atrp,\n",
    "            portfolio_trp_series=p_f_trp,\n",
    "            benchmark_trp_series=b_f_trp,\n",
    "            normalized_plot_data=self._get_normalized_plot_data(\n",
    "                tickers_to_trade, safe_start, safe_end\n",
    "            ),\n",
    "            tickers=tickers_to_trade,\n",
    "            initial_weights=_prepare_initial_weights(tickers_to_trade),\n",
    "            perf_metrics={**p_metrics, **b_metrics},\n",
    "            results_df=results_table,\n",
    "            start_date=safe_start,\n",
    "            decision_date=safe_decision,\n",
    "            buy_date=safe_buy,\n",
    "            holding_end_date=safe_end,\n",
    "            # --- PINPOINT FIX: YOU MUST PASS THE DICT HERE ---\n",
    "            debug_data=debug_dict,\n",
    "        )\n",
    "\n",
    "        return res_output\n",
    "\n",
    "    # ==============================================================================\n",
    "    # INTERNAL LOGIC MODULES\n",
    "    # ==============================================================================\n",
    "\n",
    "    def _validate_timeline(self, inputs: EngineInput):\n",
    "        cal = self.trading_calendar\n",
    "        last_idx = len(cal) - 1\n",
    "\n",
    "        if len(cal) <= inputs.lookback_period:\n",
    "            return (\n",
    "                None,\n",
    "                f\"‚ùå Dataset too small.\\nNeed > {inputs.lookback_period} days of history.\",\n",
    "            )\n",
    "\n",
    "        # 2. Check \"Past\" Constraints (Lookback)\n",
    "        min_decision_date = cal[inputs.lookback_period]\n",
    "        if inputs.start_date < min_decision_date:\n",
    "            # Added \\n here\n",
    "            return None, (\n",
    "                f\"‚ùå Not enough history for a {inputs.lookback_period}-day lookback.\\n\"\n",
    "                f\"Earliest valid Decision Date: {min_decision_date.date()}\"\n",
    "            )\n",
    "\n",
    "        # 3. Check \"Future\" Constraints (Entry T+1 and Holding Period)\n",
    "        required_future_days = 1 + inputs.holding_period\n",
    "        latest_valid_idx = last_idx - required_future_days\n",
    "\n",
    "        if latest_valid_idx < 0:\n",
    "            return (\n",
    "                None,\n",
    "                f\"‚ùå Holding period too long.\\n{inputs.holding_period} days exceeds available data.\",\n",
    "            )\n",
    "\n",
    "        # If user picked a date beyond the available \"future\" runway\n",
    "        if inputs.start_date > cal[latest_valid_idx]:\n",
    "            latest_date = cal[latest_valid_idx].date()\n",
    "            # Added \\n here and shortened the text slightly to fit better\n",
    "            return None, (\n",
    "                f\"‚ùå Decision Date too late for a {inputs.holding_period}-day hold.\\n\"\n",
    "                f\"Latest valid date: {latest_date}. Please move picker back.\"\n",
    "            )\n",
    "\n",
    "        # 4. Map the safe indices\n",
    "        decision_idx = cal.searchsorted(inputs.start_date)\n",
    "        if decision_idx > latest_valid_idx:\n",
    "            decision_idx = latest_valid_idx\n",
    "\n",
    "        start_idx = decision_idx - inputs.lookback_period\n",
    "        entry_idx = decision_idx + 1\n",
    "        end_idx = entry_idx + inputs.holding_period\n",
    "\n",
    "        return (cal[start_idx], cal[decision_idx], cal[entry_idx], cal[end_idx]), None\n",
    "\n",
    "    def _select_tickers(self, inputs: EngineInput, start_date, decision_date):\n",
    "        debug_dict = {}\n",
    "\n",
    "        # --- PATH A: MANUAL LIST ---\n",
    "        if inputs.mode == \"Manual List\":\n",
    "            validation_errors = []\n",
    "            valid_tickers = []\n",
    "            for t in inputs.manual_tickers:\n",
    "                if t not in self.df_close.columns:\n",
    "                    validation_errors.append(f\"‚ùå {t}: Not found.\")\n",
    "                    continue\n",
    "                if pd.isna(self.df_close.at[start_date, t]):\n",
    "                    validation_errors.append(f\"‚ö†Ô∏è {t}: No data on start date.\")\n",
    "                    continue\n",
    "                valid_tickers.append(t)\n",
    "\n",
    "            if validation_errors:\n",
    "                return [], pd.DataFrame(), {}, \"\\n\".join(validation_errors)\n",
    "            if not valid_tickers:\n",
    "                return [], pd.DataFrame(), {}, \"No valid tickers found.\"\n",
    "            return valid_tickers, pd.DataFrame(index=valid_tickers), {}, None\n",
    "\n",
    "        # --- PATH B: RANKING ---\n",
    "        else:\n",
    "            audit_info = {}\n",
    "            eligible_tickers = self._filter_universe(\n",
    "                decision_date, inputs.quality_thresholds, audit_info\n",
    "            )\n",
    "            debug_dict[\"audit_liquidity\"] = audit_info\n",
    "\n",
    "            if not eligible_tickers:\n",
    "                return (\n",
    "                    [],\n",
    "                    pd.DataFrame(),\n",
    "                    debug_dict,\n",
    "                    \"No tickers passed quality filters.\",\n",
    "                )\n",
    "\n",
    "            lookback_close = self.df_close.loc[\n",
    "                start_date:decision_date, eligible_tickers\n",
    "            ]\n",
    "\n",
    "            # 1. Get the Snapshot of Features for the Decision Date\n",
    "            feat_slice_current = self.features_df.xs(\n",
    "                decision_date, level=\"Date\"\n",
    "            ).reindex(eligible_tickers)\n",
    "\n",
    "            # Calculate mean ATRP over the lookback period\n",
    "            idx_product = pd.MultiIndex.from_product(\n",
    "                [eligible_tickers, lookback_close.index], names=[\"Ticker\", \"Date\"]\n",
    "            )\n",
    "            feat_slice_period = self.features_df.reindex(idx_product)\n",
    "            atrp_value_for_obs = (\n",
    "                feat_slice_period[\"ATRP\"].groupby(level=\"Ticker\").mean()\n",
    "            )\n",
    "\n",
    "            # --- PINPOINT CHANGE: Calculate mean TRP over lookback ---\n",
    "            trp_value_for_obs = feat_slice_period[\"TRP\"].groupby(level=\"Ticker\").mean()\n",
    "\n",
    "            # Update the observation dictionary\n",
    "            observation: MarketObservation = {\n",
    "                # ...\n",
    "                \"atrp\": atrp_value_for_obs,\n",
    "                \"trp\": trp_value_for_obs,  # <--- PINPOINT CHANGE: Pass the lookback mean\n",
    "                # ...\n",
    "            }\n",
    "\n",
    "            # 2. Package the Observation (The 'State')\n",
    "            observation: MarketObservation = {\n",
    "                # Time Series Data\n",
    "                \"lookback_close\": lookback_close,\n",
    "                \"lookback_returns\": lookback_close.ffill().pct_change(),\n",
    "                # Snapshot Data (Scalar values for today)\n",
    "                \"atrp\": atrp_value_for_obs,  # <--- USES THE TOGGLED VALUE HERE\n",
    "                \"trp\": trp_value_for_obs,  # <--- PINPOINT CHANGE: Pass the lookback mean\n",
    "                \"rsi\": feat_slice_current[\"RSI\"],\n",
    "                \"rel_strength\": feat_slice_current[\"RelStrength\"],\n",
    "                \"vol_regime\": feat_slice_current[\"VolRegime\"],\n",
    "                \"rvol\": feat_slice_current[\"RVol\"],\n",
    "                \"spy_rvol\": feat_slice_current[\"Spy_RVol\"],\n",
    "                \"obv_score\": feat_slice_current[\"OBV_Score\"],\n",
    "                \"spy_obv_score\": feat_slice_current[\"Spy_OBV_Score\"],\n",
    "                # Momentum Vectors\n",
    "                \"roc_1\": feat_slice_current[\"ROC_1\"],\n",
    "                \"roc_3\": feat_slice_current[\"ROC_3\"],\n",
    "                \"roc_5\": feat_slice_current[\"ROC_5\"],\n",
    "                \"roc_10\": feat_slice_current[\"ROC_10\"],\n",
    "                \"roc_21\": feat_slice_current[\"ROC_21\"],\n",
    "            }\n",
    "\n",
    "            # 3. Run the Strategy (The 'Agent')\n",
    "            if inputs.metric not in METRIC_REGISTRY:\n",
    "                return [], pd.DataFrame(), {}, f\"Strategy '{inputs.metric}' not found.\"\n",
    "\n",
    "            metric_vals = METRIC_REGISTRY[inputs.metric](observation)\n",
    "            sorted_tickers = metric_vals.sort_values(ascending=False)\n",
    "            start_r = max(0, inputs.rank_start - 1)\n",
    "            end_r = inputs.rank_end\n",
    "            selected_tickers = sorted_tickers.iloc[start_r:end_r].index.tolist()\n",
    "\n",
    "            # Audit\n",
    "            debug_dict[\"full_universe_ranking\"] = pd.DataFrame(\n",
    "                {\n",
    "                    \"Strategy_Score\": metric_vals,\n",
    "                    \"Lookback_Return_Ann\": observation[\"lookback_returns\"].mean() * 252,\n",
    "                    \"Lookback_ATRP\": observation[\"atrp\"],\n",
    "                }\n",
    "            )\n",
    "\n",
    "            if not selected_tickers:\n",
    "                return (\n",
    "                    [],\n",
    "                    pd.DataFrame(),\n",
    "                    debug_dict,\n",
    "                    \"No tickers generated from ranking.\",\n",
    "                )\n",
    "\n",
    "            results_table = pd.DataFrame(\n",
    "                {\n",
    "                    \"Rank\": range(\n",
    "                        inputs.rank_start, inputs.rank_start + len(selected_tickers)\n",
    "                    ),\n",
    "                    \"Ticker\": selected_tickers,\n",
    "                    \"Strategy Value\": sorted_tickers.loc[selected_tickers].values,\n",
    "                }\n",
    "            ).set_index(\"Ticker\")\n",
    "\n",
    "            return selected_tickers, results_table, debug_dict, None\n",
    "\n",
    "    def _filter_universe(self, date_ts, thresholds, audit_container=None):\n",
    "        avail_dates = (\n",
    "            self.features_df.index.get_level_values(\"Date\").unique().sort_values()\n",
    "        )\n",
    "        valid_dates = avail_dates[avail_dates <= date_ts]\n",
    "        if valid_dates.empty:\n",
    "            return []\n",
    "        target_date = valid_dates[-1]\n",
    "        day_features = self.features_df.xs(target_date, level=\"Date\")\n",
    "\n",
    "        vol_cutoff = thresholds.get(\"min_median_dollar_volume\", 0)\n",
    "        percentile_used = \"N/A\"\n",
    "        if \"min_liquidity_percentile\" in thresholds:\n",
    "            percentile_used = thresholds[\"min_liquidity_percentile\"]\n",
    "            dynamic_val = day_features[\"RollMedDollarVol\"].quantile(percentile_used)\n",
    "            vol_cutoff = max(vol_cutoff, dynamic_val)\n",
    "\n",
    "        mask = (\n",
    "            (day_features[\"RollMedDollarVol\"] >= vol_cutoff)\n",
    "            & (day_features[\"RollingStalePct\"] <= thresholds[\"max_stale_pct\"])\n",
    "            & (day_features[\"RollingSameVolCount\"] <= thresholds[\"max_same_vol_count\"])\n",
    "        )\n",
    "\n",
    "        if audit_container is not None:\n",
    "            audit_container[\"date\"] = target_date\n",
    "            audit_container[\"total_tickers_available\"] = len(day_features)\n",
    "            audit_container[\"percentile_setting\"] = percentile_used\n",
    "            audit_container[\"final_cutoff_usd\"] = vol_cutoff\n",
    "            audit_container[\"tickers_passed\"] = mask.sum()\n",
    "            snapshot = day_features.copy()\n",
    "            snapshot[\"Calculated_Cutoff\"] = vol_cutoff\n",
    "            snapshot[\"Passed_Vol_Check\"] = snapshot[\"RollMedDollarVol\"] >= vol_cutoff\n",
    "            snapshot[\"Passed_Final\"] = mask\n",
    "            snapshot = snapshot.sort_values(\"RollMedDollarVol\", ascending=False)\n",
    "            audit_container[\"universe_snapshot\"] = snapshot\n",
    "\n",
    "        return day_features[mask].index.tolist()\n",
    "\n",
    "    def _calculate_period_metrics(\n",
    "        self,\n",
    "        f_val,\n",
    "        f_ret,\n",
    "        f_atrp,\n",
    "        f_trp,\n",
    "        decision_date,\n",
    "        h_val,\n",
    "        h_ret,\n",
    "        h_atrp,\n",
    "        h_trp,\n",
    "        prefix,  # <--- Added trp args\n",
    "    ):\n",
    "        metrics = {}\n",
    "        slices = {}\n",
    "\n",
    "        # 1. Temporal Slicing (Routing)\n",
    "        lb_val, lb_ret, lb_atrp, lb_trp = (\n",
    "            f_val.loc[:decision_date],\n",
    "            f_ret.loc[:decision_date],\n",
    "            f_atrp.loc[:decision_date],\n",
    "            f_trp.loc[:decision_date],\n",
    "        )\n",
    "\n",
    "        # Use the new unified QuantUtils calls\n",
    "        metrics[f\"full_{prefix}_gain\"] = QuantUtils.calculate_gain(f_val)\n",
    "        metrics[f\"full_{prefix}_sharpe\"] = QuantUtils.calculate_sharpe(f_ret)\n",
    "        metrics[f\"full_{prefix}_sharpe_atrp\"] = QuantUtils.calculate_sharpe_vol(\n",
    "            f_ret, f_atrp\n",
    "        )\n",
    "        metrics[f\"full_{prefix}_sharpe_trp\"] = QuantUtils.calculate_sharpe_vol(\n",
    "            f_ret, f_trp\n",
    "        )\n",
    "\n",
    "        metrics[f\"lookback_{prefix}_gain\"] = QuantUtils.calculate_gain(lb_val)\n",
    "        metrics[f\"lookback_{prefix}_sharpe\"] = QuantUtils.calculate_sharpe(lb_ret)\n",
    "        metrics[f\"lookback_{prefix}_sharpe_atrp\"] = QuantUtils.calculate_sharpe_vol(\n",
    "            lb_ret, lb_atrp\n",
    "        )\n",
    "        metrics[f\"lookback_{prefix}_sharpe_trp\"] = QuantUtils.calculate_sharpe_vol(\n",
    "            lb_ret, lb_trp\n",
    "        )\n",
    "\n",
    "        metrics[f\"holding_{prefix}_gain\"] = QuantUtils.calculate_gain(h_val)\n",
    "        metrics[f\"holding_{prefix}_sharpe\"] = QuantUtils.calculate_sharpe(h_ret)\n",
    "        metrics[f\"holding_{prefix}_sharpe_atrp\"] = QuantUtils.calculate_sharpe_vol(\n",
    "            h_ret, h_atrp\n",
    "        )\n",
    "        metrics[f\"holding_{prefix}_sharpe_trp\"] = QuantUtils.calculate_sharpe_vol(\n",
    "            h_ret, h_trp\n",
    "        )\n",
    "\n",
    "        # 5. Metadata Collection\n",
    "        (\n",
    "            slices[\"full_val\"],\n",
    "            slices[\"full_ret\"],\n",
    "            slices[\"full_atrp\"],\n",
    "            slices[\"full_trp\"],\n",
    "        ) = (\n",
    "            f_val,\n",
    "            f_ret,\n",
    "            f_atrp,\n",
    "            f_trp,\n",
    "        )\n",
    "        (\n",
    "            slices[\"lookback_val\"],\n",
    "            slices[\"lookback_ret\"],\n",
    "            slices[\"lookback_atrp\"],\n",
    "            slices[\"lookback_trp\"],\n",
    "        ) = (\n",
    "            lb_val,\n",
    "            lb_ret,\n",
    "            lb_atrp,\n",
    "            lb_trp,\n",
    "        )\n",
    "        (\n",
    "            slices[\"holding_val\"],\n",
    "            slices[\"holding_ret\"],\n",
    "            slices[\"holding_atrp\"],\n",
    "            slices[\"holding_trp\"],\n",
    "        ) = (\n",
    "            h_val,\n",
    "            h_ret,\n",
    "            h_atrp,\n",
    "            h_trp,\n",
    "        )\n",
    "\n",
    "        return metrics, slices\n",
    "\n",
    "    def _get_normalized_plot_data(self, tickers, start_date, end_date):\n",
    "        if not tickers:\n",
    "            return pd.DataFrame()\n",
    "        data = self.df_close[list(set(tickers))].loc[start_date:end_date]\n",
    "        if data.empty:\n",
    "            return pd.DataFrame()\n",
    "        return data / data.bfill().iloc[0]\n",
    "\n",
    "    def _error_result(self, msg):\n",
    "        return EngineOutput(\n",
    "            portfolio_series=pd.Series(dtype=float),\n",
    "            benchmark_series=pd.Series(dtype=float),\n",
    "            normalized_plot_data=pd.DataFrame(),\n",
    "            tickers=[],\n",
    "            initial_weights=pd.Series(dtype=float),\n",
    "            perf_metrics={},\n",
    "            results_df=pd.DataFrame(),\n",
    "            start_date=pd.Timestamp.min,\n",
    "            decision_date=pd.Timestamp.min,\n",
    "            buy_date=pd.Timestamp.min,\n",
    "            holding_end_date=pd.Timestamp.min,\n",
    "            error_msg=msg,\n",
    "        )\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION E: THE UI (Visualization)\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "def plot_walk_forward_analyzer(\n",
    "    df_ohlcv,\n",
    "    features_df=None,\n",
    "    df_close_wide=None,\n",
    "    df_atrp_wide=None,\n",
    "    df_trp_wide=None,\n",
    "    default_start_date=\"2025-01-17\",\n",
    "    default_lookback=10,\n",
    "    default_holding=5,\n",
    "    default_strategy=\"Sharpe (ATRP)\",\n",
    "    default_rank_start=1,\n",
    "    default_rank_end=10,\n",
    "    default_benchmark_ticker=GLOBAL_SETTINGS[\"benchmark_ticker\"],\n",
    "    master_calendar_ticker=GLOBAL_SETTINGS[\"calendar_ticker\"],\n",
    "    quality_thresholds=GLOBAL_SETTINGS[\"thresholds\"],\n",
    "    debug=False,\n",
    "):\n",
    "\n",
    "    engine = AlphaEngine(\n",
    "        df_ohlcv,\n",
    "        features_df=features_df,\n",
    "        df_close_wide=df_close_wide,\n",
    "        df_atrp_wide=df_atrp_wide,\n",
    "        df_trp_wide=df_trp_wide,  # <--- Update your class to accept this\n",
    "        master_ticker=master_calendar_ticker,\n",
    "    )\n",
    "\n",
    "    # Initialize containers\n",
    "    audit_pack = [None]  # Unified container\n",
    "\n",
    "    # If no thresholds passed, use the global Source of Truth\n",
    "    if quality_thresholds is None:\n",
    "        quality_thresholds = GLOBAL_SETTINGS[\"thresholds\"]\n",
    "\n",
    "    # --- Widgets ---\n",
    "    mode_selector = widgets.RadioButtons(\n",
    "        options=[\"Ranking\", \"Manual List\"],\n",
    "        value=\"Ranking\",\n",
    "        description=\"Mode:\",\n",
    "        layout={\"width\": \"max-content\"},\n",
    "        style={\"description_width\": \"initial\"},\n",
    "    )\n",
    "    lookback_input = widgets.IntText(\n",
    "        value=default_lookback,\n",
    "        description=\"Lookback (Days):\",\n",
    "        layout={\"width\": \"200px\"},\n",
    "        style={\"description_width\": \"initial\"},\n",
    "    )\n",
    "    decision_date_picker = widgets.DatePicker(\n",
    "        description=\"Decision Date:\",\n",
    "        value=pd.to_datetime(default_start_date),\n",
    "        layout={\"width\": \"auto\"},\n",
    "        style={\"description_width\": \"initial\"},\n",
    "    )\n",
    "    holding_input = widgets.IntText(\n",
    "        value=default_holding,\n",
    "        description=\"Holding (Days):\",\n",
    "        layout={\"width\": \"200px\"},\n",
    "        style={\"description_width\": \"initial\"},\n",
    "    )\n",
    "    strategy_dropdown = widgets.Dropdown(\n",
    "        options=list(METRIC_REGISTRY.keys()),\n",
    "        value=default_strategy,\n",
    "        description=\"Strategy:\",\n",
    "        layout={\"width\": \"220px\"},\n",
    "        style={\"description_width\": \"initial\"},\n",
    "    )\n",
    "    benchmark_input = widgets.Text(\n",
    "        value=default_benchmark_ticker,\n",
    "        description=\"Benchmark:\",\n",
    "        placeholder=\"Enter Ticker\",\n",
    "        layout={\"width\": \"180px\"},\n",
    "        style={\"description_width\": \"initial\"},\n",
    "    )\n",
    "    rank_start_input = widgets.IntText(\n",
    "        value=default_rank_start,\n",
    "        description=\"Rank Start:\",\n",
    "        layout={\"width\": \"150px\"},\n",
    "        style={\"description_width\": \"initial\"},\n",
    "    )\n",
    "    rank_end_input = widgets.IntText(\n",
    "        value=default_rank_end,\n",
    "        description=\"Rank End:\",\n",
    "        layout={\"width\": \"150px\"},\n",
    "        style={\"description_width\": \"initial\"},\n",
    "    )\n",
    "    manual_tickers_input = widgets.Textarea(\n",
    "        value=\"\",\n",
    "        placeholder=\"Enter tickers...\",\n",
    "        description=\"Manual Tickers:\",\n",
    "        layout={\"width\": \"400px\", \"height\": \"80px\"},\n",
    "        style={\"description_width\": \"initial\"},\n",
    "    )\n",
    "    update_button = widgets.Button(description=\"Run Simulation\", button_style=\"primary\")\n",
    "    ticker_list_output = widgets.Output()\n",
    "\n",
    "    # --- Layouts ---\n",
    "    timeline_box = widgets.HBox(\n",
    "        [lookback_input, decision_date_picker, holding_input],\n",
    "        layout=widgets.Layout(\n",
    "            justify_content=\"space-between\",\n",
    "            border=\"1px solid #ddd\",\n",
    "            padding=\"10px\",\n",
    "            margin=\"5px\",\n",
    "        ),\n",
    "    )\n",
    "    strategy_box = widgets.HBox([strategy_dropdown, benchmark_input])\n",
    "    ranking_box = widgets.HBox([rank_start_input, rank_end_input])\n",
    "\n",
    "    def on_mode_change(c):\n",
    "        ranking_box.layout.display = \"flex\" if c[\"new\"] == \"Ranking\" else \"none\"\n",
    "        manual_tickers_input.layout.display = (\n",
    "            \"none\" if c[\"new\"] == \"Ranking\" else \"flex\"\n",
    "        )\n",
    "        strategy_dropdown.disabled = c[\"new\"] == \"Manual List\"\n",
    "\n",
    "    mode_selector.observe(on_mode_change, names=\"value\")\n",
    "    on_mode_change({\"new\": mode_selector.value})\n",
    "\n",
    "    ui = widgets.VBox(\n",
    "        [\n",
    "            widgets.HTML(\n",
    "                \"<b>1. Timeline Configuration:</b> (Past <--- Decision ---> Future)\"\n",
    "            ),\n",
    "            timeline_box,\n",
    "            widgets.HTML(\"<b>2. Strategy Settings:</b>\"),\n",
    "            widgets.HBox([mode_selector, strategy_box]),\n",
    "            ranking_box,\n",
    "            manual_tickers_input,\n",
    "            widgets.HTML(\"<hr>\"),\n",
    "            update_button,\n",
    "            ticker_list_output,\n",
    "        ],\n",
    "        layout=widgets.Layout(margin=\"10px 0 20px 0\"),\n",
    "    )\n",
    "\n",
    "    fig = go.FigureWidget()\n",
    "    fig.update_layout(\n",
    "        title=\"Event-Driven Walk-Forward Analysis\",\n",
    "        height=600,\n",
    "        template=\"plotly_white\",\n",
    "        hovermode=\"x unified\",\n",
    "    )\n",
    "    for i in range(50):\n",
    "        fig.add_trace(go.Scatter(visible=False, line=dict(width=2)))\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            name=\"Benchmark\",\n",
    "            visible=True,\n",
    "            line=dict(color=\"black\", width=3, dash=\"dash\"),\n",
    "        )\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            name=\"Group Portfolio\", visible=True, line=dict(color=\"green\", width=3)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # --- Update Logic ---\n",
    "    def update_plot(b):\n",
    "        ticker_list_output.clear_output()\n",
    "        manual_list = [\n",
    "            t.strip().upper()\n",
    "            for t in manual_tickers_input.value.split(\",\")\n",
    "            if t.strip()\n",
    "        ]\n",
    "        decision_date_raw = pd.to_datetime(decision_date_picker.value)\n",
    "\n",
    "        inputs = EngineInput(\n",
    "            mode=mode_selector.value,\n",
    "            start_date=decision_date_raw,\n",
    "            lookback_period=lookback_input.value,\n",
    "            holding_period=holding_input.value,\n",
    "            metric=strategy_dropdown.value,\n",
    "            benchmark_ticker=benchmark_input.value.strip().upper(),\n",
    "            rank_start=rank_start_input.value,\n",
    "            rank_end=rank_end_input.value,\n",
    "            quality_thresholds=quality_thresholds,\n",
    "            manual_tickers=manual_list,\n",
    "            debug=debug,\n",
    "        )\n",
    "\n",
    "        # --- CAPTURE INPUTS FOR AUDIT ---\n",
    "        with ticker_list_output:\n",
    "            res = engine.run(inputs)\n",
    "            audit_pack[0] = {\"inputs\": inputs, \"results\": res, \"debug\": res.debug_data}\n",
    "\n",
    "            if res.error_msg:\n",
    "                print(f\"‚ö†Ô∏è Simulation Stopped: {res.error_msg}\")\n",
    "                return\n",
    "\n",
    "            # Plotting\n",
    "            with fig.batch_update():\n",
    "                cols = res.normalized_plot_data.columns.tolist()\n",
    "                for i in range(50):\n",
    "                    if i < len(cols):\n",
    "                        fig.data[i].update(\n",
    "                            x=res.normalized_plot_data.index,\n",
    "                            y=res.normalized_plot_data[cols[i]],\n",
    "                            name=cols[i],\n",
    "                            visible=True,\n",
    "                        )\n",
    "                    else:\n",
    "                        fig.data[i].visible = False\n",
    "\n",
    "                fig.data[50].update(\n",
    "                    x=res.benchmark_series.index,\n",
    "                    y=res.benchmark_series.values,\n",
    "                    name=f\"Benchmark ({inputs.benchmark_ticker})\",\n",
    "                    visible=not res.benchmark_series.empty,\n",
    "                )\n",
    "                fig.data[51].update(\n",
    "                    x=res.portfolio_series.index,\n",
    "                    y=res.portfolio_series.values,\n",
    "                    visible=True,\n",
    "                )\n",
    "\n",
    "                # Visual Lines\n",
    "                fig.layout.shapes = [\n",
    "                    dict(\n",
    "                        type=\"line\",\n",
    "                        x0=res.decision_date,\n",
    "                        y0=0,\n",
    "                        x1=res.decision_date,\n",
    "                        y1=1,\n",
    "                        xref=\"x\",\n",
    "                        yref=\"paper\",\n",
    "                        line=dict(color=\"red\", width=2, dash=\"dash\"),\n",
    "                    ),\n",
    "                    dict(\n",
    "                        type=\"line\",\n",
    "                        x0=res.buy_date,\n",
    "                        y0=0,\n",
    "                        x1=res.buy_date,\n",
    "                        y1=1,\n",
    "                        xref=\"x\",\n",
    "                        yref=\"paper\",\n",
    "                        line=dict(color=\"blue\", width=2, dash=\"dot\"),\n",
    "                    ),\n",
    "                ]\n",
    "\n",
    "                fig.layout.annotations = [\n",
    "                    dict(\n",
    "                        x=res.decision_date,\n",
    "                        y=0.05,\n",
    "                        xref=\"x\",\n",
    "                        yref=\"paper\",\n",
    "                        text=\"DECISION\",\n",
    "                        showarrow=False,\n",
    "                        bgcolor=\"red\",\n",
    "                        font=dict(color=\"white\"),\n",
    "                    ),\n",
    "                    dict(\n",
    "                        x=res.buy_date,\n",
    "                        y=1.0,\n",
    "                        xref=\"x\",\n",
    "                        yref=\"paper\",\n",
    "                        text=\"ENTRY (T+1)\",\n",
    "                        showarrow=False,\n",
    "                        bgcolor=\"blue\",\n",
    "                        font=dict(color=\"white\"),\n",
    "                    ),\n",
    "                ]\n",
    "\n",
    "            start_date = res.start_date.date()\n",
    "            act_date = res.decision_date.date()\n",
    "            entry_date = res.buy_date.date()\n",
    "\n",
    "            # Liquidity Audit Print\n",
    "            if (\n",
    "                inputs.mode == \"Ranking\"\n",
    "                and res.debug_data\n",
    "                and \"audit_liquidity\" in res.debug_data\n",
    "            ):\n",
    "                audit = res.debug_data[\"audit_liquidity\"]\n",
    "                if audit:\n",
    "                    raw_percentile = audit.get(\"percentile_setting\", 0)\n",
    "                    keep_pct = (\n",
    "                        1 - raw_percentile\n",
    "                    ) * 100  # Calculates the actual portion kept\n",
    "                    cut_val = audit.get(\"final_cutoff_usd\", 0)\n",
    "\n",
    "                    print(\"-\" * 60)\n",
    "                    print(f\"üîç LIQUIDITY CHECK (On Decision Date: {act_date})\")\n",
    "                    print(\n",
    "                        f\"   Universe Size: {audit.get('total_tickers_available')} tickers\"\n",
    "                    )\n",
    "                    print(\n",
    "                        f\"   Liquidity Threshold: {raw_percentile*100:.0f}th Percentile\"\n",
    "                    )\n",
    "                    print(f\"   Action: Keeping the Top {keep_pct:.0f}% of Market\")\n",
    "                    print(f\"   Calculated Cutoff: ${cut_val:,.0f} / day\")\n",
    "                    print(f\"   Tickers Remaining: {audit.get('tickers_passed')}\")\n",
    "                    print(\"-\" * 60)\n",
    "\n",
    "            # --- UPDATED TIMELINE PRINT ---\n",
    "            print(\n",
    "                f\"Timeline: Start [ {start_date} ] --> Decision [ {act_date} ] --> Cash (1d) --> Entry [ {entry_date} ] --> End [ {res.holding_end_date.date()} ]\"\n",
    "            )\n",
    "\n",
    "            if inputs.mode == \"Ranking\":\n",
    "                print(f\"Ranked Tickers ({len(res.tickers)}):\")\n",
    "                for i in range(0, len(res.tickers), 10):\n",
    "                    print(\", \".join(res.tickers[i : i + 10]))\n",
    "            else:\n",
    "                print(\"Manual Portfolio Tickers:\")\n",
    "                for i in range(0, len(res.tickers), 10):\n",
    "                    print(\", \".join(res.tickers[i : i + 10]))\n",
    "\n",
    "            m = res.perf_metrics\n",
    "\n",
    "            # --- DRY UI GENERATION ---\n",
    "            # 1. Define the metrics we want to display\n",
    "            metrics_to_show = [\n",
    "                (\"Gain\", \"gain\"),\n",
    "                (\"Sharpe\", \"sharpe\"),\n",
    "                (\"Sharpe (ATRP)\", \"sharpe_atrp\"),\n",
    "                (\"Sharpe (TRP)\", \"sharpe_trp\"),  # <--- PINPOINT CHANGE: Add this line\n",
    "            ]\n",
    "\n",
    "            rows = []\n",
    "            for label, key in metrics_to_show:\n",
    "                p_row = {\n",
    "                    \"Metric\": f\"Group {label}\",\n",
    "                    \"Full\": m.get(f\"full_p_{key}\"),\n",
    "                    \"Lookback\": m.get(f\"lookback_p_{key}\"),\n",
    "                    \"Holding\": m.get(f\"holding_p_{key}\"),\n",
    "                }\n",
    "                b_row = {\n",
    "                    \"Metric\": f\"Benchmark {label}\",\n",
    "                    \"Full\": m.get(f\"full_b_{key}\"),\n",
    "                    \"Lookback\": m.get(f\"lookback_b_{key}\"),\n",
    "                    \"Holding\": m.get(f\"holding_b_{key}\"),\n",
    "                }\n",
    "\n",
    "                # Delta calculation\n",
    "                d_row = {\"Metric\": f\"== {label} Delta\"}\n",
    "                for col in [\"Full\", \"Lookback\", \"Holding\"]:\n",
    "                    d_row[col] = (p_row[col] or 0) - (b_row[col] or 0)\n",
    "\n",
    "                rows.extend([p_row, b_row, d_row])\n",
    "\n",
    "            df_report = pd.DataFrame(rows).set_index(\"Metric\")\n",
    "\n",
    "            # --- 2. STYLING (The \"Senior\" Design) ---\n",
    "            # --- 1. PREP DATA (Flattening the Index) ---\n",
    "            # We convert the index to a column so \"Metric\" sits on the same row as other headers\n",
    "            df_report = pd.DataFrame(rows)\n",
    "            df_report = df_report.set_index(\"Metric\")\n",
    "\n",
    "            # --- 2. THE STYLING (Sleek & Proportional) ---\n",
    "            def apply_sleek_style(styler):\n",
    "                # Match notebook font size (usually 13px)\n",
    "                styler.format(\"{:+.4f}\", na_rep=\"N/A\")\n",
    "\n",
    "                # Dynamic Row Highlighting\n",
    "                def row_logic(row):\n",
    "                    if \"Delta\" in row.name:\n",
    "                        return [\n",
    "                            \"background-color: #f9f9f9; font-weight: 600; border-top: 1px solid #ddd\"\n",
    "                        ] * len(row)\n",
    "                    if \"Group\" in row.name:\n",
    "                        return [\"color: #2c5e8f; background-color: #fcfdfe\"] * len(row)\n",
    "                    return [\"color: #555\"] * len(\n",
    "                        row\n",
    "                    )  # Benchmark rows are slightly muted\n",
    "\n",
    "                styler.apply(row_logic, axis=1)\n",
    "\n",
    "                styler.set_table_styles(\n",
    "                    [\n",
    "                        # Base Table Font - Scaling down to match standard text\n",
    "                        {\n",
    "                            \"selector\": \"\",\n",
    "                            \"props\": [\n",
    "                                (\"font-family\", \"inherit\"),\n",
    "                                (\"font-size\", \"12px\"),\n",
    "                                (\"border-collapse\", \"collapse\"),\n",
    "                                (\"width\", \"auto\"),\n",
    "                                (\"margin-left\", \"0\"),\n",
    "                            ],\n",
    "                        },\n",
    "                        # Header Row - Flattened and Muted\n",
    "                        {\n",
    "                            \"selector\": \"th\",\n",
    "                            \"props\": [\n",
    "                                (\"background-color\", \"white\"),\n",
    "                                (\"color\", \"#222\"),\n",
    "                                (\"font-weight\", \"600\"),\n",
    "                                (\"padding\", \"6px 12px\"),\n",
    "                                (\"border-bottom\", \"2px solid #444\"),\n",
    "                                (\"text-align\", \"center\"),\n",
    "                                (\n",
    "                                    \"vertical-align\",\n",
    "                                    \"bottom\",\n",
    "                                ),  # Aligns 'Metric' with others\n",
    "                            ],\n",
    "                        },\n",
    "                        # Index Column (The \"Metric\" labels)\n",
    "                        {\n",
    "                            \"selector\": \"th.row_heading\",\n",
    "                            \"props\": [\n",
    "                                (\"text-align\", \"left\"),\n",
    "                                (\"padding-right\", \"30px\"),\n",
    "                                (\"border-bottom\", \"1px solid #eee\"),\n",
    "                            ],\n",
    "                        },\n",
    "                        # Cell Data - Tighter padding\n",
    "                        {\n",
    "                            \"selector\": \"td\",\n",
    "                            \"props\": [\n",
    "                                (\"padding\", \"4px 12px\"),\n",
    "                                (\"border-bottom\", \"1px solid #eee\"),\n",
    "                            ],\n",
    "                        },\n",
    "                        # Remove the extra \"Index Name\" row completely\n",
    "                        {\n",
    "                            \"selector\": \"thead tr:nth-child(1) th\",\n",
    "                            \"props\": [(\"display\", \"table-cell\")],\n",
    "                        },\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "                # Hack to fix the 'Metric' alignment:\n",
    "                # We remove the index name and set it as the horizontal label for the index\n",
    "                styler.index.name = None\n",
    "\n",
    "                return styler\n",
    "\n",
    "            display(apply_sleek_style(df_report.style))\n",
    "\n",
    "    update_button.on_click(update_plot)\n",
    "    update_plot(None)\n",
    "    display(ui, fig)\n",
    "    return audit_pack  # <--- Return ONLY ONE\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION F: INSPECTION TOOLS\n",
    "# ==============================================================================\n",
    "\n",
    "from dataclasses import is_dataclass, fields\n",
    "import pandas as pd\n",
    "\n",
    "_AUDIT_REGISTRY = {}\n",
    "_SEEN_MEM_IDS = {}\n",
    "\n",
    "\n",
    "def inspect_audit(item, name=\"audit_pack\", indent=0, reset=True, path=\"\"):\n",
    "    \"\"\"\n",
    "    Categorized audit inspector with Breadcrumb Path tracking.\n",
    "    \"\"\"\n",
    "    global _AUDIT_REGISTRY, _SEEN_MEM_IDS\n",
    "\n",
    "    if reset and indent == 0:\n",
    "        _AUDIT_REGISTRY = {}\n",
    "        _SEEN_MEM_IDS = {}\n",
    "        path = name\n",
    "        print(f\"\\n{'='*65}\\nüîç HIGH-TRANSPARENCY AUDIT MAP\\n{'='*65}\")\n",
    "\n",
    "    mem_id = id(item)\n",
    "\n",
    "    # 1. Deduplication Guard\n",
    "    if mem_id in _SEEN_MEM_IDS and isinstance(\n",
    "        item, (dict, list, pd.DataFrame, pd.Series)\n",
    "    ):\n",
    "        prev_idx = _SEEN_MEM_IDS[mem_id]\n",
    "        print(f\"      {'  '*indent}‚ï∞‚îÄ‚îÄ {name} --> [See ID {prev_idx}]\")\n",
    "        return\n",
    "\n",
    "    # 2. Register with Breadcrumbs\n",
    "    idx = len(_AUDIT_REGISTRY)\n",
    "    current_path = f\"{path} -> {name}\" if indent > 0 else name\n",
    "    _AUDIT_REGISTRY[idx] = {\"data\": item, \"path\": current_path}\n",
    "    _SEEN_MEM_IDS[mem_id] = idx\n",
    "\n",
    "    # 3. Visual Headers for Major Sections\n",
    "    if indent == 1:\n",
    "        headers = {\n",
    "            \"inputs\": \"‚öôÔ∏è SYSTEM CONFIGURATION & THRESHOLDS\",\n",
    "            \"results\": \"üìà PERFORMANCE & EQUITY CURVES\",\n",
    "            \"debug\": \"üîç AUDIT TRAIL & VERIFICATION MATRICES\",\n",
    "        }\n",
    "        if name in headers:\n",
    "            print(f\"\\n{headers[name]}\\n{'-'*45}\")\n",
    "\n",
    "    # 4. Icon & Metadata Resolution\n",
    "    icon = \"üìÇ\"\n",
    "    if isinstance(item, pd.DataFrame):\n",
    "        icon = \"üßÆ\"\n",
    "    elif isinstance(item, pd.Series):\n",
    "        icon = \"üìà\"\n",
    "    elif isinstance(item, (pd.Timestamp, datetime, date)):\n",
    "        icon = \"üìÖ\"\n",
    "    elif isinstance(item, (int, float)):\n",
    "        icon = \"üî¢\"\n",
    "    elif is_dataclass(item):\n",
    "        icon = \"üì¶\"\n",
    "\n",
    "    # 5. Print Tree Line\n",
    "    spacing = \"  \" * indent\n",
    "    shape = f\" shape={item.shape}\" if hasattr(item, \"shape\") else \"\"\n",
    "\n",
    "    # Handle Simple Lists\n",
    "    if isinstance(item, list) and all(isinstance(x, (str, int, float)) for x in item):\n",
    "        content = (\n",
    "            str(item) if len(str(item)) < 50 else f\"{item[:3]}... [+ {len(item)-3}]\"\n",
    "        )\n",
    "        print(f\"[{idx: >3}] {spacing}{icon} {name}: {content}\")\n",
    "        return\n",
    "\n",
    "    print(f\"[{idx: >3}] {spacing}{icon} {name} ({type(item).__name__}{shape})\")\n",
    "\n",
    "    # 6. Recursion\n",
    "    if isinstance(item, list):\n",
    "        for i, val in enumerate(item):\n",
    "            inspect_audit(\n",
    "                val,\n",
    "                name=f\"index_{i}\",\n",
    "                indent=indent + 1,\n",
    "                reset=False,\n",
    "                path=current_path,\n",
    "            )\n",
    "    elif isinstance(item, dict):\n",
    "        for k in sorted(item.keys()):\n",
    "            inspect_audit(\n",
    "                item[k], name=k, indent=indent + 1, reset=False, path=current_path\n",
    "            )\n",
    "    elif is_dataclass(item):\n",
    "        for field in fields(item):\n",
    "            inspect_audit(\n",
    "                getattr(item, field.name),\n",
    "                name=field.name,\n",
    "                indent=indent + 1,\n",
    "                reset=False,\n",
    "                path=current_path,\n",
    "            )\n",
    "\n",
    "\n",
    "def get_audit(idx: int):\n",
    "    \"\"\"\n",
    "    Fetches an object with a 'Smart Header' breadcrumb trail.\n",
    "    \"\"\"\n",
    "    entry = _AUDIT_REGISTRY.get(idx)\n",
    "    if not entry:\n",
    "        print(f\"‚ùå ID [{idx}] not found.\")\n",
    "        return None\n",
    "\n",
    "    obj = entry[\"data\"]\n",
    "    path = entry[\"path\"]\n",
    "\n",
    "    # Generate the Smart Header\n",
    "    type_name = type(obj).__name__\n",
    "    meta = f\"Shape: {obj.shape}\" if hasattr(obj, \"shape\") else f\"Value: {obj}\"\n",
    "\n",
    "    print(f\"\\nüìå FETCHING ID [{idx}]: {path}\")\n",
    "    print(f\"   [Type: {type_name} | {meta}]\")\n",
    "    print(\"-\" * 75)\n",
    "\n",
    "    return obj\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# INTEGRITY PROTECTION: THE TRIPWIRE\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "def verify_math_integrity():\n",
    "    \"\"\"\n",
    "    üõ°Ô∏è TRIPWIRE: Ensures Sample Boundary Integrity.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- üõ°Ô∏è Starting Final Integrity Audit ---\")\n",
    "\n",
    "    try:\n",
    "        # Test 1: Series Input\n",
    "        mock_series = pd.Series([100.0, 102.0, 101.0])\n",
    "        rets_s = QuantUtils.compute_returns(mock_series)\n",
    "        # Verify first value is actually NaN\n",
    "        if not pd.isna(rets_s.iloc[0]):\n",
    "            raise ValueError(\"Series Leading NaN missing\")\n",
    "        print(\"‚úÖ Series Boundary: OK\")\n",
    "\n",
    "        # Test 2: DataFrame Input\n",
    "        mock_df = pd.DataFrame({\"A\": [100, 101], \"B\": [200, 202]})\n",
    "        rets_df = QuantUtils.compute_returns(mock_df)\n",
    "        if not rets_df.iloc[0].isna().all():\n",
    "            raise ValueError(\"DataFrame Leading NaN missing\")\n",
    "        print(\"‚úÖ DataFrame Boundary: OK\")\n",
    "\n",
    "        print(\"‚úÖ AUDIT PASSED: Mathematical boundaries are strictly enforced.\")\n",
    "    except Exception as e:\n",
    "        print(f\"üî• SYSTEM BREACH: {str(e)}\")\n",
    "        raise e\n",
    "\n",
    "\n",
    "def verify_feature_engineering_integrity():\n",
    "    \"\"\"\n",
    "    üõ°Ô∏è TRIPWIRE: Validates Feature Engineering Logic.\n",
    "    Enforces:\n",
    "    1. Day 1 ATR must be NaN (No PrevClose).\n",
    "    2. Wilder's Smoothing must use Alpha = 1/Period.\n",
    "    3. Recursion must match manual calculation.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- üõ°Ô∏è Starting Feature Engineering Audit ---\")\n",
    "\n",
    "    # 1. Create Synthetic Data (3 Days)\n",
    "    # Day 1: High-Low = 10. No PrevClose.\n",
    "    # Day 2: High-Low = 20. Gap up implies TR might be larger.\n",
    "    # Day 3: High-Low = 10.\n",
    "    dates = pd.to_datetime([\"2020-01-01\", \"2020-01-02\", \"2020-01-03\"])\n",
    "    idx = pd.MultiIndex.from_product([[\"TEST\"], dates], names=[\"Ticker\", \"Date\"])\n",
    "\n",
    "    df_mock = pd.DataFrame(\n",
    "        {\n",
    "            \"Adj Open\": [100, 110, 110],\n",
    "            \"Adj High\": [110, 130, 120],\n",
    "            \"Adj Low\": [100, 110, 110],\n",
    "            \"Adj Close\": [105, 120, 115],  # PrevClose: NaN, 105, 120\n",
    "            \"Volume\": [1000, 1000, 1000],\n",
    "        },\n",
    "        index=idx,\n",
    "    )\n",
    "\n",
    "    # 2. Run the Generator\n",
    "    # We use Period=2 to make manual math easy (Alpha = 1/2 = 0.5)\n",
    "    feats = generate_features(\n",
    "        df_mock, atr_period=2, rsi_period=2, quality_min_periods=1\n",
    "    )\n",
    "    atr_series = feats[\"ATR\"]\n",
    "\n",
    "    # 3. MANUAL CALCULATION (The \"Truth\")\n",
    "    # Day 1:\n",
    "    #   TR = Max(H-L, |H-PC|, |L-PC|)\n",
    "    #   TR = Max(10, NaN, NaN) -> NaN (Because skipna=False)\n",
    "    #   Expected ATR: NaN\n",
    "\n",
    "    # Day 2:\n",
    "    #   PrevClose = 105\n",
    "    #   H-L=20, |130-105|=25, |110-105|=5\n",
    "    #   TR = 25\n",
    "    #   Expected ATR: First valid observation = 25.0\n",
    "\n",
    "    # Day 3:\n",
    "    #   PrevClose = 120\n",
    "    #   H-L=10, |120-120|=0, |110-120|=10\n",
    "    #   TR = 10\n",
    "    #   Wilder's Smoothing (Alpha=0.5):\n",
    "    #   ATR_3 = (TR_3 * alpha) + (ATR_2 * (1-alpha))\n",
    "    #   ATR_3 = (10 * 0.5) + (25 * 0.5) = 5 + 12.5 = 17.5\n",
    "\n",
    "    print(f\"Audit Values:\\n{atr_series.values}\")\n",
    "\n",
    "    # 4. ASSERTIONS\n",
    "    try:\n",
    "        # Check Day 1\n",
    "        if not np.isnan(atr_series.iloc[0]):\n",
    "            raise AssertionError(\n",
    "                f\"Day 1 Regression: Expected NaN, got {atr_series.iloc[0]}. (Check skipna=False)\"\n",
    "            )\n",
    "\n",
    "        # Check Day 2 (Initialization)\n",
    "        if not np.isclose(atr_series.iloc[1], 25.0):\n",
    "            raise AssertionError(\n",
    "                f\"Initialization Regression: Expected 25.0, got {atr_series.iloc[1]}.\"\n",
    "            )\n",
    "\n",
    "        # Check Day 3 (Recursion)\n",
    "        if not np.isclose(atr_series.iloc[2], 17.5):\n",
    "            raise AssertionError(\n",
    "                f\"Wilder's Logic Regression: Expected 17.5, got {atr_series.iloc[2]}. (Check Alpha=1/N)\"\n",
    "            )\n",
    "\n",
    "        print(\"‚úÖ FEATURE INTEGRITY PASSED: Wilder's ATR logic is strictly enforced.\")\n",
    "\n",
    "    except AssertionError as e:\n",
    "        print(f\"üî• LOGIC FAILURE: {str(e)}\")\n",
    "        raise e\n",
    "\n",
    "\n",
    "def verify_ranking_integrity():\n",
    "    \"\"\"\n",
    "    üõ°Ô∏è TRIPWIRE: Prevents 'Momentum Collapse' in Volatility-Adjusted Ranking.\n",
    "    Ensures that Sharpe(Vol) distinguishes between High-Vol and Low-Vol stocks.\n",
    "    \"\"\"\n",
    "    print(\"--- üõ°Ô∏è Starting Ranking Kernel Audit ---\")\n",
    "\n",
    "    # 1. Setup Mock Universe (2 Tickers, 2 Days)\n",
    "    # Ticker 'VOLATILE': 10% return, but 10% Volatility\n",
    "    # Ticker 'STABLE': 2% return, but 1% Volatility (The 'Sharpe' Winner)\n",
    "    data = {\"VOLATILE\": [1.0, 1.10], \"STABLE\": [1.0, 1.02]}  # +10%  # +2%\n",
    "    df_returns = pd.DataFrame(data).pct_change().dropna()\n",
    "\n",
    "    # Pre-calculated Mean Volatility per ticker (as provided by Engine Observation)\n",
    "    vol_series = pd.Series({\"VOLATILE\": 0.10, \"STABLE\": 0.01})\n",
    "\n",
    "    # 2. Run Kernel\n",
    "    results = QuantUtils.calculate_sharpe_vol(df_returns, vol_series)\n",
    "\n",
    "    # 3. CALCULATE EXPECTED (Pure Math)\n",
    "    # Volatile Sharpe: 0.10 / 0.10 = 1.0\n",
    "    # Stable Sharpe:   0.02 / 0.01 = 2.0\n",
    "\n",
    "    try:\n",
    "        # Check A: Diversity. If they are the same, normalization didn't happen.\n",
    "        if np.isclose(results[\"VOLATILE\"], results[\"STABLE\"]):\n",
    "            raise AssertionError(\n",
    "                \"RANKING COLLAPSE: Both tickers have the same normalized score.\"\n",
    "            )\n",
    "\n",
    "        # Check B: Direction. STABLE must rank higher than VOLATILE.\n",
    "        if results[\"STABLE\"] < results[\"VOLATILE\"]:\n",
    "            # This is exactly what happens when the bug turns it into Momentum\n",
    "            raise AssertionError(\n",
    "                f\"MOMENTUM REGRESSION: 'STABLE' ({results['STABLE']:.2f}) \"\n",
    "                f\"ranked below 'VOLATILE' ({results['VOLATILE']:.2f}). \"\n",
    "                \"The denominator was likely collapsed to a market average.\"\n",
    "            )\n",
    "\n",
    "        # Check C: Absolute Precision\n",
    "        if not np.isclose(results[\"STABLE\"], 2.0):\n",
    "            raise AssertionError(\n",
    "                f\"MATH ERROR: Expected 2.0 for STABLE, got {results['STABLE']}\"\n",
    "            )\n",
    "\n",
    "        print(\n",
    "            \"‚úÖ RANKING INTEGRITY PASSED: Volatility normalization is strictly enforced.\"\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"üî• KERNEL BREACH: {str(e)}\")\n",
    "        raise e\n",
    "\n",
    "\n",
    "def verify_vol_alignment_integrity():\n",
    "    \"\"\"\n",
    "    üõ°Ô∏è TRIPWIRE: Verifies Temporal Coupling between Returns and Volatility.\n",
    "    Ensures that the volatility average is only calculated over days where a return exists.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- üõ°Ô∏è Starting Volatility Alignment Audit ---\")\n",
    "\n",
    "    # 1. SETUP SYNTHETIC DATA (2 Days)\n",
    "    # Day 1: Return = NaN, Vol = 0.90 (Extreme 'Trap' Volatility)\n",
    "    # Day 2: Return = 0.10, Vol = 0.10 (Target Reward/Risk)\n",
    "    rets_s = pd.Series([np.nan, 0.10])\n",
    "    vol_s = pd.Series([0.90, 0.10])\n",
    "\n",
    "    # 2. RUN KERNEL (Series Mode)\n",
    "    # Calculation Logic:\n",
    "    # If aligned: 0.10 / 0.10 = 1.0\n",
    "    # If misaligned: 0.10 / mean(0.90, 0.10) = 0.10 / 0.50 = 0.2\n",
    "    res_series = QuantUtils.calculate_sharpe_vol(rets_s, vol_s)\n",
    "\n",
    "    # 3. RUN KERNEL (DataFrame Mode)\n",
    "    # Ensures vectorized alignment works across columns\n",
    "    rets_df = pd.DataFrame({\"A\": [np.nan, 0.10], \"B\": [np.nan, 0.20]})\n",
    "    vol_df = pd.DataFrame({\"A\": [0.90, 0.10], \"B\": [0.05, 0.20]})\n",
    "    res_df = QuantUtils.calculate_sharpe_vol(rets_df, vol_df)\n",
    "\n",
    "    try:\n",
    "        # Check Series Alignment\n",
    "        if not np.isclose(res_series, 1.0):\n",
    "            raise AssertionError(\n",
    "                f\"DENOMINATOR MISMATCH: Series result {res_series:.2f} != 1.0. \"\n",
    "                \"The volatility denominator is likely including the leading NaN day.\"\n",
    "            )\n",
    "        print(\"‚úÖ Series Temporal Coupling: OK\")\n",
    "\n",
    "        # Check DataFrame Alignment (Ticker A: 0.1/0.1=1.0 | Ticker B: 0.2/0.2=1.0)\n",
    "        if not (np.isclose(res_df[\"A\"], 1.0) and np.isclose(res_df[\"B\"], 1.0)):\n",
    "            raise AssertionError(\n",
    "                f\"VECTORIZED MISMATCH: DataFrame results {res_df.values} != [1.0, 1.0]. \"\n",
    "                \"The logic is failing to align individual columns.\"\n",
    "            )\n",
    "        print(\"‚úÖ DataFrame Temporal Coupling: OK\")\n",
    "\n",
    "        print(\"‚úÖ AUDIT PASSED: Reward and Risk are strictly synchronized.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"üî• ALIGNMENT BREACH: {str(e)}\")\n",
    "        raise e\n",
    "\n",
    "\n",
    "# Auto-run the checks\n",
    "verify_math_integrity()\n",
    "\n",
    "verify_feature_engineering_integrity()\n",
    "\n",
    "verify_ranking_integrity()\n",
    "\n",
    "verify_vol_alignment_integrity()\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a48017a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# SECTION G: UTILITIES\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "def export_debug_to_csv(audit_pack, source_label=\"Audit\"):\n",
    "    \"\"\"\n",
    "    High-Transparency Exporter (Hardened Version).\n",
    "    Dumps the entire simulation state into a folder for manual Excel verification.\n",
    "    \"\"\"\n",
    "    if not audit_pack or not audit_pack[0]:\n",
    "        print(\"‚ùå Error: Audit Pack is empty. Run a simulation first.\")\n",
    "        return\n",
    "\n",
    "    data = audit_pack[0]\n",
    "    # Handle the fact that 'inputs' might be a key or a dataclass attribute\n",
    "    inputs = data.get(\"inputs\")\n",
    "\n",
    "    # 1. Folder Setup\n",
    "    date_str = inputs.start_date.strftime(\"%Y-%m-%d\")\n",
    "    strat = inputs.metric.replace(\" \", \"\").replace(\"(\", \"\").replace(\")\", \"\")\n",
    "    folder_name = f\"{source_label}_{strat}_{date_str}\"\n",
    "\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "\n",
    "    print(f\"üìÇ [AUDIT EXPORT] Folder: ./{folder_name}/\")\n",
    "\n",
    "    def process_item(item, path_prefix=\"\"):\n",
    "        # A. Handle Nested Dicts\n",
    "        if isinstance(item, dict):\n",
    "            for k, v in item.items():\n",
    "                process_item(v, f\"{path_prefix}{k}_\" if path_prefix else f\"{k}_\")\n",
    "\n",
    "        # B. Handle DataFrames (Matrices - High Precision)\n",
    "        elif isinstance(item, pd.DataFrame):\n",
    "            fn = f\"Matrix_{path_prefix.strip('_')}.csv\"\n",
    "            item.to_csv(os.path.join(folder_name, fn), float_format=\"%.8f\")\n",
    "            print(f\"   ‚úÖ Matrix: {fn}\")\n",
    "\n",
    "        # C. Handle Series (Vectors)\n",
    "        elif isinstance(item, pd.Series):\n",
    "            fn = f\"Vector_{path_prefix.strip('_')}.csv\"\n",
    "            item.to_frame().to_csv(os.path.join(folder_name, fn), float_format=\"%.8f\")\n",
    "            print(f\"   ‚úÖ Vector: {fn}\")\n",
    "\n",
    "        # D. Handle Dataclasses (Metadata & Results)\n",
    "        elif is_dataclass(item):\n",
    "            class_name = item.__class__.__name__\n",
    "            fn = f\"Summary_{class_name}_{path_prefix.strip('_')}\".strip(\"_\") + \".csv\"\n",
    "\n",
    "            # --- THE FIX: Create a Safe Dictionary for Pandas ---\n",
    "            raw_dict = asdict(item)\n",
    "            summary_ready_dict = {}\n",
    "\n",
    "            for k, v in raw_dict.items():\n",
    "                # If it's a big data object, just note its existence in the summary\n",
    "                if isinstance(v, (pd.DataFrame, pd.Series)):\n",
    "                    summary_ready_dict[k] = f\"<{v.__class__.__name__} shape={v.shape}>\"\n",
    "                # If it's a list or dict (the crash cause), stringify it for Excel\n",
    "                elif isinstance(v, (list, dict)):\n",
    "                    summary_ready_dict[k] = str(v)\n",
    "                else:\n",
    "                    summary_ready_dict[k] = v\n",
    "\n",
    "            # Save the clean key-value summary\n",
    "            pd.DataFrame.from_dict(\n",
    "                summary_ready_dict, orient=\"index\", columns=[\"Value\"]\n",
    "            ).to_csv(os.path.join(folder_name, fn))\n",
    "            print(f\"   üìë Summary: {fn}\")\n",
    "\n",
    "            # E. RECURSION: Now find the actual DataFrames inside the dataclass\n",
    "            # We iterate the object attributes directly to avoid the 'asdict' list confusion\n",
    "            for k in item.__dataclass_fields__.keys():\n",
    "                val = getattr(item, k)\n",
    "                if isinstance(val, (pd.DataFrame, pd.Series, dict)):\n",
    "                    process_item(val, f\"{path_prefix}{k}_\")\n",
    "\n",
    "    # 3. Execute Extraction\n",
    "    process_item(data)\n",
    "    print(f\"\\n‚ú® Export Complete. Open ./{folder_name}/ to verify results.\")\n",
    "\n",
    "\n",
    "def export_audit_to_excel(audit_pack, filename=\"Audit_Verification_Report.xlsx\"):\n",
    "    \"\"\"\n",
    "    Consolidates the audit_pack into a multi-sheet Excel workbook.\n",
    "    Organizes data by shared axes (Date vs Ticker) for manual formula checking.\n",
    "    \"\"\"\n",
    "    if not audit_pack or not audit_pack[0]:\n",
    "        print(\"‚ùå Error: Audit Pack is empty.\")\n",
    "        return\n",
    "\n",
    "    data = audit_pack[0]\n",
    "    res = data[\"results\"]\n",
    "    inputs = data[\"inputs\"]\n",
    "    debug = data.get(\"debug\", {})\n",
    "\n",
    "    print(f\"üìÇ [EXCEL AUDIT] Creating Report: {filename}\")\n",
    "\n",
    "    with pd.ExcelWriter(filename, engine=\"openpyxl\") as writer:\n",
    "\n",
    "        # --- SHEET 1: OVERVIEW (The Settings & Final Totals) ---\n",
    "        # Combines Input settings and Result scalars into one vertical table\n",
    "        meta_dict = {\n",
    "            **asdict(inputs),\n",
    "            **{\n",
    "                k: v\n",
    "                for k, v in asdict(res).items()\n",
    "                if not isinstance(v, (pd.DataFrame, pd.Series, dict))\n",
    "            },\n",
    "        }\n",
    "        # Stringify lists/dicts to prevent Excel/Pandas export crashes\n",
    "        clean_meta = {\n",
    "            k: (str(v) if isinstance(v, (list, dict)) else v)\n",
    "            for k, v in meta_dict.items()\n",
    "        }\n",
    "\n",
    "        df_overview = pd.DataFrame.from_dict(\n",
    "            clean_meta, orient=\"index\", columns=[\"Value\"]\n",
    "        )\n",
    "        df_overview.to_excel(writer, sheet_name=\"OVERVIEW\")\n",
    "\n",
    "        # --- SHEET 2: DAILY_AUDIT (Axis = Date) ---\n",
    "        # Concatenates everything that happens day-by-day\n",
    "        daily_items = {\n",
    "            \"Port_Value\": res.portfolio_series,\n",
    "            \"Port_Ret\": QuantUtils.compute_returns(res.portfolio_series),\n",
    "            \"Port_ATRP\": res.portfolio_atrp_series,  # <--- DIRECT ACCESS\n",
    "            \"Port_TRP\": res.portfolio_trp_series,  # <--- DIRECT ACCESS\n",
    "            \"Bench_Value\": res.benchmark_series,\n",
    "            \"Bench_Ret\": QuantUtils.compute_returns(res.benchmark_series),\n",
    "            \"Bench_ATRP\": res.benchmark_atrp_series,  # <--- DIRECT ACCESS\n",
    "            \"Bench_TRP\": res.benchmark_trp_series,  # <--- DIRECT ACCESS\n",
    "        }\n",
    "\n",
    "        # Filter out None values and concatenate side-by-side\n",
    "        df_daily = pd.concat(\n",
    "            {k: v for k, v in daily_items.items() if v is not None}, axis=1\n",
    "        )\n",
    "        df_daily.to_excel(writer, sheet_name=\"DAILY_AUDIT\", float_format=\"%.8f\")\n",
    "\n",
    "        # --- SHEET 3: SELECTION_SNAPSHOT (Axis = Ticker) ---\n",
    "        # Focuses on the selected 10-20 tickers and their performance\n",
    "        if \"full_universe_ranking\" in debug:\n",
    "            df_rank = debug[\"full_universe_ranking\"]\n",
    "            # Filter the leaderboard for only the tickers we actually bought\n",
    "            df_composition = df_rank.reindex(res.tickers)\n",
    "            df_composition.to_excel(\n",
    "                writer, sheet_name=\"PORTFOLIO_SNAPSHOT\", float_format=\"%.8f\"\n",
    "            )\n",
    "\n",
    "        # --- SHEET 4: FULL_UNIVERSE_RANKING ---\n",
    "        if \"full_universe_ranking\" in debug:\n",
    "            debug[\"full_universe_ranking\"].to_excel(\n",
    "                writer, sheet_name=\"FULL_RANKING\", float_format=\"%.8f\"\n",
    "            )\n",
    "\n",
    "        # --- SHEET 5: RAW_PRICES_MATRIX ---\n",
    "        if \"portfolio_raw_components\" in debug:\n",
    "            raw_p = debug[\"portfolio_raw_components\"].get(\"prices\")\n",
    "            if raw_p is not None:\n",
    "                raw_p.to_excel(writer, sheet_name=\"RAW_PRICES\", float_format=\"%.8f\")\n",
    "\n",
    "        # --- SHEET 6: RAW_VOL_MATRIX (TRP) ---\n",
    "        if \"portfolio_raw_components\" in debug:\n",
    "            # Re-extracting TRP matrix for the specific tickers\n",
    "            raw_v = debug[\"portfolio_raw_components\"].get(\n",
    "                \"atrp\"\n",
    "            )  # Or trp if stored specifically\n",
    "            if raw_v is not None:\n",
    "                raw_v.to_excel(writer, sheet_name=\"RAW_VOL_DATA\", float_format=\"%.8f\")\n",
    "\n",
    "    print(f\"‚ú® Audit Report Complete. Manual verification ready in {filename}\")\n",
    "\n",
    "\n",
    "def print_nested(d, indent=0, width=4):\n",
    "    \"\"\"Pretty-print nested containers.\n",
    "    Leaves are rendered as two lines:  key\\\\nvalue .\"\"\"\n",
    "    spacing = \" \" * indent\n",
    "\n",
    "    def _kind(node):\n",
    "        if not isinstance(node, dict):\n",
    "            return None\n",
    "        return \"sep\" if all(isinstance(v, dict) for v in node.values()) else \"nest\"\n",
    "\n",
    "    if isinstance(d, dict):\n",
    "        for k, v in d.items():\n",
    "            kind = _kind(v)\n",
    "            tag = \"\" if kind is None else f\"  [{'SEP' if kind == 'sep' else 'NEST'}]\"\n",
    "            print(f\"{spacing}{k}{tag}\")\n",
    "            print_nested(v, indent + width, width)\n",
    "\n",
    "    elif isinstance(d, (list, tuple)):\n",
    "        for idx, item in enumerate(d):\n",
    "            print(f\"{spacing}[{idx}]\")\n",
    "            print_nested(item, indent + width, width)\n",
    "\n",
    "    else:  # leaf ‚Äì primitive value\n",
    "        print(f\"{spacing}{d}\")\n",
    "\n",
    "\n",
    "def get_ticker_OHLCV(\n",
    "    df_ohlcv: pd.DataFrame,\n",
    "    tickers: Union[str, List[str]],\n",
    "    date_start: str,\n",
    "    date_end: str,\n",
    "    return_format: str = \"dataframe\",\n",
    "    verbose: bool = True,\n",
    ") -> Union[pd.DataFrame, dict]:\n",
    "    \"\"\"\n",
    "    Get OHLCV data for specified tickers within a date range.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_ohlcv : pd.DataFrame\n",
    "        DataFrame with MultiIndex of (ticker, date) and OHLCV columns\n",
    "    tickers : str or list of str\n",
    "        Ticker symbol(s) to retrieve\n",
    "    date_start : str\n",
    "        Start date in 'YYYY-MM-DD' format\n",
    "    date_end : str\n",
    "        End date in 'YYYY-MM-DD' format\n",
    "    return_format : str, optional\n",
    "        Format to return data in. Options:\n",
    "        - 'dataframe': Single DataFrame with MultiIndex (default)\n",
    "        - 'dict': Dictionary with tickers as keys and DataFrames as values\n",
    "        - 'separate': List of separate DataFrames for each ticker\n",
    "    verbose : bool, optional\n",
    "        Whether to print summary information (default: True)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Union[pd.DataFrame, dict, list]\n",
    "        Filtered OHLCV data in specified format\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If input parameters are invalid\n",
    "    KeyError\n",
    "        If tickers not found in DataFrame\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> # Get data for single ticker\n",
    "    >>> vlo_data = get_ticker_OHLCV(df_ohlcv, 'VLO', '2025-08-13', '2025-09-04')\n",
    "\n",
    "    >>> # Get data for multiple tickers\n",
    "    >>> multi_data = get_ticker_OHLCV(df_ohlcv, ['VLO', 'JPST'], '2025-08-13', '2025-09-04')\n",
    "\n",
    "    >>> # Get data as dictionary\n",
    "    >>> data_dict = get_ticker_OHLCV(df_ohlcv, ['VLO', 'JPST'], '2025-08-13',\n",
    "    ...                              '2025-09-04', return_format='dict')\n",
    "    \"\"\"\n",
    "\n",
    "    # Input validation\n",
    "    if not isinstance(df_ohlcv, pd.DataFrame):\n",
    "        raise TypeError(\"df_ohlcv must be a pandas DataFrame\")\n",
    "\n",
    "    if not isinstance(df_ohlcv.index, pd.MultiIndex):\n",
    "        raise ValueError(\"DataFrame must have MultiIndex of (ticker, date)\")\n",
    "\n",
    "    if len(df_ohlcv.index.levels) != 2:\n",
    "        raise ValueError(\"MultiIndex must have exactly 2 levels: (ticker, date)\")\n",
    "\n",
    "    # Convert single ticker to list for consistent processing\n",
    "    if isinstance(tickers, str):\n",
    "        tickers = [tickers]\n",
    "    elif not isinstance(tickers, list):\n",
    "        raise TypeError(\"tickers must be a string or list of strings\")\n",
    "\n",
    "    # Convert dates to Timestamps\n",
    "    try:\n",
    "        start_date = pd.Timestamp(date_start)\n",
    "        end_date = pd.Timestamp(date_end)\n",
    "    except ValueError as e:\n",
    "        raise ValueError(f\"Invalid date format. Use 'YYYY-MM-DD': {e}\")\n",
    "\n",
    "    if start_date > end_date:\n",
    "        raise ValueError(\"date_start must be before or equal to date_end\")\n",
    "\n",
    "    # Check if tickers exist in the DataFrame\n",
    "    available_tickers = df_ohlcv.index.get_level_values(0).unique()\n",
    "    missing_tickers = [t for t in tickers if t not in available_tickers]\n",
    "\n",
    "    if missing_tickers:\n",
    "        raise KeyError(f\"Ticker(s) not found in DataFrame: {missing_tickers}\")\n",
    "\n",
    "    # Filter the data using MultiIndex slicing\n",
    "    try:\n",
    "        filtered_data = df_ohlcv.loc[(tickers, slice(date_start, date_end)), :]\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error filtering data: {e}\")\n",
    "\n",
    "    # Handle empty results\n",
    "    if filtered_data.empty:\n",
    "        if verbose:\n",
    "            print(\n",
    "                f\"No data found for tickers {tickers} in date range {date_start} to {date_end}\"\n",
    "            )\n",
    "        return filtered_data\n",
    "\n",
    "    # Print summary if verbose\n",
    "    if verbose:\n",
    "        print(\n",
    "            f\"Data retrieved for {len(tickers)} ticker(s) from {date_start} to {date_end}\"\n",
    "        )\n",
    "        print(f\"Total rows: {len(filtered_data)}\")\n",
    "        print(\n",
    "            f\"Date range in data: {filtered_data.index.get_level_values(1).min()} to \"\n",
    "            f\"{filtered_data.index.get_level_values(1).max()}\"\n",
    "        )\n",
    "\n",
    "        # Print ticker-specific counts\n",
    "        ticker_counts = filtered_data.index.get_level_values(0).value_counts()\n",
    "        for ticker in tickers:\n",
    "            count = ticker_counts.get(ticker, 0)\n",
    "            if count > 0:\n",
    "                print(f\"  {ticker}: {count} rows\")\n",
    "            else:\n",
    "                print(f\"  {ticker}: No data in range\")\n",
    "\n",
    "    # Return in requested format\n",
    "    if return_format == \"dict\":\n",
    "        result = {}\n",
    "        for ticker in tickers:\n",
    "            try:\n",
    "                result[ticker] = filtered_data.xs(ticker, level=0).loc[\n",
    "                    date_start:date_end\n",
    "                ]\n",
    "            except KeyError:\n",
    "                result[ticker] = pd.DataFrame()\n",
    "        return result\n",
    "\n",
    "    elif return_format == \"separate\":\n",
    "        result = []\n",
    "        for ticker in tickers:\n",
    "            try:\n",
    "                result.append(\n",
    "                    filtered_data.xs(ticker, level=0).loc[date_start:date_end]\n",
    "                )\n",
    "            except KeyError:\n",
    "                result.append(pd.DataFrame())\n",
    "        return result\n",
    "\n",
    "    elif return_format == \"dataframe\":\n",
    "        return filtered_data\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Invalid return_format: {return_format}. \"\n",
    "            f\"Must be 'dataframe', 'dict', or 'separate'\"\n",
    "        )\n",
    "\n",
    "\n",
    "def get_ticker_features(\n",
    "    features_df: pd.DataFrame,\n",
    "    tickers: Union[str, List[str]],\n",
    "    date_start: str,\n",
    "    date_end: str,\n",
    "    return_format: str = \"dataframe\",\n",
    "    verbose: bool = True,\n",
    ") -> Union[pd.DataFrame, dict]:\n",
    "    \"\"\"\n",
    "    Get features data for specified tickers within a date range.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    features_df : pd.DataFrame\n",
    "        DataFrame with MultiIndex of (ticker, date) and feature columns\n",
    "    tickers : str or list of str\n",
    "        Ticker symbol(s) to retrieve\n",
    "    date_start : str\n",
    "        Start date in 'YYYY-MM-DD' format\n",
    "    date_end : str\n",
    "        End date in 'YYYY-MM-DD' format\n",
    "    return_format : str, optional\n",
    "        Format to return data in. Options:\n",
    "        - 'dataframe': Single DataFrame with MultiIndex (default)\n",
    "        - 'dict': Dictionary with tickers as keys and DataFrames as values\n",
    "        - 'separate': List of separate DataFrames for each ticker\n",
    "    verbose : bool, optional\n",
    "        Whether to print summary information (default: True)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Union[pd.DataFrame, dict, list]\n",
    "        Filtered features data in specified format\n",
    "    \"\"\"\n",
    "    # Convert single ticker to list for consistent processing\n",
    "    if isinstance(tickers, str):\n",
    "        tickers = [tickers]\n",
    "\n",
    "    # Filter the data using MultiIndex slicing\n",
    "    try:\n",
    "        filtered_data = features_df.loc[(tickers, slice(date_start, date_end)), :]\n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(f\"Error filtering data: {e}\")\n",
    "        return pd.DataFrame() if return_format == \"dataframe\" else {}\n",
    "\n",
    "    # Handle empty results\n",
    "    if filtered_data.empty:\n",
    "        if verbose:\n",
    "            print(\n",
    "                f\"No data found for tickers {tickers} in date range {date_start} to {date_end}\"\n",
    "            )\n",
    "        return filtered_data\n",
    "\n",
    "    # Print summary if verbose\n",
    "    if verbose:\n",
    "        print(\n",
    "            f\"Features data retrieved for {len(tickers)} ticker(s) from {date_start} to {date_end}\"\n",
    "        )\n",
    "        print(f\"Total rows: {len(filtered_data)}\")\n",
    "        print(\n",
    "            f\"Date range in data: {filtered_data.index.get_level_values(1).min()} to \"\n",
    "            f\"{filtered_data.index.get_level_values(1).max()}\"\n",
    "        )\n",
    "        print(f\"Available features: {', '.join(filtered_data.columns.tolist())}\")\n",
    "\n",
    "        # Print ticker-specific counts\n",
    "        ticker_counts = filtered_data.index.get_level_values(0).value_counts()\n",
    "        for ticker in tickers:\n",
    "            count = ticker_counts.get(ticker, 0)\n",
    "            if count > 0:\n",
    "                print(f\"  {ticker}: {count} rows\")\n",
    "            else:\n",
    "                print(f\"  {ticker}: No data in range\")\n",
    "\n",
    "    # Return in requested format\n",
    "    if return_format == \"dict\":\n",
    "        result = {}\n",
    "        for ticker in tickers:\n",
    "            try:\n",
    "                result[ticker] = filtered_data.xs(ticker, level=0).loc[\n",
    "                    date_start:date_end\n",
    "                ]\n",
    "            except KeyError:\n",
    "                result[ticker] = pd.DataFrame()\n",
    "        return result\n",
    "\n",
    "    elif return_format == \"separate\":\n",
    "        result = []\n",
    "        for ticker in tickers:\n",
    "            try:\n",
    "                result.append(\n",
    "                    filtered_data.xs(ticker, level=0).loc[date_start:date_end]\n",
    "                )\n",
    "            except KeyError:\n",
    "                result.append(pd.DataFrame())\n",
    "        return result\n",
    "\n",
    "    elif return_format == \"dataframe\":\n",
    "        return filtered_data\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Invalid return_format: {return_format}. \"\n",
    "            f\"Must be 'dataframe', 'dict', or 'separate'\"\n",
    "        )\n",
    "\n",
    "\n",
    "def create_combined_dict(\n",
    "    df_ohlcv: pd.DataFrame,\n",
    "    features_df: pd.DataFrame,\n",
    "    tickers: Union[str, List[str]],\n",
    "    date_start: str,\n",
    "    date_end: str,\n",
    "    verbose: bool = True,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Create a combined dictionary with both OHLCV and features data for each ticker.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_ohlcv : pd.DataFrame\n",
    "        DataFrame with OHLCV data (MultiIndex: ticker, date)\n",
    "    features_df : pd.DataFrame\n",
    "        DataFrame with features data (MultiIndex: ticker, date)\n",
    "    tickers : str or list of str\n",
    "        Ticker symbol(s) to retrieve\n",
    "    date_start : str\n",
    "        Start date in 'YYYY-MM-DD' format\n",
    "    date_end : str\n",
    "        End date in 'YYYY-MM-DD' format\n",
    "    verbose : bool, optional\n",
    "        Whether to print progress information (default: True)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with tickers as keys and combined DataFrames (OHLCV + features) as values\n",
    "    \"\"\"\n",
    "    # Convert single ticker to list\n",
    "    if isinstance(tickers, str):\n",
    "        tickers = [tickers]\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Creating combined dictionary for {len(tickers)} ticker(s)\")\n",
    "        print(f\"Date range: {date_start} to {date_end}\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "    # Get OHLCV data as dictionary\n",
    "    ohlcv_dict = get_ticker_OHLCV(\n",
    "        df_ohlcv, tickers, date_start, date_end, return_format=\"dict\", verbose=verbose\n",
    "    )\n",
    "\n",
    "    # Get features data as dictionary\n",
    "    features_dict = get_ticker_features(\n",
    "        features_df,\n",
    "        tickers,\n",
    "        date_start,\n",
    "        date_end,\n",
    "        return_format=\"dict\",\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    # Create combined_dict\n",
    "    combined_dict = {}\n",
    "\n",
    "    for ticker in tickers:\n",
    "        if verbose:\n",
    "            print(f\"\\nProcessing {ticker}...\")\n",
    "\n",
    "        # Check if ticker exists in both dictionaries\n",
    "        if ticker in ohlcv_dict and ticker in features_dict:\n",
    "            ohlcv_data = ohlcv_dict[ticker]\n",
    "            features_data = features_dict[ticker]\n",
    "\n",
    "            # Check if both dataframes have data\n",
    "            if not ohlcv_data.empty and not features_data.empty:\n",
    "                # Combine OHLCV and features data\n",
    "                # Note: Both dataframes have the same index (dates), so we can concatenate\n",
    "                combined_df = pd.concat([ohlcv_data, features_data], axis=1)\n",
    "\n",
    "                # Ensure proper index naming\n",
    "                combined_df.index.name = \"Date\"\n",
    "\n",
    "                # Store in combined_dict\n",
    "                combined_dict[ticker] = combined_df\n",
    "\n",
    "                if verbose:\n",
    "                    print(f\"  ‚úì Successfully combined data\")\n",
    "                    print(f\"  OHLCV shape: {ohlcv_data.shape}\")\n",
    "                    print(f\"  Features shape: {features_data.shape}\")\n",
    "                    print(f\"  Combined shape: {combined_df.shape}\")\n",
    "                    print(\n",
    "                        f\"  Date range: {combined_df.index.min()} to {combined_df.index.max()}\"\n",
    "                    )\n",
    "            else:\n",
    "                if verbose:\n",
    "                    print(f\"  ‚úó Cannot combine: One or both dataframes are empty\")\n",
    "                    print(f\"    OHLCV empty: {ohlcv_data.empty}\")\n",
    "                    print(f\"    Features empty: {features_data.empty}\")\n",
    "                combined_dict[ticker] = pd.DataFrame()\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(f\"  ‚úó Ticker not found in both dictionaries\")\n",
    "                if ticker not in ohlcv_dict:\n",
    "                    print(f\"    Not in OHLCV data\")\n",
    "                if ticker not in features_dict:\n",
    "                    print(f\"    Not in features data\")\n",
    "            combined_dict[ticker] = pd.DataFrame()\n",
    "\n",
    "    # Print summary\n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"SUMMARY\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Total tickers processed: {len(tickers)}\")\n",
    "\n",
    "        tickers_with_data = [\n",
    "            ticker for ticker, df in combined_dict.items() if not df.empty\n",
    "        ]\n",
    "        print(f\"Tickers with combined data: {len(tickers_with_data)}\")\n",
    "\n",
    "        if tickers_with_data:\n",
    "            print(\"\\nTicker details:\")\n",
    "            for ticker in tickers_with_data:\n",
    "                df = combined_dict[ticker]\n",
    "                print(f\"  {ticker}: {df.shape} - {df.index.min()} to {df.index.max()}\")\n",
    "                print(f\"    Columns: {len(df.columns)}\")\n",
    "\n",
    "        empty_tickers = [ticker for ticker, df in combined_dict.items() if df.empty]\n",
    "        if empty_tickers:\n",
    "            print(f\"\\nTickers with no data: {', '.join(empty_tickers)}\")\n",
    "\n",
    "    return combined_dict\n",
    "\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6d867e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r\"c:\\Users\\ping\\Files_win10\\python\\py311\\stocks\\data\\df_indices.parquet\"\n",
    "\n",
    "df_indices = pd.read_parquet(data_path, engine=\"pyarrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf150180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 144085 entries, ('^AXJO', Timestamp('1992-11-22 00:00:00')) to ('^VIX3M', Timestamp('2026-01-05 00:00:00'))\n",
      "Data columns (total 5 columns):\n",
      " #   Column     Non-Null Count   Dtype  \n",
      "---  ------     --------------   -----  \n",
      " 0   Adj Open   144085 non-null  float64\n",
      " 1   Adj High   144085 non-null  float64\n",
      " 2   Adj Low    144085 non-null  float64\n",
      " 3   Adj Close  144085 non-null  float64\n",
      " 4   Volume     144085 non-null  int64  \n",
      "dtypes: float64(4), int64(1)\n",
      "memory usage: 6.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df_indices.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "232740e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = (\n",
    "    r\"c:\\Users\\ping\\Files_win10\\python\\py311\\stocks\\data\\df_OHLCV_stocks_etfs.parquet\"\n",
    ")\n",
    "\n",
    "df_ohlcv = pd.read_parquet(data_path, engine=\"pyarrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "239275a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 9462502 entries, ('A', Timestamp('1999-11-18 00:00:00')) to ('ZWS', Timestamp('2026-01-05 00:00:00'))\n",
      "Data columns (total 5 columns):\n",
      " #   Column     Dtype  \n",
      "---  ------     -----  \n",
      " 0   Adj Open   float64\n",
      " 1   Adj High   float64\n",
      " 2   Adj Low    float64\n",
      " 3   Adj Close  float64\n",
      " 4   Volume     int64  \n",
      "dtypes: float64(4), int64(1)\n",
      "memory usage: 397.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df_ohlcv.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88954119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating features... this might take about 3 minutes...\n",
      "‚ö° Generating SOTA Quant Features (Benchmark: SPY)...\n",
      "üöÄ Generating Wide Matrices for Instant Backtesting...\n",
      "   - Unstacking ATRP...\n",
      "   - Unstacking TRP...\n",
      "‚úÖ Pre-computation Complete. df_close_wide, df_atrp_wide, and df_trp_wide are ready.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# DATA PRE-COMPUTATION (The \"Fast-Track\" Setup)\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"Calculating features... this might take about 3 minutes...\")\n",
    "features_df = generate_features(\n",
    "    df_ohlcv=df_ohlcv,\n",
    "    atr_period=GLOBAL_SETTINGS[\"atr_period\"],\n",
    "    quality_window=GLOBAL_SETTINGS[\"quality_window\"],\n",
    "    quality_min_periods=GLOBAL_SETTINGS[\"quality_min_periods\"],\n",
    ")\n",
    "\n",
    "print(\"üöÄ Generating Wide Matrices for Instant Backtesting...\")\n",
    "\n",
    "# 1. Price Matrix\n",
    "df_close_wide = df_ohlcv[\"Adj Close\"].unstack(level=0)\n",
    "\n",
    "# 2. Volatility Matrices (Unstack and Align)\n",
    "# Using reindex_like ensures Dates and Tickers match df_close_wide exactly\n",
    "print(\"   - Unstacking ATRP...\")\n",
    "df_atrp_wide = features_df[\"ATRP\"].unstack(level=0).reindex_like(df_close_wide)\n",
    "\n",
    "print(\"   - Unstacking TRP...\")\n",
    "df_trp_wide = features_df[\"TRP\"].unstack(level=0).reindex_like(df_close_wide)\n",
    "\n",
    "# 3. Handle Data Gaps (Sanitize the Wide Matrices)\n",
    "# This prevents NaN propagation during matrix multiplication\n",
    "if GLOBAL_SETTINGS[\"handle_zeros_as_nan\"]:\n",
    "    df_close_wide = df_close_wide.replace(0, np.nan)\n",
    "\n",
    "# Forward fill up to the limit, then fill remaining with the \"Disaster Detection\" value\n",
    "df_close_wide = df_close_wide.ffill(limit=GLOBAL_SETTINGS[\"max_data_gap_ffill\"])\n",
    "df_close_wide = df_close_wide.fillna(GLOBAL_SETTINGS[\"nan_price_replacement\"])\n",
    "\n",
    "print(\n",
    "    \"‚úÖ Pre-computation Complete. df_close_wide, df_atrp_wide, and df_trp_wide are ready.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca6930d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ‚öôÔ∏è Initializing AlphaEngine v2.2 (Sanitized) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c31c08a3a471474184d97972a8857521",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<b>1. Timeline Configuration:</b> (Past <--- Decision ---> Future)'), HBox(children‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3beb31269a2a48e8bb6661045e718584",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'line': {'width': 2},\n",
       "              'name': 'SNX',\n",
       "              'type': 'scatter',\n",
       "              'uid': '86cd4aae-2aba-40af-ae49-8d01b0e23ec4',\n",
       "              'visible': True,\n",
       "              'x': array([datetime.datetime(2025, 1, 2, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 3, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 6, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 7, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 8, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 10, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 13, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 14, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 15, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 16, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 17, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 21, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 22, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 23, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 24, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 27, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 28, 0, 0)], dtype=object),\n",
       "              'y': array([1.        , 1.01245041, 1.01562609, 1.05794529, 1.04824424, 1.15050115,\n",
       "                          1.14045208, 1.16792824, 1.16981625, 1.17762929, 1.17702896, 1.20853344,\n",
       "                          1.2147978 , 1.22931022, 1.2271525 , 1.21587666, 1.22104476])},\n",
       "             {'line': {'width': 2},\n",
       "              'name': 'RPRX',\n",
       "              'type': 'scatter',\n",
       "              'uid': '516ca022-634f-43da-bf63-8b51586faa59',\n",
       "              'visible': True,\n",
       "              'x': array([datetime.datetime(2025, 1, 2, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 3, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 6, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 7, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 8, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 10, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 13, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 14, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 15, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 16, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 17, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 21, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 22, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 23, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 24, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 27, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 28, 0, 0)], dtype=object),\n",
       "              'y': array([1.        , 1.00193989, 0.99844968, 1.00969546, 1.01589674, 1.14346818,\n",
       "                          1.16285513, 1.16440545, 1.19193754, 1.20007871, 1.19232313, 1.17487607,\n",
       "                          1.16634534, 1.18728261, 1.20434408, 1.22993628, 1.23846701])},\n",
       "             {'line': {'width': 2},\n",
       "              'name': 'MINT',\n",
       "              'type': 'scatter',\n",
       "              'uid': '28127feb-d1cf-4295-95b1-5ec40b2c5d33',\n",
       "              'visible': True,\n",
       "              'x': array([datetime.datetime(2025, 1, 2, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 3, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 6, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 7, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 8, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 10, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 13, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 14, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 15, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 16, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 17, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 21, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 22, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 23, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 24, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 27, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 28, 0, 0)], dtype=object),\n",
       "              'y': array([1.        , 1.00049801, 1.00069742, 1.00059719, 1.00099602, 1.00109625,\n",
       "                          1.00129566, 1.00149507, 1.00179367, 1.00209331, 1.00259132, 1.00259132,\n",
       "                          1.00279073, 1.00299014, 1.00348815, 1.00348815, 1.00348815])},\n",
       "             {'line': {'width': 2},\n",
       "              'name': 'LNG',\n",
       "              'type': 'scatter',\n",
       "              'uid': 'e9b5eaec-e4f3-4a8b-9f85-b30620d89ca3',\n",
       "              'visible': True,\n",
       "              'x': array([datetime.datetime(2025, 1, 2, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 3, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 6, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 7, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 8, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 10, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 13, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 14, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 15, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 16, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 17, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 21, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 22, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 23, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 24, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 27, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 28, 0, 0)], dtype=object),\n",
       "              'y': array([1.        , 1.0085232 , 1.01500595, 1.01319425, 1.02080703, 1.02094428,\n",
       "                          1.04007228, 1.10194894, 1.11459877, 1.14986733, 1.14578644, 1.13159484,\n",
       "                          1.05784152, 1.06205966, 1.0510431 , 1.01749931, 1.02661268])},\n",
       "             {'line': {'width': 2},\n",
       "              'name': 'USFR',\n",
       "              'type': 'scatter',\n",
       "              'uid': '8ed43ec4-17ce-4137-bf64-79d090df2dcd',\n",
       "              'visible': True,\n",
       "              'x': array([datetime.datetime(2025, 1, 2, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 3, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 6, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 7, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 8, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 10, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 13, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 14, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 15, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 16, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 17, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 21, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 22, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 23, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 24, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 27, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 28, 0, 0)], dtype=object),\n",
       "              'y': array([1.        , 1.00019881, 1.00039762, 1.00059642, 1.00079523, 1.00139165,\n",
       "                          1.00159046, 1.001986  , 1.001986  , 1.001986  , 1.00238362, 1.00258243,\n",
       "                          1.00238362, 1.00258243, 1.00298004, 1.00317885, 1.00341908])},\n",
       "             {'line': {'width': 2},\n",
       "              'name': 'QRVO',\n",
       "              'type': 'scatter',\n",
       "              'uid': '1218406e-c78f-43d5-83ce-1eff063a1622',\n",
       "              'visible': True,\n",
       "              'x': array([datetime.datetime(2025, 1, 2, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 3, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 6, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 7, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 8, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 10, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 13, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 14, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 15, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 16, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 17, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 21, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 22, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 23, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 24, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 27, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 28, 0, 0)], dtype=object),\n",
       "              'y': array([1.        , 1.01247133, 1.03411697, 1.05547592, 1.04759174, 1.02766628,\n",
       "                          1.03081995, 1.04386468, 1.06307339, 1.05490252, 1.20713876, 1.25544725,\n",
       "                          1.24842317, 1.29830849, 1.27494266, 1.25616399, 1.2296445 ])},\n",
       "             {'line': {'width': 2},\n",
       "              'name': 'SGOV',\n",
       "              'type': 'scatter',\n",
       "              'uid': '9d4a77e5-5b79-4d9f-8f56-4e471802f3a0',\n",
       "              'visible': True,\n",
       "              'x': array([datetime.datetime(2025, 1, 2, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 3, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 6, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 7, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 8, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 10, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 13, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 14, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 15, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 16, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 17, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 21, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 22, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 23, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 24, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 27, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 28, 0, 0)], dtype=object),\n",
       "              'y': array([1.        , 1.00029902, 1.00029902, 1.00049836, 1.00069771, 1.0010964 ,\n",
       "                          1.00129575, 1.00139542, 1.00149509, 1.00149509, 1.00199346, 1.00209313,\n",
       "                          1.00229248, 1.00239215, 1.00269117, 1.00269117, 1.00299019])},\n",
       "             {'line': {'width': 2},\n",
       "              'name': 'SHV',\n",
       "              'type': 'scatter',\n",
       "              'uid': '11580876-0b68-48a3-b55d-48c8b70302f6',\n",
       "              'visible': True,\n",
       "              'x': array([datetime.datetime(2025, 1, 2, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 3, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 6, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 7, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 8, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 10, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 13, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 14, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 15, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 16, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 17, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 21, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 22, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 23, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 24, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 27, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 28, 0, 0)], dtype=object),\n",
       "              'y': array([1.        , 1.00027431, 1.00045403, 1.00063374, 1.00063374, 1.00090805,\n",
       "                          1.00108777, 1.00118236, 1.00136208, 1.00145666, 1.00191069, 1.00209041,\n",
       "                          1.00209041, 1.00227013, 1.00263902, 1.00281874, 1.00290387])},\n",
       "             {'line': {'width': 2},\n",
       "              'name': 'BIL',\n",
       "              'type': 'scatter',\n",
       "              'uid': 'ed3808b3-d411-4d17-836e-0f4563b2c657',\n",
       "              'visible': True,\n",
       "              'x': array([datetime.datetime(2025, 1, 2, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 3, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 6, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 7, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 8, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 10, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 13, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 14, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 15, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 16, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 17, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 21, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 22, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 23, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 24, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 27, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 28, 0, 0)], dtype=object),\n",
       "              'y': array([1.        , 1.00010939, 1.00021764, 1.00043755, 1.00065519, 1.0009845 ,\n",
       "                          1.00109275, 1.00131152, 1.00142091, 1.00142091, 1.00196786, 1.00196786,\n",
       "                          1.00218663, 1.00218663, 1.00262419, 1.00273358, 1.00273358])},\n",
       "             {'line': {'width': 2},\n",
       "              'name': 'TRGP',\n",
       "              'type': 'scatter',\n",
       "              'uid': '445231af-5631-4ac5-bd3c-09e52cc74659',\n",
       "              'visible': True,\n",
       "              'x': array([datetime.datetime(2025, 1, 2, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 3, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 6, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 7, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 8, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 10, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 13, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 14, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 15, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 16, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 17, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 21, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 22, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 23, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 24, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 27, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 28, 0, 0)], dtype=object),\n",
       "              'y': array([1.        , 1.01699117, 1.01693534, 1.02600325, 1.04872887, 1.055943  ,\n",
       "                          1.07871888, 1.10008208, 1.12258436, 1.16328388, 1.17841568, 1.18660696,\n",
       "                          1.15410429, 1.15476317, 1.14208819, 1.08838983, 1.10548709])},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '1fee364c-c254-4874-93eb-4f78eff89512', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '1dc5d7b6-b621-4b35-8c18-0ace8de1bbb8', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': 'ed66acdd-828b-42bc-81a2-5a9172ed9573', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '229aebf8-f584-412d-8703-017ff0f1d6ef', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '1c38faee-e5d0-4e7f-9940-9919823580c3', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': 'a821237a-ea53-45bf-890f-96605e7b5678', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': 'acc64ace-f2fa-4609-a744-958bdb50adc0', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '2ec2a8ec-5301-4b45-b5bf-fe6363bd4061', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': 'd97f8e4b-6f8e-447d-afae-dbe09dd3d58f', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': 'f3b61457-1614-40ac-aba8-ac3c20c2f20b', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': 'a1deceec-4093-4522-baf5-d0c48934a944', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '8797d327-3770-4c8d-88c2-850efdb62f5b', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': 'f258aca0-ef8a-4835-82c1-4028935b7369', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '373fe7ab-d3a0-407e-ada9-e66b78faceca', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '2ad53136-8359-40e3-ae1d-0ff115efb684', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '8cb699b4-aee4-4c20-8f3f-3c205db693d3', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': 'd7197c6c-8df3-4ac0-8515-28b65b28ef3d', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '2d2a4587-81c2-4500-8f5f-aeee35cf8dff', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '161de971-40bf-43e5-b371-b72b6cf11203', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': 'ac702646-110d-4c87-b4e0-f77a017d9158', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '56273dbd-e3e6-440b-9df0-972828ef44c1', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': 'cac8cf57-9966-46fc-aafd-82250db7f5ca', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': 'ca554123-0e11-445c-8ea8-769ec865f936', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '5cbc34e2-0052-482a-a6c4-6c4ca3e2f2a2', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '71f36d9f-a5bd-4257-9140-37295a79c48a', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '26b9dce0-8ade-4c6d-babe-559df411cfcc', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': 'c07a9b31-d504-4d0b-a6d5-666badd9e864', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': 'f12f0275-f844-4969-be1e-21490dc6f178', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '9d6227aa-bf2a-4692-9d7c-2fba012a3a23', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': 'a17304d5-43e1-4a88-8206-cc26715d34de', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '18bb35dc-f1e0-4aa0-9dfc-9ffed46042a2', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': 'a72f7da5-705a-4ee8-837d-ef20fc864d38', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': 'cd47d38d-4952-45c7-b782-55bd27da1279', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '8152cba7-06ca-4a1d-b96e-94cc95c4d060', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': 'd49959fe-2d0c-4e8e-9340-5a6712737795', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '409eae70-cc0d-412f-b14e-91b55c3450d5', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '8294ed1b-1363-4e72-bad5-1ce8c806dd57', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': 'b3c7ad0d-594a-40b2-8235-bcd7e992ac20', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': 'a2559d12-23f1-430e-aa50-b1fe7b447ca4', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': 'b6e807b8-7902-4e47-8de3-89cfc7149620', 'visible': False},\n",
       "             {'line': {'color': 'black', 'dash': 'dash', 'width': 3},\n",
       "              'name': 'Benchmark (SPY)',\n",
       "              'type': 'scatter',\n",
       "              'uid': '1b936091-65e2-4501-b6f5-b6415d5c20c3',\n",
       "              'visible': True,\n",
       "              'x': array([datetime.datetime(2025, 1, 2, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 3, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 6, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 7, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 8, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 10, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 13, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 14, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 15, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 16, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 17, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 21, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 22, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 23, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 24, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 27, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 28, 0, 0)], dtype=object),\n",
       "              'y': array([1.        , 1.01250316, 1.01833681, 1.00682525, 1.00829621, 0.99290132,\n",
       "                          0.9944415 , 0.99581036, 1.01392393, 1.01197361, 1.02213362, 1.03148892,\n",
       "                          1.03728797, 1.0429503 , 1.03990454, 1.02519495, 1.0340034 ])},\n",
       "             {'line': {'color': 'green', 'width': 3},\n",
       "              'name': 'Group Portfolio',\n",
       "              'type': 'scatter',\n",
       "              'uid': '8ffda822-5968-4982-a3a2-4dd2aba3525b',\n",
       "              'visible': True,\n",
       "              'x': array([datetime.datetime(2025, 1, 2, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 3, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 6, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 7, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 8, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 10, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 13, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 14, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 15, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 16, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 17, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 21, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 22, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 23, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 24, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 27, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 28, 0, 0)], dtype=object),\n",
       "              'y': array([1.        , 1.00537555, 1.00821997, 1.01650775, 1.01850465, 1.04039997,\n",
       "                          1.04592807, 1.05855998, 1.06700681, 1.07542137, 1.09115399, 1.09683837,\n",
       "                          1.0853256 , 1.09441456, 1.09139931, 1.08227766, 1.08367909])}],\n",
       "    'layout': {'annotations': [{'bgcolor': 'red',\n",
       "                                'font': {'color': 'white'},\n",
       "                                'showarrow': False,\n",
       "                                'text': 'DECISION',\n",
       "                                'x': Timestamp('2025-01-17 00:00:00'),\n",
       "                                'xref': 'x',\n",
       "                                'y': 0.05,\n",
       "                                'yref': 'paper'},\n",
       "                               {'bgcolor': 'blue',\n",
       "                                'font': {'color': 'white'},\n",
       "                                'showarrow': False,\n",
       "                                'text': 'ENTRY (T+1)',\n",
       "                                'x': Timestamp('2025-01-21 00:00:00'),\n",
       "                                'xref': 'x',\n",
       "                                'y': 1.0,\n",
       "                                'yref': 'paper'}],\n",
       "               'height': 600,\n",
       "               'hovermode': 'x unified',\n",
       "               'shapes': [{'line': {'color': 'red', 'dash': 'dash', 'width': 2},\n",
       "                           'type': 'line',\n",
       "                           'x0': Timestamp('2025-01-17 00:00:00'),\n",
       "                           'x1': Timestamp('2025-01-17 00:00:00'),\n",
       "                           'xref': 'x',\n",
       "                           'y0': 0,\n",
       "                           'y1': 1,\n",
       "                           'yref': 'paper'},\n",
       "                          {'line': {'color': 'blue', 'dash': 'dot', 'width': 2},\n",
       "                           'type': 'line',\n",
       "                           'x0': Timestamp('2025-01-21 00:00:00'),\n",
       "                           'x1': Timestamp('2025-01-21 00:00:00'),\n",
       "                           'xref': 'x',\n",
       "                           'y0': 0,\n",
       "                           'y1': 1,\n",
       "                           'yref': 'paper'}],\n",
       "               'template': '...',\n",
       "               'title': {'text': 'Event-Driven Walk-Forward Analysis'}}\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "audit_pack = plot_walk_forward_analyzer(\n",
    "    df_ohlcv=df_ohlcv,\n",
    "    features_df=features_df,\n",
    "    df_close_wide=df_close_wide,\n",
    "    df_atrp_wide=df_atrp_wide,\n",
    "    df_trp_wide=df_trp_wide,  # <--- Update your class to accept this\n",
    "    default_start_date=\"2025-01-17\",\n",
    "    default_lookback=10,\n",
    "    default_holding=5,\n",
    "    default_strategy=\"Sharpe (ATRP)\",\n",
    "    default_rank_start=1,\n",
    "    default_rank_end=10,\n",
    "    default_benchmark_ticker=GLOBAL_SETTINGS[\"benchmark_ticker\"],\n",
    "    master_calendar_ticker=GLOBAL_SETTINGS[\"calendar_ticker\"],\n",
    "    quality_thresholds=GLOBAL_SETTINGS[\"thresholds\"],\n",
    "    debug=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ac4da5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=================================================================\n",
      "üîç HIGH-TRANSPARENCY AUDIT MAP\n",
      "=================================================================\n",
      "[  0] üìÇ audit_pack (list)\n",
      "[  1]   üìÇ index_0 (dict)\n",
      "[  2]     üìÇ debug (dict)\n",
      "[  3]       üìÇ audit_liquidity (dict)\n",
      "[  4]         üìÖ date (Timestamp)\n",
      "[  5]         üî¢ final_cutoff_usd (float64 shape=())\n",
      "[  6]         üî¢ percentile_setting (float)\n",
      "[  7]         üìÇ tickers_passed (int64 shape=())\n",
      "[  8]         üî¢ total_tickers_available (int)\n",
      "[  9]         üßÆ universe_snapshot (DataFrame shape=(1559, 21))\n",
      "[ 10]       üìÇ benchmark_raw_components (dict)\n",
      "[ 11]         üßÆ atrp (DataFrame shape=(17, 1))\n",
      "[ 12]         üßÆ ohlcv_raw (DataFrame shape=(17, 5))\n",
      "[ 13]         üßÆ prices (DataFrame shape=(17, 1))\n",
      "[ 14]         üßÆ trp (DataFrame shape=(17, 1))\n",
      "[ 15]       üßÆ full_universe_ranking (DataFrame shape=(927, 3))\n",
      "[ 16]       üìÇ portfolio_raw_components (dict)\n",
      "[ 17]         üßÆ atrp (DataFrame shape=(17, 10))\n",
      "[ 18]         üßÆ ohlcv_raw (DataFrame shape=(170, 5))\n",
      "[ 19]         üßÆ prices (DataFrame shape=(17, 10))\n",
      "[ 20]         üßÆ trp (DataFrame shape=(17, 10))\n",
      "            ‚ï∞‚îÄ‚îÄ selection_audit --> [See ID 15]\n",
      "[ 21]       üìÇ verification (dict)\n",
      "[ 22]         üìÇ benchmark (dict)\n",
      "[ 23]           üìà full_atrp (Series shape=(17,))\n",
      "[ 24]           üìà full_ret (Series shape=(17,))\n",
      "[ 25]           üìà full_trp (Series shape=(17,))\n",
      "[ 26]           üìà full_val (Series shape=(17,))\n",
      "[ 27]           üìà holding_atrp (Series shape=(6,))\n",
      "[ 28]           üìà holding_ret (Series shape=(6,))\n",
      "[ 29]           üìà holding_trp (Series shape=(6,))\n",
      "[ 30]           üìà holding_val (Series shape=(6,))\n",
      "[ 31]           üìà lookback_atrp (Series shape=(11,))\n",
      "[ 32]           üìà lookback_ret (Series shape=(11,))\n",
      "[ 33]           üìà lookback_trp (Series shape=(11,))\n",
      "[ 34]           üìà lookback_val (Series shape=(11,))\n",
      "[ 35]         üìÇ portfolio (dict)\n",
      "[ 36]           üìà full_atrp (Series shape=(17,))\n",
      "[ 37]           üìà full_ret (Series shape=(17,))\n",
      "[ 38]           üìà full_trp (Series shape=(17,))\n",
      "[ 39]           üìà full_val (Series shape=(17,))\n",
      "[ 40]           üìà holding_atrp (Series shape=(6,))\n",
      "[ 41]           üìà holding_ret (Series shape=(6,))\n",
      "[ 42]           üìà holding_trp (Series shape=(6,))\n",
      "[ 43]           üìà holding_val (Series shape=(6,))\n",
      "[ 44]           üìà lookback_atrp (Series shape=(11,))\n",
      "[ 45]           üìà lookback_ret (Series shape=(11,))\n",
      "[ 46]           üìà lookback_trp (Series shape=(11,))\n",
      "[ 47]           üìà lookback_val (Series shape=(11,))\n",
      "[ 48]     üì¶ inputs (EngineInput)\n",
      "[ 49]       üìÇ mode (str)\n",
      "[ 50]       üìÖ start_date (Timestamp)\n",
      "[ 51]       üî¢ lookback_period (int)\n",
      "[ 52]       üî¢ holding_period (int)\n",
      "[ 53]       üìÇ metric (str)\n",
      "[ 54]       üìÇ benchmark_ticker (str)\n",
      "[ 55]       üî¢ rank_start (int)\n",
      "[ 56]       üî¢ rank_end (int)\n",
      "[ 57]       üìÇ quality_thresholds (dict)\n",
      "[ 58]         üî¢ max_same_vol_count (int)\n",
      "[ 59]         üî¢ max_stale_pct (float)\n",
      "[ 60]         üî¢ min_liquidity_percentile (float)\n",
      "[ 61]         üî¢ min_median_dollar_volume (int)\n",
      "[ 62]       üìÇ manual_tickers: []\n",
      "[ 63]       üî¢ debug (bool)\n",
      "[ 64]     üì¶ results (EngineOutput)\n",
      "            ‚ï∞‚îÄ‚îÄ portfolio_series --> [See ID 39]\n",
      "            ‚ï∞‚îÄ‚îÄ benchmark_series --> [See ID 26]\n",
      "[ 65]       üßÆ normalized_plot_data (DataFrame shape=(17, 10))\n",
      "[ 66]       üìÇ tickers: ['RPRX', 'SGOV', 'SHV']... [+ 7]\n",
      "[ 67]       üìà initial_weights (Series shape=(10,))\n",
      "[ 68]       üìÇ perf_metrics (dict)\n",
      "[ 69]         üî¢ full_b_gain (float)\n",
      "[ 70]         üî¢ full_b_sharpe (float)\n",
      "[ 71]         üî¢ full_b_sharpe_atrp (float)\n",
      "[ 72]         üî¢ full_b_sharpe_trp (float)\n",
      "[ 73]         üî¢ full_p_gain (float)\n",
      "[ 74]         üî¢ full_p_sharpe (float)\n",
      "[ 75]         üî¢ full_p_sharpe_atrp (float)\n",
      "[ 76]         üî¢ full_p_sharpe_trp (float)\n",
      "[ 77]         üî¢ holding_b_gain (float)\n",
      "[ 78]         üî¢ holding_b_sharpe (float)\n",
      "[ 79]         üî¢ holding_b_sharpe_atrp (float)\n",
      "[ 80]         üî¢ holding_b_sharpe_trp (float)\n",
      "[ 81]         üî¢ holding_p_gain (float)\n",
      "[ 82]         üî¢ holding_p_sharpe (float)\n",
      "[ 83]         üî¢ holding_p_sharpe_atrp (float)\n",
      "[ 84]         üî¢ holding_p_sharpe_trp (float)\n",
      "[ 85]         üî¢ lookback_b_gain (float)\n",
      "[ 86]         üî¢ lookback_b_sharpe (float)\n",
      "[ 87]         üî¢ lookback_b_sharpe_atrp (float)\n",
      "[ 88]         üî¢ lookback_b_sharpe_trp (float)\n",
      "[ 89]         üî¢ lookback_p_gain (float)\n",
      "[ 90]         üî¢ lookback_p_sharpe (float)\n",
      "[ 91]         üî¢ lookback_p_sharpe_atrp (float)\n",
      "[ 92]         üî¢ lookback_p_sharpe_trp (float)\n",
      "[ 93]       üßÆ results_df (DataFrame shape=(10, 3))\n",
      "[ 94]       üìÖ start_date (Timestamp)\n",
      "[ 95]       üìÖ decision_date (Timestamp)\n",
      "[ 96]       üìÖ buy_date (Timestamp)\n",
      "[ 97]       üìÖ holding_end_date (Timestamp)\n",
      "            ‚ï∞‚îÄ‚îÄ portfolio_atrp_series --> [See ID 36]\n",
      "            ‚ï∞‚îÄ‚îÄ benchmark_atrp_series --> [See ID 23]\n",
      "            ‚ï∞‚îÄ‚îÄ portfolio_trp_series --> [See ID 38]\n",
      "            ‚ï∞‚îÄ‚îÄ benchmark_trp_series --> [See ID 25]\n",
      "[ 98]       üìÇ error_msg (NoneType)\n",
      "            ‚ï∞‚îÄ‚îÄ debug_data --> [See ID 2]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# inspect_audit(audit_pack)\n",
    "print(inspect_audit(audit_pack))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea81de21",
   "metadata": {},
   "source": [
    "#### üìÇ CHAPTER 1: The Capital Audit (Weight Drift)\n",
    "*Goal: Prove the portfolio value and drifted weights are correct before checking volatility.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52e5d7dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìå FETCHING ID [19]: audit_pack -> index_0 -> debug -> portfolio_raw_components -> prices\n",
      "   [Type: DataFrame | Shape: (17, 10)]\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "üìå FETCHING ID [67]: audit_pack -> index_0 -> results -> initial_weights\n",
      "   [Type: Series | Shape: (10,)]\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "üìå FETCHING ID [39]: audit_pack -> index_0 -> debug -> verification -> portfolio -> full_val\n",
      "   [Type: Series | Shape: (17,)]\n",
      "---------------------------------------------------------------------------\n",
      "üìù CHAPTER 1: CAPITAL & WEIGHT DRIFT\n",
      "Date,RAW_Price_RPRX,RAW_Price_SGOV,RAW_Price_SHV,RAW_Price_BIL,RAW_Price_SNX,RAW_Price_MINT,RAW_Price_TRGP,RAW_Price_QRVO,RAW_Price_LNG,RAW_Price_USFR,DRIFT_Weight_RPRX,DRIFT_Weight_SGOV,DRIFT_Weight_SHV,DRIFT_Weight_BIL,DRIFT_Weight_SNX,DRIFT_Weight_MINT,DRIFT_Weight_TRGP,DRIFT_Weight_QRVO,DRIFT_Weight_LNG,DRIFT_Weight_USFR,MANUAL_Equity,ENGINE_Equity,DELTA_Equity\n",
      "2025-01-02,1.00000000,1.00000000,1.00000000,1.00000000,1.00000000,1.00000000,1.00000000,1.00000000,1.00000000,1.00000000,0.10000000,0.10000000,0.10000000,0.10000000,0.10000000,0.10000000,0.10000000,0.10000000,0.10000000,0.10000000,1.00000000,1.00000000,0.00000000\n",
      "2025-01-03,1.00193989,1.00029902,1.00027431,1.00010939,1.01245041,1.00049801,1.01699117,1.01247133,1.00852320,1.00019881,0.09965827,0.09949506,0.09949260,0.09947620,0.10070370,0.09951485,0.10115535,0.10070578,0.10031308,0.09948509,1.00537555,1.00537555,0.00000000\n",
      "2025-01-06,0.99844968,1.00029902,1.00045403,1.00021764,1.01562609,1.00069742,1.01693534,1.03411697,1.01500595,1.00039762,0.09903094,0.09921436,0.09922974,0.09920629,0.10073457,0.09925388,0.10086443,0.10256859,0.10067306,0.09922414,1.00821997,1.00821997,0.00000000\n",
      "2025-01-07,1.00969546,1.00049836,1.00063374,1.00043755,1.05794529,1.00059719,1.02600325,1.05547592,1.01319425,1.00059642,0.09932983,0.09842506,0.09843838,0.09841908,0.10407646,0.09843478,0.10093413,0.10383353,0.09967403,0.09843471,1.01650775,1.01650775,0.00000000\n",
      "2025-01-08,1.01589674,1.00069771,1.00063374,1.00065519,1.04824424,1.00099602,1.04872887,1.04759174,1.02080703,1.00079523,0.09974395,0.09825166,0.09824538,0.09824748,0.10291993,0.09828095,0.10296751,0.10285586,0.10022605,0.09826123,1.01850465,1.01850465,0.00000000\n",
      "2025-01-10,1.14346818,1.00109640,1.00090805,1.00098450,1.15050115,1.00109625,1.05594300,1.02766628,1.02094428,1.00139165,0.10990659,0.09622226,0.09620416,0.09621151,0.11058258,0.09622225,0.10149395,0.09877608,0.09812998,0.09625064,1.04039997,1.04039997,0.00000000\n",
      "2025-01-13,1.16285513,1.00129575,1.00108777,1.00109275,1.14045208,1.00129566,1.07871888,1.03081995,1.04007228,1.00159046,0.11117926,0.09573275,0.09571287,0.09571335,0.10903733,0.09573275,0.10313509,0.09855553,0.09944013,0.09576093,1.04592807,1.04592807,0.00000000\n",
      "2025-01-14,1.16440545,1.00139542,1.00118236,1.00131152,1.16792824,1.00149507,1.10008208,1.04386468,1.10194894,1.00198600,0.10999901,0.09459978,0.09457965,0.09459186,0.11033180,0.09460919,0.10392251,0.09861177,0.10409887,0.09465557,1.05855998,1.05855998,0.00000000\n",
      "2025-01-15,1.19193754,1.00149509,1.00136208,1.00142091,1.16981625,1.00179367,1.12258436,1.06307339,1.11459877,1.00198600,0.11170852,0.09386023,0.09384777,0.09385328,0.10963531,0.09388822,0.10520873,0.09963136,0.10446032,0.09390624,1.06700681,1.06700681,0.00000000\n",
      "2025-01-16,1.20007871,1.00149509,1.00145666,1.00142091,1.17762929,1.00209331,1.16328388,1.05490252,1.14986733,1.00198600,0.11159149,0.09312583,0.09312226,0.09311893,0.10950399,0.09318146,0.10817005,0.09809202,0.10692249,0.09317148,1.07542137,1.07542137,0.00000000\n",
      "2025-01-17,1.19232313,1.00199346,1.00191069,1.00196786,1.17702896,1.00259132,1.17841568,1.20713876,1.14578644,1.00238362,0.10927176,0.09182879,0.09182120,0.09182644,0.10787010,0.09188358,0.10799719,0.11062955,0.10500685,0.09186454,1.09115399,1.09115399,0.00000000\n",
      "2025-01-21,1.17487607,1.00209313,1.00209041,1.00196786,1.20853344,1.00259132,1.18660696,1.25544725,1.13159484,1.00258243,0.10711479,0.09136197,0.09136172,0.09135055,0.11018337,0.09140739,0.10818430,0.11446055,0.10316879,0.09140658,1.09683837,1.09683837,0.00000000\n",
      "2025-01-22,1.16634534,1.00229248,1.00209041,1.00218663,1.21479780,1.00279073,1.15410429,1.24842317,1.05784152,1.00238362,0.10746502,0.09234947,0.09233086,0.09233972,0.11192934,0.09239538,0.10633715,0.11502752,0.09746767,0.09235787,1.08532560,1.08532560,0.00000000\n",
      "2025-01-23,1.18728261,1.00239215,1.00227013,1.00218663,1.22931022,1.00299014,1.15476317,1.29830849,1.06205966,1.00258243,0.10848564,0.09159163,0.09158048,0.09157285,0.11232583,0.09164627,0.10551424,0.11863041,0.09704363,0.09160902,1.09441456,1.09441456,0.00000000\n",
      "2025-01-24,1.20434408,1.00269117,1.00263902,1.00262419,1.22715250,1.00348815,1.14208819,1.27494266,1.05104310,1.00298004,0.11034862,0.09187207,0.09186729,0.09186594,0.11243845,0.09194510,0.10464439,0.11681725,0.09630234,0.09189854,1.09139931,1.09139931,0.00000000\n",
      "2025-01-27,1.22993628,1.00269117,1.00281874,1.00273358,1.21587666,1.00348815,1.08838983,1.25616399,1.01749931,1.00317885,0.11364332,0.09264639,0.09265818,0.09265031,0.11234425,0.09272003,0.10056475,0.11606670,0.09401463,0.09269145,1.08227766,1.08227766,0.00000000\n",
      "2025-01-28,1.23846701,1.00299019,1.00290387,1.00273358,1.22104476,1.00348815,1.10548709,1.22964450,1.02661268,1.00341908,0.11428356,0.09255417,0.09254620,0.09253049,0.11267586,0.09260012,0.10201240,0.11346943,0.09473401,0.09259375,1.08367909,1.08367909,0.00000000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Fetch\n",
    "prices = get_audit(19)\n",
    "init_weights = get_audit(67)\n",
    "stored_equity = get_audit(39)\n",
    "\n",
    "# 2. Manual Math\n",
    "norm_prices = prices.div(prices.bfill().iloc[0])\n",
    "weighted_vals = norm_prices.mul(init_weights, axis=1)\n",
    "manual_equity = weighted_vals.sum(axis=1)\n",
    "manual_weights = weighted_vals.div(manual_equity, axis=0)\n",
    "\n",
    "# 3. Align for Verification\n",
    "df_cap_audit = pd.concat(\n",
    "    [\n",
    "        norm_prices.add_prefix(\"RAW_Price_\"),\n",
    "        manual_weights.add_prefix(\"DRIFT_Weight_\"),\n",
    "        manual_equity.to_frame(\"MANUAL_Equity\"),\n",
    "        stored_equity.to_frame(\"ENGINE_Equity\"),\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "df_cap_audit[\"DELTA_Equity\"] = (\n",
    "    df_cap_audit[\"MANUAL_Equity\"] - df_cap_audit[\"ENGINE_Equity\"]\n",
    ")\n",
    "\n",
    "print(\"üìù CHAPTER 1: CAPITAL & WEIGHT DRIFT\")\n",
    "print(df_cap_audit.to_csv(float_format=\"%.8f\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73fdf18",
   "metadata": {},
   "source": [
    "#### üìÇ CHAPTER 2: The TRP Audit (Instant Risk)\n",
    "*Goal: Verify the daily True Range calculation and weighted portfolio risk.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f82aa0c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìå FETCHING ID [18]: audit_pack -> index_0 -> debug -> portfolio_raw_components -> ohlcv_raw\n",
      "   [Type: DataFrame | Shape: (170, 5)]\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "üìå FETCHING ID [38]: audit_pack -> index_0 -> debug -> verification -> portfolio -> full_trp\n",
      "   [Type: Series | Shape: (17,)]\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "üìù CHAPTER 2: TRP (INSTANT RISK) AUDIT\n",
      "Date,MANUAL_TickerTRP_RPRX,MANUAL_TickerTRP_SGOV,MANUAL_TickerTRP_SHV,MANUAL_TickerTRP_BIL,MANUAL_TickerTRP_SNX,MANUAL_TickerTRP_MINT,MANUAL_TickerTRP_TRGP,MANUAL_TickerTRP_QRVO,MANUAL_TickerTRP_LNG,MANUAL_TickerTRP_USFR,MANUAL_PortfolioTRP,ENGINE_PortfolioTRP,DELTA_TRP\n",
      "2025-01-02,0.01318567,0.00009967,0.00008513,0.00010939,0.02008944,0.00029964,0.02201649,0.02551606,0.02606826,0.00039762,0.01078674,0.01211415,-0.00132742\n",
      "2025-01-03,0.00909744,0.00039857,0.00036879,0.00032813,0.01831275,0.00069707,0.02444327,0.02548492,0.01869871,0.00039754,0.00988348,0.00988348,0.00000000\n",
      "2025-01-06,0.02698958,0.00019929,0.00017964,0.00021759,0.02603399,0.00019927,0.01750986,0.03105073,0.01357613,0.00019873,0.01171171,0.01171171,0.00000000\n",
      "2025-01-07,0.01958661,0.00029887,0.00017960,0.00032916,0.04016579,0.00029842,0.02715646,0.02797773,0.01937109,0.00039738,0.01385067,0.01385067,0.00000000\n",
      "2025-01-08,0.02309438,0.00019921,0.00008508,0.00021750,0.02751471,0.00039843,0.02625386,0.02463054,0.02131512,0.00019865,0.01261633,0.01261633,0.00000000\n",
      "2025-01-10,0.12496002,0.00049782,0.00036856,0.00043826,0.09238169,0.00059654,0.02503807,0.02524759,0.02646555,0.00059559,0.03182212,0.03182212,0.00000000\n",
      "2025-01-13,0.04301454,0.00019909,0.00017952,0.00021740,0.02898252,0.00029821,0.02633145,0.02593520,0.02615037,0.00019849,0.01591926,0.01591926,0.00000000\n",
      "2025-01-14,0.01764652,0.00009953,0.00009448,0.00021849,0.02653516,0.00029815,0.03322048,0.01750893,0.05750963,0.00059317,0.01615777,0.01615777,0.00000000\n",
      "2025-01-15,0.02635028,0.00009952,0.00017947,0.00010923,0.02620227,0.00039811,0.02160191,0.03000270,0.02245217,0.00019841,0.01351597,0.01351597,0.00000000\n",
      "2025-01-16,0.03160060,0.00019905,0.00009445,0.00010923,0.01785714,0.00039799,0.03568754,0.01902432,0.03824332,0.00019841,0.01539039,0.01539039,0.00000000\n",
      "2025-01-17,0.01824353,0.00059685,0.00045316,0.00054587,0.01298759,0.00059669,0.03189352,0.12931956,0.02476781,0.00039667,0.02398405,0.02398405,0.00000000\n",
      "2025-01-21,0.01798662,0.00019893,0.00017934,0.00010917,0.03572971,0.00009997,0.01717542,0.05617721,0.02479947,0.00039659,0.01680009,0.01680009,0.00000000\n",
      "2025-01-22,0.01911679,0.00019889,0.00009439,0.00021830,0.01441729,0.00039771,0.04676524,0.02698358,0.07211653,0.00019833,0.01887618,0.01887618,-0.00000000\n",
      "2025-01-23,0.01975733,0.00009944,0.00017931,0.00010915,0.01990205,0.00029875,0.02095643,0.04598653,0.01702815,0.00039659,0.01379721,0.01379721,0.00000000\n",
      "2025-01-24,0.01577405,0.00039762,0.00045283,0.00043641,0.01690962,0.00049628,0.02200548,0.03241511,0.03429139,0.00059465,0.01325217,0.01325217,0.00000000\n",
      "2025-01-27,0.02553619,0.00019881,0.00017921,0.00010909,0.01459055,0.00019872,0.06058290,0.04598882,0.04445943,0.00039636,0.02025159,0.02025159,0.00000000\n",
      "2025-01-28,0.01624779,0.00029813,0.00008488,0.00010909,0.01219877,0.00009988,0.02050660,0.04534857,0.02128371,0.00039832,0.01257693,0.01257693,-0.00000000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Fetch\n",
    "ohlcv = get_audit(18)  # Raw High/Low/Close\n",
    "stored_trp = get_audit(38)  # Engine Portfolio TRP\n",
    "\n",
    "# 2. Manual Ticker Math (Vectorized)\n",
    "df_t = ohlcv.copy()\n",
    "df_t[\"PC\"] = df_t.groupby(level=\"Ticker\")[\"Adj Close\"].shift(1)\n",
    "# True Range = max(H-L, |H-PC|, |L-PC|)\n",
    "tr_m = pd.concat(\n",
    "    [\n",
    "        df_t[\"Adj High\"] - df_t[\"Adj Low\"],\n",
    "        (df_t[\"Adj High\"] - df_t[\"PC\"]).abs(),\n",
    "        (df_t[\"Adj Low\"] - df_t[\"PC\"]).abs(),\n",
    "    ],\n",
    "    axis=1,\n",
    ").max(axis=1)\n",
    "\n",
    "ticker_trp = (tr_m / df_t[\"Adj Close\"]).unstack(level=\"Ticker\")\n",
    "\n",
    "# 3. Manual Portfolio Math (Using weights from Chapter 1)\n",
    "manual_port_trp = (manual_weights * ticker_trp).sum(axis=1, min_count=1)\n",
    "\n",
    "# 4. Create Calculation Trail\n",
    "df_trp_audit = pd.concat(\n",
    "    [\n",
    "        ticker_trp.add_prefix(\"MANUAL_TickerTRP_\"),\n",
    "        manual_port_trp.to_frame(\"MANUAL_PortfolioTRP\"),\n",
    "        stored_trp.to_frame(\"ENGINE_PortfolioTRP\"),\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "df_trp_audit[\"DELTA_TRP\"] = (\n",
    "    df_trp_audit[\"MANUAL_PortfolioTRP\"] - df_trp_audit[\"ENGINE_PortfolioTRP\"]\n",
    ")\n",
    "\n",
    "print(\"\\nüìù CHAPTER 2: TRP (INSTANT RISK) AUDIT\")\n",
    "print(df_trp_audit.to_csv(float_format=\"%.8f\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d13f85c",
   "metadata": {},
   "source": [
    "#### üìÇ CHAPTER 3: The ATRP Audit (Smoothed Risk)\n",
    "*Goal: Verify the Wilder's Smoothing (EMA) logic independently.*\n",
    "\n",
    "To truly verify the **AlphaEngine's** logic for ATRP, we shouldn't try to recalculate the ticker-level ATRP from raw OHLCV (because we lack the history). Instead, we should use the **Engine's own ticker-level ATRP** and see if our **Weighting/Drift math** matches the Portfolio Total.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5096bb40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìå FETCHING ID [36]: audit_pack -> index_0 -> debug -> verification -> portfolio -> full_atrp\n",
      "   [Type: Series | Shape: (17,)]\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "üìå FETCHING ID [17]: audit_pack -> index_0 -> debug -> portfolio_raw_components -> atrp\n",
      "   [Type: DataFrame | Shape: (17, 10)]\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "üìù CHAPTER 3: ATRP (SMOOTHED RISK) AUDIT - WARM START ALIGNED\n",
      "Date,ENGINE_TickerATRP_RPRX,ENGINE_TickerATRP_SGOV,ENGINE_TickerATRP_SHV,ENGINE_TickerATRP_BIL,ENGINE_TickerATRP_SNX,ENGINE_TickerATRP_MINT,ENGINE_TickerATRP_TRGP,ENGINE_TickerATRP_QRVO,ENGINE_TickerATRP_LNG,ENGINE_TickerATRP_USFR,MANUAL_PortfolioATRP,ENGINE_PortfolioATRP,DELTA_ATRP\n",
      "2025-01-02,0.01840948,0.00024401,0.00025458,0.00024206,0.02136244,0.00034734,0.02449333,0.02976644,0.01927523,0.00035705,0.01147520,0.01147520,0.00000000\n",
      "2025-01-03,0.01771124,0.00025498,0.00026267,0.00024818,0.02090067,0.00037216,0.02410977,0.02912015,0.01908279,0.00035988,0.01130453,0.01130453,0.00000000\n",
      "2025-01-06,0.01843147,0.00025100,0.00025670,0.00024597,0.02120665,0.00035974,0.02363957,0.02869206,0.01857628,0.00034830,0.01130400,0.01130400,0.00000000\n",
      "2025-01-07,0.01832335,0.00025437,0.00025115,0.00025187,0.02177318,0.00035539,0.02369677,0.02810189,0.01866390,0.00035174,0.01140032,0.01140032,0.00000000\n",
      "2025-01-08,0.01856028,0.00025039,0.00023929,0.00024936,0.02237040,0.00035834,0.02340260,0.02805032,0.01872402,0.00034074,0.01146643,0.01146643,0.00000000\n",
      "2025-01-10,0.02423748,0.00026797,0.00024846,0.00026278,0.02552493,0.00037532,0.02337095,0.02835515,0.01927465,0.00035876,0.01269633,0.01269633,0.00000000\n",
      "2025-01-13,0.02520348,0.00026300,0.00024349,0.00025951,0.02598075,0.00036974,0.02312422,0.02810174,0.01943662,0.00034724,0.01286423,0.01286423,0.00000000\n",
      "2025-01-14,0.02463254,0.00025130,0.00023283,0.00025653,0.02545280,0.00036456,0.02342839,0.02701902,0.02114267,0.00036468,0.01295693,0.01295693,0.00000000\n",
      "2025-01-15,0.02422690,0.00024043,0.00022898,0.00024598,0.02546819,0.00036685,0.02286185,0.02677881,0.02101339,0.00035281,0.01290162,0.01290162,0.00000000\n",
      "2025-01-16,0.02460098,0.00023748,0.00021935,0.00023621,0.02476764,0.00036898,0.02303524,0.02641752,0.02164562,0.00034178,0.01298565,0.01298565,0.00000000\n",
      "2025-01-17,0.02429546,0.00026304,0.00023596,0.00025821,0.02393794,0.00038507,0.02339331,0.03067403,0.02194022,0.00034557,0.01359739,0.01359739,0.00000000\n",
      "2025-01-21,0.02417985,0.00025843,0.00023187,0.00024756,0.02420076,0.00036471,0.02279922,0.03139969,0.02239996,0.00034915,0.01376072,0.01376072,0.00000000\n",
      "2025-01-22,0.02398243,0.00025413,0.00022205,0.00024542,0.02338606,0.00036700,0.02510731,0.03124830,0.02740132,0.00033845,0.01426166,0.01426166,0.00000000\n",
      "2025-01-23,0.02328792,0.00024306,0.00021896,0.00023569,0.02288084,0.00036205,0.02479751,0.03118613,0.02655933,0.00034254,0.01411850,0.01411850,0.00000000\n",
      "2025-01-24,0.02244487,0.00025403,0.00023559,0.00024993,0.02249168,0.00037148,0.02485363,0.03180463,0.02737012,0.00036042,0.01409284,0.01409284,0.00000000\n",
      "2025-01-27,0.02223201,0.00025009,0.00023153,0.00023985,0.02212100,0.00035914,0.02854435,0.03325928,0.02942864,0.00036292,0.01464304,0.01464304,0.00000000\n",
      "2025-01-28,0.02166237,0.00025345,0.00022104,0.00023051,0.02132533,0.00034062,0.02756029,0.03478886,0.02860428,0.00036537,0.01447788,0.01447788,0.00000000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### üìÇ CHAPTER 3: ATRP (SMOOTHED RISK) AUDIT - FIXED FOR \"WARM START\"\n",
    "# 1. Fetch\n",
    "stored_atrp_port = get_audit(36)  # The Portfolio Total ATRP\n",
    "# --- PINPOINT: Use the Engine's pre-calculated Ticker ATRP to avoid cold start ---\n",
    "ticker_atrp_engine = get_audit(17)\n",
    "\n",
    "# 2. Manual Portfolio Math (Using drifted weights from Chapter 1)\n",
    "# This proves the WEIGHTING logic is correct, even if we accept the Engine's Ticker ATRP\n",
    "manual_port_atrp = (manual_weights * ticker_atrp_engine).sum(axis=1, min_count=1)\n",
    "\n",
    "# 3. Create Calculation Trail\n",
    "df_atrp_audit = pd.concat(\n",
    "    [\n",
    "        ticker_atrp_engine.add_prefix(\"ENGINE_TickerATRP_\"),\n",
    "        manual_port_atrp.to_frame(\"MANUAL_PortfolioATRP\"),\n",
    "        stored_atrp_port.to_frame(\"ENGINE_PortfolioATRP\"),\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "df_atrp_audit[\"DELTA_ATRP\"] = (\n",
    "    df_atrp_audit[\"MANUAL_PortfolioATRP\"] - df_atrp_audit[\"ENGINE_PortfolioATRP\"]\n",
    ")\n",
    "\n",
    "print(\"\\nüìù CHAPTER 3: ATRP (SMOOTHED RISK) AUDIT - WARM START ALIGNED\")\n",
    "print(df_atrp_audit.to_csv(float_format=\"%.8f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb016682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìå FETCHING ID [18]: audit_pack -> index_0 -> debug -> portfolio_raw_components -> ohlcv_raw\n",
      "   [Type: DataFrame | Shape: (170, 5)]\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "üìå FETCHING ID [19]: audit_pack -> index_0 -> debug -> portfolio_raw_components -> prices\n",
      "   [Type: DataFrame | Shape: (17, 10)]\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "üìå FETCHING ID [12]: audit_pack -> index_0 -> debug -> benchmark_raw_components -> ohlcv_raw\n",
      "   [Type: DataFrame | Shape: (17, 5)]\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "üìå FETCHING ID [13]: audit_pack -> index_0 -> debug -> benchmark_raw_components -> prices\n",
      "   [Type: DataFrame | Shape: (17, 1)]\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "üìå FETCHING ID [17]: audit_pack -> index_0 -> debug -> portfolio_raw_components -> atrp\n",
      "   [Type: DataFrame | Shape: (17, 10)]\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "üìå FETCHING ID [11]: audit_pack -> index_0 -> debug -> benchmark_raw_components -> atrp\n",
      "   [Type: DataFrame | Shape: (17, 1)]\n",
      "---------------------------------------------------------------------------\n",
      "üìù CHAPTER 4: FINAL INTEGRITY REPORT (MODULAR)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Period</th>\n",
       "      <th>Full</th>\n",
       "      <th>Holding</th>\n",
       "      <th>Lookback</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Entity</th>\n",
       "      <th>Metric</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">Benchmark</th>\n",
       "      <th>Gain</th>\n",
       "      <td>‚úÖ PASS</td>\n",
       "      <td>‚úÖ PASS</td>\n",
       "      <td>‚úÖ PASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sharpe</th>\n",
       "      <td>‚úÖ PASS</td>\n",
       "      <td>‚úÖ PASS</td>\n",
       "      <td>‚úÖ PASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sharpe (ATRP)</th>\n",
       "      <td>‚úÖ PASS</td>\n",
       "      <td>‚úÖ PASS</td>\n",
       "      <td>‚úÖ PASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sharpe (TRP)</th>\n",
       "      <td>‚úÖ PASS</td>\n",
       "      <td>‚úÖ PASS</td>\n",
       "      <td>‚úÖ PASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">Group</th>\n",
       "      <th>Gain</th>\n",
       "      <td>‚úÖ PASS</td>\n",
       "      <td>‚úÖ PASS</td>\n",
       "      <td>‚úÖ PASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sharpe</th>\n",
       "      <td>‚úÖ PASS</td>\n",
       "      <td>‚úÖ PASS</td>\n",
       "      <td>‚úÖ PASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sharpe (ATRP)</th>\n",
       "      <td>‚úÖ PASS</td>\n",
       "      <td>‚úÖ PASS</td>\n",
       "      <td>‚úÖ PASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sharpe (TRP)</th>\n",
       "      <td>‚úÖ PASS</td>\n",
       "      <td>‚úÖ PASS</td>\n",
       "      <td>‚úÖ PASS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Period                     Full Holding Lookback\n",
       "Entity    Metric                                \n",
       "Benchmark Gain           ‚úÖ PASS  ‚úÖ PASS   ‚úÖ PASS\n",
       "          Sharpe         ‚úÖ PASS  ‚úÖ PASS   ‚úÖ PASS\n",
       "          Sharpe (ATRP)  ‚úÖ PASS  ‚úÖ PASS   ‚úÖ PASS\n",
       "          Sharpe (TRP)   ‚úÖ PASS  ‚úÖ PASS   ‚úÖ PASS\n",
       "Group     Gain           ‚úÖ PASS  ‚úÖ PASS   ‚úÖ PASS\n",
       "          Sharpe         ‚úÖ PASS  ‚úÖ PASS   ‚úÖ PASS\n",
       "          Sharpe (ATRP)  ‚úÖ PASS  ‚úÖ PASS   ‚úÖ PASS\n",
       "          Sharpe (TRP)   ‚úÖ PASS  ‚úÖ PASS   ‚úÖ PASS"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_3d427_row0_col5, #T_3d427_row1_col5, #T_3d427_row2_col5, #T_3d427_row3_col5, #T_3d427_row4_col5, #T_3d427_row5_col5, #T_3d427_row6_col5, #T_3d427_row7_col5, #T_3d427_row8_col5, #T_3d427_row9_col5, #T_3d427_row10_col5, #T_3d427_row11_col5, #T_3d427_row12_col5, #T_3d427_row13_col5, #T_3d427_row14_col5, #T_3d427_row15_col5, #T_3d427_row16_col5, #T_3d427_row17_col5, #T_3d427_row18_col5, #T_3d427_row19_col5, #T_3d427_row20_col5, #T_3d427_row21_col5, #T_3d427_row22_col5, #T_3d427_row23_col5 {\n",
       "  color: gray;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_3d427\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_3d427_level0_col0\" class=\"col_heading level0 col0\" >Entity</th>\n",
       "      <th id=\"T_3d427_level0_col1\" class=\"col_heading level0 col1\" >Period</th>\n",
       "      <th id=\"T_3d427_level0_col2\" class=\"col_heading level0 col2\" >Metric</th>\n",
       "      <th id=\"T_3d427_level0_col3\" class=\"col_heading level0 col3\" >Engine</th>\n",
       "      <th id=\"T_3d427_level0_col4\" class=\"col_heading level0 col4\" >Manual</th>\n",
       "      <th id=\"T_3d427_level0_col5\" class=\"col_heading level0 col5\" >Delta</th>\n",
       "      <th id=\"T_3d427_level0_col6\" class=\"col_heading level0 col6\" >Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_3d427_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_3d427_row0_col0\" class=\"data row0 col0\" >Group</td>\n",
       "      <td id=\"T_3d427_row0_col1\" class=\"data row0 col1\" >Full</td>\n",
       "      <td id=\"T_3d427_row0_col2\" class=\"data row0 col2\" >Gain</td>\n",
       "      <td id=\"T_3d427_row0_col3\" class=\"data row0 col3\" >0.08367909</td>\n",
       "      <td id=\"T_3d427_row0_col4\" class=\"data row0 col4\" >0.08367909</td>\n",
       "      <td id=\"T_3d427_row0_col5\" class=\"data row0 col5\" >0.00000000</td>\n",
       "      <td id=\"T_3d427_row0_col6\" class=\"data row0 col6\" >‚úÖ PASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3d427_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_3d427_row1_col0\" class=\"data row1 col0\" >Benchmark</td>\n",
       "      <td id=\"T_3d427_row1_col1\" class=\"data row1 col1\" >Full</td>\n",
       "      <td id=\"T_3d427_row1_col2\" class=\"data row1 col2\" >Gain</td>\n",
       "      <td id=\"T_3d427_row1_col3\" class=\"data row1 col3\" >0.03400340</td>\n",
       "      <td id=\"T_3d427_row1_col4\" class=\"data row1 col4\" >0.03400340</td>\n",
       "      <td id=\"T_3d427_row1_col5\" class=\"data row1 col5\" >0.00000000</td>\n",
       "      <td id=\"T_3d427_row1_col6\" class=\"data row1 col6\" >‚úÖ PASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3d427_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_3d427_row2_col0\" class=\"data row2 col0\" >Group</td>\n",
       "      <td id=\"T_3d427_row2_col1\" class=\"data row2 col1\" >Full</td>\n",
       "      <td id=\"T_3d427_row2_col2\" class=\"data row2 col2\" >Sharpe</td>\n",
       "      <td id=\"T_3d427_row2_col3\" class=\"data row2 col3\" >10.04809914</td>\n",
       "      <td id=\"T_3d427_row2_col4\" class=\"data row2 col4\" >10.04809914</td>\n",
       "      <td id=\"T_3d427_row2_col5\" class=\"data row2 col5\" >0.00000000</td>\n",
       "      <td id=\"T_3d427_row2_col6\" class=\"data row2 col6\" >‚úÖ PASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3d427_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_3d427_row3_col0\" class=\"data row3 col0\" >Benchmark</td>\n",
       "      <td id=\"T_3d427_row3_col1\" class=\"data row3 col1\" >Full</td>\n",
       "      <td id=\"T_3d427_row3_col2\" class=\"data row3 col2\" >Sharpe</td>\n",
       "      <td id=\"T_3d427_row3_col3\" class=\"data row3 col3\" >3.57519881</td>\n",
       "      <td id=\"T_3d427_row3_col4\" class=\"data row3 col4\" >3.57519881</td>\n",
       "      <td id=\"T_3d427_row3_col5\" class=\"data row3 col5\" >0.00000000</td>\n",
       "      <td id=\"T_3d427_row3_col6\" class=\"data row3 col6\" >‚úÖ PASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3d427_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_3d427_row4_col0\" class=\"data row4 col0\" >Group</td>\n",
       "      <td id=\"T_3d427_row4_col1\" class=\"data row4 col1\" >Full</td>\n",
       "      <td id=\"T_3d427_row4_col2\" class=\"data row4 col2\" >Sharpe (ATRP)</td>\n",
       "      <td id=\"T_3d427_row4_col3\" class=\"data row4 col3\" >0.38807339</td>\n",
       "      <td id=\"T_3d427_row4_col4\" class=\"data row4 col4\" >0.38807339</td>\n",
       "      <td id=\"T_3d427_row4_col5\" class=\"data row4 col5\" >0.00000000</td>\n",
       "      <td id=\"T_3d427_row4_col6\" class=\"data row4 col6\" >‚úÖ PASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3d427_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_3d427_row5_col0\" class=\"data row5 col0\" >Benchmark</td>\n",
       "      <td id=\"T_3d427_row5_col1\" class=\"data row5 col1\" >Full</td>\n",
       "      <td id=\"T_3d427_row5_col2\" class=\"data row5 col2\" >Sharpe (ATRP)</td>\n",
       "      <td id=\"T_3d427_row5_col3\" class=\"data row5 col3\" >0.17446186</td>\n",
       "      <td id=\"T_3d427_row5_col4\" class=\"data row5 col4\" >0.17446186</td>\n",
       "      <td id=\"T_3d427_row5_col5\" class=\"data row5 col5\" >0.00000000</td>\n",
       "      <td id=\"T_3d427_row5_col6\" class=\"data row5 col6\" >‚úÖ PASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3d427_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_3d427_row6_col0\" class=\"data row6 col0\" >Group</td>\n",
       "      <td id=\"T_3d427_row6_col1\" class=\"data row6 col1\" >Full</td>\n",
       "      <td id=\"T_3d427_row6_col2\" class=\"data row6 col2\" >Sharpe (TRP)</td>\n",
       "      <td id=\"T_3d427_row6_col3\" class=\"data row6 col3\" >0.31121478</td>\n",
       "      <td id=\"T_3d427_row6_col4\" class=\"data row6 col4\" >0.31121478</td>\n",
       "      <td id=\"T_3d427_row6_col5\" class=\"data row6 col5\" >0.00000000</td>\n",
       "      <td id=\"T_3d427_row6_col6\" class=\"data row6 col6\" >‚úÖ PASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3d427_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_3d427_row7_col0\" class=\"data row7 col0\" >Benchmark</td>\n",
       "      <td id=\"T_3d427_row7_col1\" class=\"data row7 col1\" >Full</td>\n",
       "      <td id=\"T_3d427_row7_col2\" class=\"data row7 col2\" >Sharpe (TRP)</td>\n",
       "      <td id=\"T_3d427_row7_col3\" class=\"data row7 col3\" >0.17062179</td>\n",
       "      <td id=\"T_3d427_row7_col4\" class=\"data row7 col4\" >0.17062179</td>\n",
       "      <td id=\"T_3d427_row7_col5\" class=\"data row7 col5\" >0.00000000</td>\n",
       "      <td id=\"T_3d427_row7_col6\" class=\"data row7 col6\" >‚úÖ PASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3d427_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_3d427_row8_col0\" class=\"data row8 col0\" >Group</td>\n",
       "      <td id=\"T_3d427_row8_col1\" class=\"data row8 col1\" >Lookback</td>\n",
       "      <td id=\"T_3d427_row8_col2\" class=\"data row8 col2\" >Gain</td>\n",
       "      <td id=\"T_3d427_row8_col3\" class=\"data row8 col3\" >0.09115399</td>\n",
       "      <td id=\"T_3d427_row8_col4\" class=\"data row8 col4\" >0.09115399</td>\n",
       "      <td id=\"T_3d427_row8_col5\" class=\"data row8 col5\" >0.00000000</td>\n",
       "      <td id=\"T_3d427_row8_col6\" class=\"data row8 col6\" >‚úÖ PASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3d427_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_3d427_row9_col0\" class=\"data row9 col0\" >Benchmark</td>\n",
       "      <td id=\"T_3d427_row9_col1\" class=\"data row9 col1\" >Lookback</td>\n",
       "      <td id=\"T_3d427_row9_col2\" class=\"data row9 col2\" >Gain</td>\n",
       "      <td id=\"T_3d427_row9_col3\" class=\"data row9 col3\" >0.02213362</td>\n",
       "      <td id=\"T_3d427_row9_col4\" class=\"data row9 col4\" >0.02213362</td>\n",
       "      <td id=\"T_3d427_row9_col5\" class=\"data row9 col5\" >0.00000000</td>\n",
       "      <td id=\"T_3d427_row9_col6\" class=\"data row9 col6\" >‚úÖ PASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3d427_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_3d427_row10_col0\" class=\"data row10 col0\" >Group</td>\n",
       "      <td id=\"T_3d427_row10_col1\" class=\"data row10 col1\" >Lookback</td>\n",
       "      <td id=\"T_3d427_row10_col2\" class=\"data row10 col2\" >Sharpe</td>\n",
       "      <td id=\"T_3d427_row10_col3\" class=\"data row10 col3\" >23.58765760</td>\n",
       "      <td id=\"T_3d427_row10_col4\" class=\"data row10 col4\" >23.58765760</td>\n",
       "      <td id=\"T_3d427_row10_col5\" class=\"data row10 col5\" >0.00000000</td>\n",
       "      <td id=\"T_3d427_row10_col6\" class=\"data row10 col6\" >‚úÖ PASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3d427_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_3d427_row11_col0\" class=\"data row11 col0\" >Benchmark</td>\n",
       "      <td id=\"T_3d427_row11_col1\" class=\"data row11 col1\" >Lookback</td>\n",
       "      <td id=\"T_3d427_row11_col2\" class=\"data row11 col2\" >Sharpe</td>\n",
       "      <td id=\"T_3d427_row11_col3\" class=\"data row11 col3\" >3.47348826</td>\n",
       "      <td id=\"T_3d427_row11_col4\" class=\"data row11 col4\" >3.47348826</td>\n",
       "      <td id=\"T_3d427_row11_col5\" class=\"data row11 col5\" >0.00000000</td>\n",
       "      <td id=\"T_3d427_row11_col6\" class=\"data row11 col6\" >‚úÖ PASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3d427_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_3d427_row12_col0\" class=\"data row12 col0\" >Group</td>\n",
       "      <td id=\"T_3d427_row12_col1\" class=\"data row12 col1\" >Lookback</td>\n",
       "      <td id=\"T_3d427_row12_col2\" class=\"data row12 col2\" >Sharpe (ATRP)</td>\n",
       "      <td id=\"T_3d427_row12_col3\" class=\"data row12 col3\" >0.71083893</td>\n",
       "      <td id=\"T_3d427_row12_col4\" class=\"data row12 col4\" >0.71083893</td>\n",
       "      <td id=\"T_3d427_row12_col5\" class=\"data row12 col5\" >0.00000000</td>\n",
       "      <td id=\"T_3d427_row12_col6\" class=\"data row12 col6\" >‚úÖ PASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3d427_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_3d427_row13_col0\" class=\"data row13 col0\" >Benchmark</td>\n",
       "      <td id=\"T_3d427_row13_col1\" class=\"data row13 col1\" >Lookback</td>\n",
       "      <td id=\"T_3d427_row13_col2\" class=\"data row13 col2\" >Sharpe (ATRP)</td>\n",
       "      <td id=\"T_3d427_row13_col3\" class=\"data row13 col3\" >0.17822337</td>\n",
       "      <td id=\"T_3d427_row13_col4\" class=\"data row13 col4\" >0.17822337</td>\n",
       "      <td id=\"T_3d427_row13_col5\" class=\"data row13 col5\" >0.00000000</td>\n",
       "      <td id=\"T_3d427_row13_col6\" class=\"data row13 col6\" >‚úÖ PASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3d427_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "      <td id=\"T_3d427_row14_col0\" class=\"data row14 col0\" >Group</td>\n",
       "      <td id=\"T_3d427_row14_col1\" class=\"data row14 col1\" >Lookback</td>\n",
       "      <td id=\"T_3d427_row14_col2\" class=\"data row14 col2\" >Sharpe (TRP)</td>\n",
       "      <td id=\"T_3d427_row14_col3\" class=\"data row14 col3\" >0.53243332</td>\n",
       "      <td id=\"T_3d427_row14_col4\" class=\"data row14 col4\" >0.53243332</td>\n",
       "      <td id=\"T_3d427_row14_col5\" class=\"data row14 col5\" >0.00000000</td>\n",
       "      <td id=\"T_3d427_row14_col6\" class=\"data row14 col6\" >‚úÖ PASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3d427_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "      <td id=\"T_3d427_row15_col0\" class=\"data row15 col0\" >Benchmark</td>\n",
       "      <td id=\"T_3d427_row15_col1\" class=\"data row15 col1\" >Lookback</td>\n",
       "      <td id=\"T_3d427_row15_col2\" class=\"data row15 col2\" >Sharpe (TRP)</td>\n",
       "      <td id=\"T_3d427_row15_col3\" class=\"data row15 col3\" >0.16703731</td>\n",
       "      <td id=\"T_3d427_row15_col4\" class=\"data row15 col4\" >0.16703731</td>\n",
       "      <td id=\"T_3d427_row15_col5\" class=\"data row15 col5\" >0.00000000</td>\n",
       "      <td id=\"T_3d427_row15_col6\" class=\"data row15 col6\" >‚úÖ PASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3d427_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "      <td id=\"T_3d427_row16_col0\" class=\"data row16 col0\" >Group</td>\n",
       "      <td id=\"T_3d427_row16_col1\" class=\"data row16 col1\" >Holding</td>\n",
       "      <td id=\"T_3d427_row16_col2\" class=\"data row16 col2\" >Gain</td>\n",
       "      <td id=\"T_3d427_row16_col3\" class=\"data row16 col3\" >-0.01130108</td>\n",
       "      <td id=\"T_3d427_row16_col4\" class=\"data row16 col4\" >-0.01130108</td>\n",
       "      <td id=\"T_3d427_row16_col5\" class=\"data row16 col5\" >0.00000000</td>\n",
       "      <td id=\"T_3d427_row16_col6\" class=\"data row16 col6\" >‚úÖ PASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3d427_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "      <td id=\"T_3d427_row17_col0\" class=\"data row17 col0\" >Benchmark</td>\n",
       "      <td id=\"T_3d427_row17_col1\" class=\"data row17 col1\" >Holding</td>\n",
       "      <td id=\"T_3d427_row17_col2\" class=\"data row17 col2\" >Gain</td>\n",
       "      <td id=\"T_3d427_row17_col3\" class=\"data row17 col3\" >0.00243771</td>\n",
       "      <td id=\"T_3d427_row17_col4\" class=\"data row17 col4\" >0.00243771</td>\n",
       "      <td id=\"T_3d427_row17_col5\" class=\"data row17 col5\" >0.00000000</td>\n",
       "      <td id=\"T_3d427_row17_col6\" class=\"data row17 col6\" >‚úÖ PASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3d427_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
       "      <td id=\"T_3d427_row18_col0\" class=\"data row18 col0\" >Group</td>\n",
       "      <td id=\"T_3d427_row18_col1\" class=\"data row18 col1\" >Holding</td>\n",
       "      <td id=\"T_3d427_row18_col2\" class=\"data row18 col2\" >Sharpe</td>\n",
       "      <td id=\"T_3d427_row18_col3\" class=\"data row18 col3\" >-5.07128664</td>\n",
       "      <td id=\"T_3d427_row18_col4\" class=\"data row18 col4\" >-5.07128664</td>\n",
       "      <td id=\"T_3d427_row18_col5\" class=\"data row18 col5\" >0.00000000</td>\n",
       "      <td id=\"T_3d427_row18_col6\" class=\"data row18 col6\" >‚úÖ PASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3d427_level0_row19\" class=\"row_heading level0 row19\" >19</th>\n",
       "      <td id=\"T_3d427_row19_col0\" class=\"data row19 col0\" >Benchmark</td>\n",
       "      <td id=\"T_3d427_row19_col1\" class=\"data row19 col1\" >Holding</td>\n",
       "      <td id=\"T_3d427_row19_col2\" class=\"data row19 col2\" >Sharpe</td>\n",
       "      <td id=\"T_3d427_row19_col3\" class=\"data row19 col3\" >0.89461095</td>\n",
       "      <td id=\"T_3d427_row19_col4\" class=\"data row19 col4\" >0.89461095</td>\n",
       "      <td id=\"T_3d427_row19_col5\" class=\"data row19 col5\" >0.00000000</td>\n",
       "      <td id=\"T_3d427_row19_col6\" class=\"data row19 col6\" >‚úÖ PASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3d427_level0_row20\" class=\"row_heading level0 row20\" >20</th>\n",
       "      <td id=\"T_3d427_row20_col0\" class=\"data row20 col0\" >Group</td>\n",
       "      <td id=\"T_3d427_row20_col1\" class=\"data row20 col1\" >Holding</td>\n",
       "      <td id=\"T_3d427_row20_col2\" class=\"data row20 col2\" >Sharpe (ATRP)</td>\n",
       "      <td id=\"T_3d427_row20_col3\" class=\"data row20 col3\" >-0.17096660</td>\n",
       "      <td id=\"T_3d427_row20_col4\" class=\"data row20 col4\" >-0.17096660</td>\n",
       "      <td id=\"T_3d427_row20_col5\" class=\"data row20 col5\" >0.00000000</td>\n",
       "      <td id=\"T_3d427_row20_col6\" class=\"data row20 col6\" >‚úÖ PASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3d427_level0_row21\" class=\"row_heading level0 row21\" >21</th>\n",
       "      <td id=\"T_3d427_row21_col0\" class=\"data row21 col0\" >Benchmark</td>\n",
       "      <td id=\"T_3d427_row21_col1\" class=\"data row21 col1\" >Holding</td>\n",
       "      <td id=\"T_3d427_row21_col2\" class=\"data row21 col2\" >Sharpe (ATRP)</td>\n",
       "      <td id=\"T_3d427_row21_col3\" class=\"data row21 col3\" >0.04493774</td>\n",
       "      <td id=\"T_3d427_row21_col4\" class=\"data row21 col4\" >0.04493774</td>\n",
       "      <td id=\"T_3d427_row21_col5\" class=\"data row21 col5\" >0.00000000</td>\n",
       "      <td id=\"T_3d427_row21_col6\" class=\"data row21 col6\" >‚úÖ PASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3d427_level0_row22\" class=\"row_heading level0 row22\" >22</th>\n",
       "      <td id=\"T_3d427_row22_col0\" class=\"data row22 col0\" >Group</td>\n",
       "      <td id=\"T_3d427_row22_col1\" class=\"data row22 col1\" >Holding</td>\n",
       "      <td id=\"T_3d427_row22_col2\" class=\"data row22 col2\" >Sharpe (TRP)</td>\n",
       "      <td id=\"T_3d427_row22_col3\" class=\"data row22 col3\" >-0.15524399</td>\n",
       "      <td id=\"T_3d427_row22_col4\" class=\"data row22 col4\" >-0.15524399</td>\n",
       "      <td id=\"T_3d427_row22_col5\" class=\"data row22 col5\" >0.00000000</td>\n",
       "      <td id=\"T_3d427_row22_col6\" class=\"data row22 col6\" >‚úÖ PASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3d427_level0_row23\" class=\"row_heading level0 row23\" >23</th>\n",
       "      <td id=\"T_3d427_row23_col0\" class=\"data row23 col0\" >Benchmark</td>\n",
       "      <td id=\"T_3d427_row23_col1\" class=\"data row23 col1\" >Holding</td>\n",
       "      <td id=\"T_3d427_row23_col2\" class=\"data row23 col2\" >Sharpe (TRP)</td>\n",
       "      <td id=\"T_3d427_row23_col3\" class=\"data row23 col3\" >0.04572424</td>\n",
       "      <td id=\"T_3d427_row23_col4\" class=\"data row23 col4\" >0.04572424</td>\n",
       "      <td id=\"T_3d427_row23_col5\" class=\"data row23 col5\" >0.00000000</td>\n",
       "      <td id=\"T_3d427_row23_col6\" class=\"data row23 col6\" >‚úÖ PASS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x161c4254950>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# üìÇ CHAPTER 4 (RE-ENGINEERED): MODULAR EVENT-DRIVEN AUDIT\n",
    "# ==============================================================================\n",
    "\n",
    "# 1. SETUP: Identical dates and Engine results\n",
    "res = audit_pack[0][\"results\"]\n",
    "m = res.perf_metrics\n",
    "init_weights = res.initial_weights\n",
    "\n",
    "# Period Definitions\n",
    "periods = {\n",
    "    \"Full\": (res.start_date, res.holding_end_date),\n",
    "    \"Lookback\": (res.start_date, res.decision_date),\n",
    "    \"Holding\": (res.buy_date, res.holding_end_date),\n",
    "}\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. FETCH RAW DATA (The \"Double-Blind\" Source)\n",
    "# ==============================================================================\n",
    "# Instead of taking the Engine's ATRP/TRP, we take the raw High/Low/Close\n",
    "p_ohlcv = get_audit(18)\n",
    "p_raw_price = get_audit(19)\n",
    "\n",
    "b_ohlcv = get_audit(12)\n",
    "b_raw_price = get_audit(13)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. CALCULATE VOLATILITY MANUALLY (From Raw High/Low/Close)\n",
    "# ==============================================================================\n",
    "def calculate_manual_vol(df_ohlcv, atr_period=14):\n",
    "    df = df_ohlcv.copy()\n",
    "    df[\"PC\"] = df.groupby(level=\"Ticker\")[\"Adj Close\"].shift(1)\n",
    "\n",
    "    # True Range math\n",
    "    tr = pd.concat(\n",
    "        [\n",
    "            df[\"Adj High\"] - df[\"Adj Low\"],\n",
    "            (df[\"Adj High\"] - df[\"PC\"]).abs(),\n",
    "            (df[\"Adj Low\"] - df[\"PC\"]).abs(),\n",
    "        ],\n",
    "        axis=1,\n",
    "    ).max(axis=1)\n",
    "\n",
    "    # Wilder's ATR math\n",
    "    atr = tr.groupby(level=\"Ticker\").ewm(alpha=1 / atr_period, adjust=False).mean()\n",
    "    atr = atr.reset_index(level=0, drop=True)\n",
    "\n",
    "    # Convert to Ticker-Wide Matrices\n",
    "    manual_trp = (tr / df[\"Adj Close\"]).unstack(level=\"Ticker\")\n",
    "    manual_atrp = (atr / df[\"Adj Close\"]).unstack(level=\"Ticker\")\n",
    "    return manual_atrp, manual_trp\n",
    "\n",
    "\n",
    "# Generate our own \"Verified Ground Truth\" matrices\n",
    "p_manual_atrp, p_manual_trp = calculate_manual_vol(p_ohlcv)\n",
    "b_manual_atrp, b_manual_trp = calculate_manual_vol(b_ohlcv)\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. CONSOLIDATE INTO THE AUDIT INPUTS\n",
    "#    Use Engine's pre-aligned Ticker ATRP (Warm Start)\n",
    "#    Do not use manual_atrp (Cold Start)\n",
    "# ==============================================================================\n",
    "p_raw = {\n",
    "    \"price\": p_raw_price,\n",
    "    \"atrp\": get_audit(17),  # <--- Use Engine's \"Deep\" ATRP\n",
    "    \"trp\": p_manual_trp,  # <--- Keep your \"Double-Blind\" TRP\n",
    "}\n",
    "b_raw = {\n",
    "    \"price\": b_raw_price,\n",
    "    \"atrp\": get_audit(11),  # <--- Use Engine's \"Deep\" ATRP\n",
    "    \"trp\": b_manual_trp,  # <--- Keep your \"Double-Blind\" TRP\n",
    "}\n",
    "\n",
    "\n",
    "# 2. THE MODULAR AUDIT KERNEL\n",
    "def run_period_audit(df_p, df_atrp, df_trp, weights):\n",
    "    \"\"\"\n",
    "    Replicates the Engine's logic for a specific 'Slate Reset' window.\n",
    "    \"\"\"\n",
    "    # A. Price Normalization (Fresh Capital Reset)\n",
    "    # We divide by the first valid price in THIS specific slice\n",
    "    norm_prices = df_p.div(df_p.bfill().iloc[0])\n",
    "\n",
    "    # B. Equity & Drifted Weights\n",
    "    weighted_components = norm_prices.mul(weights, axis=1)\n",
    "    equity_curve = weighted_components.sum(axis=1)\n",
    "    # Drifted weights are used to aggregate volatility (weighted average)\n",
    "    current_weights = weighted_components.div(equity_curve, axis=0)\n",
    "\n",
    "    # C. Volatility Aggregation\n",
    "    port_atrp = (current_weights * df_atrp).sum(axis=1)\n",
    "    port_trp = (current_weights * df_trp).sum(axis=1)\n",
    "\n",
    "    # D. Return Calculation (Produces leading NaN)\n",
    "    returns = equity_curve.pct_change()\n",
    "    valid_mask = returns.notna()  # ALIGNMENT FIX: Identify days where returns exist\n",
    "\n",
    "    # E. Statistical Kernels (Applying the Aligned Math)\n",
    "    # Gain: Last / First - 1\n",
    "    gain = (equity_curve.iloc[-1] / equity_curve.iloc[0]) - 1\n",
    "\n",
    "    # Sharpe: Mean(Returns) / Std(Returns) * sqrt(252)\n",
    "    sharpe = (returns.mean() / returns.std()) * np.sqrt(252) if returns.std() > 0 else 0\n",
    "\n",
    "    # Sharpe(Vol): Mean(Returns) / Mean(Vol[Only on Return Days])\n",
    "    # This aligns the N-1 denominator for both numerator and denominator\n",
    "    s_atrp = returns.mean() / port_atrp.where(valid_mask).mean()\n",
    "    s_trp = returns.mean() / port_trp.where(valid_mask).mean()\n",
    "\n",
    "    return gain, sharpe, s_atrp, s_trp\n",
    "\n",
    "\n",
    "# 3. EXECUTION LOOP\n",
    "audit_rows = []\n",
    "\n",
    "for p_label, (d_start, d_end) in periods.items():\n",
    "    # --- AUDIT GROUP (PORTFOLIO) ---\n",
    "    g, s, sa, st = run_period_audit(\n",
    "        p_raw[\"price\"].loc[d_start:d_end],\n",
    "        p_raw[\"atrp\"].loc[d_start:d_end],\n",
    "        p_raw[\"trp\"].loc[d_start:d_end],\n",
    "        init_weights,\n",
    "    )\n",
    "\n",
    "    # --- AUDIT BENCHMARK ---\n",
    "    # Dynamically grab the ticker name from the benchmark price columns\n",
    "    b_ticker_name = b_raw[\"price\"].columns[0]\n",
    "    b_weights = pd.Series({b_ticker_name: 1.0})  # ‚úÖ Guaranteed to match the index\n",
    "\n",
    "    bg, bs, bsa, bst = run_period_audit(\n",
    "        b_raw[\"price\"].loc[d_start:d_end],\n",
    "        b_raw[\"atrp\"].loc[d_start:d_end],\n",
    "        b_raw[\"trp\"].loc[d_start:d_end],\n",
    "        b_weights,\n",
    "    )\n",
    "\n",
    "    # Mapping to Engine Metric Keys\n",
    "    mapping = [\n",
    "        (\"Gain\", g, f\"{p_label.lower()}_p_gain\", bg, f\"{p_label.lower()}_b_gain\"),\n",
    "        (\"Sharpe\", s, f\"{p_label.lower()}_p_sharpe\", bs, f\"{p_label.lower()}_b_sharpe\"),\n",
    "        (\n",
    "            \"Sharpe (ATRP)\",\n",
    "            sa,\n",
    "            f\"{p_label.lower()}_p_sharpe_atrp\",\n",
    "            bsa,\n",
    "            f\"{p_label.lower()}_b_sharpe_atrp\",\n",
    "        ),\n",
    "        (\n",
    "            \"Sharpe (TRP)\",\n",
    "            st,\n",
    "            f\"{p_label.lower()}_p_sharpe_trp\",\n",
    "            bst,\n",
    "            f\"{p_label.lower()}_b_sharpe_trp\",\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    for metric, manual_p, key_p, manual_b, key_b in mapping:\n",
    "        # Compare Portfolio\n",
    "        eng_p = m.get(key_p, 0)\n",
    "        audit_rows.append(\n",
    "            {\n",
    "                \"Entity\": \"Group\",\n",
    "                \"Period\": p_label,\n",
    "                \"Metric\": metric,\n",
    "                \"Engine\": eng_p,\n",
    "                \"Manual\": manual_p,\n",
    "                \"Delta\": eng_p - manual_p,\n",
    "            }\n",
    "        )\n",
    "        # Compare Benchmark\n",
    "        eng_b = m.get(key_b, 0)\n",
    "        audit_rows.append(\n",
    "            {\n",
    "                \"Entity\": \"Benchmark\",\n",
    "                \"Period\": p_label,\n",
    "                \"Metric\": metric,\n",
    "                \"Engine\": eng_b,\n",
    "                \"Manual\": manual_b,\n",
    "                \"Delta\": eng_b - manual_b,\n",
    "            }\n",
    "        )\n",
    "\n",
    "# 4. REPORTING\n",
    "df_final = pd.DataFrame(audit_rows)\n",
    "df_final[\"Status\"] = df_final[\"Delta\"].apply(\n",
    "    lambda x: \"‚úÖ PASS\" if abs(x) < 1e-7 else \"‚ùå FAIL\"\n",
    ")\n",
    "\n",
    "\n",
    "# --- 5. DETAILED PRECISION REPORT (FIXED FOR PANDAS 2.1+) ---\n",
    "# This replicates the table you saw, but displays all values with 8 decimals.\n",
    "styled_final = df_final.style.format(\n",
    "    {\"Engine\": \"{:.8f}\", \"Manual\": \"{:.8f}\", \"Delta\": \"{:.8f}\"}\n",
    ")\n",
    "\n",
    "\n",
    "def highlight_noise(val):\n",
    "    # If the delta is exactly zero, keep it gray.\n",
    "    # If there is even a tiny floating point difference, turn it red.\n",
    "    color = \"red\" if abs(val) > 1e-12 else \"gray\"\n",
    "    return f\"color: {color}\"\n",
    "\n",
    "\n",
    "print(f\"üìù CHAPTER 4: FINAL INTEGRITY REPORT (MODULAR)\")\n",
    "display(\n",
    "    df_final.pivot_table(\n",
    "        index=[\"Entity\", \"Metric\"], columns=\"Period\", values=\"Status\", aggfunc=\"first\"\n",
    "    )\n",
    ")\n",
    "\n",
    "display(styled_final.map(highlight_noise, subset=[\"Delta\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bdccd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c200aac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä WEIGHT DRIFT ANALYSIS\n",
      "----------------------------------------\n",
      "        Initial (Jan 02)  Drifted (Jan 21)  Change\n",
      "Ticker                                            \n",
      "RPRX                 0.1            0.1071  0.0071\n",
      "SGOV                 0.1            0.0914 -0.0086\n",
      "SHV                  0.1            0.0914 -0.0086\n",
      "BIL                  0.1            0.0914 -0.0086\n",
      "SNX                  0.1            0.1102  0.0102\n",
      "MINT                 0.1            0.0914 -0.0086\n",
      "TRGP                 0.1            0.1082  0.0082\n",
      "QRVO                 0.1            0.1145  0.0145\n",
      "LNG                  0.1            0.1032  0.0032\n",
      "USFR                 0.1            0.0914 -0.0086\n"
     ]
    }
   ],
   "source": [
    "# 1. Get the weights at the start (Decision Date / Jan 02 context)\n",
    "start_w = manual_weights.loc[res.start_date]\n",
    "\n",
    "# 2. Get the weights at the Entry Date (Jan 21)\n",
    "entry_w = manual_weights.loc[res.buy_date]\n",
    "\n",
    "print(\"üìä WEIGHT DRIFT ANALYSIS\")\n",
    "print(\"-\" * 40)\n",
    "drift_df = pd.DataFrame(\n",
    "    {\n",
    "        \"Initial (Jan 02)\": start_w,\n",
    "        \"Drifted (Jan 21)\": entry_w,\n",
    "        \"Change\": entry_w - start_w,\n",
    "    }\n",
    ")\n",
    "print(drift_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "387e0507",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bench_atrp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# bench_rets\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mbench_atrp\u001b[49m\n",
      "\u001b[31mNameError\u001b[39m: name 'bench_atrp' is not defined"
     ]
    }
   ],
   "source": [
    "# bench_rets\n",
    "bench_atrp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4033da25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# üîç PROOF: THE DENOMINATOR MISMATCH\n",
    "# ==============================================================================\n",
    "\n",
    "# 1. Fetch Benchmark \"Full\" Series (Shape: 17)\n",
    "bench_rets = get_audit(24)  # Benchmark Returns (full_ret)\n",
    "bench_atrp = get_audit(23)  # Benchmark Volatility (full_atrp)\n",
    "\n",
    "print(f\"üìä DATASET INSPECTION (Period: Full)\")\n",
    "print(f\"Total Rows in Index: {len(bench_rets)}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# 2. Inspect the Returns (The Numerator)\n",
    "# .count() returns the number of non-NaN observations\n",
    "ret_count = bench_rets.count()\n",
    "ret_sum = bench_rets.sum()\n",
    "ret_mean_calc = bench_rets.mean()\n",
    "\n",
    "print(f\"RETURNS (.mean() logic):\")\n",
    "print(f\"  First 2 values:\\n{bench_rets.head(2)}\")\n",
    "print(f\"  Non-NaN Count: {ret_count}  <-- (N-1 because of leading NaN)\")\n",
    "print(f\"  Manual Mean (Sum/{ret_count}): {ret_sum/ret_count:.8f}\")\n",
    "print(f\"  Pandas .mean():              {ret_mean_calc:.8f}\")\n",
    "\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# 3. Inspect the Volatility (The Denominator)\n",
    "vol_count = bench_atrp.count()\n",
    "vol_sum = bench_atrp.sum()\n",
    "vol_mean_calc = bench_atrp.mean()\n",
    "\n",
    "print(f\"VOLATILITY (.mean() logic):\")\n",
    "print(f\"  First 2 values:\\n{bench_atrp.head(2)}\")\n",
    "print(f\"  Non-NaN Count: {vol_count}  <-- (Full N)\")\n",
    "print(f\"  Manual Mean (Sum/{vol_count}): {vol_sum/vol_count:.8f}\")\n",
    "print(f\"  Pandas .mean():              {vol_mean_calc:.8f}\")\n",
    "\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# 4. THE SMOKING GUN: The Engine's Sharpe(ATRP) Calculation\n",
    "# Logic: average_return / average_volatility\n",
    "engine_sharpe_atrp = ret_mean_calc / vol_mean_calc\n",
    "stored_engine_val = m.get(\"full_b_sharpe_atrp\")\n",
    "\n",
    "print(f\"üéØ THE RESULT:\")\n",
    "print(f\"  Calculated (Mean_Ret_16 / Mean_Vol_17): {engine_sharpe_atrp:.8f}\")\n",
    "print(f\"  Stored in Engine Metrics:               {stored_engine_val:.8f}\")\n",
    "print(f\"  DELTA: {engine_sharpe_atrp - stored_engine_val:.12f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba85f376",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea115b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291b2f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = get_audit(66)\n",
    "start_date = get_audit(94)\n",
    "decision_date = get_audit(95)\n",
    "buy_date = get_audit(96)\n",
    "end_date = get_audit(97)\n",
    "\n",
    "print(f\"tickers: {tickers}\")\n",
    "print(f\"start_date: {start_date}\")\n",
    "print(f\"decision_date: {decision_date}\")\n",
    "print(f\"buy_date: {buy_date}\")\n",
    "print(f\"end_date: {end_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = create_combined_dict(\n",
    "    df_ohlcv=df_ohlcv,\n",
    "    features_df=features_df,\n",
    "    tickers=tickers,\n",
    "    date_start=start_date,\n",
    "    date_end=end_date,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811ae76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ticker in tickers:\n",
    "    with pd.option_context(\"display.float_format\", \"{:.8f}\".format):\n",
    "        print(f\"{ticker}:\\n{combined[ticker]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6446988",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"df_atrp_wide:\\n{df_atrp_wide}\\n\")\n",
    "df_atrp_wide.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1e6cf3",
   "metadata": {},
   "source": [
    "#####  Output Debug Data to csv Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab55888",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_debug_to_csv(audit_pack, source_label=\"_Audit_bot_v52\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370bf5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_audit_to_excel(audit_pack)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631da036",
   "metadata": {},
   "source": [
    "#####  Get Subset Data, Copy Cell Output and Paste into Excel with 'Import Wizard'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c34f99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ticker = \"TRGP\"\n",
    "_ticker = \"QRVO\"\n",
    "_start_date = \"2025-01-02\"\n",
    "_end_date = \"2025-01-28\"\n",
    "\n",
    "_df = df_ohlcv.loc[_ticker][_start_date:_end_date]\n",
    "print(_df.to_csv())\n",
    "\n",
    "_df = features_df.loc[_ticker][_start_date:_end_date]\n",
    "print(_df.to_csv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3fd34f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cc3007",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e3cc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Fetch\n",
    "stored_atrp = get_audit(36)  # Engine Portfolio ATRP\n",
    "# tr_m from Chapter 2 is reused as the base 'True Range'\n",
    "\n",
    "# 2. Manual Wilder's Smoothing (Alpha = 1/N)\n",
    "alpha = 1 / GLOBAL_SETTINGS[\"atr_period\"]\n",
    "\n",
    "# Calculate ATR for each ticker\n",
    "ticker_atr = tr_m.groupby(level=\"Ticker\").transform(\n",
    "    lambda x: x.ewm(alpha=alpha, adjust=False).mean()\n",
    ")\n",
    "ticker_atrp = (ticker_atr / df_t[\"Adj Close\"]).unstack(level=\"Ticker\")\n",
    "\n",
    "# 3. Manual Portfolio Math\n",
    "manual_port_atrp = (manual_weights * ticker_atrp).sum(axis=1, min_count=1)\n",
    "\n",
    "# 4. Create Calculation Trail\n",
    "df_atrp_audit = pd.concat(\n",
    "    [\n",
    "        ticker_atrp.add_prefix(\"MANUAL_TickerATRP_\"),\n",
    "        manual_port_atrp.to_frame(\"MANUAL_PortfolioATRP\"),\n",
    "        stored_atrp.to_frame(\"ENGINE_PortfolioATRP\"),\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "df_atrp_audit[\"DELTA_ATRP\"] = (\n",
    "    df_atrp_audit[\"MANUAL_PortfolioATRP\"] - df_atrp_audit[\"ENGINE_PortfolioATRP\"]\n",
    ")\n",
    "\n",
    "print(\"\\nüìù CHAPTER 3: ATRP (SMOOTHED RISK) AUDIT\")\n",
    "print(df_atrp_audit.to_csv(float_format=\"%.8f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee64dbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cc9e0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f590ca2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2206e615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# üïµÔ∏è INTERMEDIATE CALCULATION AUDIT (The \"Logic Trail\")\n",
    "# ==============================================================================\n",
    "\n",
    "# 1. GROWTH & INFLUENCE (Price Norm -> Drifted Weights)\n",
    "# Shows how much each stock grew and how that changed its \"vote\" in the portfolio\n",
    "drift_audit = pd.concat(\n",
    "    [norm_prices, drifted_weights], axis=1, keys=[\"Normal Price\", \"Current_Weight\"]\n",
    ")\n",
    "\n",
    "print(\"\\nSTEP 1: GROWTH & DRIFT (How capital shifted over time)\")\n",
    "print(\"=\" * 80)\n",
    "print(drift_audit.to_csv(float_format=\"%.8f\"))\n",
    "\n",
    "\n",
    "# 2. TICKER-LEVEL RISK (Raw TRP vs Smoothed ATRP)\n",
    "# Shows the daily risk for each stock before weighting\n",
    "risk_audit = pd.concat(\n",
    "    [trp_matrix, atrp_matrix], axis=1, keys=[\"Daily_TRP_Raw\", \"Daily_ATRP_Smoothed\"]\n",
    ")\n",
    "\n",
    "print(\"\\nSTEP 2: TICKER RISK (Instant vs. Smoothed Volatility per Stock)\")\n",
    "print(\"=\" * 80)\n",
    "print(risk_audit.to_csv(float_format=\"%.8f\"))\n",
    "\n",
    "\n",
    "# 3. CONTRIBUTION (Weight * Vol)\n",
    "# This shows exactly how many \"basis points\" of risk each stock added to the total\n",
    "contrib_trp = drifted_weights * trp_matrix\n",
    "contrib_atrp = drifted_weights * atrp_matrix\n",
    "contrib_audit = pd.concat(\n",
    "    [contrib_trp, contrib_atrp], axis=1, keys=[\"TRP_Contrib\", \"ATRP_Contrib\"]\n",
    ")\n",
    "\n",
    "print(\"\\nSTEP 3: CONTRIBUTION (Weight * Individual Vol = Portfolio Risk Component)\")\n",
    "print(\"=\" * 80)\n",
    "print(contrib_audit.to_csv(float_format=\"%.8f\"))\n",
    "\n",
    "\n",
    "# 4. FINAL RECONCILIATION\n",
    "# Shows your manual math vs. the Engine's stored result\n",
    "print(\"\\nSTEP 4: FINAL RECONCILIATION (Manual Math vs. Engine Stored Result)\")\n",
    "print(\"=\" * 80)\n",
    "# Re-printing the final audit_df for completeness\n",
    "print(\n",
    "    audit_df[\n",
    "        [\"PORT_TRP_MANUAL\", \"PORT_TRP_STORED\", \"PORT_ATRP_MANUAL\", \"PORT_ATRP_STORED\"]\n",
    "    ].to_csv(float_format=\"%.8f\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f036fdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 1. BASE DATA & WEIGHT DRIFT (SHARED)\n",
    "# ==============================================================================\n",
    "ohlcv_raw = get_audit(18)  # Raw High/Low/Close/Volume\n",
    "prices_wide = get_audit(19)  # Sanitized Adj Closes\n",
    "init_weights = get_audit(67)  # Initial 50/50 weights\n",
    "tickers = init_weights.index.tolist()\n",
    "\n",
    "# Calculate Price Drift (The \"Influence\" of each stock)\n",
    "norm_prices = prices_wide.div(prices_wide.bfill().iloc[0])\n",
    "weighted_val = norm_prices.mul(init_weights, axis=1)\n",
    "total_equity = weighted_val.sum(axis=1)\n",
    "drifted_weights = weighted_val.div(total_equity, axis=0)\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. TICKER-LEVEL MATH (TR vs ATR)\n",
    "# ==============================================================================\n",
    "df_raw = ohlcv_raw.copy()\n",
    "df_raw[\"PC\"] = df_raw.groupby(level=\"Ticker\")[\"Adj Close\"].shift(1)\n",
    "\n",
    "# A. Calculate True Range (The Base Unit of Risk)\n",
    "tr1 = df_raw[\"Adj High\"] - df_raw[\"Adj Low\"]\n",
    "tr2 = (df_raw[\"Adj High\"] - df_raw[\"PC\"]).abs()\n",
    "tr3 = (df_raw[\"Adj Low\"] - df_raw[\"PC\"]).abs()\n",
    "df_raw[\"TR\"] = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)\n",
    "\n",
    "# B. Instant Volatility: TRP Calculation\n",
    "df_raw[\"TRP_Indiv\"] = df_raw[\"TR\"] / df_raw[\"Adj Close\"]\n",
    "trp_matrix = df_raw[\"TRP_Indiv\"].unstack(level=\"Ticker\")\n",
    "\n",
    "# C. Smoothed Volatility: ATRP Calculation\n",
    "# Wilder's Smoothing: Alpha = 1/N\n",
    "atr_period = GLOBAL_SETTINGS(atr_period)  # Default from Global Settings\n",
    "df_raw[\"ATR\"] = df_raw.groupby(level=\"Ticker\")[\"TR\"].transform(\n",
    "    lambda x: x.ewm(alpha=1 / atr_period, adjust=False).mean()\n",
    ")\n",
    "df_raw[\"ATRP_Indiv\"] = df_raw[\"ATR\"] / df_raw[\"Adj Close\"]\n",
    "atrp_matrix = df_raw[\"ATRP_Indiv\"].unstack(level=\"Ticker\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. PORTFOLIO-LEVEL RECONSTRUCTION\n",
    "# ==============================================================================\n",
    "# Multiply Ticker Vol by drifted weights and sum\n",
    "port_trp_manual = (drifted_weights * trp_matrix).sum(axis=1, min_count=1)\n",
    "port_atrp_manual = (drifted_weights * atrp_matrix).sum(axis=1, min_count=1)\n",
    "\n",
    "# Fetch Stored Results for final verification\n",
    "port_trp_stored = get_audit(38)\n",
    "port_atrp_stored = get_audit(36)\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. THE VERIFICATION MATRIX\n",
    "# ==============================================================================\n",
    "final_report_cols = []\n",
    "for t in tickers:\n",
    "    final_report_cols.append(\n",
    "        pd.DataFrame(\n",
    "            {\n",
    "                f\"{t}_Weight\": drifted_weights[t],\n",
    "                f\"{t}_TRP\": trp_matrix[t],\n",
    "                f\"{t}_ATRP\": atrp_matrix[t],\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "\n",
    "audit_df = pd.concat(final_report_cols, axis=1)\n",
    "audit_df[\"PORT_TRP_MANUAL\"] = port_trp_manual\n",
    "audit_df[\"PORT_TRP_STORED\"] = port_trp_stored\n",
    "audit_df[\"PORT_ATRP_MANUAL\"] = port_atrp_manual\n",
    "audit_df[\"PORT_ATRP_STORED\"] = port_atrp_stored\n",
    "\n",
    "print(\"‚ú® DUAL VOLATILITY AUDIT: TRP (Instant) vs ATRP (Smoothed)\")\n",
    "print(\"-\" * 100)\n",
    "print(audit_df.to_csv(float_format=\"%.8f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7851b4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect_audit(audit_pack)\n",
    "print(inspect_audit(audit_pack))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fa0811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Fetch Source Data\n",
    "ohlcv_raw = get_audit(18)  # Raw High/Low/Close/Volume\n",
    "prices_wide = get_audit(19)  # Sanitized Adj Closes (Wide)\n",
    "init_weights = get_audit(67)  # Initial weights\n",
    "stored_trp = get_audit(38)  # The final result to verify\n",
    "\n",
    "# 2. Calculate Individual Ticker TRP (Vectorized & Precise)\n",
    "df_temp = ohlcv_raw.copy()\n",
    "# Get Previous Close for each ticker\n",
    "df_temp[\"PC\"] = df_temp.groupby(level=\"Ticker\")[\"Adj Close\"].shift(1)\n",
    "\n",
    "# True Range Components (Vectorized is faster and avoids warnings)\n",
    "tr1 = df_temp[\"Adj High\"] - df_temp[\"Adj Low\"]\n",
    "tr2 = (df_temp[\"Adj High\"] - df_temp[\"PC\"]).abs()\n",
    "tr3 = (df_temp[\"Adj Low\"] - df_temp[\"PC\"]).abs()\n",
    "\n",
    "# TR = max(H-L, |H-PC|, |L-PC|)\n",
    "df_temp[\"TR\"] = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)\n",
    "df_temp[\"TRP_Individual\"] = df_temp[\"TR\"] / df_temp[\"Adj Close\"]\n",
    "\n",
    "# Move to Wide format for easy matrix math\n",
    "trp_matrix = df_temp[\"TRP_Individual\"].unstack(level=\"Ticker\")\n",
    "\n",
    "# 3. Calculate Weight Drift (The \"Concentration\" Logic)\n",
    "# Normalize prices so they all start at 1.0\n",
    "norm_prices = prices_wide.div(prices_wide.bfill().iloc[0])\n",
    "# Dollar value of each position\n",
    "weighted_val = norm_prices.mul(init_weights, axis=1)\n",
    "# Total Portfolio Value\n",
    "total_equity = weighted_val.sum(axis=1)\n",
    "# Current Weight of each stock (Percentage of total value)\n",
    "drifted_weights = weighted_val.div(total_equity, axis=0)\n",
    "\n",
    "# 4. CONSTRUCT PROGRESSION DATAFRAME\n",
    "progression_cols = []\n",
    "tickers = init_weights.index.tolist()\n",
    "\n",
    "for t in tickers:\n",
    "    # Build a 4-column block for each ticker\n",
    "    t_df = pd.DataFrame(\n",
    "        {\n",
    "            f\"{t}_Price_Norm\": norm_prices[t],\n",
    "            f\"{t}_Weight\": drifted_weights[t],\n",
    "            f\"{t}_TRP\": trp_matrix[t],\n",
    "            f\"{t}_Contrib\": drifted_weights[t]\n",
    "            * trp_matrix[t],  # (Weight * Individual Vol)\n",
    "        },\n",
    "        index=prices_wide.index,\n",
    "    )\n",
    "    progression_cols.append(t_df)\n",
    "\n",
    "# Combine everything\n",
    "verification_df = pd.concat(progression_cols, axis=1)\n",
    "\n",
    "# Sum the contributions to get the Portfolio Volatility\n",
    "verification_df[\"PORTFOLIO_TRP_CALC\"] = verification_df[\n",
    "    [f\"{t}_Contrib\" for t in tickers]\n",
    "].sum(axis=1, min_count=1)\n",
    "verification_df[\"STORED_TRP_VALUE\"] = stored_trp\n",
    "\n",
    "# 5. Output for Verification\n",
    "print(\"‚úÖ Verification Matrix Generated (Positional Warnings Fixed)\")\n",
    "print(\"-\" * 80)\n",
    "# Export to CSV format for high-precision review\n",
    "print(verification_df.to_csv(float_format=\"%.8f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc0e16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Fetch Source Data\n",
    "ohlcv_raw = get_audit(18)  # Raw High/Low/Close/Volume\n",
    "prices_wide = get_audit(19)  # Sanitized Adj Closes\n",
    "init_weights = get_audit(67)  # Initial 50/50 weights\n",
    "stored_trp = get_audit(38)  # The final result to verify against\n",
    "\n",
    "# 2. Calculate Individual Ticker TRP\n",
    "# We calculate TR = max(H-L, |H-PC|, |L-PC|)\n",
    "df_temp = ohlcv_raw.copy()\n",
    "df_temp[\"PC\"] = df_temp.groupby(level=\"Ticker\")[\"Adj Close\"].shift(1)\n",
    "df_temp[\"TR\"] = df_temp[[\"Adj High\", \"Adj Low\", \"PC\"]].apply(\n",
    "    lambda x: max(x[0] - x[1], abs(x[0] - x[2]), abs(x[1] - x[2])), axis=1\n",
    ")\n",
    "df_temp[\"TRP_Individual\"] = df_temp[\"TR\"] / df_temp[\"Adj Close\"]\n",
    "trp_matrix = df_temp[\"TRP_Individual\"].unstack(level=\"Ticker\")\n",
    "\n",
    "# 3. Calculate Weight Drift (How much capital is in each stock)\n",
    "norm_prices = prices_wide.div(prices_wide.bfill().iloc[0])\n",
    "weighted_val = norm_prices.mul(init_weights, axis=1)\n",
    "total_equity = weighted_val.sum(axis=1)\n",
    "drifted_weights = weighted_val.div(total_equity, axis=0)\n",
    "\n",
    "# 4. CONSTRUCT PROGRESSION DATAFRAME\n",
    "# We will create a column group for each ticker: [TRP] * [Weight]\n",
    "progression_cols = []\n",
    "tickers = init_weights.index.tolist()\n",
    "\n",
    "for t in tickers:\n",
    "    t_df = pd.DataFrame(\n",
    "        {\n",
    "            f\"{t}_Price_Norm\": norm_prices[t],\n",
    "            f\"{t}_Weight\": drifted_weights[t],\n",
    "            f\"{t}_TRP\": trp_matrix[t],\n",
    "            f\"{t}_Contrib\": drifted_weights[t]\n",
    "            * trp_matrix[t],  # This is the 'Weighted Vol'\n",
    "        }\n",
    "    )\n",
    "    progression_cols.append(t_df)\n",
    "\n",
    "# Combine all ticker columns and add the Portfolio Totals\n",
    "verification_df = pd.concat(progression_cols, axis=1)\n",
    "verification_df[\"PORTFOLIO_TRP_CALC\"] = verification_df[\n",
    "    [f\"{t}_Contrib\" for t in tickers]\n",
    "].sum(axis=1, min_count=1)\n",
    "verification_df[\"STORED_TRP_VALUE\"] = stored_trp\n",
    "\n",
    "# 5. Display the Progression\n",
    "print(\"üìä VOLATILITY CALCULATION PROGRESSION\")\n",
    "print(\"-\" * 100)\n",
    "# Showing a slice of the columns for readability in the console\n",
    "print(verification_df.to_csv(float_format=\"%.6f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239cc369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# üîç THE ULTIMATE VOLATILITY AUDIT: RAW OHLCV -> PORTFOLIO TRP\n",
    "# ==============================================================================\n",
    "\n",
    "# 1. Fetch the absolute raw components\n",
    "ohlcv_raw = get_audit(18)  # Raw High/Low/Close/Volume (Long form)\n",
    "prices_wide = get_audit(19)  # Sanitized Adj Closes (Wide form)\n",
    "init_weights = get_audit(67)  # Starting 50/50 allocation\n",
    "\n",
    "# 2. MANUALLY CALCULATE TICKER TRP FROM OHLCV (The \"Hard\" Way)\n",
    "# We need to calculate True Range: max(H-L, |H-PC|, |L-PC|)\n",
    "df_vol_audit = ohlcv_raw.copy()\n",
    "df_vol_audit[\"PrevClose\"] = df_vol_audit.groupby(level=\"Ticker\")[\"Adj Close\"].shift(1)\n",
    "\n",
    "# TR Components\n",
    "df_vol_audit[\"H_L\"] = df_vol_audit[\"Adj High\"] - df_vol_audit[\"Adj Low\"]\n",
    "df_vol_audit[\"H_PC\"] = (df_vol_audit[\"Adj High\"] - df_vol_audit[\"PrevClose\"]).abs()\n",
    "df_vol_audit[\"L_PC\"] = (df_vol_audit[\"Adj Low\"] - df_vol_audit[\"PrevClose\"]).abs()\n",
    "\n",
    "# TR Calculation\n",
    "df_vol_audit[\"Manual_TR\"] = df_vol_audit[[\"H_L\", \"H_PC\", \"L_PC\"]].max(axis=1)\n",
    "# Convert to Percentage (TRP)\n",
    "df_vol_audit[\"Manual_TRP\"] = df_vol_audit[\"Manual_TR\"] / df_vol_audit[\"Adj Close\"]\n",
    "\n",
    "# 3. PIVOT TO WIDE FORMAT\n",
    "manual_trp_matrix = df_vol_audit[\"Manual_TRP\"].unstack(level=\"Ticker\")\n",
    "\n",
    "# 4. CALCULATE DRIFTED WEIGHTS (Follow the Money)\n",
    "norm_prices = prices_wide.div(prices_wide.bfill().iloc[0])\n",
    "weighted_comp = norm_prices.mul(init_weights, axis=1)\n",
    "equity_curve = weighted_comp.sum(axis=1)\n",
    "drifted_weights = weighted_comp.div(equity_curve, axis=0)\n",
    "\n",
    "# 5. COMBINE: DRIFTED WEIGHTS * MANUAL TRP MATRIX\n",
    "manual_portfolio_trp = (drifted_weights * manual_trp_matrix).sum(axis=1, min_count=1)\n",
    "\n",
    "# 6. FINAL COMPARISON TABLE\n",
    "stored_portfolio_trp = get_audit(38)  # This is the \"full_trp\" from the Pack\n",
    "\n",
    "audit_final = pd.DataFrame(\n",
    "    {\n",
    "        \"Manual_Reconstruction\": manual_portfolio_trp,\n",
    "        \"Engine_Stored_Result\": stored_portfolio_trp,\n",
    "        \"Difference\": manual_portfolio_trp - stored_portfolio_trp,\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"‚ú® VOLATILITY AUDIT COMPLETE\")\n",
    "print(audit_final.to_csv(float_format=\"%.8f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acabe861",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d873f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Fetch the raw components\n",
    "prices = get_audit(16)  # Raw Adj Closes for TRGP, QRVO\n",
    "weights = get_audit(63)  # Initial Weights (e.g., 0.5, 0.5)\n",
    "raw_vols = get_audit(15)  # Raw Daily Volatility (Matrix) for TRGP, QRVO\n",
    "\n",
    "# 2. Calculate the \"Drifted\" (Current) Weights\n",
    "# First, get the cumulative value of each $1.00 \"pile\" put into each stock\n",
    "norm_prices = prices.div(prices.bfill().iloc[0])\n",
    "weighted_components = norm_prices.mul(weights, axis=1)\n",
    "\n",
    "# Second, get the total portfolio value (the denominator)\n",
    "equity_curve = weighted_components.sum(axis=1)\n",
    "\n",
    "# Third, find the percentage of the portfolio each stock represents TODAY\n",
    "# This is where the 'Drift' is captured.\n",
    "drifted_weights = weighted_components.div(equity_curve, axis=0)\n",
    "\n",
    "# 3. Calculate Portfolio Volatility (Weighted Average of Daily Volatilities)\n",
    "# We multiply the daily volatility of each stock by its DRIFTED weight\n",
    "portfolio_vol_series = (drifted_weights * raw_vols).sum(axis=1)\n",
    "\n",
    "# 4. Create the Combined DataFrame for Audit\n",
    "audit_df = drifted_weights.copy()\n",
    "# Add columns for clarity\n",
    "audit_df.columns = [f\"Weight_{c}\" for c in audit_df.columns]\n",
    "audit_df[\"== PORTFOLIO_TRP ==\"] = portfolio_vol_series\n",
    "\n",
    "# Show the results\n",
    "# Note: Compare 'PORTFOLIO_TRP' here to 'Port_TRP' in your Excel DAILY_LOG tab.\n",
    "print(audit_df.to_csv(index=True, float_format=\"%.8f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647e3010",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_audit(34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9165c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Fetch the raw components\n",
    "prices = get_audit(16)  # The raw Adj Closes for TRGP, QRVO\n",
    "weights = get_audit(63)  # The starting weights (e.g., 0.5, 0.5)\n",
    "\n",
    "# 2. Normalize Prices (Start every stock at 1.0 for a fair comparison)\n",
    "# We divide every row by the first valid row\n",
    "norm_prices = prices.div(prices.bfill().iloc[0])\n",
    "\n",
    "# 3. Calculate the Portfolio Total (Sum of Weighted Normalized Prices)\n",
    "manual_portfolio = (norm_prices * weights).sum(axis=1)\n",
    "\n",
    "# 4. Create the Combined DataFrame\n",
    "# We copy norm_prices and add the total as a new column\n",
    "reconstruction_df = norm_prices.copy()\n",
    "reconstruction_df[\"== PORTFOLIO_TOTAL ==\"] = manual_portfolio\n",
    "\n",
    "# Display with high precision\n",
    "# reconstruction_df.style.format(\"{:.6f}\")\n",
    "# print(reconstruction_df.style.format(\"{:.6f}\"))\n",
    "print(reconstruction_df.to_csv(index=False, float_format=\"%.8f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339a7ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "stored_portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973bbbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Fetch the raw data\n",
    "prices = get_audit(16)  # Daily prices for TRGP and QRVO\n",
    "weights = get_audit(63)  # Allocation (e.g., 0.5 and 0.5)\n",
    "\n",
    "# 2. Normalize the prices (Start every stock at 1.0)\n",
    "norm_prices = prices.div(prices.bfill().iloc[0])\n",
    "\n",
    "# 3. Apply weights and sum\n",
    "# This creates the daily equity curve\n",
    "manual_portfolio = (norm_prices * weights).sum(axis=1)\n",
    "\n",
    "# 4. Compare to the stored result\n",
    "stored_portfolio = get_audit(35)\n",
    "print(\"Difference:\", (manual_portfolio - stored_portfolio).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9476141",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525fcaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_audit(63)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eddcbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_verify = get_audit(17)\n",
    "df_verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e10c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "browse_audit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28735f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_audit_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06681d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'active_audit_item' is now whatever you last clicked in the browser\n",
    "# 'active_audit_item' is now whatever you last clicked in the browser\n",
    "active_audit_item.query(\"Strategy_Score > 0.5\").query(\"Strategy_Score > 0.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4453fb30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c63e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import is_dataclass, fields\n",
    "\n",
    "\n",
    "def print_audit_structure(item, indent=0):\n",
    "    \"\"\"\n",
    "    Recursively prints the keys and structure of the audit_pack.\n",
    "    Handles Dictionaries, Dataclasses, and DataFrames.\n",
    "    \"\"\"\n",
    "    spacing = \"  \" * indent\n",
    "\n",
    "    # 1. Handle the Top-Level List (audit_pack is [dict])\n",
    "    if isinstance(item, list):\n",
    "        print(f\"{spacing}List (Length: {len(item)})\")\n",
    "        for i, val in enumerate(item):\n",
    "            print(f\"{spacing}[{i}]:\")\n",
    "            print_audit_structure(val, indent + 1)\n",
    "\n",
    "    # 2. Handle Dictionaries (like 'debug_data')\n",
    "    elif isinstance(item, dict):\n",
    "        for key, value in item.items():\n",
    "            if isinstance(value, (dict, list)) or is_dataclass(value):\n",
    "                print(f\"{spacing}{key}:\")\n",
    "                print_audit_structure(value, indent + 1)\n",
    "            else:\n",
    "                type_name = type(value).__name__\n",
    "                shape = f\" shape={value.shape}\" if hasattr(value, \"shape\") else \"\"\n",
    "                print(f\"{spacing}{key} ({type_name}{shape})\")\n",
    "\n",
    "    # 3. Handle Dataclasses (like 'results' or 'inputs')\n",
    "    elif is_dataclass(item):\n",
    "        class_name = item.__class__.__name__\n",
    "        print(f\"{spacing}Dataclass: {class_name}\")\n",
    "        for field in fields(item):\n",
    "            value = getattr(item, field.name)\n",
    "            if isinstance(value, (dict, list)) or is_dataclass(value):\n",
    "                print(f\"{spacing}{field.name}:\")\n",
    "                print_audit_structure(value, indent + 1)\n",
    "            else:\n",
    "                type_name = type(value).__name__\n",
    "                shape = f\" shape={value.shape}\" if hasattr(value, \"shape\") else \"\"\n",
    "                print(f\"{spacing}{field.name} ({type_name}{shape})\")\n",
    "\n",
    "\n",
    "# Usage:\n",
    "print_audit_structure(audit_pack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de2903a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_nested(audit_pack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1b374f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_idx = \"SPY\"\n",
    "print(df_ohlcv.loc[_idx].head(50).to_csv(index=True))\n",
    "\n",
    "# save the slice as a real CSV file\n",
    "start, end = \"2004-08-19\", \"2025-12-30\"\n",
    "# df_ohlcv.loc[_idx, start:end].to_csv(f\"{_idx}.csv\", index=True)\n",
    "# df_ohlcv.loc[_idx].to_csv(f\"{_idx}.csv\", index=True)\n",
    "(df_ohlcv.loc[_idx].loc[\"2004-08-19\":\"2025-12-30\"].to_csv(f\"_{_idx}.csv\", index=True))\n",
    "print(df_ohlcv.loc[_ticker].head(50).to_csv(index=True))\n",
    "# save the slice as a real CSV file\n",
    "df_ohlcv.loc[_ticker].to_csv(f\"_{_ticker}.csv\", index=True)\n",
    "features_df.loc[_ticker].to_csv(f\"_{_ticker}_features.csv\", index=True)\n",
    "print(features_df.loc[_ticker].head(70), \"\\n\")\n",
    "# print(features_df.loc[_ticker].tail(70))\n",
    "print(f\" features_df:\\n{features_df}\\n\")\n",
    "features_df.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
