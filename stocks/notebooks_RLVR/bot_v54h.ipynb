{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9716794",
   "metadata": {},
   "source": [
    "v52  \n",
    "- **verify_engine_results_short_form**\n",
    "- **verify_engine_results_long_form**\n",
    "-  **The Temporal Alignment Fix:** We synchronized the \"Reward\" (Returns) and \"Risk\" (Volatility) by implementing the $N-1$ denominator logic. This ensures that Day 1's volatility no longer dilutes your Sharpe scores.\n",
    "-  **The Event-Driven Re-normalization:** We verified that the Engine correctly resets capital and weights at the start of the Holding period, giving you an accurate \"Fresh Start\" performance metric.\n",
    "-  **The Double-Blind Verification:** We proved that the Engine's True Range (TRP) math is flawless by recreating it from raw High/Low/Close data and achieving an 8-decimal match.\n",
    "-  **Mathematical Fortification:** We centralized all logic into a polymorphic `QuantUtils` kernel that handles both single-portfolio reports and whole-universe rankings with built-in numerical safety.\n",
    "-  **Volatility Evolution:** We successfully added `TRP` (True Range Percent) and the `Sharpe (TRP)` metric, giving you a raw, high-frequency alternative to the smoothed ATR.\n",
    "-  **Data Integrity:** We implemented the \"Momentum Collapse\" tripwire (`verify_ranking_integrity`) to ensure that your risk-adjusted rankings never accidentally devolve into simple price momentum.\n",
    "-  **The \"Audit Pack\" Architecture:** We collapsed fragmented results into a single, atomic container, ensuring that your inputs, results, and debug data are always perfectly synchronized.\n",
    "-  **Total Transparency:** We replaced scattered CSV files with a unified **Excel Audit Report**, allowing for 1-to-1 manual verification of every calculation in the system.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9817eb05",
   "metadata": {},
   "source": [
    "v51\n",
    "\n",
    "UNDO v50, Calculate Sharpe(ATR) using mean over lookback period.  \n",
    "\n",
    "Comment out ``# --- PINPOINT START: ATRP SWITCH ---`` in function ``_select_tickers`` can switch between ``Averaged ATRP over lookback period`` and ``Current ATRP``  \n",
    "    # --- PINPOINT START: ATRP SWITCH ---  \n",
    "    # To switch between Old (Averaged ATRP) and New (Current ATRP):  \n",
    "    # 1. Comment out the logic you DON'T want.  \n",
    "    # 2. Uncomment the logic you DO want.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15eb349d",
   "metadata": {},
   "source": [
    "v50\n",
    "\n",
    "Ticker selection based on atrp_value_for_obs based on decision day, was based on average over lookback period. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31dde13",
   "metadata": {},
   "source": [
    "v48  \n",
    "### Summary of what you just accomplished:\n",
    "1.  **Strict Math:** `QuantUtils` now contains an `assert` that prevents any dev (or AI) from filling the first day with 0.0.\n",
    "2.  **Semantic Protection:** Variables are now named `returns_WITH_BOUNDARY_NAN`, signaling to the AI that the Null value is part of its identity.\n",
    "3.  **Complete SOLID Separation:** The Engine CONDUCTS the simulation, while `QuantUtils` CALCULATES the results. They no longer share logic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61923b0e",
   "metadata": {},
   "source": [
    "**1. Data Flow of `plot_walk_forward_analyzer`**\n",
    "The function acts as a **UI wrapper** around the `AlphaEngine` class. The flow is:\n",
    "1.  **Input:** User selects parameters (Dates, Lookback, Strategy).\n",
    "2.  **State Construction:** `AlphaEngine` slices the historical data (`df_ohlcv`, `df_atrp`) up to the `decision_date`.\n",
    "3.  **Policy Execution (Hardcoded):** The engine applies the logic (e.g., `METRIC_REGISTRY['Sharpe']`) to rank stocks based *only* on the Lookback window.\n",
    "4.  **Environment Step:** It simulates a \"Buy\" at `decision_date + 1` and calculates the returns over the `holding_period`.\n",
    "5.  **Reward Generation:** It outputs performance metrics (`holding_p_gain`, `holding_p_sharpe`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1110f2e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- üõ°Ô∏è Starting Final Integrity Audit ---\n",
      "‚úÖ Series Boundary: OK\n",
      "‚úÖ DataFrame Boundary: OK\n",
      "‚úÖ AUDIT PASSED: Mathematical boundaries are strictly enforced.\n",
      "\n",
      "--- üõ°Ô∏è Starting Feature Engineering Audit ---\n",
      "‚ö° Generating Features (Base: 21d, Ratio Clip: 10.0)...\n",
      "Audit Values:\n",
      "[ nan 25.  17.5]\n",
      "‚úÖ FEATURE INTEGRITY PASSED: Wilder's ATR logic is strictly enforced.\n",
      "--- üõ°Ô∏è Starting Ranking Kernel Audit ---\n",
      "‚úÖ RANKING INTEGRITY PASSED: Volatility normalization is strictly enforced.\n",
      "\n",
      "--- üõ°Ô∏è Starting Volatility Alignment Audit ---\n",
      "‚úÖ Series Temporal Coupling: OK\n",
      "‚úÖ DataFrame Temporal Coupling: OK\n",
      "‚úÖ AUDIT PASSED: Reward and Risk are strictly synchronized.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from dataclasses import dataclass, field, asdict, is_dataclass\n",
    "from typing import List, Dict, Optional, Any, Union, TypedDict, Tuple\n",
    "from collections import Counter\n",
    "from datetime import datetime, date\n",
    "from pandas.testing import assert_series_equal\n",
    "\n",
    "\n",
    "# pd.set_option('display.max_rows', None)  display all rows\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", 1000)\n",
    "pd.set_option(\"display.max_colwidth\", 50)\n",
    "pd.set_option(\"display.precision\", 4)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# GLOBAL SETTINGS: The \"Control Panel\" for the Strategy\n",
    "# ==============================================================================\n",
    "\n",
    "GLOBAL_SETTINGS = {\n",
    "    # ENVIRONMENT (The \"Where\")\n",
    "    \"benchmark_ticker\": \"SPY\",\n",
    "    \"calendar_ticker\": \"SPY\",  # Used as the \"Master Clock\" for trading days\n",
    "    # DATA SANITIZER (The \"Glitches & Gaps\" Protector)\n",
    "    \"handle_zeros_as_nan\": True,  # Convert 0.0 prices to NaN to prevent math errors\n",
    "    \"max_data_gap_ffill\": 1,  # Max consecutive days to \"Forward Fill\" missing data\n",
    "    # IMPLICATION OF nan_price_replacement:\n",
    "    # - This defines what happens if the \"Forward Fill\" limit is exceeded.\n",
    "    # - If set to 0.0: A permanent data gap will look like a \"total loss\" (-100%).\n",
    "    #   The equity curve will plummet. Good for \"disaster detection.\"\n",
    "    #   Sharpe and Sharpe(ATR) drop because: return (gets smaller) / std (gets larger)\n",
    "    # - If set to np.nan: A permanent gap will cause portfolio calculations to return NaN.\n",
    "    #   The chart may break or show gaps. Good for \"math integrity.\"\n",
    "    \"nan_price_replacement\": 0.0,\n",
    "    # STRATEGY & MATH\n",
    "    \"annual_period\": 252,  # Replaces hardcoded 252 in Sharpe calculations\n",
    "    \"atr_period\": 14,  # Used for volatility normalization\n",
    "    \"rsi_period\": 14,  # <--- NEW: Control for RSI logic\n",
    "    # FEATURE ENGINE WINDOWS\n",
    "    \"features_base_window\": 21,  # Replaces hardcoded 21 (The \"Monthly\" anchor)\n",
    "    \"features_fast_window\": 5,  # Replaces hardcoded 5 (The \"Weekly\" anchor)\n",
    "    # FEATURE GUARDRAILS (CLIPS)\n",
    "    \"feature_zscore_clip\": 5.0,  # Replaces hardcoded 5.0 in OBV Z-Scores\n",
    "    \"feature_ratio_clip\": 10.0,  # Replaces hardcoded 10.0 in RVol ratios\n",
    "    # QUALITY/LIQUIDITY\n",
    "    \"quality_window\": 252,  # 1 year lookback for liquidity/quality stats\n",
    "    \"quality_min_periods\": 126,  # min period that ticker has to meet quality thresholds\n",
    "    # QUALITY THRESHOLDS (The \"Rules\")\n",
    "    \"thresholds\": {\n",
    "        # HARD LIQUIDITY FLOOR\n",
    "        # Logic: Calculates (Adj Close * Volume) daily, then takes the ROLLING MEDIAN\n",
    "        # over the quality_window (252 days). Filters out stocks where the\n",
    "        # typical daily dollar turnover is below this absolute value.\n",
    "        \"min_median_dollar_volume\": 1_000_000,\n",
    "        # DYNAMIC LIQUIDITY CUTOFF (Relative to Universe)\n",
    "        # Logic: On the decision date, the engine calculates the X-quantile\n",
    "        # of 'RollMedDollarVol' across ALL available stocks.\n",
    "        # Setting this to 0.40 calculates the 60th percentile and requires\n",
    "        # stocks to be above it‚Äîeffectively keeping only the TOP 60% of the market.\n",
    "        \"min_liquidity_percentile\": 0.40,\n",
    "        # PRICE/VOLUME STALENESS\n",
    "        # Logic: Creates a binary flag (1 if Volume is 0 OR High equals Low).\n",
    "        # It then calculates the ROLLING MEAN of this flag.\n",
    "        # A value of 0.05 means the stock is rejected if it was \"stale\"\n",
    "        # for more than 5% of the trading days in the rolling window.\n",
    "        \"max_stale_pct\": 0.05,\n",
    "        # DATA INTEGRITY (FROZEN VOLUME)\n",
    "        # Logic: Checks if Volume is identical to the previous day (Volume.diff() == 0).\n",
    "        # It calculates the ROLLING SUM of these occurrences over the window.\n",
    "        # If the exact same volume is reported more than 10 times, the stock\n",
    "        # is rejected as having \"frozen\" or low-quality data.\n",
    "        \"max_same_vol_count\": 10,\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION A: CORE KERNELS & QUANT UTILITIES (THE SAFE ROOM)\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "class QuantUtils:\n",
    "    \"\"\"\n",
    "    MATHEMATICAL KERNEL REGISTRY: THE SINGLE SOURCE OF TRUTH.\n",
    "    Handles both pd.Series (Report) and pd.DataFrame (Ranking) robustly.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_returns(\n",
    "        data: Union[pd.Series, pd.DataFrame],\n",
    "    ) -> Union[pd.Series, pd.DataFrame]:\n",
    "        return data.pct_change().replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_gain(data: Union[pd.Series, pd.DataFrame]) -> Union[float, pd.Series]:\n",
    "        if data.empty:\n",
    "            return 0.0\n",
    "        res = (data.ffill().iloc[-1] / data.bfill().iloc[0]) - 1\n",
    "\n",
    "        if isinstance(res, (pd.Series, pd.DataFrame)):\n",
    "            return res.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "        return float(res) if np.isfinite(res) else 0.0\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_sharpe(\n",
    "        data: Union[pd.Series, pd.DataFrame],\n",
    "        periods: int = None,  # Default to None to trigger global lookup\n",
    "    ) -> Union[float, pd.Series]:\n",
    "        if periods is None:\n",
    "            periods = GLOBAL_SETTINGS[\"annual_period\"]\n",
    "        mu, std = data.mean(), data.std()\n",
    "        # Use np.maximum for universal floor (works on scalars and Series)\n",
    "        res = (mu / np.maximum(std, 1e-8)) * np.sqrt(periods)\n",
    "\n",
    "        if isinstance(res, (pd.Series, pd.DataFrame)):\n",
    "            return res.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "        return float(res) if np.isfinite(res) else 0.0\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_sharpe_vol(\n",
    "        returns: Union[pd.Series, pd.DataFrame],\n",
    "        vol_data: Union[pd.Series, pd.DataFrame],\n",
    "    ) -> Union[float, pd.Series]:\n",
    "        \"\"\"\n",
    "        Aligned Reward / Risk.\n",
    "        Filters out volatility observations where no return exists (e.g. Day 1 NaN).\n",
    "        \"\"\"\n",
    "        # 1. Identify valid timestamps (Pandas .mean() skips NaNs in returns)\n",
    "        # but we must manually force the volatility denominator to skip those same rows.\n",
    "        mask = returns.notna()\n",
    "        avg_ret = returns.mean()\n",
    "\n",
    "        # 2. Handle Logic Branches\n",
    "        if isinstance(returns, pd.DataFrame) and isinstance(vol_data, pd.Series):\n",
    "            # RANKING MODE: vol_data is usually a pre-calculated snapshot Series\n",
    "            avg_vol = vol_data\n",
    "        else:\n",
    "            # REPORT MODE (Series) or Cross-Sectional DataFrame\n",
    "            # Filter vol_data to only include rows where returns exist\n",
    "            avg_vol = vol_data.where(mask).mean()\n",
    "\n",
    "        # 3. Final Division\n",
    "        res = avg_ret / np.maximum(avg_vol, 1e-8)\n",
    "\n",
    "        if isinstance(res, (pd.Series, pd.DataFrame)):\n",
    "            return res.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "        return float(res) if np.isfinite(res) else 0.0\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_portfolio_stats(\n",
    "        prices: pd.DataFrame,\n",
    "        atrp_matrix: pd.DataFrame,\n",
    "        trp_matrix: pd.DataFrame,\n",
    "        weights: pd.Series,\n",
    "    ) -> Tuple[pd.Series, pd.Series, pd.Series, pd.Series]:\n",
    "        \"\"\"\n",
    "        MATRIX KERNEL: Calculates equity curve and weighted volatility.\n",
    "        \"\"\"\n",
    "        # 1. Equity Curve Logic (Price-Weighted Drift)\n",
    "        norm_prices = prices.div(prices.bfill().iloc[0])\n",
    "        weighted_components = norm_prices.mul(weights, axis=1)\n",
    "        equity_curve = weighted_components.sum(axis=1)\n",
    "\n",
    "        # MANDATORY: Use internal compute_returns to preserve boundary NaN\n",
    "        returns_WITH_BOUNDARY_NAN = QuantUtils.compute_returns(equity_curve)\n",
    "\n",
    "        # 2. Portfolio Volatility Logic (Weighted Average)\n",
    "        # We calculate current_weights (rebalanced daily by price drift)\n",
    "        current_weights = weighted_components.div(equity_curve, axis=0)\n",
    "\n",
    "        # Weighted average of ATRP and TRP\n",
    "        portfolio_atrp = (current_weights * atrp_matrix).sum(axis=1, min_count=1)\n",
    "        portfolio_trp = (current_weights * trp_matrix).sum(axis=1, min_count=1)\n",
    "\n",
    "        return equity_curve, returns_WITH_BOUNDARY_NAN, portfolio_atrp, portfolio_trp\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION B: STRATEGY HELPERS & FEATURES\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "def generate_features(\n",
    "    df_ohlcv: pd.DataFrame,\n",
    "    df_indices: pd.DataFrame = None,\n",
    "    benchmark_ticker: str = None,\n",
    "    atr_period: int = None,\n",
    "    rsi_period: int = None,\n",
    "    features_base_window: int = None,\n",
    "    features_fast_window: int = None,\n",
    "    feature_zscore_clip: float = None,\n",
    "    feature_ratio_clip: float = None,\n",
    "    quality_window: int = None,\n",
    "    quality_min_periods: int = None,\n",
    ") -> pd.DataFrame:\n",
    "    # --- 1. RESOLVE GLOBALS ---\n",
    "    benchmark_ticker = benchmark_ticker or GLOBAL_SETTINGS[\"benchmark_ticker\"]\n",
    "    atr_period = atr_period or GLOBAL_SETTINGS[\"atr_period\"]\n",
    "    rsi_period = rsi_period or GLOBAL_SETTINGS[\"rsi_period\"]\n",
    "    features_base_window = (\n",
    "        features_base_window or GLOBAL_SETTINGS[\"features_base_window\"]\n",
    "    )\n",
    "    features_fast_window = (\n",
    "        features_fast_window or GLOBAL_SETTINGS[\"features_fast_window\"]\n",
    "    )\n",
    "    feature_zscore_clip = feature_zscore_clip or GLOBAL_SETTINGS[\"feature_zscore_clip\"]\n",
    "    feature_ratio_clip = feature_ratio_clip or GLOBAL_SETTINGS[\"feature_ratio_clip\"]\n",
    "    quality_window = quality_window or GLOBAL_SETTINGS[\"quality_window\"]\n",
    "    quality_min_periods = quality_min_periods or GLOBAL_SETTINGS[\"quality_min_periods\"]\n",
    "\n",
    "    print(\n",
    "        f\"‚ö° Generating Features (Base: {features_base_window}d, Ratio Clip: {feature_ratio_clip})...\"\n",
    "    )\n",
    "\n",
    "    if not df_ohlcv.index.is_monotonic_increasing:\n",
    "        df_ohlcv = df_ohlcv.sort_index()\n",
    "    grouped = df_ohlcv.groupby(level=\"Ticker\")\n",
    "\n",
    "    # 2. VECTORIZED ATR\n",
    "    prev_close = grouped[\"Adj Close\"].shift(1)\n",
    "    tr = pd.concat(\n",
    "        [\n",
    "            df_ohlcv[\"Adj High\"] - df_ohlcv[\"Adj Low\"],\n",
    "            abs(df_ohlcv[\"Adj High\"] - prev_close),\n",
    "            abs(df_ohlcv[\"Adj Low\"] - prev_close),\n",
    "        ],\n",
    "        axis=1,\n",
    "    ).max(axis=1, skipna=False)\n",
    "\n",
    "    atr = (\n",
    "        tr.groupby(level=\"Ticker\")\n",
    "        .ewm(alpha=1 / atr_period, adjust=False)\n",
    "        .mean()\n",
    "        .reset_index(level=0, drop=True)\n",
    "    )\n",
    "    atrp = (atr / df_ohlcv[\"Adj Close\"]).replace([np.inf, -np.inf], np.nan)\n",
    "    trp = (tr / df_ohlcv[\"Adj Close\"]).replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    # 3. VECTORIZED RSI\n",
    "    delta = grouped[\"Adj Close\"].diff()\n",
    "    up, down = delta.clip(lower=0), -1 * delta.clip(upper=0)\n",
    "    ma_up = (\n",
    "        up.groupby(level=\"Ticker\")\n",
    "        .ewm(alpha=1 / rsi_period, adjust=False)\n",
    "        .mean()\n",
    "        .reset_index(level=0, drop=True)\n",
    "    )\n",
    "    ma_down = (\n",
    "        down.groupby(level=\"Ticker\")\n",
    "        .ewm(alpha=1 / rsi_period, adjust=False)\n",
    "        .mean()\n",
    "        .reset_index(level=0, drop=True)\n",
    "    )\n",
    "    rsi = 100 - (100 / (1 + (ma_up / ma_down))).replace([np.inf, -np.inf], 50).fillna(\n",
    "        50\n",
    "    )\n",
    "\n",
    "    # 4. OBV Score (Ticker Specific)\n",
    "    direction = np.sign(delta).fillna(0)\n",
    "    obv_raw = (direction * df_ohlcv[\"Volume\"]).groupby(level=\"Ticker\").cumsum()\n",
    "    obv_roll_mean = (\n",
    "        obv_raw.groupby(level=\"Ticker\")\n",
    "        .rolling(features_base_window)\n",
    "        .mean()\n",
    "        .reset_index(level=0, drop=True)\n",
    "    )\n",
    "    obv_roll_std = (\n",
    "        obv_raw.groupby(level=\"Ticker\")\n",
    "        .rolling(features_base_window)\n",
    "        .std()\n",
    "        .reset_index(level=0, drop=True)\n",
    "    )\n",
    "    obv_score = (\n",
    "        ((obv_raw - obv_roll_mean) / obv_roll_std)\n",
    "        .fillna(0.0)\n",
    "        .clip(lower=-feature_zscore_clip, upper=feature_zscore_clip)\n",
    "    )\n",
    "\n",
    "    # 5. BENCHMARK SENSORS\n",
    "    dollar_vol_series = df_ohlcv[\"Adj Close\"] * df_ohlcv[\"Volume\"]\n",
    "    rel_strength_base = pd.Series(0.0, index=df_ohlcv.index)\n",
    "    spy_rvol = pd.Series(1.0, index=df_ohlcv.index)\n",
    "    spy_obv_score = pd.Series(0.0, index=df_ohlcv.index)\n",
    "\n",
    "    found_bench = False\n",
    "    if (\n",
    "        df_indices is not None\n",
    "        and benchmark_ticker in df_indices.index.get_level_values(0)\n",
    "    ):\n",
    "        bench_close = df_indices.xs(benchmark_ticker, level=0)[\"Adj Close\"]\n",
    "        bench_vol = df_indices.xs(benchmark_ticker, level=0)[\"Volume\"]\n",
    "        found_bench = True\n",
    "    elif benchmark_ticker in df_ohlcv.index.get_level_values(0):\n",
    "        bench_close = df_ohlcv.xs(benchmark_ticker, level=0)[\"Adj Close\"]\n",
    "        bench_vol = df_ohlcv.xs(benchmark_ticker, level=0)[\"Volume\"]\n",
    "        found_bench = True\n",
    "\n",
    "    if found_bench:\n",
    "        try:\n",
    "            # Relative Strength (Using Base Window)\n",
    "            bench_close_aligned = bench_close.reindex(\n",
    "                df_ohlcv.index.get_level_values(\"Date\")\n",
    "            ).values\n",
    "            rel_strength_base = (\n",
    "                (df_ohlcv[\"Adj Close\"] / bench_close_aligned)\n",
    "                .groupby(level=\"Ticker\")\n",
    "                .pct_change(features_base_window, fill_method=None)\n",
    "                .fillna(0.0)\n",
    "            )\n",
    "\n",
    "            # SPY RVol\n",
    "            bench_dvol = bench_close * bench_vol\n",
    "            bench_dvol_avg = bench_dvol.rolling(features_base_window).mean()\n",
    "            spy_rvol_vals = (\n",
    "                (bench_dvol / bench_dvol_avg)\n",
    "                .reindex(df_ohlcv.index.get_level_values(\"Date\"))\n",
    "                .fillna(1.0)\n",
    "                .values\n",
    "            )\n",
    "            spy_rvol = pd.Series(spy_rvol_vals, index=df_ohlcv.index).clip(\n",
    "                upper=feature_ratio_clip\n",
    "            )\n",
    "\n",
    "            # SPY OBV\n",
    "            spy_dir = np.sign(bench_close.diff()).fillna(0)\n",
    "            spy_obv = (spy_dir * bench_vol).cumsum()\n",
    "            spy_obv_z = (\n",
    "                (\n",
    "                    (spy_obv - spy_obv.rolling(features_base_window).mean())\n",
    "                    / spy_obv.rolling(features_base_window).std()\n",
    "                )\n",
    "                .fillna(0.0)\n",
    "                .clip(lower=-feature_zscore_clip, upper=feature_zscore_clip)\n",
    "            )\n",
    "            spy_obv_score = pd.Series(\n",
    "                spy_obv_z.reindex(df_ohlcv.index.get_level_values(\"Date\")).values,\n",
    "                index=df_ohlcv.index,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Benchmark Math Error: {e}\")\n",
    "\n",
    "    # 6. TICKER RVol\n",
    "    dvol_avg = (\n",
    "        dollar_vol_series.groupby(level=\"Ticker\")\n",
    "        .rolling(features_base_window)\n",
    "        .mean()\n",
    "        .reset_index(level=0, drop=True)\n",
    "    )\n",
    "    ticker_rvol = (\n",
    "        (dollar_vol_series / dvol_avg)\n",
    "        .replace([np.inf, -np.inf], 1.0)\n",
    "        .fillna(1.0)\n",
    "        .clip(upper=feature_ratio_clip)\n",
    "    )\n",
    "\n",
    "    # 7. MOMENTUM VECTORS (Static as requested)\n",
    "    roc_1 = grouped[\"Adj Close\"].pct_change(1, fill_method=None)\n",
    "    roc_3 = grouped[\"Adj Close\"].pct_change(3, fill_method=None)\n",
    "    roc_5 = grouped[\"Adj Close\"].pct_change(5, fill_method=None)\n",
    "    roc_10 = grouped[\"Adj Close\"].pct_change(10, fill_method=None)\n",
    "    roc_21 = grouped[\"Adj Close\"].pct_change(21, fill_method=None)\n",
    "\n",
    "    # 8. VOLATILITY REGIME (Fast / Base)\n",
    "    rets = grouped[\"Adj Close\"].pct_change(1, fill_method=None)\n",
    "    std_fast = (\n",
    "        rets.groupby(level=\"Ticker\")\n",
    "        .rolling(features_fast_window)\n",
    "        .std()\n",
    "        .reset_index(level=0, drop=True)\n",
    "    )\n",
    "    std_base = (\n",
    "        rets.groupby(level=\"Ticker\")\n",
    "        .rolling(features_base_window)\n",
    "        .std()\n",
    "        .reset_index(level=0, drop=True)\n",
    "    )\n",
    "    vol_regime = (std_fast / std_base).replace([np.inf, -np.inf], 1.0)\n",
    "\n",
    "    # 9. OUTPUT\n",
    "    indicator_df = pd.DataFrame(\n",
    "        {\n",
    "            \"ATR\": atr,\n",
    "            \"ATRP\": atrp,\n",
    "            \"TRP\": trp,\n",
    "            \"RSI\": rsi,\n",
    "            \"RelStrength\": rel_strength_base,\n",
    "            \"VolRegime\": vol_regime,\n",
    "            \"RVol\": ticker_rvol,\n",
    "            \"Spy_RVol\": spy_rvol,\n",
    "            \"OBV_Score\": obv_score,\n",
    "            \"Spy_OBV_Score\": spy_obv_score,\n",
    "            \"ROC_1\": roc_1,\n",
    "            \"ROC_3\": roc_3,\n",
    "            \"ROC_5\": roc_5,\n",
    "            \"ROC_10\": roc_10,\n",
    "            \"ROC_21\": roc_21,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # 10. QUALITY/LIQUIDITY\n",
    "    quality_temp = pd.DataFrame(\n",
    "        {\n",
    "            \"IsStale\": np.where(\n",
    "                (df_ohlcv[\"Volume\"] == 0)\n",
    "                | (df_ohlcv[\"Adj High\"] == df_ohlcv[\"Adj Low\"]),\n",
    "                1,\n",
    "                0,\n",
    "            ),\n",
    "            \"DollarVolume\": dollar_vol_series,\n",
    "            \"HasSameVolume\": (grouped[\"Volume\"].diff() == 0).astype(int),\n",
    "        },\n",
    "        index=df_ohlcv.index,\n",
    "    )\n",
    "\n",
    "    rolling_quality = (\n",
    "        quality_temp.groupby(level=\"Ticker\")\n",
    "        .rolling(window=quality_window, min_periods=quality_min_periods)\n",
    "        .agg({\"IsStale\": \"mean\", \"DollarVolume\": \"median\", \"HasSameVolume\": \"sum\"})\n",
    "        .rename(\n",
    "            columns={\n",
    "                \"IsStale\": \"RollingStalePct\",\n",
    "                \"DollarVolume\": \"RollMedDollarVol\",\n",
    "                \"HasSameVolume\": \"RollingSameVolCount\",\n",
    "            }\n",
    "        )\n",
    "        .reset_index(level=0, drop=True)\n",
    "    )\n",
    "\n",
    "    return pd.concat([indicator_df, rolling_quality], axis=1)\n",
    "\n",
    "\n",
    "def _prepare_initial_weights(tickers: List[str]) -> pd.Series:\n",
    "    \"\"\"\n",
    "    METADATA: Converts a list of tickers into a weight map.\n",
    "    Example: ['AAPL', 'AAPL', 'TSLA'] -> {'AAPL': 0.66, 'TSLA': 0.33}\n",
    "    \"\"\"\n",
    "    ticker_counts = Counter(tickers)\n",
    "    total = len(tickers)\n",
    "    return pd.Series({t: c / total for t, c in ticker_counts.items()})\n",
    "\n",
    "\n",
    "def calculate_buy_and_hold_performance(\n",
    "    df_close_wide: pd.DataFrame,  # Use the WIDE version\n",
    "    df_atrp_wide: pd.DataFrame,  # Use the WIDE version\n",
    "    df_trp_wide: pd.DataFrame,  # <--- Added\n",
    "    tickers: List[str],\n",
    "    start_date: pd.Timestamp,\n",
    "    end_date: pd.Timestamp,\n",
    "):\n",
    "    if not tickers:\n",
    "        return pd.Series(), pd.Series(), pd.Series()\n",
    "\n",
    "    initial_weights = _prepare_initial_weights(tickers)\n",
    "\n",
    "    # SLICE (Fix Part B)\n",
    "    ticker_list = initial_weights.index.tolist()\n",
    "    p_slice = df_close_wide.reindex(columns=ticker_list).loc[start_date:end_date]\n",
    "    a_slice = df_atrp_wide.reindex(columns=ticker_list).loc[start_date:end_date]\n",
    "    t_slice = df_trp_wide.reindex(columns=ticker_list).loc[start_date:end_date]\n",
    "    # KERNEL - Pure Math\n",
    "    return QuantUtils.compute_portfolio_stats(\n",
    "        p_slice, a_slice, t_slice, initial_weights\n",
    "    )\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION C: METRIC REGISTRY\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "class MarketObservation(TypedDict):\n",
    "    \"\"\"\n",
    "    The 'STATE' (Observation) in Reinforcement Learning.\n",
    "    This defines the context given to the agent to make a decision.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- The Movie (Time Series) ---\n",
    "    lookback_returns: pd.DataFrame  # (Time x Tickers)\n",
    "    lookback_close: pd.DataFrame  # (Time x Tickers)\n",
    "\n",
    "    # --- The Snapshot (Scalar values at Decision Time) ---\n",
    "    atrp: pd.Series  # Volatility (Mean over lookback)\n",
    "    trp: pd.Series  # Volatility (Snapshot)\n",
    "\n",
    "    # NEW SENSORS\n",
    "    rsi: pd.Series  # Internal Momentum (0-100)\n",
    "    rel_strength: pd.Series  # Performance vs SPY\n",
    "    vol_regime: pd.Series  # Volatility Expansion/Compression\n",
    "    rvol: pd.Series  # Ticker Conviction\n",
    "    spy_rvol: pd.Series  # Market Participation\n",
    "    obv_score: pd.Series  # Ticker Accumulation/Distribution\n",
    "    spy_obv_score: pd.Series  # Market Tide\n",
    "\n",
    "    # MOMENTUM VECTORS\n",
    "    roc_1: pd.Series\n",
    "    roc_3: pd.Series\n",
    "    roc_5: pd.Series\n",
    "    roc_10: pd.Series\n",
    "    roc_21: pd.Series\n",
    "\n",
    "\n",
    "METRIC_REGISTRY = {\n",
    "    # --- CLASSIC METRICS ---\n",
    "    \"Price\": lambda obs: QuantUtils.calculate_gain(obs[\"lookback_close\"]),\n",
    "    \"Sharpe\": lambda obs: QuantUtils.calculate_sharpe(obs[\"lookback_returns\"]),\n",
    "    \"Sharpe (ATRP)\": lambda obs: QuantUtils.calculate_sharpe_vol(\n",
    "        obs[\"lookback_returns\"], obs[\"atrp\"]\n",
    "    ),\n",
    "    \"Sharpe (TRP)\": lambda obs: QuantUtils.calculate_sharpe_vol(\n",
    "        obs[\"lookback_returns\"], obs[\"trp\"]\n",
    "    ),  # <--- New Strategy\n",
    "    # --- MOMENTUM VECTORS ---\n",
    "    \"Momentum 1D\": lambda obs: obs[\"roc_1\"],\n",
    "    \"Momentum 3D\": lambda obs: obs[\"roc_3\"],\n",
    "    \"Momentum 5D\": lambda obs: obs[\"roc_5\"],\n",
    "    \"Momentum 10D\": lambda obs: obs[\"roc_10\"],\n",
    "    \"Momentum 1M\": lambda obs: obs[\"roc_21\"],\n",
    "    # --- PULLBACK VECTORS ---\n",
    "    \"Pullback 1D\": lambda obs: -obs[\"roc_1\"],\n",
    "    \"Pullback 3D\": lambda obs: -obs[\"roc_3\"],\n",
    "    \"Pullback 5D\": lambda obs: -obs[\"roc_5\"],\n",
    "    \"Pullback 10D\": lambda obs: -obs[\"roc_10\"],\n",
    "    \"Pullback 1M\": lambda obs: -obs[\"roc_21\"],\n",
    "    # --- NEW SOTA SENSORS ---\n",
    "    \"RSI (Reversal)\": lambda obs: -obs[\"rsi\"],  # Rank Low RSI (Oversold) higher\n",
    "    \"RSI (Trend)\": lambda obs: obs[\"rsi\"],  # Rank High RSI (Strong Trend) higher\n",
    "    \"Alpha (RelStrength)\": lambda obs: obs[\"rel_strength\"],  # Rank stocks beating SPY\n",
    "    \"OBV_Score (Accumulation)\": lambda obs: obs[\"obv_score\"],  # Rank High OBV Score\n",
    "    \"$V Inc (Cur$V / Avg$V21)\": lambda obs: obs[\"rvol\"],  # Rank High Relative Volume\n",
    "    \"Vol Inc (5dStDev / 21dStDev)\": lambda obs: obs[\n",
    "        \"vol_regime\"\n",
    "    ],  # Rank High Volatility Expansion\n",
    "}\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION D: DATA CONTRACTS\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EngineInput:\n",
    "    mode: str\n",
    "    start_date: pd.Timestamp\n",
    "    lookback_period: int\n",
    "    holding_period: int\n",
    "    metric: str\n",
    "    benchmark_ticker: str\n",
    "    rank_start: int = 1\n",
    "    rank_end: int = 10\n",
    "    # Default factory pulls from Global thresholds\n",
    "    quality_thresholds: Dict[str, float] = field(\n",
    "        default_factory=lambda: GLOBAL_SETTINGS[\"thresholds\"].copy()\n",
    "    )\n",
    "    manual_tickers: List[str] = field(default_factory=list)\n",
    "    debug: bool = False\n",
    "    universe_subset: Optional[List[str]] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EngineOutput:\n",
    "    # 1. CORE DATA (Required - No Defaults)\n",
    "    portfolio_series: pd.Series\n",
    "    benchmark_series: pd.Series\n",
    "    normalized_plot_data: pd.DataFrame\n",
    "    tickers: List[str]\n",
    "    initial_weights: pd.Series\n",
    "    perf_metrics: Dict[str, float]\n",
    "    results_df: pd.DataFrame\n",
    "\n",
    "    # 2. TIMELINE (Required - No Defaults)\n",
    "    start_date: pd.Timestamp\n",
    "    decision_date: pd.Timestamp\n",
    "    buy_date: pd.Timestamp\n",
    "    holding_end_date: pd.Timestamp\n",
    "\n",
    "    # 3. OPTIONAL / AUDIT DATA (Must be at the bottom because they have defaults)\n",
    "    portfolio_atrp_series: Optional[pd.Series] = None\n",
    "    benchmark_atrp_series: Optional[pd.Series] = None\n",
    "    portfolio_trp_series: Optional[pd.Series] = None\n",
    "    benchmark_trp_series: Optional[pd.Series] = None\n",
    "    error_msg: Optional[str] = None\n",
    "    debug_data: Optional[Dict[str, Any]] = None\n",
    "\n",
    "\n",
    "class AlphaEngine:\n",
    "    def __init__(\n",
    "        self,\n",
    "        df_ohlcv: pd.DataFrame,\n",
    "        features_df: pd.DataFrame = None,\n",
    "        df_close_wide: pd.DataFrame = None,\n",
    "        df_atrp_wide: pd.DataFrame = None,\n",
    "        df_trp_wide: pd.DataFrame = None,\n",
    "        master_ticker: str = GLOBAL_SETTINGS[\"calendar_ticker\"],\n",
    "    ):\n",
    "        print(\"--- ‚öôÔ∏è Initializing AlphaEngine v2.2 (Sanitized) ---\")\n",
    "\n",
    "        # 1. SETUP PRICES (CLEAN-AT-ENTRY)\n",
    "        if df_close_wide is not None:\n",
    "            self.df_close = df_close_wide\n",
    "        else:\n",
    "            print(\"üê¢ Pivoting and Sanitizing Price Data...\")\n",
    "            self.df_close = df_ohlcv[\"Adj Close\"].unstack(level=0)\n",
    "\n",
    "        # STORE RAW SOURCE (The \"Transparency\" Line)\n",
    "        self.df_ohlcv_raw = df_ohlcv\n",
    "\n",
    "        # 3. DATA SANITIZER STEP 1: Handle Zeros\n",
    "        if GLOBAL_SETTINGS[\"handle_zeros_as_nan\"]:\n",
    "            # Replace 0.0 with NaN so math functions (mean/std) ignore them\n",
    "            self.df_close = self.df_close.replace(0, np.nan)\n",
    "\n",
    "        # Smooth over 1-2 day glitches (The \"FNV\" Fix)\n",
    "        self.df_close = self.df_close.ffill(limit=GLOBAL_SETTINGS[\"max_data_gap_ffill\"])\n",
    "\n",
    "        # Handle the remaining \"unfillable\" gaps\n",
    "        self.df_close = self.df_close.fillna(GLOBAL_SETTINGS[\"nan_price_replacement\"])\n",
    "\n",
    "        # 2. SETUP FEATURES\n",
    "        if features_df is not None:\n",
    "            self.features_df = features_df\n",
    "        else:\n",
    "            # We pass the cleaned price data if needed, or calculate from raw\n",
    "            self.features_df = generate_features(\n",
    "                df_ohlcv,\n",
    "                atr_period=GLOBAL_SETTINGS[\"atr_period\"],\n",
    "                quality_window=GLOBAL_SETTINGS[\"quality_window\"],\n",
    "                quality_min_periods=GLOBAL_SETTINGS[\"quality_min_periods\"],\n",
    "            )\n",
    "\n",
    "        # 1. SETUP ATRP\n",
    "        if df_atrp_wide is not None:\n",
    "            # INSTANT: Use the matrix precomputed outside the UI\n",
    "            self.df_atrp = df_atrp_wide\n",
    "        else:\n",
    "            # SLOW FALLBACK: Only runs if you forget to precompute\n",
    "            print(\"üöÄ Pre-aligning Volatility (ATRP) Matrix (Slow Fallback)...\")\n",
    "            self.df_atrp = self.features_df[\"ATRP\"].unstack(level=0)\n",
    "\n",
    "        # 2. SETUP TRP\n",
    "        if df_trp_wide is not None:\n",
    "            self.df_trp = df_trp_wide\n",
    "        else:\n",
    "            print(\"üöÄ Pre-aligning Volatility (TRP) Matrix (Slow Fallback)...\")\n",
    "            self.df_trp = self.features_df[\"TRP\"].unstack(level=0)\n",
    "\n",
    "        # 3. FINAL ALIGNMENT (The \"Safety Seal\")\n",
    "        # Ensures all matrices have the exact same Dimensions, Tickers, and Dates\n",
    "        common_idx = self.df_close.index\n",
    "        common_cols = self.df_close.columns\n",
    "\n",
    "        self.df_atrp = self.df_atrp.reindex(index=common_idx, columns=common_cols)\n",
    "        self.df_trp = self.df_trp.reindex(index=common_idx, columns=common_cols)\n",
    "\n",
    "        # 3. Setup Calendar\n",
    "        if master_ticker not in self.df_close.columns:\n",
    "            master_ticker = self.df_close.columns[0]\n",
    "        self.trading_calendar = (\n",
    "            self.df_close[master_ticker].dropna().index.unique().sort_values()\n",
    "        )\n",
    "\n",
    "    def run(self, inputs: EngineInput) -> EngineOutput:\n",
    "        dates, error = self._validate_timeline(inputs)\n",
    "        if error:\n",
    "            return self._error_result(error)\n",
    "        (safe_start, safe_decision, safe_buy, safe_end) = dates\n",
    "\n",
    "        # 1. Pass the debug flag into the selection logic\n",
    "        tickers_to_trade, results_table, debug_dict, error = self._select_tickers(\n",
    "            inputs, safe_start, safe_decision\n",
    "        )\n",
    "        if error:\n",
    "            return self._error_result(error)\n",
    "\n",
    "        # 2. CORE PERFORMANCE CALCULATION (Required for all modes)\n",
    "        p_f_val, p_f_ret, p_f_atrp, p_f_trp = calculate_buy_and_hold_performance(\n",
    "            self.df_close,\n",
    "            self.df_atrp,\n",
    "            self.df_trp,\n",
    "            tickers_to_trade,\n",
    "            safe_start,\n",
    "            safe_end,\n",
    "        )\n",
    "        b_f_val, b_f_ret, b_f_atrp, b_f_trp = calculate_buy_and_hold_performance(\n",
    "            self.df_close,\n",
    "            self.df_atrp,\n",
    "            self.df_trp,\n",
    "            [inputs.benchmark_ticker],\n",
    "            safe_start,\n",
    "            safe_end,\n",
    "        )\n",
    "\n",
    "        p_h_val, p_h_ret, p_h_atrp, p_h_trp = calculate_buy_and_hold_performance(\n",
    "            self.df_close,\n",
    "            self.df_atrp,\n",
    "            self.df_trp,\n",
    "            tickers_to_trade,\n",
    "            safe_buy,\n",
    "            safe_end,\n",
    "        )\n",
    "        b_h_val, b_h_ret, b_h_atrp, b_h_trp = calculate_buy_and_hold_performance(\n",
    "            self.df_close,\n",
    "            self.df_atrp,\n",
    "            self.df_trp,\n",
    "            [inputs.benchmark_ticker],\n",
    "            safe_buy,\n",
    "            safe_end,\n",
    "        )\n",
    "\n",
    "        p_metrics, p_slices = self._calculate_period_metrics(\n",
    "            p_f_val,\n",
    "            p_f_ret,\n",
    "            p_f_atrp,\n",
    "            p_f_trp,\n",
    "            safe_decision,\n",
    "            p_h_val,\n",
    "            p_h_ret,\n",
    "            p_h_atrp,\n",
    "            p_h_trp,\n",
    "            prefix=\"p\",\n",
    "        )\n",
    "        b_metrics, b_slices = self._calculate_period_metrics(\n",
    "            b_f_val,\n",
    "            b_f_ret,\n",
    "            b_f_atrp,\n",
    "            b_f_trp,\n",
    "            safe_decision,\n",
    "            b_h_val,\n",
    "            b_h_ret,\n",
    "            b_h_atrp,\n",
    "            b_h_trp,\n",
    "            prefix=\"b\",\n",
    "        )\n",
    "\n",
    "        # 3. CONDITIONAL DEBUG GATING (The RL \"Fast Path\")\n",
    "        # If debug is False, we skip the most memory-intensive tasks\n",
    "        final_debug_data = None\n",
    "        if inputs.debug:\n",
    "            idx_slice = pd.IndexSlice\n",
    "            debug_dict[\"inputs_snapshot\"] = inputs\n",
    "            debug_dict[\"verification\"] = {\"portfolio\": p_slices, \"benchmark\": b_slices}\n",
    "\n",
    "            debug_dict[\"portfolio_raw_components\"] = {\n",
    "                \"prices\": self.df_close[tickers_to_trade].loc[safe_start:safe_end],\n",
    "                \"atrp\": self.df_atrp[tickers_to_trade].loc[safe_start:safe_end],\n",
    "                \"trp\": self.df_trp[tickers_to_trade].loc[safe_start:safe_end],\n",
    "                \"ohlcv_raw\": self.df_ohlcv_raw.loc[\n",
    "                    idx_slice[tickers_to_trade, safe_start:safe_end], :\n",
    "                ],\n",
    "            }\n",
    "\n",
    "            debug_dict[\"benchmark_raw_components\"] = {\n",
    "                \"prices\": self.df_close[[inputs.benchmark_ticker]].loc[\n",
    "                    safe_start:safe_end\n",
    "                ],\n",
    "                \"atrp\": self.df_atrp[[inputs.benchmark_ticker]].loc[\n",
    "                    safe_start:safe_end\n",
    "                ],\n",
    "                \"trp\": self.df_trp[[inputs.benchmark_ticker]].loc[safe_start:safe_end],\n",
    "                \"ohlcv_raw\": self.df_ohlcv_raw.loc[\n",
    "                    idx_slice[[inputs.benchmark_ticker], safe_start:safe_end], :\n",
    "                ],\n",
    "            }\n",
    "            debug_dict[\"selection_audit\"] = debug_dict.get(\"full_universe_ranking\")\n",
    "            final_debug_data = debug_dict\n",
    "\n",
    "        # 4. FINAL OUTPUT SEAL\n",
    "        return EngineOutput(\n",
    "            portfolio_series=p_f_val,\n",
    "            benchmark_series=b_f_val,\n",
    "            portfolio_atrp_series=p_f_atrp if inputs.debug else None,\n",
    "            benchmark_atrp_series=b_f_atrp if inputs.debug else None,\n",
    "            portfolio_trp_series=p_f_trp if inputs.debug else None,\n",
    "            benchmark_trp_series=b_f_trp if inputs.debug else None,\n",
    "            normalized_plot_data=(\n",
    "                self._get_normalized_plot_data(tickers_to_trade, safe_start, safe_end)\n",
    "                if inputs.debug\n",
    "                else pd.DataFrame()\n",
    "            ),\n",
    "            tickers=tickers_to_trade,\n",
    "            initial_weights=_prepare_initial_weights(tickers_to_trade),\n",
    "            perf_metrics={**p_metrics, **b_metrics},\n",
    "            results_df=results_table,\n",
    "            start_date=safe_start,\n",
    "            decision_date=safe_decision,\n",
    "            buy_date=safe_buy,\n",
    "            holding_end_date=safe_end,\n",
    "            debug_data=final_debug_data,  # None if RL search mode\n",
    "        )\n",
    "\n",
    "    # ==============================================================================\n",
    "    # INTERNAL LOGIC MODULES\n",
    "    # ==============================================================================\n",
    "\n",
    "    def _validate_timeline(self, inputs: EngineInput):\n",
    "        cal = self.trading_calendar\n",
    "        last_idx = len(cal) - 1\n",
    "\n",
    "        if len(cal) <= inputs.lookback_period:\n",
    "            return (\n",
    "                None,\n",
    "                f\"‚ùå Dataset too small.\\nNeed > {inputs.lookback_period} days of history.\",\n",
    "            )\n",
    "\n",
    "        # 2. Check \"Past\" Constraints (Lookback)\n",
    "        min_decision_date = cal[inputs.lookback_period]\n",
    "        if inputs.start_date < min_decision_date:\n",
    "            # Added \\n here\n",
    "            return None, (\n",
    "                f\"‚ùå Not enough history for a {inputs.lookback_period}-day lookback.\\n\"\n",
    "                f\"Earliest valid Decision Date: {min_decision_date.date()}\"\n",
    "            )\n",
    "\n",
    "        # 3. Check \"Future\" Constraints (Entry T+1 and Holding Period)\n",
    "        required_future_days = 1 + inputs.holding_period\n",
    "        latest_valid_idx = last_idx - required_future_days\n",
    "\n",
    "        if latest_valid_idx < 0:\n",
    "            return (\n",
    "                None,\n",
    "                f\"‚ùå Holding period too long.\\n{inputs.holding_period} days exceeds available data.\",\n",
    "            )\n",
    "\n",
    "        # If user picked a date beyond the available \"future\" runway\n",
    "        if inputs.start_date > cal[latest_valid_idx]:\n",
    "            latest_date = cal[latest_valid_idx].date()\n",
    "            # Added \\n here and shortened the text slightly to fit better\n",
    "            return None, (\n",
    "                f\"‚ùå Decision Date too late for a {inputs.holding_period}-day hold.\\n\"\n",
    "                f\"Latest valid date: {latest_date}. Please move picker back.\"\n",
    "            )\n",
    "\n",
    "        # 4. Map the safe indices\n",
    "        decision_idx = cal.searchsorted(inputs.start_date)\n",
    "        if decision_idx > latest_valid_idx:\n",
    "            decision_idx = latest_valid_idx\n",
    "\n",
    "        start_idx = decision_idx - inputs.lookback_period\n",
    "        entry_idx = decision_idx + 1\n",
    "        end_idx = entry_idx + inputs.holding_period\n",
    "\n",
    "        return (cal[start_idx], cal[decision_idx], cal[entry_idx], cal[end_idx]), None\n",
    "\n",
    "    ############################\n",
    "    ############################\n",
    "\n",
    "    def _select_tickers_old_0(self, inputs: EngineInput, start_date, decision_date):\n",
    "        debug_dict = {}\n",
    "\n",
    "        # --- PATH A: MANUAL LIST ---\n",
    "        if inputs.mode == \"Manual List\":\n",
    "            validation_errors = []\n",
    "            valid_tickers = []\n",
    "            for t in inputs.manual_tickers:\n",
    "                if t not in self.df_close.columns:\n",
    "                    validation_errors.append(f\"‚ùå {t}: Not found.\")\n",
    "                    continue\n",
    "                if pd.isna(self.df_close.at[start_date, t]):\n",
    "                    validation_errors.append(f\"‚ö†Ô∏è {t}: No data on start date.\")\n",
    "                    continue\n",
    "                valid_tickers.append(t)\n",
    "\n",
    "            if validation_errors:\n",
    "                return [], pd.DataFrame(), {}, \"\\n\".join(validation_errors)\n",
    "            if not valid_tickers:\n",
    "                return [], pd.DataFrame(), {}, \"No valid tickers found.\"\n",
    "            return valid_tickers, pd.DataFrame(index=valid_tickers), {}, None\n",
    "\n",
    "        # --- PATH B: RANKING ---\n",
    "        else:\n",
    "            audit_info = {}\n",
    "            eligible_tickers = self._filter_universe(\n",
    "                decision_date, inputs.quality_thresholds, audit_info\n",
    "            )\n",
    "            debug_dict[\"audit_liquidity\"] = audit_info\n",
    "\n",
    "            if not eligible_tickers:\n",
    "                return (\n",
    "                    [],\n",
    "                    pd.DataFrame(),\n",
    "                    debug_dict,\n",
    "                    \"No tickers passed quality filters.\",\n",
    "                )\n",
    "\n",
    "            lookback_close = self.df_close.loc[\n",
    "                start_date:decision_date, eligible_tickers\n",
    "            ]\n",
    "\n",
    "            # 1. Get the Snapshot of Features for the Decision Date\n",
    "            feat_slice_current = self.features_df.xs(\n",
    "                decision_date, level=\"Date\"\n",
    "            ).reindex(eligible_tickers)\n",
    "\n",
    "            # Calculate mean ATRP over the lookback period\n",
    "            idx_product = pd.MultiIndex.from_product(\n",
    "                [eligible_tickers, lookback_close.index], names=[\"Ticker\", \"Date\"]\n",
    "            )\n",
    "            feat_slice_period = self.features_df.reindex(idx_product)\n",
    "            atrp_value_for_obs = (\n",
    "                feat_slice_period[\"ATRP\"].groupby(level=\"Ticker\").mean()\n",
    "            )\n",
    "\n",
    "            # --- PINPOINT CHANGE: Calculate mean TRP over lookback ---\n",
    "            trp_value_for_obs = feat_slice_period[\"TRP\"].groupby(level=\"Ticker\").mean()\n",
    "\n",
    "            # Update the observation dictionary\n",
    "            observation: MarketObservation = {\n",
    "                # ...\n",
    "                \"atrp\": atrp_value_for_obs,\n",
    "                \"trp\": trp_value_for_obs,  # <--- PINPOINT CHANGE: Pass the lookback mean\n",
    "                # ...\n",
    "            }\n",
    "\n",
    "            # 2. Package the Observation (The 'State')\n",
    "            observation: MarketObservation = {\n",
    "                # Time Series Data\n",
    "                \"lookback_close\": lookback_close,\n",
    "                \"lookback_returns\": lookback_close.ffill().pct_change(),\n",
    "                # Snapshot Data (Scalar values for today)\n",
    "                \"atrp\": atrp_value_for_obs,  # <--- USES THE TOGGLED VALUE HERE\n",
    "                \"trp\": trp_value_for_obs,  # <--- PINPOINT CHANGE: Pass the lookback mean\n",
    "                \"rsi\": feat_slice_current[\"RSI\"],\n",
    "                \"rel_strength\": feat_slice_current[\"RelStrength\"],\n",
    "                \"vol_regime\": feat_slice_current[\"VolRegime\"],\n",
    "                \"rvol\": feat_slice_current[\"RVol\"],\n",
    "                \"spy_rvol\": feat_slice_current[\"Spy_RVol\"],\n",
    "                \"obv_score\": feat_slice_current[\"OBV_Score\"],\n",
    "                \"spy_obv_score\": feat_slice_current[\"Spy_OBV_Score\"],\n",
    "                # Momentum Vectors\n",
    "                \"roc_1\": feat_slice_current[\"ROC_1\"],\n",
    "                \"roc_3\": feat_slice_current[\"ROC_3\"],\n",
    "                \"roc_5\": feat_slice_current[\"ROC_5\"],\n",
    "                \"roc_10\": feat_slice_current[\"ROC_10\"],\n",
    "                \"roc_21\": feat_slice_current[\"ROC_21\"],\n",
    "            }\n",
    "\n",
    "            # 3. Run the Strategy (The 'Agent')\n",
    "            if inputs.metric not in METRIC_REGISTRY:\n",
    "                return [], pd.DataFrame(), {}, f\"Strategy '{inputs.metric}' not found.\"\n",
    "\n",
    "            metric_vals = METRIC_REGISTRY[inputs.metric](observation)\n",
    "            sorted_tickers = metric_vals.sort_values(ascending=False)\n",
    "            start_r = max(0, inputs.rank_start - 1)\n",
    "            end_r = inputs.rank_end\n",
    "            selected_tickers = sorted_tickers.iloc[start_r:end_r].index.tolist()\n",
    "\n",
    "            # Audit\n",
    "            debug_dict[\"full_universe_ranking\"] = pd.DataFrame(\n",
    "                {\n",
    "                    \"Strategy_Score\": metric_vals,\n",
    "                    \"Lookback_Return_Ann\": observation[\"lookback_returns\"].mean() * 252,\n",
    "                    \"Lookback_ATRP\": observation[\"atrp\"],\n",
    "                }\n",
    "            )\n",
    "\n",
    "            if not selected_tickers:\n",
    "                return (\n",
    "                    [],\n",
    "                    pd.DataFrame(),\n",
    "                    debug_dict,\n",
    "                    \"No tickers generated from ranking.\",\n",
    "                )\n",
    "\n",
    "            results_table = pd.DataFrame(\n",
    "                {\n",
    "                    \"Rank\": range(\n",
    "                        inputs.rank_start, inputs.rank_start + len(selected_tickers)\n",
    "                    ),\n",
    "                    \"Ticker\": selected_tickers,\n",
    "                    \"Strategy Value\": sorted_tickers.loc[selected_tickers].values,\n",
    "                }\n",
    "            ).set_index(\"Ticker\")\n",
    "\n",
    "            return selected_tickers, results_table, debug_dict, None\n",
    "\n",
    "    def _select_tickers_old_1(self, inputs: EngineInput, start_date, decision_date):\n",
    "        \"\"\"\n",
    "        Logic:\n",
    "        1. Identifies the initial universe (Manual, Subset, or Full Filtered).\n",
    "        2. Validates data availability.\n",
    "        3. Executes the Strategy Metric (Ranking).\n",
    "        \"\"\"\n",
    "\n",
    "        debug_dict = {}\n",
    "\n",
    "        # ------------------- DEBUG PRINTS START -------------------\n",
    "        print(f\"\\nüïµÔ∏è DEBUG PROBE:\")\n",
    "        print(f\"   - Input Mode: {inputs.mode}\")\n",
    "        print(f\"   - Universe Subset Type: {type(inputs.universe_subset)}\")\n",
    "        if inputs.universe_subset:\n",
    "            print(f\"   - Universe Subset Length: {len(inputs.universe_subset)}\")\n",
    "        else:\n",
    "            print(f\"   - Universe Subset is EMPTY or NONE\")\n",
    "        # ------------------- DEBUG PRINTS END ---------------------\n",
    "\n",
    "        # ==============================================================================\n",
    "        # PHASE 1: UNIVERSE DETERMINATION (The Gate)\n",
    "        # ==============================================================================\n",
    "\n",
    "        # --- PATH A: MANUAL LIST ---\n",
    "        if inputs.mode == \"Manual List\":\n",
    "            validation_errors = []\n",
    "            valid_tickers = []\n",
    "            for t in inputs.manual_tickers:\n",
    "                if t not in self.df_close.columns:\n",
    "                    validation_errors.append(f\"‚ùå {t}: Not found.\")\n",
    "                    continue\n",
    "                valid_tickers.append(t)\n",
    "            if not valid_tickers:\n",
    "                return (\n",
    "                    [],\n",
    "                    pd.DataFrame(),\n",
    "                    {},\n",
    "                    (\n",
    "                        \"\\n\".join(validation_errors)\n",
    "                        if validation_errors\n",
    "                        else \"No valid tickers.\"\n",
    "                    ),\n",
    "                )\n",
    "            return valid_tickers, pd.DataFrame(index=valid_tickers), {}, None\n",
    "\n",
    "        # --- PATH B: RANKING ---\n",
    "        else:\n",
    "            # CHECK FOR SUBSET (Stage 2 / Cascade Mode)\n",
    "            if inputs.universe_subset is not None:\n",
    "                print(\"‚úÖ DEBUG: ENTERING CASCADE PATH (Using Subset)\")  # <--- PRINT 2\n",
    "                # üõ°Ô∏è This ensures Stage 2 is strictly limited to Stage 1 results\n",
    "                eligible_tickers = [\n",
    "                    t for t in inputs.universe_subset if t in self.df_close.columns\n",
    "                ]\n",
    "                if inputs.debug:\n",
    "                    debug_dict[\"audit_liquidity\"] = {\n",
    "                        \"mode\": \"Cascade/Subset\",\n",
    "                        \"tickers_passed\": len(eligible_tickers),\n",
    "                        \"universe_snapshot\": pd.DataFrame(\n",
    "                            index=eligible_tickers\n",
    "                        ),  # Minimal snapshot for audit\n",
    "                    }\n",
    "                    # Force \"Passed_Final\" column for the Audit functions to see\n",
    "                    debug_dict[\"audit_liquidity\"][\"universe_snapshot\"][\n",
    "                        \"Passed_Final\"\n",
    "                    ] = True\n",
    "\n",
    "            # NO SUBSET (Stage 1 / Discovery Mode)\n",
    "            else:\n",
    "                print(\n",
    "                    \"‚ö†Ô∏è DEBUG: ENTERING DISCOVERY PATH (Full Market Scan)\"\n",
    "                )  # <--- PRINT 3\n",
    "                audit_info = {}\n",
    "                eligible_tickers = self._filter_universe(\n",
    "                    decision_date, inputs.quality_thresholds, audit_info\n",
    "                )\n",
    "                if inputs.debug:\n",
    "                    debug_dict[\"audit_liquidity\"] = audit_info\n",
    "\n",
    "            if not eligible_tickers:\n",
    "                return (\n",
    "                    [],\n",
    "                    pd.DataFrame(),\n",
    "                    debug_dict,\n",
    "                    \"No tickers passed universe filters.\",\n",
    "                )\n",
    "\n",
    "            # ==============================================================================\n",
    "            # PHASE 2: RANKING (The Strategy)\n",
    "            # ==============================================================================\n",
    "            # A. Slicing Price History & Features\n",
    "            lookback_close = self.df_close.loc[\n",
    "                start_date:decision_date, eligible_tickers\n",
    "            ]\n",
    "            feat_slice_current = self.features_df.xs(\n",
    "                decision_date, level=\"Date\"\n",
    "            ).reindex(eligible_tickers)\n",
    "\n",
    "            # B. Volatility means\n",
    "            idx_slice = pd.IndexSlice\n",
    "            feat_slice_period = self.features_df.loc[\n",
    "                idx_slice[eligible_tickers, start_date:decision_date], :\n",
    "            ]\n",
    "            atrp_mean = feat_slice_period[\"ATRP\"].groupby(level=\"Ticker\").mean()\n",
    "            trp_mean = feat_slice_period[\"TRP\"].groupby(level=\"Ticker\").mean()\n",
    "\n",
    "            # C. Market Observation\n",
    "            observation: MarketObservation = {\n",
    "                \"lookback_close\": lookback_close,\n",
    "                \"lookback_returns\": lookback_close.ffill().pct_change(),\n",
    "                \"atrp\": atrp_mean,\n",
    "                \"trp\": trp_mean,\n",
    "                \"rsi\": feat_slice_current[\"RSI\"],\n",
    "                \"rel_strength\": feat_slice_current[\"RelStrength\"],\n",
    "                \"vol_regime\": feat_slice_current[\"VolRegime\"],\n",
    "                \"rvol\": feat_slice_current[\"RVol\"],\n",
    "                \"obv_score\": feat_slice_current[\"OBV_Score\"],\n",
    "                \"roc_1\": feat_slice_current[\"ROC_1\"],\n",
    "                \"roc_3\": feat_slice_current[\"ROC_3\"],\n",
    "                \"roc_5\": feat_slice_current[\"ROC_5\"],\n",
    "                \"roc_10\": feat_slice_current[\"ROC_10\"],\n",
    "                \"roc_21\": feat_slice_current[\"ROC_21\"],\n",
    "            }\n",
    "\n",
    "            if inputs.metric not in METRIC_REGISTRY:\n",
    "                return [], pd.DataFrame(), {}, f\"Strategy '{inputs.metric}' not found.\"\n",
    "\n",
    "            # D. Execute & Sort\n",
    "            metric_vals = METRIC_REGISTRY[inputs.metric](observation)\n",
    "            sorted_tickers = metric_vals.sort_values(ascending=False)\n",
    "\n",
    "            start_r = max(0, inputs.rank_start - 1)\n",
    "            end_r = inputs.rank_end\n",
    "            selected_tickers = sorted_tickers.iloc[start_r:end_r].index.tolist()\n",
    "\n",
    "            # E. Audit\n",
    "            if inputs.debug:\n",
    "                debug_dict[\"full_universe_ranking\"] = pd.DataFrame(\n",
    "                    {\n",
    "                        \"Strategy_Score\": metric_vals,\n",
    "                        \"Lookback_Return_Ann\": observation[\"lookback_returns\"].mean()\n",
    "                        * 252,\n",
    "                        \"Lookback_ATRP\": observation[\"atrp\"],\n",
    "                    }\n",
    "                ).sort_values(\"Strategy_Score\", ascending=False)\n",
    "\n",
    "            results_table = pd.DataFrame(\n",
    "                {\n",
    "                    \"Rank\": range(\n",
    "                        inputs.rank_start, inputs.rank_start + len(selected_tickers)\n",
    "                    ),\n",
    "                    \"Ticker\": selected_tickers,\n",
    "                    \"Strategy Value\": sorted_tickers.loc[selected_tickers].values,\n",
    "                }\n",
    "            ).set_index(\"Ticker\")\n",
    "\n",
    "            return selected_tickers, results_table, debug_dict, None\n",
    "\n",
    "    def _select_tickers_old_2(self, inputs: EngineInput, start_date, decision_date):\n",
    "        debug_dict = {}\n",
    "\n",
    "        # ==============================================================================\n",
    "        # PHASE 1: UNIVERSE DETERMINATION\n",
    "        # ==============================================================================\n",
    "\n",
    "        # --- PATH A: MANUAL LIST ---\n",
    "        if inputs.mode == \"Manual List\":\n",
    "            # (Standard manual handling...)\n",
    "            valid_tickers = [\n",
    "                t for t in inputs.manual_tickers if t in self.df_close.columns\n",
    "            ]\n",
    "            return valid_tickers, pd.DataFrame(index=valid_tickers), {}, None\n",
    "\n",
    "        # --- PATH B: RANKING ---\n",
    "        else:\n",
    "            # 1. CHECK FOR SUBSET (The Critical Fix)\n",
    "            # If universe_subset is provided, we SKIP the liquidity filters completely.\n",
    "            if inputs.universe_subset is not None:\n",
    "                # CASCADE MODE: Strictly accept the list (filtering out only those with no data)\n",
    "                eligible_tickers = [\n",
    "                    t for t in inputs.universe_subset if t in self.df_close.columns\n",
    "                ]\n",
    "\n",
    "                if inputs.debug:\n",
    "                    debug_dict[\"audit_liquidity\"] = {\n",
    "                        \"mode\": \"Cascade/Subset\",\n",
    "                        \"tickers_passed\": len(eligible_tickers),\n",
    "                        \"forced_list\": True,\n",
    "                    }\n",
    "\n",
    "            # 2. NO SUBSET -> RUN DISCOVERY\n",
    "            else:\n",
    "                # DISCOVERY MODE: Run the standard liquidity/quality filters\n",
    "                audit_info = {}\n",
    "                eligible_tickers = self._filter_universe(\n",
    "                    decision_date, inputs.quality_thresholds, audit_info\n",
    "                )\n",
    "                if inputs.debug:\n",
    "                    debug_dict[\"audit_liquidity\"] = audit_info\n",
    "\n",
    "            if not eligible_tickers:\n",
    "                return (\n",
    "                    [],\n",
    "                    pd.DataFrame(),\n",
    "                    debug_dict,\n",
    "                    \"No tickers passed universe filters.\",\n",
    "                )\n",
    "\n",
    "            # ==============================================================================\n",
    "            # PHASE 2: RANKING (The Strategy)\n",
    "            # ==============================================================================\n",
    "\n",
    "            # A. Slicing Price History\n",
    "            lookback_close = self.df_close.loc[\n",
    "                start_date:decision_date, eligible_tickers\n",
    "            ]\n",
    "\n",
    "            # B. Feature Snapshot (As of Today)\n",
    "            feat_slice_current = self.features_df.xs(\n",
    "                decision_date, level=\"Date\"\n",
    "            ).reindex(eligible_tickers)\n",
    "\n",
    "            # C. Period Means for Volatility\n",
    "            idx_slice = pd.IndexSlice\n",
    "            feat_slice_period = self.features_df.loc[\n",
    "                idx_slice[eligible_tickers, start_date:decision_date], :\n",
    "            ]\n",
    "\n",
    "            atrp_mean = feat_slice_period[\"ATRP\"].groupby(level=\"Ticker\").mean()\n",
    "            trp_mean = feat_slice_period[\"TRP\"].groupby(level=\"Ticker\").mean()\n",
    "\n",
    "            # D. Package Market Observation\n",
    "            observation = {\n",
    "                \"lookback_close\": lookback_close,\n",
    "                \"lookback_returns\": lookback_close.ffill().pct_change(),\n",
    "                \"atrp\": atrp_mean,\n",
    "                \"trp\": trp_mean,\n",
    "                \"rsi\": feat_slice_current.get(\"RSI\"),\n",
    "                \"rel_strength\": feat_slice_current.get(\"RelStrength\"),\n",
    "                \"vol_regime\": feat_slice_current.get(\"VolRegime\"),\n",
    "                \"rvol\": feat_slice_current.get(\"RVol\"),\n",
    "                \"obv_score\": feat_slice_current.get(\"OBV_Score\"),\n",
    "                \"roc_1\": feat_slice_current.get(\"ROC_1\"),\n",
    "                \"roc_3\": feat_slice_current.get(\"ROC_3\"),\n",
    "                \"roc_5\": feat_slice_current.get(\"ROC_5\"),\n",
    "                \"roc_10\": feat_slice_current.get(\"ROC_10\"),\n",
    "                \"roc_21\": feat_slice_current.get(\"ROC_21\"),\n",
    "            }\n",
    "\n",
    "            if inputs.metric not in METRIC_REGISTRY:\n",
    "                return [], pd.DataFrame(), {}, f\"Strategy '{inputs.metric}' not found.\"\n",
    "\n",
    "            # E. Calculate & Sort\n",
    "            metric_vals = METRIC_REGISTRY[inputs.metric](observation)\n",
    "            sorted_tickers = metric_vals.sort_values(ascending=False)\n",
    "\n",
    "            start_r = max(0, inputs.rank_start - 1)\n",
    "            end_r = inputs.rank_end\n",
    "            selected_tickers = sorted_tickers.iloc[start_r:end_r].index.tolist()\n",
    "\n",
    "            # F. Prepare Output\n",
    "            results_table = pd.DataFrame(\n",
    "                {\n",
    "                    \"Rank\": range(\n",
    "                        inputs.rank_start, inputs.rank_start + len(selected_tickers)\n",
    "                    ),\n",
    "                    \"Ticker\": selected_tickers,\n",
    "                    \"Strategy Value\": sorted_tickers.loc[selected_tickers].values,\n",
    "                }\n",
    "            ).set_index(\"Ticker\")\n",
    "\n",
    "            return selected_tickers, results_table, debug_dict, None\n",
    "\n",
    "    def _select_tickers(self, inputs: EngineInput, start_date, decision_date):\n",
    "        debug_dict = {}\n",
    "\n",
    "        # ==============================================================================\n",
    "        # PHASE 1: UNIVERSE DETERMINATION\n",
    "        # ==============================================================================\n",
    "\n",
    "        # --- PATH A: MANUAL LIST ---\n",
    "        if inputs.mode == \"Manual List\":\n",
    "            valid_tickers = [\n",
    "                t for t in inputs.manual_tickers if t in self.df_close.columns\n",
    "            ]\n",
    "            return valid_tickers, pd.DataFrame(index=valid_tickers), {}, None\n",
    "\n",
    "        # --- PATH B: RANKING ---\n",
    "        else:\n",
    "            # 1. CHECK FOR SUBSET (Cascade Mode)\n",
    "            if inputs.universe_subset is not None:\n",
    "                # Filter subset to what is actually in our data\n",
    "                eligible_tickers = [\n",
    "                    t for t in inputs.universe_subset if t in self.df_close.columns\n",
    "                ]\n",
    "\n",
    "                # --- FIX: CREATE SYNTHETIC SNAPSHOT FOR VERIFIER ---\n",
    "                # The verification function expects a DataFrame called 'universe_snapshot'\n",
    "                # with a column 'Passed_Final'. We generate it manually here.\n",
    "                if inputs.debug:\n",
    "                    synthetic_snapshot = pd.DataFrame(index=eligible_tickers)\n",
    "                    synthetic_snapshot[\"Passed_Final\"] = True\n",
    "                    synthetic_snapshot[\"RollMedDollarVol\"] = (\n",
    "                        0  # Placeholder to satisfy verifier\n",
    "                    )\n",
    "                    synthetic_snapshot[\"Calculated_Cutoff\"] = 0  # Placeholder\n",
    "\n",
    "                    debug_dict[\"audit_liquidity\"] = {\n",
    "                        \"mode\": \"Cascade/Subset\",\n",
    "                        \"tickers_passed\": len(eligible_tickers),\n",
    "                        \"forced_list\": True,\n",
    "                        \"universe_snapshot\": synthetic_snapshot,  # <--- THE MISSING KEY\n",
    "                    }\n",
    "\n",
    "            # 2. NO SUBSET (Discovery Mode)\n",
    "            else:\n",
    "                audit_info = {}\n",
    "                eligible_tickers = self._filter_universe(\n",
    "                    decision_date, inputs.quality_thresholds, audit_info\n",
    "                )\n",
    "                if inputs.debug:\n",
    "                    debug_dict[\"audit_liquidity\"] = audit_info\n",
    "\n",
    "            if not eligible_tickers:\n",
    "                return (\n",
    "                    [],\n",
    "                    pd.DataFrame(),\n",
    "                    debug_dict,\n",
    "                    \"No tickers passed universe filters.\",\n",
    "                )\n",
    "\n",
    "            # ==============================================================================\n",
    "            # PHASE 2: RANKING (The Strategy)\n",
    "            # ==============================================================================\n",
    "\n",
    "            # A. Slice Price History\n",
    "            lookback_close = self.df_close.loc[\n",
    "                start_date:decision_date, eligible_tickers\n",
    "            ]\n",
    "\n",
    "            # B. Feature Snapshot\n",
    "            feat_slice_current = self.features_df.xs(\n",
    "                decision_date, level=\"Date\"\n",
    "            ).reindex(eligible_tickers)\n",
    "\n",
    "            # C. Period Means\n",
    "            idx_slice = pd.IndexSlice\n",
    "            feat_slice_period = self.features_df.loc[\n",
    "                idx_slice[eligible_tickers, start_date:decision_date], :\n",
    "            ]\n",
    "\n",
    "            atrp_mean = feat_slice_period[\"ATRP\"].groupby(level=\"Ticker\").mean()\n",
    "            trp_mean = feat_slice_period[\"TRP\"].groupby(level=\"Ticker\").mean()\n",
    "\n",
    "            # D. Market Observation\n",
    "            observation = {\n",
    "                \"lookback_close\": lookback_close,\n",
    "                \"lookback_returns\": lookback_close.ffill().pct_change(),\n",
    "                \"atrp\": atrp_mean,\n",
    "                \"trp\": trp_mean,\n",
    "                \"rsi\": feat_slice_current.get(\"RSI\"),\n",
    "                \"rel_strength\": feat_slice_current.get(\"RelStrength\"),\n",
    "                \"vol_regime\": feat_slice_current.get(\"VolRegime\"),\n",
    "                \"rvol\": feat_slice_current.get(\"RVol\"),\n",
    "                \"obv_score\": feat_slice_current.get(\"OBV_Score\"),\n",
    "                \"roc_1\": feat_slice_current.get(\"ROC_1\"),\n",
    "                \"roc_3\": feat_slice_current.get(\"ROC_3\"),\n",
    "                \"roc_5\": feat_slice_current.get(\"ROC_5\"),\n",
    "                \"roc_10\": feat_slice_current.get(\"ROC_10\"),\n",
    "                \"roc_21\": feat_slice_current.get(\"ROC_21\"),\n",
    "            }\n",
    "\n",
    "            if inputs.metric not in METRIC_REGISTRY:\n",
    "                return [], pd.DataFrame(), {}, f\"Strategy '{inputs.metric}' not found.\"\n",
    "\n",
    "            # E. Execute & Sort\n",
    "            metric_vals = METRIC_REGISTRY[inputs.metric](observation)\n",
    "            sorted_tickers = metric_vals.sort_values(ascending=False)\n",
    "\n",
    "            start_r = max(0, inputs.rank_start - 1)\n",
    "            end_r = inputs.rank_end\n",
    "            selected_tickers = sorted_tickers.iloc[start_r:end_r].index.tolist()\n",
    "\n",
    "            # F. Audit Data (Ranking)\n",
    "            if inputs.debug:\n",
    "                debug_dict[\"full_universe_ranking\"] = pd.DataFrame(\n",
    "                    {\n",
    "                        \"Strategy_Score\": metric_vals,\n",
    "                        \"Lookback_Return_Ann\": observation[\"lookback_returns\"].mean()\n",
    "                        * 252,\n",
    "                        \"Lookback_ATRP\": observation[\"atrp\"],\n",
    "                    }\n",
    "                ).sort_values(\"Strategy_Score\", ascending=False)\n",
    "\n",
    "            results_table = pd.DataFrame(\n",
    "                {\n",
    "                    \"Rank\": range(\n",
    "                        inputs.rank_start, inputs.rank_start + len(selected_tickers)\n",
    "                    ),\n",
    "                    \"Ticker\": selected_tickers,\n",
    "                    \"Strategy Value\": sorted_tickers.loc[selected_tickers].values,\n",
    "                }\n",
    "            ).set_index(\"Ticker\")\n",
    "\n",
    "            return selected_tickers, results_table, debug_dict, None\n",
    "\n",
    "    # from GOLDEN_bot_v52\n",
    "    def _select_tickers(self, inputs: EngineInput, start_date, decision_date):\n",
    "        debug_dict = {}\n",
    "\n",
    "        # --- PATH A: MANUAL LIST ---\n",
    "        if inputs.mode == \"Manual List\":\n",
    "            validation_errors = []\n",
    "            valid_tickers = []\n",
    "            for t in inputs.manual_tickers:\n",
    "                if t not in self.df_close.columns:\n",
    "                    validation_errors.append(f\"‚ùå {t}: Not found.\")\n",
    "                    continue\n",
    "                if pd.isna(self.df_close.at[start_date, t]):\n",
    "                    validation_errors.append(f\"‚ö†Ô∏è {t}: No data on start date.\")\n",
    "                    continue\n",
    "                valid_tickers.append(t)\n",
    "\n",
    "            if validation_errors:\n",
    "                return [], pd.DataFrame(), {}, \"\\n\".join(validation_errors)\n",
    "            if not valid_tickers:\n",
    "                return [], pd.DataFrame(), {}, \"No valid tickers found.\"\n",
    "            return valid_tickers, pd.DataFrame(index=valid_tickers), {}, None\n",
    "\n",
    "        # --- PATH B: RANKING ---\n",
    "        else:\n",
    "            audit_info = {}\n",
    "            eligible_tickers = self._filter_universe(\n",
    "                decision_date, inputs.quality_thresholds, audit_info\n",
    "            )\n",
    "            debug_dict[\"audit_liquidity\"] = audit_info\n",
    "\n",
    "            if not eligible_tickers:\n",
    "                return (\n",
    "                    [],\n",
    "                    pd.DataFrame(),\n",
    "                    debug_dict,\n",
    "                    \"No tickers passed quality filters.\",\n",
    "                )\n",
    "\n",
    "            lookback_close = self.df_close.loc[\n",
    "                start_date:decision_date, eligible_tickers\n",
    "            ]\n",
    "\n",
    "            # 1. Get the Snapshot of Features for the Decision Date\n",
    "            feat_slice_current = self.features_df.xs(\n",
    "                decision_date, level=\"Date\"\n",
    "            ).reindex(eligible_tickers)\n",
    "\n",
    "            # Calculate mean ATRP over the lookback period\n",
    "            idx_product = pd.MultiIndex.from_product(\n",
    "                [eligible_tickers, lookback_close.index], names=[\"Ticker\", \"Date\"]\n",
    "            )\n",
    "            feat_slice_period = self.features_df.reindex(idx_product)\n",
    "            atrp_value_for_obs = (\n",
    "                feat_slice_period[\"ATRP\"].groupby(level=\"Ticker\").mean()\n",
    "            )\n",
    "\n",
    "            # --- PINPOINT CHANGE: Calculate mean TRP over lookback ---\n",
    "            trp_value_for_obs = feat_slice_period[\"TRP\"].groupby(level=\"Ticker\").mean()\n",
    "\n",
    "            # Update the observation dictionary\n",
    "            observation: MarketObservation = {\n",
    "                # ...\n",
    "                \"atrp\": atrp_value_for_obs,\n",
    "                \"trp\": trp_value_for_obs,  # <--- PINPOINT CHANGE: Pass the lookback mean\n",
    "                # ...\n",
    "            }\n",
    "\n",
    "            # 2. Package the Observation (The 'State')\n",
    "            observation: MarketObservation = {\n",
    "                # Time Series Data\n",
    "                \"lookback_close\": lookback_close,\n",
    "                \"lookback_returns\": lookback_close.ffill().pct_change(),\n",
    "                # Snapshot Data (Scalar values for today)\n",
    "                \"atrp\": atrp_value_for_obs,  # <--- USES THE TOGGLED VALUE HERE\n",
    "                \"trp\": trp_value_for_obs,  # <--- PINPOINT CHANGE: Pass the lookback mean\n",
    "                \"rsi\": feat_slice_current[\"RSI\"],\n",
    "                \"rel_strength\": feat_slice_current[\"RelStrength\"],\n",
    "                \"vol_regime\": feat_slice_current[\"VolRegime\"],\n",
    "                \"rvol\": feat_slice_current[\"RVol\"],\n",
    "                \"spy_rvol\": feat_slice_current[\"Spy_RVol\"],\n",
    "                \"obv_score\": feat_slice_current[\"OBV_Score\"],\n",
    "                \"spy_obv_score\": feat_slice_current[\"Spy_OBV_Score\"],\n",
    "                # Momentum Vectors\n",
    "                \"roc_1\": feat_slice_current[\"ROC_1\"],\n",
    "                \"roc_3\": feat_slice_current[\"ROC_3\"],\n",
    "                \"roc_5\": feat_slice_current[\"ROC_5\"],\n",
    "                \"roc_10\": feat_slice_current[\"ROC_10\"],\n",
    "                \"roc_21\": feat_slice_current[\"ROC_21\"],\n",
    "            }\n",
    "\n",
    "            # 3. Run the Strategy (The 'Agent')\n",
    "            if inputs.metric not in METRIC_REGISTRY:\n",
    "                return [], pd.DataFrame(), {}, f\"Strategy '{inputs.metric}' not found.\"\n",
    "\n",
    "            metric_vals = METRIC_REGISTRY[inputs.metric](observation)\n",
    "            sorted_tickers = metric_vals.sort_values(ascending=False)\n",
    "            start_r = max(0, inputs.rank_start - 1)\n",
    "            end_r = inputs.rank_end\n",
    "            selected_tickers = sorted_tickers.iloc[start_r:end_r].index.tolist()\n",
    "\n",
    "            # Audit\n",
    "            debug_dict[\"full_universe_ranking\"] = pd.DataFrame(\n",
    "                {\n",
    "                    \"Strategy_Score\": metric_vals,\n",
    "                    \"Lookback_Return_Ann\": observation[\"lookback_returns\"].mean() * 252,\n",
    "                    \"Lookback_ATRP\": observation[\"atrp\"],\n",
    "                }\n",
    "            )\n",
    "\n",
    "            if not selected_tickers:\n",
    "                return (\n",
    "                    [],\n",
    "                    pd.DataFrame(),\n",
    "                    debug_dict,\n",
    "                    \"No tickers generated from ranking.\",\n",
    "                )\n",
    "\n",
    "            results_table = pd.DataFrame(\n",
    "                {\n",
    "                    \"Rank\": range(\n",
    "                        inputs.rank_start, inputs.rank_start + len(selected_tickers)\n",
    "                    ),\n",
    "                    \"Ticker\": selected_tickers,\n",
    "                    \"Strategy Value\": sorted_tickers.loc[selected_tickers].values,\n",
    "                }\n",
    "            ).set_index(\"Ticker\")\n",
    "\n",
    "            return selected_tickers, results_table, debug_dict, None\n",
    "\n",
    "    def _select_tickers(self, inputs: EngineInput, start_date, decision_date):\n",
    "        ####################################\n",
    "        print(\"=========== _select_tickers_01 fix  =================\\n\")\n",
    "        ####################################\n",
    "        debug_dict = {}\n",
    "\n",
    "        # --- PATH A: MANUAL LIST ---\n",
    "        if inputs.mode == \"Manual List\":\n",
    "            validation_errors = []\n",
    "            valid_tickers = []\n",
    "            for t in inputs.manual_tickers:\n",
    "                if t not in self.df_close.columns:\n",
    "                    validation_errors.append(f\"‚ùå {t}: Not found.\")\n",
    "                    continue\n",
    "                valid_tickers.append(t)\n",
    "            if not valid_tickers:\n",
    "                return [], pd.DataFrame(), {}, \"\\n\".join(validation_errors)\n",
    "            return valid_tickers, pd.DataFrame(index=valid_tickers), {}, None\n",
    "\n",
    "        # --- PATH B: RANKING ---\n",
    "        else:\n",
    "            # -----------------------------------------------------------\n",
    "            # [FIX] CHECK FOR SUBSET (Stage 2 / Cascade Mode)\n",
    "            # -----------------------------------------------------------\n",
    "            if inputs.universe_subset is not None:\n",
    "                # CASCADE MODE: Strictly limit to the provided subset\n",
    "                # We skip _filter_universe completely here!\n",
    "                eligible_tickers = [\n",
    "                    t for t in inputs.universe_subset if t in self.df_close.columns\n",
    "                ]\n",
    "                if inputs.debug:\n",
    "                    debug_dict[\"audit_liquidity\"] = {\n",
    "                        \"mode\": \"Cascade/Subset\",\n",
    "                        \"tickers_passed\": len(eligible_tickers),\n",
    "                        # Create a dummy snapshot so the audit function doesn't crash\n",
    "                        \"universe_snapshot\": pd.DataFrame(\n",
    "                            {\"Passed_Final\": True}, index=eligible_tickers\n",
    "                        ),\n",
    "                    }\n",
    "\n",
    "            # -----------------------------------------------------------\n",
    "            # NO SUBSET (Stage 1 / Discovery Mode)\n",
    "            # -----------------------------------------------------------\n",
    "            else:\n",
    "                audit_info = {}\n",
    "                eligible_tickers = self._filter_universe(\n",
    "                    decision_date, inputs.quality_thresholds, audit_info\n",
    "                )\n",
    "                if inputs.debug:\n",
    "                    debug_dict[\"audit_liquidity\"] = audit_info\n",
    "\n",
    "            # --- Common Exit ---\n",
    "            if not eligible_tickers:\n",
    "                return (\n",
    "                    [],\n",
    "                    pd.DataFrame(),\n",
    "                    debug_dict,\n",
    "                    \"No tickers passed universe filters.\",\n",
    "                )\n",
    "\n",
    "            # ==============================================================================\n",
    "            # PHASE 2: RANKING (The Strategy)\n",
    "            # ==============================================================================\n",
    "            lookback_close = self.df_close.loc[\n",
    "                start_date:decision_date, eligible_tickers\n",
    "            ]\n",
    "\n",
    "            # 1. Get the Snapshot of Features for the Decision Date\n",
    "            feat_slice_current = self.features_df.xs(\n",
    "                decision_date, level=\"Date\"\n",
    "            ).reindex(eligible_tickers)\n",
    "\n",
    "            # Calculate mean ATRP over the lookback period\n",
    "            idx_product = pd.MultiIndex.from_product(\n",
    "                [eligible_tickers, lookback_close.index], names=[\"Ticker\", \"Date\"]\n",
    "            )\n",
    "            feat_slice_period = self.features_df.reindex(idx_product)\n",
    "            atrp_value_for_obs = (\n",
    "                feat_slice_period[\"ATRP\"].groupby(level=\"Ticker\").mean()\n",
    "            )\n",
    "            trp_value_for_obs = feat_slice_period[\"TRP\"].groupby(level=\"Ticker\").mean()\n",
    "\n",
    "            # 2. Package the Observation (The 'State')\n",
    "            observation: MarketObservation = {\n",
    "                \"lookback_close\": lookback_close,\n",
    "                \"lookback_returns\": lookback_close.ffill().pct_change(),\n",
    "                \"atrp\": atrp_value_for_obs,\n",
    "                \"trp\": trp_value_for_obs,\n",
    "                \"rsi\": feat_slice_current[\"RSI\"],\n",
    "                \"rel_strength\": feat_slice_current[\"RelStrength\"],\n",
    "                \"vol_regime\": feat_slice_current[\"VolRegime\"],\n",
    "                \"rvol\": feat_slice_current[\"RVol\"],\n",
    "                \"spy_rvol\": feat_slice_current.get(\"Spy_RVol\"),\n",
    "                \"obv_score\": feat_slice_current[\"OBV_Score\"],\n",
    "                \"roc_1\": feat_slice_current[\"ROC_1\"],\n",
    "                \"roc_3\": feat_slice_current[\"ROC_3\"],\n",
    "                \"roc_5\": feat_slice_current[\"ROC_5\"],\n",
    "                \"roc_10\": feat_slice_current[\"ROC_10\"],\n",
    "                \"roc_21\": feat_slice_current[\"ROC_21\"],\n",
    "            }\n",
    "\n",
    "            # 3. Run the Strategy\n",
    "            if inputs.metric not in METRIC_REGISTRY:\n",
    "                return [], pd.DataFrame(), {}, f\"Strategy '{inputs.metric}' not found.\"\n",
    "\n",
    "            metric_vals = METRIC_REGISTRY[inputs.metric](observation)\n",
    "            sorted_tickers = metric_vals.sort_values(ascending=False)\n",
    "            start_r = max(0, inputs.rank_start - 1)\n",
    "            end_r = inputs.rank_end\n",
    "            selected_tickers = sorted_tickers.iloc[start_r:end_r].index.tolist()\n",
    "\n",
    "            # Audit\n",
    "            debug_dict[\"full_universe_ranking\"] = pd.DataFrame(\n",
    "                {\n",
    "                    \"Strategy_Score\": metric_vals,\n",
    "                    \"Lookback_Return_Ann\": observation[\"lookback_returns\"].mean() * 252,\n",
    "                    \"Lookback_ATRP\": observation[\"atrp\"],\n",
    "                }\n",
    "            )\n",
    "\n",
    "            results_table = pd.DataFrame(\n",
    "                {\n",
    "                    \"Rank\": range(\n",
    "                        inputs.rank_start, inputs.rank_start + len(selected_tickers)\n",
    "                    ),\n",
    "                    \"Ticker\": selected_tickers,\n",
    "                    \"Strategy Value\": sorted_tickers.loc[selected_tickers].values,\n",
    "                }\n",
    "            ).set_index(\"Ticker\")\n",
    "\n",
    "            return selected_tickers, results_table, debug_dict, None\n",
    "\n",
    "    ############################\n",
    "    ############################\n",
    "\n",
    "    def _filter_universe(self, date_ts, thresholds, audit_container=None):\n",
    "        avail_dates = (\n",
    "            self.features_df.index.get_level_values(\"Date\").unique().sort_values()\n",
    "        )\n",
    "        valid_dates = avail_dates[avail_dates <= date_ts]\n",
    "        if valid_dates.empty:\n",
    "            return []\n",
    "        target_date = valid_dates[-1]\n",
    "        day_features = self.features_df.xs(target_date, level=\"Date\")\n",
    "\n",
    "        vol_cutoff = thresholds.get(\"min_median_dollar_volume\", 0)\n",
    "        percentile_used = \"N/A\"\n",
    "        if \"min_liquidity_percentile\" in thresholds:\n",
    "            percentile_used = thresholds[\"min_liquidity_percentile\"]\n",
    "            dynamic_val = day_features[\"RollMedDollarVol\"].quantile(percentile_used)\n",
    "            vol_cutoff = max(vol_cutoff, dynamic_val)\n",
    "\n",
    "        mask = (\n",
    "            (day_features[\"RollMedDollarVol\"] >= vol_cutoff)\n",
    "            & (day_features[\"RollingStalePct\"] <= thresholds[\"max_stale_pct\"])\n",
    "            & (day_features[\"RollingSameVolCount\"] <= thresholds[\"max_same_vol_count\"])\n",
    "        )\n",
    "\n",
    "        if audit_container is not None:\n",
    "            audit_container[\"date\"] = target_date\n",
    "            audit_container[\"total_tickers_available\"] = len(day_features)\n",
    "            audit_container[\"percentile_setting\"] = percentile_used\n",
    "            audit_container[\"final_cutoff_usd\"] = vol_cutoff\n",
    "            audit_container[\"tickers_passed\"] = mask.sum()\n",
    "            snapshot = day_features.copy()\n",
    "            snapshot[\"Calculated_Cutoff\"] = vol_cutoff\n",
    "            snapshot[\"Passed_Vol_Check\"] = snapshot[\"RollMedDollarVol\"] >= vol_cutoff\n",
    "            snapshot[\"Passed_Final\"] = mask\n",
    "            snapshot = snapshot.sort_values(\"RollMedDollarVol\", ascending=False)\n",
    "            audit_container[\"universe_snapshot\"] = snapshot\n",
    "\n",
    "        return day_features[mask].index.tolist()\n",
    "\n",
    "    def _calculate_period_metrics(\n",
    "        self,\n",
    "        f_val,\n",
    "        f_ret,\n",
    "        f_atrp,\n",
    "        f_trp,\n",
    "        decision_date,\n",
    "        h_val,\n",
    "        h_ret,\n",
    "        h_atrp,\n",
    "        h_trp,\n",
    "        prefix,  # <--- Added trp args\n",
    "    ):\n",
    "        metrics = {}\n",
    "        slices = {}\n",
    "\n",
    "        # 1. Temporal Slicing (Routing)\n",
    "        lb_val, lb_ret, lb_atrp, lb_trp = (\n",
    "            f_val.loc[:decision_date],\n",
    "            f_ret.loc[:decision_date],\n",
    "            f_atrp.loc[:decision_date],\n",
    "            f_trp.loc[:decision_date],\n",
    "        )\n",
    "\n",
    "        # Use the new unified QuantUtils calls\n",
    "        metrics[f\"full_{prefix}_gain\"] = QuantUtils.calculate_gain(f_val)\n",
    "        metrics[f\"full_{prefix}_sharpe\"] = QuantUtils.calculate_sharpe(f_ret)\n",
    "        metrics[f\"full_{prefix}_sharpe_atrp\"] = QuantUtils.calculate_sharpe_vol(\n",
    "            f_ret, f_atrp\n",
    "        )\n",
    "        metrics[f\"full_{prefix}_sharpe_trp\"] = QuantUtils.calculate_sharpe_vol(\n",
    "            f_ret, f_trp\n",
    "        )\n",
    "\n",
    "        metrics[f\"lookback_{prefix}_gain\"] = QuantUtils.calculate_gain(lb_val)\n",
    "        metrics[f\"lookback_{prefix}_sharpe\"] = QuantUtils.calculate_sharpe(lb_ret)\n",
    "        metrics[f\"lookback_{prefix}_sharpe_atrp\"] = QuantUtils.calculate_sharpe_vol(\n",
    "            lb_ret, lb_atrp\n",
    "        )\n",
    "        metrics[f\"lookback_{prefix}_sharpe_trp\"] = QuantUtils.calculate_sharpe_vol(\n",
    "            lb_ret, lb_trp\n",
    "        )\n",
    "\n",
    "        metrics[f\"holding_{prefix}_gain\"] = QuantUtils.calculate_gain(h_val)\n",
    "        metrics[f\"holding_{prefix}_sharpe\"] = QuantUtils.calculate_sharpe(h_ret)\n",
    "        metrics[f\"holding_{prefix}_sharpe_atrp\"] = QuantUtils.calculate_sharpe_vol(\n",
    "            h_ret, h_atrp\n",
    "        )\n",
    "        metrics[f\"holding_{prefix}_sharpe_trp\"] = QuantUtils.calculate_sharpe_vol(\n",
    "            h_ret, h_trp\n",
    "        )\n",
    "\n",
    "        # 5. Metadata Collection\n",
    "        (\n",
    "            slices[\"full_val\"],\n",
    "            slices[\"full_ret\"],\n",
    "            slices[\"full_atrp\"],\n",
    "            slices[\"full_trp\"],\n",
    "        ) = (\n",
    "            f_val,\n",
    "            f_ret,\n",
    "            f_atrp,\n",
    "            f_trp,\n",
    "        )\n",
    "        (\n",
    "            slices[\"lookback_val\"],\n",
    "            slices[\"lookback_ret\"],\n",
    "            slices[\"lookback_atrp\"],\n",
    "            slices[\"lookback_trp\"],\n",
    "        ) = (\n",
    "            lb_val,\n",
    "            lb_ret,\n",
    "            lb_atrp,\n",
    "            lb_trp,\n",
    "        )\n",
    "        (\n",
    "            slices[\"holding_val\"],\n",
    "            slices[\"holding_ret\"],\n",
    "            slices[\"holding_atrp\"],\n",
    "            slices[\"holding_trp\"],\n",
    "        ) = (\n",
    "            h_val,\n",
    "            h_ret,\n",
    "            h_atrp,\n",
    "            h_trp,\n",
    "        )\n",
    "\n",
    "        return metrics, slices\n",
    "\n",
    "    def _get_normalized_plot_data(self, tickers, start_date, end_date):\n",
    "        if not tickers:\n",
    "            return pd.DataFrame()\n",
    "        data = self.df_close[list(set(tickers))].loc[start_date:end_date]\n",
    "        if data.empty:\n",
    "            return pd.DataFrame()\n",
    "        return data / data.bfill().iloc[0]\n",
    "\n",
    "    def _error_result(self, msg):\n",
    "        return EngineOutput(\n",
    "            portfolio_series=pd.Series(dtype=float),\n",
    "            benchmark_series=pd.Series(dtype=float),\n",
    "            normalized_plot_data=pd.DataFrame(),\n",
    "            tickers=[],\n",
    "            initial_weights=pd.Series(dtype=float),\n",
    "            perf_metrics={},\n",
    "            results_df=pd.DataFrame(),\n",
    "            start_date=pd.Timestamp.min,\n",
    "            decision_date=pd.Timestamp.min,\n",
    "            buy_date=pd.Timestamp.min,\n",
    "            holding_end_date=pd.Timestamp.min,\n",
    "            error_msg=msg,\n",
    "        )\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION E: THE UI (Visualization)\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "def plot_walk_forward_analyzer(\n",
    "    df_ohlcv=None,\n",
    "    features_df=None,\n",
    "    df_close_wide=None,\n",
    "    df_atrp_wide=None,\n",
    "    df_trp_wide=None,\n",
    "    engine=None,\n",
    "    universe_subset=None,  # Logic integrated\n",
    "    default_start_date=\"2025-01-17\",\n",
    "    default_lookback=10,\n",
    "    default_holding=5,\n",
    "    default_strategy=\"Sharpe (ATRP)\",\n",
    "    default_rank_start=1,\n",
    "    default_rank_end=10,\n",
    "    default_benchmark_ticker=GLOBAL_SETTINGS[\"benchmark_ticker\"],\n",
    "    master_calendar_ticker=GLOBAL_SETTINGS[\"calendar_ticker\"],\n",
    "    quality_thresholds=GLOBAL_SETTINGS[\"thresholds\"],\n",
    "    debug=True,\n",
    "):\n",
    "\n",
    "    # 1. THE PERSISTENT ENGINE GATE\n",
    "    if engine is None:\n",
    "        if df_ohlcv is None:\n",
    "            raise ValueError(\n",
    "                \"‚ùå No engine provided AND no df_ohlcv provided to build one.\"\n",
    "            )\n",
    "        engine = AlphaEngine(\n",
    "            df_ohlcv,\n",
    "            features_df=features_df,\n",
    "            df_close_wide=df_close_wide,\n",
    "            df_atrp_wide=df_atrp_wide,\n",
    "            df_trp_wide=df_trp_wide,\n",
    "            master_ticker=master_calendar_ticker,\n",
    "        )\n",
    "\n",
    "    audit_pack = [None]\n",
    "\n",
    "    if quality_thresholds is None:\n",
    "        quality_thresholds = GLOBAL_SETTINGS[\"thresholds\"]\n",
    "\n",
    "    # --- Widgets ---\n",
    "    mode_selector = widgets.RadioButtons(\n",
    "        options=[\"Ranking\", \"Manual List\"],\n",
    "        value=\"Ranking\",\n",
    "        description=\"Mode:\",\n",
    "        layout={\"width\": \"max-content\"},\n",
    "        style={\"description_width\": \"initial\"},\n",
    "    )\n",
    "    lookback_input = widgets.IntText(\n",
    "        value=default_lookback,\n",
    "        description=\"Lookback (Days):\",\n",
    "        layout={\"width\": \"200px\"},\n",
    "        style={\"description_width\": \"initial\"},\n",
    "    )\n",
    "    decision_date_picker = widgets.DatePicker(\n",
    "        description=\"Decision Date:\",\n",
    "        value=pd.to_datetime(default_start_date),\n",
    "        layout={\"width\": \"auto\"},\n",
    "        style={\"description_width\": \"initial\"},\n",
    "    )\n",
    "    holding_input = widgets.IntText(\n",
    "        value=default_holding,\n",
    "        description=\"Holding (Days):\",\n",
    "        layout={\"width\": \"200px\"},\n",
    "        style={\"description_width\": \"initial\"},\n",
    "    )\n",
    "    strategy_dropdown = widgets.Dropdown(\n",
    "        options=list(METRIC_REGISTRY.keys()),\n",
    "        value=default_strategy,\n",
    "        description=\"Strategy:\",\n",
    "        layout={\"width\": \"220px\"},\n",
    "        style={\"description_width\": \"initial\"},\n",
    "    )\n",
    "    benchmark_input = widgets.Text(\n",
    "        value=default_benchmark_ticker,\n",
    "        description=\"Benchmark:\",\n",
    "        placeholder=\"Enter Ticker\",\n",
    "        layout={\"width\": \"180px\"},\n",
    "        style={\"description_width\": \"initial\"},\n",
    "    )\n",
    "    rank_start_input = widgets.IntText(\n",
    "        value=default_rank_start,\n",
    "        description=\"Rank Start:\",\n",
    "        layout={\"width\": \"150px\"},\n",
    "        style={\"description_width\": \"initial\"},\n",
    "    )\n",
    "    rank_end_input = widgets.IntText(\n",
    "        value=default_rank_end,\n",
    "        description=\"Rank End:\",\n",
    "        layout={\"width\": \"150px\"},\n",
    "        style={\"description_width\": \"initial\"},\n",
    "    )\n",
    "    manual_tickers_input = widgets.Textarea(\n",
    "        value=\"\",\n",
    "        placeholder=\"Enter tickers...\",\n",
    "        description=\"Manual Tickers:\",\n",
    "        layout={\"width\": \"400px\", \"height\": \"80px\"},\n",
    "        style={\"description_width\": \"initial\"},\n",
    "    )\n",
    "    update_button = widgets.Button(description=\"Run Simulation\", button_style=\"primary\")\n",
    "    ticker_list_output = widgets.Output()\n",
    "\n",
    "    # --- Layouts ---\n",
    "    timeline_box = widgets.HBox(\n",
    "        [lookback_input, decision_date_picker, holding_input],\n",
    "        layout=widgets.Layout(\n",
    "            justify_content=\"space-between\",\n",
    "            border=\"1px solid #ddd\",\n",
    "            padding=\"10px\",\n",
    "            margin=\"5px\",\n",
    "        ),\n",
    "    )\n",
    "    strategy_box = widgets.HBox([strategy_dropdown, benchmark_input])\n",
    "    ranking_box = widgets.HBox([rank_start_input, rank_end_input])\n",
    "\n",
    "    def on_mode_change(c):\n",
    "        ranking_box.layout.display = \"flex\" if c[\"new\"] == \"Ranking\" else \"none\"\n",
    "        manual_tickers_input.layout.display = (\n",
    "            \"none\" if c[\"new\"] == \"Ranking\" else \"flex\"\n",
    "        )\n",
    "        strategy_dropdown.disabled = c[\"new\"] == \"Manual List\"\n",
    "\n",
    "    mode_selector.observe(on_mode_change, names=\"value\")\n",
    "    on_mode_change({\"new\": mode_selector.value})\n",
    "\n",
    "    ui = widgets.VBox(\n",
    "        [\n",
    "            widgets.HTML(\n",
    "                \"<b>1. Timeline Configuration:</b> (Past <--- Decision ---> Future)\"\n",
    "            ),\n",
    "            timeline_box,\n",
    "            widgets.HTML(\"<b>2. Strategy Settings:</b>\"),\n",
    "            widgets.HBox([mode_selector, strategy_box]),\n",
    "            ranking_box,\n",
    "            manual_tickers_input,\n",
    "            widgets.HTML(\"<hr>\"),\n",
    "            update_button,\n",
    "            ticker_list_output,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    fig = go.FigureWidget()\n",
    "    fig.update_layout(\n",
    "        title=\"Event-Driven Walk-Forward Analysis\",\n",
    "        height=600,\n",
    "        template=\"plotly_white\",\n",
    "        hovermode=\"x unified\",\n",
    "    )\n",
    "    for i in range(50):\n",
    "        fig.add_trace(go.Scatter(visible=False, line=dict(width=2)))\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            name=\"Benchmark\",\n",
    "            visible=True,\n",
    "            line=dict(color=\"black\", width=3, dash=\"dash\"),\n",
    "        )\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            name=\"Group Portfolio\", visible=True, line=dict(color=\"green\", width=3)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # --- Update Logic ---\n",
    "    def update_plot(b):\n",
    "        ticker_list_output.clear_output()\n",
    "        manual_list = [\n",
    "            t.strip().upper()\n",
    "            for t in manual_tickers_input.value.split(\",\")\n",
    "            if t.strip()\n",
    "        ]\n",
    "        decision_date_raw = pd.to_datetime(decision_date_picker.value)\n",
    "\n",
    "        # PINPOINT CHANGE: universe_subset now passed to EngineInput\n",
    "        inputs = EngineInput(\n",
    "            mode=mode_selector.value,\n",
    "            start_date=decision_date_raw,\n",
    "            lookback_period=lookback_input.value,\n",
    "            holding_period=holding_input.value,\n",
    "            metric=strategy_dropdown.value,\n",
    "            benchmark_ticker=benchmark_input.value.strip().upper(),\n",
    "            rank_start=rank_start_input.value,\n",
    "            rank_end=rank_end_input.value,\n",
    "            quality_thresholds=quality_thresholds,\n",
    "            manual_tickers=manual_list,\n",
    "            universe_subset=universe_subset,  # <--- Integrated from V2\n",
    "            debug=debug,\n",
    "        )\n",
    "\n",
    "        with ticker_list_output:\n",
    "            res = engine.run(inputs)\n",
    "            audit_pack[0] = res\n",
    "\n",
    "            if res.error_msg:\n",
    "                print(f\"‚ö†Ô∏è Simulation Stopped: {res.error_msg}\")\n",
    "                return\n",
    "\n",
    "            if not res.normalized_plot_data.empty:\n",
    "                with fig.batch_update():\n",
    "                    cols = res.normalized_plot_data.columns.tolist()\n",
    "                    for i in range(50):\n",
    "                        if i < len(cols):\n",
    "                            fig.data[i].update(\n",
    "                                x=res.normalized_plot_data.index,\n",
    "                                y=res.normalized_plot_data[cols[i]],\n",
    "                                name=cols[i],\n",
    "                                visible=True,\n",
    "                            )\n",
    "                        else:\n",
    "                            fig.data[i].visible = False\n",
    "\n",
    "                    fig.data[50].update(\n",
    "                        x=res.benchmark_series.index,\n",
    "                        y=res.benchmark_series.values,\n",
    "                        name=f\"Benchmark ({inputs.benchmark_ticker})\",\n",
    "                        visible=not res.benchmark_series.empty,\n",
    "                    )\n",
    "                    fig.data[51].update(\n",
    "                        x=res.portfolio_series.index,\n",
    "                        y=res.portfolio_series.values,\n",
    "                        visible=True,\n",
    "                    )\n",
    "\n",
    "                    fig.layout.shapes = [\n",
    "                        dict(\n",
    "                            type=\"line\",\n",
    "                            x0=res.decision_date,\n",
    "                            y0=0,\n",
    "                            x1=res.decision_date,\n",
    "                            y1=1,\n",
    "                            xref=\"x\",\n",
    "                            yref=\"paper\",\n",
    "                            line=dict(color=\"red\", width=2, dash=\"dash\"),\n",
    "                        ),\n",
    "                        dict(\n",
    "                            type=\"line\",\n",
    "                            x0=res.buy_date,\n",
    "                            y0=0,\n",
    "                            x1=res.buy_date,\n",
    "                            y1=1,\n",
    "                            xref=\"x\",\n",
    "                            yref=\"paper\",\n",
    "                            line=dict(color=\"blue\", width=2, dash=\"dot\"),\n",
    "                        ),\n",
    "                    ]\n",
    "\n",
    "                start_date = res.start_date.date()\n",
    "                act_date = res.decision_date.date()\n",
    "                entry_date = res.buy_date.date()\n",
    "\n",
    "            # --- Status & Liquidity Output ---\n",
    "            subset_status = f\" | Subset Mode: {'ACTIVE (' + str(len(universe_subset)) + ' tickers)' if universe_subset else 'OFF (Full Market)'}\"\n",
    "            print(f\"‚úÖ Success{subset_status}\")\n",
    "\n",
    "            if (\n",
    "                inputs.mode == \"Ranking\"\n",
    "                and res.debug_data\n",
    "                and \"audit_liquidity\" in res.debug_data\n",
    "            ):\n",
    "                audit = res.debug_data[\"audit_liquidity\"]\n",
    "                if audit:\n",
    "                    print(\"-\" * 60)\n",
    "                    print(f\"üîç LIQUIDITY CHECK (On Decision Date: {act_date})\")\n",
    "                    print(f\"   Tickers Remaining: {audit.get('tickers_passed')}\")\n",
    "                    print(\"-\" * 60)\n",
    "\n",
    "            print(\n",
    "                f\"Timeline: Start [ {start_date} ] --> Decision [ {act_date} ] --> Cash (1d) --> Entry [ {entry_date} ] --> End [ {res.holding_end_date.date()} ]\"\n",
    "            )\n",
    "\n",
    "            print(f\"Tickers ({len(res.tickers)}):\")\n",
    "            for i in range(0, len(res.tickers), 10):\n",
    "                print(\", \".join(res.tickers[i : i + 10]))\n",
    "\n",
    "            # --- Metrics Table (Full, Lookback, Holding) ---\n",
    "            m = res.perf_metrics\n",
    "            metrics_to_show = [\n",
    "                (\"Gain\", \"gain\"),\n",
    "                (\"Sharpe\", \"sharpe\"),\n",
    "                (\"Sharpe (ATRP)\", \"sharpe_atrp\"),\n",
    "                (\"Sharpe (TRP)\", \"sharpe_trp\"),\n",
    "            ]\n",
    "\n",
    "            rows = []\n",
    "            for label, key in metrics_to_show:\n",
    "                p_row = {\n",
    "                    \"Metric\": f\"Group {label}\",\n",
    "                    \"Full\": m.get(f\"full_p_{key}\"),\n",
    "                    \"Lookback\": m.get(f\"lookback_p_{key}\"),\n",
    "                    \"Holding\": m.get(f\"holding_p_{key}\"),\n",
    "                }\n",
    "                b_row = {\n",
    "                    \"Metric\": f\"Benchmark {label}\",\n",
    "                    \"Full\": m.get(f\"full_b_{key}\"),\n",
    "                    \"Lookback\": m.get(f\"lookback_b_{key}\"),\n",
    "                    \"Holding\": m.get(f\"holding_b_{key}\"),\n",
    "                }\n",
    "                d_row = {\"Metric\": f\"== {label} Delta\"}\n",
    "                for col in [\"Full\", \"Lookback\", \"Holding\"]:\n",
    "                    d_row[col] = (p_row[col] or 0) - (b_row[col] or 0)\n",
    "                rows.extend([p_row, b_row, d_row])\n",
    "\n",
    "            df_report = pd.DataFrame(rows).set_index(\"Metric\")\n",
    "\n",
    "            def apply_sleek_style(styler):\n",
    "                styler.format(\"{:+.4f}\", na_rep=\"N/A\")\n",
    "\n",
    "                def row_logic(row):\n",
    "                    if \"Delta\" in row.name:\n",
    "                        return [\n",
    "                            \"background-color: #f9f9f9; font-weight: 600; border-top: 1px solid #ddd\"\n",
    "                        ] * len(row)\n",
    "                    if \"Group\" in row.name:\n",
    "                        return [\"color: #2c5e8f; background-color: #fcfdfe\"] * len(row)\n",
    "                    return [\"color: #555\"] * len(row)\n",
    "\n",
    "                styler.apply(row_logic, axis=1)\n",
    "                styler.set_table_styles(\n",
    "                    [\n",
    "                        {\n",
    "                            \"selector\": \"\",\n",
    "                            \"props\": [\n",
    "                                (\"font-family\", \"inherit\"),\n",
    "                                (\"font-size\", \"12px\"),\n",
    "                                (\"border-collapse\", \"collapse\"),\n",
    "                                (\"width\", \"auto\"),\n",
    "                            ],\n",
    "                        },\n",
    "                        {\n",
    "                            \"selector\": \"th\",\n",
    "                            \"props\": [\n",
    "                                (\"background-color\", \"white\"),\n",
    "                                (\"color\", \"#222\"),\n",
    "                                (\"font-weight\", \"600\"),\n",
    "                                (\"padding\", \"6px 12px\"),\n",
    "                                (\"border-bottom\", \"2px solid #444\"),\n",
    "                                (\"text-align\", \"center\"),\n",
    "                            ],\n",
    "                        },\n",
    "                        {\n",
    "                            \"selector\": \"th.row_heading\",\n",
    "                            \"props\": [\n",
    "                                (\"text-align\", \"left\"),\n",
    "                                (\"padding-right\", \"30px\"),\n",
    "                                (\"border-bottom\", \"1px solid #eee\"),\n",
    "                            ],\n",
    "                        },\n",
    "                        {\n",
    "                            \"selector\": \"td\",\n",
    "                            \"props\": [\n",
    "                                (\"padding\", \"4px 12px\"),\n",
    "                                (\"border-bottom\", \"1px solid #eee\"),\n",
    "                            ],\n",
    "                        },\n",
    "                    ]\n",
    "                )\n",
    "                styler.index.name = None\n",
    "                return styler\n",
    "\n",
    "            display(apply_sleek_style(df_report.style))\n",
    "\n",
    "    update_button.on_click(update_plot)\n",
    "    update_plot(None)\n",
    "    display(ui, fig)\n",
    "    return audit_pack, engine\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION F: INSPECTION TOOLS\n",
    "# ==============================================================================\n",
    "\n",
    "from dataclasses import is_dataclass, fields\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# INTEGRITY PROTECTION: THE TRIPWIRE\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "def verify_math_integrity():\n",
    "    \"\"\"\n",
    "    üõ°Ô∏è TRIPWIRE: Ensures Sample Boundary Integrity.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- üõ°Ô∏è Starting Final Integrity Audit ---\")\n",
    "\n",
    "    try:\n",
    "        # Test 1: Series Input\n",
    "        mock_series = pd.Series([100.0, 102.0, 101.0])\n",
    "        rets_s = QuantUtils.compute_returns(mock_series)\n",
    "        # Verify first value is actually NaN\n",
    "        if not pd.isna(rets_s.iloc[0]):\n",
    "            raise ValueError(\"Series Leading NaN missing\")\n",
    "        print(\"‚úÖ Series Boundary: OK\")\n",
    "\n",
    "        # Test 2: DataFrame Input\n",
    "        mock_df = pd.DataFrame({\"A\": [100, 101], \"B\": [200, 202]})\n",
    "        rets_df = QuantUtils.compute_returns(mock_df)\n",
    "        if not rets_df.iloc[0].isna().all():\n",
    "            raise ValueError(\"DataFrame Leading NaN missing\")\n",
    "        print(\"‚úÖ DataFrame Boundary: OK\")\n",
    "\n",
    "        print(\"‚úÖ AUDIT PASSED: Mathematical boundaries are strictly enforced.\")\n",
    "    except Exception as e:\n",
    "        print(f\"üî• SYSTEM BREACH: {str(e)}\")\n",
    "        raise e\n",
    "\n",
    "\n",
    "def verify_feature_engineering_integrity():\n",
    "    \"\"\"\n",
    "    üõ°Ô∏è TRIPWIRE: Validates Feature Engineering Logic.\n",
    "    Enforces:\n",
    "    1. Day 1 ATR must be NaN (No PrevClose).\n",
    "    2. Wilder's Smoothing must use Alpha = 1/Period.\n",
    "    3. Recursion must match manual calculation.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- üõ°Ô∏è Starting Feature Engineering Audit ---\")\n",
    "\n",
    "    # 1. Create Synthetic Data (3 Days)\n",
    "    # Day 1: High-Low = 10. No PrevClose.\n",
    "    # Day 2: High-Low = 20. Gap up implies TR might be larger.\n",
    "    # Day 3: High-Low = 10.\n",
    "    dates = pd.to_datetime([\"2020-01-01\", \"2020-01-02\", \"2020-01-03\"])\n",
    "    idx = pd.MultiIndex.from_product([[\"TEST\"], dates], names=[\"Ticker\", \"Date\"])\n",
    "\n",
    "    df_mock = pd.DataFrame(\n",
    "        {\n",
    "            \"Adj Open\": [100, 110, 110],\n",
    "            \"Adj High\": [110, 130, 120],\n",
    "            \"Adj Low\": [100, 110, 110],\n",
    "            \"Adj Close\": [105, 120, 115],  # PrevClose: NaN, 105, 120\n",
    "            \"Volume\": [1000, 1000, 1000],\n",
    "        },\n",
    "        index=idx,\n",
    "    )\n",
    "\n",
    "    # 2. Run the Generator\n",
    "    # We use Period=2 to make manual math easy (Alpha = 1/2 = 0.5)\n",
    "    feats = generate_features(\n",
    "        df_mock, atr_period=2, rsi_period=2, quality_min_periods=1\n",
    "    )\n",
    "    atr_series = feats[\"ATR\"]\n",
    "\n",
    "    # 3. MANUAL CALCULATION (The \"Truth\")\n",
    "    # Day 1:\n",
    "    #   TR = Max(H-L, |H-PC|, |L-PC|)\n",
    "    #   TR = Max(10, NaN, NaN) -> NaN (Because skipna=False)\n",
    "    #   Expected ATR: NaN\n",
    "\n",
    "    # Day 2:\n",
    "    #   PrevClose = 105\n",
    "    #   H-L=20, |130-105|=25, |110-105|=5\n",
    "    #   TR = 25\n",
    "    #   Expected ATR: First valid observation = 25.0\n",
    "\n",
    "    # Day 3:\n",
    "    #   PrevClose = 120\n",
    "    #   H-L=10, |120-120|=0, |110-120|=10\n",
    "    #   TR = 10\n",
    "    #   Wilder's Smoothing (Alpha=0.5):\n",
    "    #   ATR_3 = (TR_3 * alpha) + (ATR_2 * (1-alpha))\n",
    "    #   ATR_3 = (10 * 0.5) + (25 * 0.5) = 5 + 12.5 = 17.5\n",
    "\n",
    "    print(f\"Audit Values:\\n{atr_series.values}\")\n",
    "\n",
    "    # 4. ASSERTIONS\n",
    "    try:\n",
    "        # Check Day 1\n",
    "        if not np.isnan(atr_series.iloc[0]):\n",
    "            raise AssertionError(\n",
    "                f\"Day 1 Regression: Expected NaN, got {atr_series.iloc[0]}. (Check skipna=False)\"\n",
    "            )\n",
    "\n",
    "        # Check Day 2 (Initialization)\n",
    "        if not np.isclose(atr_series.iloc[1], 25.0):\n",
    "            raise AssertionError(\n",
    "                f\"Initialization Regression: Expected 25.0, got {atr_series.iloc[1]}.\"\n",
    "            )\n",
    "\n",
    "        # Check Day 3 (Recursion)\n",
    "        if not np.isclose(atr_series.iloc[2], 17.5):\n",
    "            raise AssertionError(\n",
    "                f\"Wilder's Logic Regression: Expected 17.5, got {atr_series.iloc[2]}. (Check Alpha=1/N)\"\n",
    "            )\n",
    "\n",
    "        print(\"‚úÖ FEATURE INTEGRITY PASSED: Wilder's ATR logic is strictly enforced.\")\n",
    "\n",
    "    except AssertionError as e:\n",
    "        print(f\"üî• LOGIC FAILURE: {str(e)}\")\n",
    "        raise e\n",
    "\n",
    "\n",
    "def verify_ranking_integrity():\n",
    "    \"\"\"\n",
    "    üõ°Ô∏è TRIPWIRE: Prevents 'Momentum Collapse' in Volatility-Adjusted Ranking.\n",
    "    Ensures that Sharpe(Vol) distinguishes between High-Vol and Low-Vol stocks.\n",
    "    \"\"\"\n",
    "    print(\"--- üõ°Ô∏è Starting Ranking Kernel Audit ---\")\n",
    "\n",
    "    # 1. Setup Mock Universe (2 Tickers, 2 Days)\n",
    "    # Ticker 'VOLATILE': 10% return, but 10% Volatility\n",
    "    # Ticker 'STABLE': 2% return, but 1% Volatility (The 'Sharpe' Winner)\n",
    "    data = {\"VOLATILE\": [1.0, 1.10], \"STABLE\": [1.0, 1.02]}  # +10%  # +2%\n",
    "    df_returns = pd.DataFrame(data).pct_change().dropna()\n",
    "\n",
    "    # Pre-calculated Mean Volatility per ticker (as provided by Engine Observation)\n",
    "    vol_series = pd.Series({\"VOLATILE\": 0.10, \"STABLE\": 0.01})\n",
    "\n",
    "    # 2. Run Kernel\n",
    "    results = QuantUtils.calculate_sharpe_vol(df_returns, vol_series)\n",
    "\n",
    "    # 3. CALCULATE EXPECTED (Pure Math)\n",
    "    # Volatile Sharpe: 0.10 / 0.10 = 1.0\n",
    "    # Stable Sharpe:   0.02 / 0.01 = 2.0\n",
    "\n",
    "    try:\n",
    "        # Check A: Diversity. If they are the same, normalization didn't happen.\n",
    "        if np.isclose(results[\"VOLATILE\"], results[\"STABLE\"]):\n",
    "            raise AssertionError(\n",
    "                \"RANKING COLLAPSE: Both tickers have the same normalized score.\"\n",
    "            )\n",
    "\n",
    "        # Check B: Direction. STABLE must rank higher than VOLATILE.\n",
    "        if results[\"STABLE\"] < results[\"VOLATILE\"]:\n",
    "            # This is exactly what happens when the bug turns it into Momentum\n",
    "            raise AssertionError(\n",
    "                f\"MOMENTUM REGRESSION: 'STABLE' ({results['STABLE']:.2f}) \"\n",
    "                f\"ranked below 'VOLATILE' ({results['VOLATILE']:.2f}). \"\n",
    "                \"The denominator was likely collapsed to a market average.\"\n",
    "            )\n",
    "\n",
    "        # Check C: Absolute Precision\n",
    "        if not np.isclose(results[\"STABLE\"], 2.0):\n",
    "            raise AssertionError(\n",
    "                f\"MATH ERROR: Expected 2.0 for STABLE, got {results['STABLE']}\"\n",
    "            )\n",
    "\n",
    "        print(\n",
    "            \"‚úÖ RANKING INTEGRITY PASSED: Volatility normalization is strictly enforced.\"\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"üî• KERNEL BREACH: {str(e)}\")\n",
    "        raise e\n",
    "\n",
    "\n",
    "def verify_vol_alignment_integrity():\n",
    "    \"\"\"\n",
    "    üõ°Ô∏è TRIPWIRE: Verifies Temporal Coupling between Returns and Volatility.\n",
    "    Ensures that the volatility average is only calculated over days where a return exists.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- üõ°Ô∏è Starting Volatility Alignment Audit ---\")\n",
    "\n",
    "    # 1. SETUP SYNTHETIC DATA (2 Days)\n",
    "    # Day 1: Return = NaN, Vol = 0.90 (Extreme 'Trap' Volatility)\n",
    "    # Day 2: Return = 0.10, Vol = 0.10 (Target Reward/Risk)\n",
    "    rets_s = pd.Series([np.nan, 0.10])\n",
    "    vol_s = pd.Series([0.90, 0.10])\n",
    "\n",
    "    # 2. RUN KERNEL (Series Mode)\n",
    "    # Calculation Logic:\n",
    "    # If aligned: 0.10 / 0.10 = 1.0\n",
    "    # If misaligned: 0.10 / mean(0.90, 0.10) = 0.10 / 0.50 = 0.2\n",
    "    res_series = QuantUtils.calculate_sharpe_vol(rets_s, vol_s)\n",
    "\n",
    "    # 3. RUN KERNEL (DataFrame Mode)\n",
    "    # Ensures vectorized alignment works across columns\n",
    "    rets_df = pd.DataFrame({\"A\": [np.nan, 0.10], \"B\": [np.nan, 0.20]})\n",
    "    vol_df = pd.DataFrame({\"A\": [0.90, 0.10], \"B\": [0.05, 0.20]})\n",
    "    res_df = QuantUtils.calculate_sharpe_vol(rets_df, vol_df)\n",
    "\n",
    "    try:\n",
    "        # Check Series Alignment\n",
    "        if not np.isclose(res_series, 1.0):\n",
    "            raise AssertionError(\n",
    "                f\"DENOMINATOR MISMATCH: Series result {res_series:.2f} != 1.0. \"\n",
    "                \"The volatility denominator is likely including the leading NaN day.\"\n",
    "            )\n",
    "        print(\"‚úÖ Series Temporal Coupling: OK\")\n",
    "\n",
    "        # Check DataFrame Alignment (Ticker A: 0.1/0.1=1.0 | Ticker B: 0.2/0.2=1.0)\n",
    "        if not (np.isclose(res_df[\"A\"], 1.0) and np.isclose(res_df[\"B\"], 1.0)):\n",
    "            raise AssertionError(\n",
    "                f\"VECTORIZED MISMATCH: DataFrame results {res_df.values} != [1.0, 1.0]. \"\n",
    "                \"The logic is failing to align individual columns.\"\n",
    "            )\n",
    "        print(\"‚úÖ DataFrame Temporal Coupling: OK\")\n",
    "\n",
    "        print(\"‚úÖ AUDIT PASSED: Reward and Risk are strictly synchronized.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"üî• ALIGNMENT BREACH: {str(e)}\")\n",
    "        raise e\n",
    "\n",
    "\n",
    "# Auto-run the checks\n",
    "verify_math_integrity()\n",
    "\n",
    "verify_feature_engineering_integrity()\n",
    "\n",
    "verify_ranking_integrity()\n",
    "\n",
    "verify_vol_alignment_integrity()\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301d104d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# SECTION G: AUDIT ENGINE RESULTS\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "def visualize_audit_structure(obj):\n",
    "    \"\"\"\n",
    "    Generates the Map and returns a Registry of dictionaries:\n",
    "    [{'name': str, 'path': str, 'obj': object}, ...]\n",
    "    \"\"\"\n",
    "    id_memory = {}\n",
    "    registry = []\n",
    "    output = [\n",
    "        \"====================================================================\",\n",
    "        \"üîç HIGH-TRANSPARENCY AUDIT MAP\",\n",
    "        \"====================================================================\",\n",
    "    ]\n",
    "\n",
    "    def get_icon(val):\n",
    "        if isinstance(val, pd.DataFrame):\n",
    "            return \"üßÆ\"\n",
    "        if isinstance(val, pd.Series):\n",
    "            return \"üìà\"\n",
    "        if isinstance(val, (list, tuple, dict)):\n",
    "            return \"üìÇ\"\n",
    "        if isinstance(val, pd.Timestamp):\n",
    "            return \"üìÖ\"\n",
    "        if is_dataclass(val):\n",
    "            return \"üì¶\"\n",
    "        return \"üî¢\" if isinstance(val, (int, float)) else \"üìÑ\"\n",
    "\n",
    "    def process(item, name, level=0, path=\"\"):\n",
    "        indent = \"  \" * level\n",
    "        item_id = id(item)\n",
    "\n",
    "        # Build the breadcrumb path\n",
    "        current_path = f\"{path} -> {name}\" if path else name\n",
    "\n",
    "        is_primitive = isinstance(item, (int, float, str, bool, type(None)))\n",
    "        if not is_primitive and item_id in id_memory:\n",
    "            output.append(\n",
    "                f\"{indent}          ‚ï∞‚îÄ‚îÄ {name} --> [See ID {id_memory[item_id]}]\"\n",
    "            )\n",
    "            return\n",
    "\n",
    "        # 1. Store Index, Object, Name, and Path in Registry\n",
    "        curr_idx = len(registry)\n",
    "        registry.append({\"name\": name, \"path\": current_path, \"obj\": item})\n",
    "\n",
    "        if not is_primitive:\n",
    "            id_memory[item_id] = curr_idx\n",
    "\n",
    "        # 2. Metadata for display\n",
    "        meta = f\"{type(item).__name__}\"\n",
    "        if hasattr(item, \"shape\"):\n",
    "            meta = f\"shape={item.shape}\"\n",
    "        elif isinstance(item, (list, dict)):\n",
    "            meta = f\"len={len(item)}\"\n",
    "\n",
    "        output.append(f\"[{curr_idx:>3}] {indent}{get_icon(item)} {name} ({meta})\")\n",
    "\n",
    "        # 3. Recurse\n",
    "        if isinstance(item, dict):\n",
    "            for k, v in item.items():\n",
    "                process(v, k, level + 1, current_path)\n",
    "        elif isinstance(item, (list, tuple)):\n",
    "            for i, v in enumerate(item):\n",
    "                process(v, f\"index_{i}\", level + 1, current_path)\n",
    "        elif is_dataclass(item):\n",
    "            for f in fields(item):\n",
    "                process(getattr(item, f.name), f.name, level + 1, current_path)\n",
    "\n",
    "    process(obj, \"audit_pack\")\n",
    "    print(\"\\n\".join(output))\n",
    "\n",
    "    return registry\n",
    "\n",
    "\n",
    "def peek(idx, reg):\n",
    "    \"\"\"\n",
    "    Displays metadata and RETURNS the object for further use.\n",
    "    \"\"\"\n",
    "    if idx < 0 or idx >= len(reg):\n",
    "        print(f\"‚ùå Index {idx} out of range.\")\n",
    "        return None\n",
    "\n",
    "    entry = reg[idx]\n",
    "\n",
    "    # 1. Print the Header (for humans)\n",
    "    print(f\" {'='*60}\")\n",
    "    print(f\" üìç INDEX: [{idx}]\")\n",
    "    print(f\" üè∑Ô∏è  NAME:  {entry['name']}\")\n",
    "    print(f\" üìÇ PATH:  {entry['path']}\")\n",
    "    print(f\" {'='*60}\\n\")\n",
    "\n",
    "    # 2. Display the data (for the UI)\n",
    "    from IPython.display import display\n",
    "\n",
    "    display(entry[\"obj\"])\n",
    "\n",
    "    # 3. RETURN the data (for other functions)\n",
    "    return entry[\"obj\"]\n",
    "\n",
    "\n",
    "###############################\n",
    "\n",
    "\n",
    "def verify_engine_results_short_form(audit_pack, engine):  # <--- Added engine here\n",
    "    \"\"\"\n",
    "    MASTER INTEGRITY REPORT: Performs a 3-layer reconciliation\n",
    "    between Engine Output and Raw Data.\n",
    "    \"\"\"\n",
    "    # --------------------------------------------------------------------------\n",
    "    # 0. INITIALIZATION & CONTEXT\n",
    "    # --------------------------------------------------------------------------\n",
    "    res = audit_pack[0]\n",
    "    if not res or res.debug_data is None:\n",
    "        print(\"‚ùå AUDIT ABORTED: No debug data found. Run UI with debug=True.\")\n",
    "        return\n",
    "\n",
    "    debug = res.debug_data\n",
    "    inputs = debug[\"inputs_snapshot\"]\n",
    "    thresholds = inputs.quality_thresholds\n",
    "\n",
    "    # --- TRANSPARENCY BLOCK ---\n",
    "    print(\n",
    "        f\"üïµÔ∏è  STARTING MULTI-LAYER AUDIT: {inputs.metric} @ {res.decision_date.date()}\"\n",
    "    )\n",
    "    print(\"\\n\" + \"=\" * 85)\n",
    "    print(\"*\" * 85)\n",
    "    print(\n",
    "        f\"‚ö†Ô∏è  ASSUMPTION: Verification logic is independent, but trusts Engine source DataFrames\"\n",
    "    )\n",
    "    print(\n",
    "        f\"   (engine.features_df, engine.df_close, and debug['portfolio_raw_components'])\"\n",
    "    )\n",
    "    print(\"*\" * 85 + \"\\n\" + \"=\" * 85 + \"\\n\" * 2)\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # LAYER 1: SURVIVAL AUDIT\n",
    "    # --------------------------------------------------------------------------\n",
    "    l_audit = debug[\"audit_liquidity\"]\n",
    "    snapshot = l_audit[\"universe_snapshot\"]\n",
    "\n",
    "    m_quantile = snapshot[\"RollMedDollarVol\"].quantile(\n",
    "        thresholds[\"min_liquidity_percentile\"]\n",
    "    )\n",
    "    m_cutoff = max(m_quantile, thresholds[\"min_median_dollar_volume\"])\n",
    "\n",
    "    m_survivors = snapshot[\n",
    "        (snapshot[\"RollMedDollarVol\"] >= m_cutoff)\n",
    "        & (snapshot[\"RollingStalePct\"] <= thresholds[\"max_stale_pct\"])\n",
    "        & (snapshot[\"RollingSameVolCount\"] <= thresholds[\"max_same_vol_count\"])\n",
    "    ]\n",
    "\n",
    "    s_status = \"‚úÖ PASS\" if len(m_survivors) == l_audit[\"tickers_passed\"] else \"‚ùå FAIL\"\n",
    "    print(\n",
    "        f\"LAYER 1: SURVIVAL  | Universe: {len(snapshot)} -> Survivors: {len(m_survivors)} | {s_status}\"\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # LAYER 2: SELECTION AUDIT (Ranking & Scoring)\n",
    "    # --------------------------------------------------------------------------\n",
    "    survivor_list = m_survivors.index.tolist()\n",
    "    idx = pd.IndexSlice\n",
    "\n",
    "    # Independent Data Reconstruction using the passed 'engine'\n",
    "    feat_period = engine.features_df.loc[\n",
    "        idx[survivor_list, res.start_date : res.decision_date], :\n",
    "    ]\n",
    "    atrp_mean = feat_period[\"ATRP\"].groupby(level=\"Ticker\").mean()\n",
    "    trp_mean = feat_period[\"TRP\"].groupby(level=\"Ticker\").mean()\n",
    "\n",
    "    feat_now = engine.features_df.xs(res.decision_date, level=\"Date\").reindex(\n",
    "        survivor_list\n",
    "    )\n",
    "\n",
    "    # Check if df_close or df_close_wide is the correct attribute on your engine\n",
    "    # Based on your Error it was looking for engine.df_close\n",
    "    lookback_prices = engine.df_close.loc[\n",
    "        res.start_date : res.decision_date, survivor_list\n",
    "    ]\n",
    "\n",
    "    # BUILD COMPLETE OBSERVATION\n",
    "    audit_obs = {\n",
    "        \"lookback_close\": lookback_prices,\n",
    "        \"lookback_returns\": lookback_prices.ffill().pct_change(),\n",
    "        \"atrp\": atrp_mean,\n",
    "        \"trp\": trp_mean,\n",
    "        \"rsi\": feat_now[\"RSI\"],\n",
    "        \"rel_strength\": feat_now[\"RelStrength\"],\n",
    "        \"vol_regime\": feat_now[\"VolRegime\"],\n",
    "        \"rvol\": feat_now[\"RVol\"],\n",
    "        \"obv_score\": feat_now[\"OBV_Score\"],\n",
    "        \"roc_1\": feat_now[\"ROC_1\"],\n",
    "        \"roc_3\": feat_now[\"ROC_3\"],\n",
    "        \"roc_5\": feat_now[\"ROC_5\"],\n",
    "        \"roc_10\": feat_now[\"ROC_10\"],\n",
    "        \"roc_21\": feat_now[\"ROC_21\"],\n",
    "    }\n",
    "\n",
    "    # Re-run Strategy Formula\n",
    "    manual_scores = METRIC_REGISTRY[inputs.metric](audit_obs)\n",
    "\n",
    "    # Comparison logic\n",
    "    audit_results = []\n",
    "    for rank, ticker in enumerate(res.tickers, start=inputs.rank_start):\n",
    "        eng_score = res.results_df.loc[ticker, \"Strategy Value\"]\n",
    "        audit_score = manual_scores.loc[ticker]\n",
    "        audit_results.append(\n",
    "            {\n",
    "                \"Rank\": rank,\n",
    "                \"Ticker\": ticker,\n",
    "                \"Engine\": eng_score,\n",
    "                \"Manual\": audit_score,\n",
    "                \"Delta\": eng_score - audit_score,\n",
    "                \"Match\": np.isclose(eng_score, audit_score),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    sel_df = pd.DataFrame(audit_results)\n",
    "    sel_status = \"‚úÖ PASS\" if sel_df[\"Match\"].all() else \"‚ùå FAIL\"\n",
    "    print(\n",
    "        f\"LAYER 2: SELECTION | Strategy: {inputs.metric} | Selection Match: {sel_status}\"\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # LAYER 3: PERFORMANCE AUDIT (Returns & Risk)\n",
    "    # --------------------------------------------------------------------------\n",
    "    def run_kernel(df_p, df_atrp, df_trp, w):\n",
    "        norm = df_p.div(df_p.bfill().iloc[0])\n",
    "        eq = (norm * w).sum(axis=1)\n",
    "        drift_w = (norm * w).div(eq, axis=0)\n",
    "        p_atrp = (drift_w * df_atrp).sum(axis=1)\n",
    "        rets = eq.pct_change()\n",
    "        v_mask = rets.notna()\n",
    "        g = eq.iloc[-1] / eq.iloc[0] - 1\n",
    "        s = (rets.mean() / rets.std() * np.sqrt(252)) if rets.std() > 0 else 0\n",
    "        sa = rets.mean() / p_atrp.where(v_mask).mean()\n",
    "        return g, s, sa\n",
    "\n",
    "    p_comp = debug[\"portfolio_raw_components\"]\n",
    "\n",
    "    m_gain, m_sharpe, m_s_atrp = run_kernel(\n",
    "        p_comp[\"prices\"].loc[res.buy_date : res.holding_end_date],\n",
    "        p_comp[\"atrp\"].loc[res.buy_date : res.holding_end_date],\n",
    "        p_comp[\"trp\"].loc[res.buy_date : res.holding_end_date],\n",
    "        res.initial_weights,\n",
    "    )\n",
    "\n",
    "    e_gain = res.perf_metrics[\"holding_p_gain\"]\n",
    "    perf_status = \"‚úÖ PASS\" if np.isclose(m_gain, e_gain, atol=1e-7) else \"‚ùå FAIL\"\n",
    "    print(\n",
    "        f\"LAYER 3: PERFORMANCE | Holding Gain: {e_gain:.4f} vs Manual: {m_gain:.4f} | {perf_status}\"\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # DISPLAY TABLES\n",
    "    # --------------------------------------------------------------------------\n",
    "    print(\"=\" * 85)\n",
    "    display(\n",
    "        sel_df.set_index(\"Rank\")\n",
    "        .style.format({\"Engine\": \"{:.6f}\", \"Manual\": \"{:.6f}\", \"Delta\": \"{:.8f}\"})\n",
    "        .set_caption(\"Layer 2: Selection Reconciliation\")\n",
    "    )\n",
    "\n",
    "    perf_summary = pd.DataFrame(\n",
    "        {\n",
    "            \"Metric\": [\"Holding Gain\", \"Holding Sharpe\", \"Sharpe (ATRP)\"],\n",
    "            \"Engine\": [\n",
    "                e_gain,\n",
    "                res.perf_metrics[\"holding_p_sharpe\"],\n",
    "                res.perf_metrics[\"holding_p_sharpe_atrp\"],\n",
    "            ],\n",
    "            \"Manual\": [m_gain, m_sharpe, m_s_atrp],\n",
    "        }\n",
    "    ).set_index(\"Metric\")\n",
    "    perf_summary[\"Delta\"] = perf_summary[\"Engine\"] - perf_summary[\"Manual\"]\n",
    "    display(\n",
    "        perf_summary.style.format(\"{:.8f}\").set_caption(\n",
    "            \"Layer 3: Performance Reconciliation\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def verify_engine_results_short_form(audit_pack, engine):\n",
    "    \"\"\"\n",
    "    Robust Short-Form Audit that handles both DISCOVERY (Calculated)\n",
    "    and CASCADE (Bypassed) modes without crashing.\n",
    "    \"\"\"\n",
    "    # Handle both list and single object inputs\n",
    "    results = audit_pack if isinstance(audit_pack, list) else [audit_pack]\n",
    "\n",
    "    print(f\"üõ°Ô∏è  SHORT-FORM AUDIT (Verifying {len(results)} runs)\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    for i, res in enumerate(results):\n",
    "        if res is None:\n",
    "            print(f\"Run #{i+1}: ‚ö†Ô∏è No Result\")\n",
    "            continue\n",
    "\n",
    "        debug = res.debug_data\n",
    "        if not debug:\n",
    "            print(f\"Run #{i+1}: ‚ùå No Debug Data (Run with debug=True)\")\n",
    "            continue\n",
    "\n",
    "        inputs = debug.get(\"inputs_snapshot\")\n",
    "        audit_liq = debug.get(\"audit_liquidity\", {})\n",
    "\n",
    "        print(f\"\\nüîé RUN #{i+1} | Date: {res.decision_date.date()}\")\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "        # 1. LIQUIDITY / SURVIVAL CHECK (The Fix)\n",
    "        # ------------------------------------------------------------------\n",
    "        # If we provided a subset, the engine SKIPPED the filters.\n",
    "        # We must tell the auditor to skip them too.\n",
    "        is_cascade = inputs.universe_subset is not None\n",
    "\n",
    "        if is_cascade:\n",
    "            subset_len = len(inputs.universe_subset)\n",
    "            print(f\"   Mode: CASCADE (Subset of {subset_len})\")\n",
    "            print(\"   Liquidity Audit: ‚úÖ BYPASSED (Engine skipped filters per design)\")\n",
    "            print(f\"   Survivors: {len(res.tickers)} passed to ranking\")\n",
    "\n",
    "        else:\n",
    "            # DISCOVERY MODE: We expect the full snapshot with columns\n",
    "            snapshot = audit_liq.get(\"universe_snapshot\")\n",
    "\n",
    "            # Double check the columns exist before touching them\n",
    "            if snapshot is not None and \"RollMedDollarVol\" in snapshot.columns:\n",
    "                thresholds = inputs.quality_thresholds\n",
    "                m_quantile = snapshot[\"RollMedDollarVol\"].quantile(\n",
    "                    thresholds[\"min_liquidity_percentile\"]\n",
    "                )\n",
    "                m_cutoff = max(m_quantile, thresholds[\"min_median_dollar_volume\"])\n",
    "\n",
    "                # Manual Re-calc\n",
    "                mask = (\n",
    "                    (snapshot[\"RollMedDollarVol\"] >= m_cutoff)\n",
    "                    & (snapshot[\"RollingStalePct\"] <= thresholds[\"max_stale_pct\"])\n",
    "                    & (\n",
    "                        snapshot[\"RollingSameVolCount\"]\n",
    "                        <= thresholds[\"max_same_vol_count\"]\n",
    "                    )\n",
    "                )\n",
    "                m_count = mask.sum()\n",
    "                e_count = audit_liq.get(\"tickers_passed\")\n",
    "\n",
    "                status = \"‚úÖ PASS\" if m_count == e_count else \"‚ùå FAIL\"\n",
    "                print(f\"   Mode: DISCOVERY (Full Market)\")\n",
    "                print(\n",
    "                    f\"   Liquidity Audit: {status} (Engine: {e_count} vs Manual: {m_count})\"\n",
    "                )\n",
    "            else:\n",
    "                print(\"   ‚ö†Ô∏è Liquidity snapshot missing or invalid.\")\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "        # 2. STRATEGY CHECK (Simple Score Check)\n",
    "        # ------------------------------------------------------------------\n",
    "        # Just verifying the Engine result dataframe structure\n",
    "        if not res.results_df.empty:\n",
    "            top_score = res.results_df.iloc[0][\"Strategy Value\"]\n",
    "            print(f\"   Top Ticker: {res.tickers[0]} (Score: {top_score:.4f})\")\n",
    "            print(f\"   Selection: {len(res.tickers)} tickers returned.\")\n",
    "        else:\n",
    "            print(\"   ‚ö†Ô∏è No results generated.\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "###############################\n",
    "\n",
    "\n",
    "def verify_engine_results_long_form(\n",
    "    audit_pack, engine\n",
    "):  # <--- FIXED: Added engine parameter\n",
    "    \"\"\"\n",
    "    Run independent verification of Engine run results\n",
    "    \"\"\"\n",
    "    # 1. SETUP\n",
    "    res = audit_pack[0]\n",
    "\n",
    "    print(\"=\" * 78)\n",
    "    print(\"MODULAR EVENT-DRIVEN AUDIT -- START\")\n",
    "    print(\"=\" * 78)\n",
    "\n",
    "    if res.debug_data is None:\n",
    "        raise ValueError(\"‚ùå Audit failed: The Engine was run with debug=False.\")\n",
    "\n",
    "    m = res.perf_metrics\n",
    "    init_weights = res.initial_weights\n",
    "    periods = {\n",
    "        \"Full\": (res.start_date, res.holding_end_date),\n",
    "        \"Lookback\": (res.start_date, res.decision_date),\n",
    "        \"Holding\": (res.buy_date, res.holding_end_date),\n",
    "    }\n",
    "\n",
    "    # 2. FETCH RAW DATA\n",
    "    p_raw_components = res.debug_data[\"portfolio_raw_components\"]\n",
    "    b_raw_components = res.debug_data[\"benchmark_raw_components\"]\n",
    "    p_ohlcv = p_raw_components[\"ohlcv_raw\"]\n",
    "    p_raw_price = p_raw_components[\"prices\"]\n",
    "    b_ohlcv = b_raw_components[\"ohlcv_raw\"]\n",
    "    b_raw_price = b_raw_components[\"prices\"]\n",
    "\n",
    "    # VOLATILITY MATH KERNEL\n",
    "    def calculate_manual_vol(df_ohlcv, atr_period=14):\n",
    "        df = df_ohlcv.copy()\n",
    "        df[\"PC\"] = df.groupby(level=\"Ticker\")[\"Adj Close\"].shift(1)\n",
    "        tr = pd.concat(\n",
    "            [\n",
    "                df[\"Adj High\"] - df[\"Adj Low\"],\n",
    "                (df[\"Adj High\"] - df[\"PC\"]).abs(),\n",
    "                (df[\"Adj Low\"] - df[\"PC\"]).abs(),\n",
    "            ],\n",
    "            axis=1,\n",
    "        ).max(axis=1)\n",
    "        atr = (\n",
    "            tr.groupby(level=\"Ticker\")\n",
    "            .ewm(alpha=1 / atr_period, adjust=False)\n",
    "            .mean()\n",
    "            .reset_index(level=0, drop=True)\n",
    "        )\n",
    "        return (atr / df[\"Adj Close\"]).unstack(level=\"Ticker\"), (\n",
    "            tr / df[\"Adj Close\"]\n",
    "        ).unstack(level=\"Ticker\")\n",
    "\n",
    "    _, p_manual_trp = calculate_manual_vol(p_ohlcv)\n",
    "    _, b_manual_trp = calculate_manual_vol(b_ohlcv)\n",
    "\n",
    "    p_raw = {\n",
    "        \"price\": p_raw_price,\n",
    "        \"atrp\": p_raw_components[\"atrp\"],\n",
    "        \"trp\": p_manual_trp,\n",
    "    }\n",
    "    b_raw = {\n",
    "        \"price\": b_raw_price,\n",
    "        \"atrp\": b_raw_components[\"atrp\"],\n",
    "        \"trp\": b_manual_trp,\n",
    "    }\n",
    "\n",
    "    # AUDIT KERNEL\n",
    "    def run_period_audit(df_p, df_atrp, df_trp, weights):\n",
    "        norm_prices = df_p.div(df_p.bfill().iloc[0])\n",
    "        weighted_components = norm_prices.mul(weights, axis=1)\n",
    "        equity_curve = weighted_components.sum(axis=1)\n",
    "        current_weights = weighted_components.div(equity_curve, axis=0)\n",
    "        port_atrp = (current_weights * df_atrp).sum(axis=1)\n",
    "        port_trp = (current_weights * df_trp).sum(axis=1)\n",
    "        returns = equity_curve.pct_change()\n",
    "        v_mask = returns.notna()\n",
    "        gain = (equity_curve.iloc[-1] / equity_curve.iloc[0]) - 1\n",
    "        sharpe = (\n",
    "            (returns.mean() / returns.std()) * np.sqrt(252) if returns.std() > 0 else 0\n",
    "        )\n",
    "        s_atrp = returns.mean() / port_atrp.where(v_mask).mean()\n",
    "        s_trp = returns.mean() / port_trp.where(v_mask).mean()\n",
    "        return gain, sharpe, s_atrp, s_trp\n",
    "\n",
    "    audit_rows = []\n",
    "    for p_label, (d_start, d_end) in periods.items():\n",
    "        g, s, sa, st = run_period_audit(\n",
    "            p_raw[\"price\"].loc[d_start:d_end],\n",
    "            p_raw[\"atrp\"].loc[d_start:d_end],\n",
    "            p_raw[\"trp\"].loc[d_start:d_end],\n",
    "            init_weights,\n",
    "        )\n",
    "        b_ticker = b_raw[\"price\"].columns[0]\n",
    "        bg, bs, bsa, bst = run_period_audit(\n",
    "            b_raw[\"price\"].loc[d_start:d_end],\n",
    "            b_raw[\"atrp\"].loc[d_start:d_end],\n",
    "            b_raw[\"trp\"].loc[d_start:d_end],\n",
    "            pd.Series({b_ticker: 1.0}),\n",
    "        )\n",
    "\n",
    "        mapping = [\n",
    "            (\"Gain\", g, f\"{p_label.lower()}_p_gain\", bg, f\"{p_label.lower()}_b_gain\"),\n",
    "            (\n",
    "                \"Sharpe\",\n",
    "                s,\n",
    "                f\"{p_label.lower()}_p_sharpe\",\n",
    "                bs,\n",
    "                f\"{p_label.lower()}_b_sharpe\",\n",
    "            ),\n",
    "            (\n",
    "                \"Sharpe (ATRP)\",\n",
    "                sa,\n",
    "                f\"{p_label.lower()}_p_sharpe_atrp\",\n",
    "                bsa,\n",
    "                f\"{p_label.lower()}_b_sharpe_atrp\",\n",
    "            ),\n",
    "            (\n",
    "                \"Sharpe (TRP)\",\n",
    "                st,\n",
    "                f\"{p_label.lower()}_p_sharpe_trp\",\n",
    "                bst,\n",
    "                f\"{p_label.lower()}_b_sharpe_trp\",\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "        for metric, manual_p, key_p, manual_b, key_b in mapping:\n",
    "            audit_rows.append(\n",
    "                {\n",
    "                    \"Entity\": \"Group\",\n",
    "                    \"Period\": p_label,\n",
    "                    \"Metric\": metric,\n",
    "                    \"Engine\": m.get(key_p, 0),\n",
    "                    \"Manual\": manual_p,\n",
    "                    \"Delta\": m.get(key_p, 0) - manual_p,\n",
    "                }\n",
    "            )\n",
    "            audit_rows.append(\n",
    "                {\n",
    "                    \"Entity\": \"Benchmark\",\n",
    "                    \"Period\": p_label,\n",
    "                    \"Metric\": metric,\n",
    "                    \"Engine\": m.get(key_b, 0),\n",
    "                    \"Manual\": manual_b,\n",
    "                    \"Delta\": m.get(key_b, 0) - manual_b,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    df_final = pd.DataFrame(audit_rows)\n",
    "    df_final[\"Status\"] = df_final[\"Delta\"].apply(\n",
    "        lambda x: \"‚úÖ PASS\" if abs(x) < 1e-7 else \"‚ùå FAIL\"\n",
    "    )\n",
    "    print(f\"üìù FINAL INTEGRITY REPORT (MODULAR)\")\n",
    "    display(\n",
    "        df_final.pivot_table(\n",
    "            index=[\"Entity\", \"Metric\"],\n",
    "            columns=\"Period\",\n",
    "            values=\"Status\",\n",
    "            aggfunc=\"first\",\n",
    "        )\n",
    "    )\n",
    "    display(\n",
    "        df_final.style.format(\n",
    "            {\"Engine\": \"{:.8f}\", \"Manual\": \"{:.8f}\", \"Delta\": \"{:.8f}\"}\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # ==============================================================================\n",
    "    # SURVIVAL AUDIT\n",
    "    # ==============================================================================\n",
    "    print(\"\\n\" + \"=\" * 78)\n",
    "    print(\"SURVIVAL AUDIT -- START\")\n",
    "    audit = res.debug_data[\"audit_liquidity\"]\n",
    "    snapshot = audit[\"universe_snapshot\"]\n",
    "    thresholds = res.debug_data.get(\"inputs_snapshot\").quality_thresholds\n",
    "\n",
    "    manual_quantile_val = snapshot[\"RollMedDollarVol\"].quantile(\n",
    "        thresholds[\"min_liquidity_percentile\"]\n",
    "    )\n",
    "    manual_final_cutoff = max(\n",
    "        manual_quantile_val, thresholds[\"min_median_dollar_volume\"]\n",
    "    )\n",
    "\n",
    "    manual_survivors = snapshot[\n",
    "        (snapshot[\"RollMedDollarVol\"] >= manual_final_cutoff)\n",
    "        & (snapshot[\"RollingStalePct\"] <= thresholds[\"max_stale_pct\"])\n",
    "        & (snapshot[\"RollingSameVolCount\"] <= thresholds[\"max_same_vol_count\"])\n",
    "    ]\n",
    "\n",
    "    s_match = (\n",
    "        \"‚úÖ PASS\" if audit[\"tickers_passed\"] == len(manual_survivors) else \"‚ùå FAIL\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Survival Integrity: {s_match} (Engine: {audit['tickers_passed']} vs Manual: {len(manual_survivors)})\"\n",
    "    )\n",
    "\n",
    "    # ==============================================================================\n",
    "    # UNIVERSAL SELECTION AUDIT (The logic that previously failed)\n",
    "    # ==============================================================================\n",
    "    print(\"\\n\" + \"=\" * 78)\n",
    "    print(\"UNIVERSAL SELECTION AUDIT -- START\")\n",
    "\n",
    "    debug = res.debug_data\n",
    "    inputs = debug[\"inputs_snapshot\"]\n",
    "    survivors = snapshot.query(\"Passed_Final\").index.tolist()\n",
    "    idx = pd.IndexSlice\n",
    "\n",
    "    # USES THE PASSED 'engine' INSTANCE\n",
    "    feat_period = engine.features_df.loc[\n",
    "        idx[survivors, res.start_date : res.decision_date], :\n",
    "    ]\n",
    "    atrp_lookback_mean = feat_period[\"ATRP\"].groupby(level=\"Ticker\").mean()\n",
    "    trp_lookback_mean = feat_period[\"TRP\"].groupby(level=\"Ticker\").mean()\n",
    "    feat_current = engine.features_df.xs(res.decision_date, level=\"Date\").reindex(\n",
    "        survivors\n",
    "    )\n",
    "    lookback_prices = engine.df_close.loc[res.start_date : res.decision_date, survivors]\n",
    "\n",
    "    audit_obs: MarketObservation = {\n",
    "        \"lookback_close\": lookback_prices,\n",
    "        \"lookback_returns\": lookback_prices.ffill().pct_change(),\n",
    "        \"atrp\": atrp_lookback_mean,\n",
    "        \"trp\": trp_lookback_mean,\n",
    "        \"rsi\": feat_current[\"RSI\"],\n",
    "        \"rel_strength\": feat_current[\"RelStrength\"],\n",
    "        \"vol_regime\": feat_current[\"VolRegime\"],\n",
    "        \"rvol\": feat_current[\"RVol\"],\n",
    "        \"spy_rvol\": feat_current[\"Spy_RVol\"],\n",
    "        \"obv_score\": feat_current[\"OBV_Score\"],\n",
    "        \"spy_obv_score\": feat_current[\"Spy_OBV_Score\"],\n",
    "        \"roc_1\": feat_current[\"ROC_1\"],\n",
    "        \"roc_3\": feat_current[\"ROC_3\"],\n",
    "        \"roc_5\": feat_current[\"ROC_5\"],\n",
    "        \"roc_10\": feat_current[\"ROC_10\"],\n",
    "        \"roc_21\": feat_current[\"ROC_21\"],\n",
    "    }\n",
    "\n",
    "    manual_scores = METRIC_REGISTRY[inputs.metric](audit_obs)\n",
    "    audit_results = []\n",
    "    for rank, ticker in enumerate(res.tickers, start=inputs.rank_start):\n",
    "        eng_score = res.results_df.loc[ticker, \"Strategy Value\"]\n",
    "        aud_score = manual_scores.loc[ticker]\n",
    "        audit_results.append(\n",
    "            {\n",
    "                \"Rank\": rank,\n",
    "                \"Ticker\": ticker,\n",
    "                \"Engine_Score\": eng_score,\n",
    "                \"Audit_Score\": aud_score,\n",
    "                \"Delta\": eng_score - aud_score,\n",
    "                \"Status\": \"‚úÖ PASS\" if np.isclose(eng_score, aud_score) else \"‚ùå FAIL\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "    display(\n",
    "        pd.DataFrame(audit_results)\n",
    "        .set_index(\"Rank\")\n",
    "        .style.format(\"{:.8f}\", subset=[\"Engine_Score\", \"Audit_Score\", \"Delta\"])\n",
    "    )\n",
    "    print(\"=\" * 78)\n",
    "\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a48017a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# SECTION H: UTILITIES\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "def export_debug_to_csv(audit_pack, source_label=\"Audit\"):\n",
    "    \"\"\"\n",
    "    High-Transparency Exporter (Hardened Version).\n",
    "    Dumps the entire simulation state into a folder for manual Excel verification.\n",
    "    \"\"\"\n",
    "    if not audit_pack or not audit_pack[0]:\n",
    "        print(\"‚ùå Error: Audit Pack is empty. Run a simulation first.\")\n",
    "        return\n",
    "\n",
    "    data = audit_pack[0]\n",
    "    # Handle the fact that 'inputs' might be a key or a dataclass attribute\n",
    "    inputs = data.get(\"inputs\")\n",
    "\n",
    "    # 1. Folder Setup\n",
    "    date_str = inputs.start_date.strftime(\"%Y-%m-%d\")\n",
    "    strat = inputs.metric.replace(\" \", \"\").replace(\"(\", \"\").replace(\")\", \"\")\n",
    "    folder_name = f\"{source_label}_{strat}_{date_str}\"\n",
    "\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "\n",
    "    print(f\"üìÇ [AUDIT EXPORT] Folder: ./{folder_name}/\")\n",
    "\n",
    "    def process_item(item, path_prefix=\"\"):\n",
    "        # A. Handle Nested Dicts\n",
    "        if isinstance(item, dict):\n",
    "            for k, v in item.items():\n",
    "                process_item(v, f\"{path_prefix}{k}_\" if path_prefix else f\"{k}_\")\n",
    "\n",
    "        # B. Handle DataFrames (Matrices - High Precision)\n",
    "        elif isinstance(item, pd.DataFrame):\n",
    "            fn = f\"Matrix_{path_prefix.strip('_')}.csv\"\n",
    "            item.to_csv(os.path.join(folder_name, fn), float_format=\"%.8f\")\n",
    "            print(f\"   ‚úÖ Matrix: {fn}\")\n",
    "\n",
    "        # C. Handle Series (Vectors)\n",
    "        elif isinstance(item, pd.Series):\n",
    "            fn = f\"Vector_{path_prefix.strip('_')}.csv\"\n",
    "            item.to_frame().to_csv(os.path.join(folder_name, fn), float_format=\"%.8f\")\n",
    "            print(f\"   ‚úÖ Vector: {fn}\")\n",
    "\n",
    "        # D. Handle Dataclasses (Metadata & Results)\n",
    "        elif is_dataclass(item):\n",
    "            class_name = item.__class__.__name__\n",
    "            fn = f\"Summary_{class_name}_{path_prefix.strip('_')}\".strip(\"_\") + \".csv\"\n",
    "\n",
    "            # --- THE FIX: Create a Safe Dictionary for Pandas ---\n",
    "            raw_dict = asdict(item)\n",
    "            summary_ready_dict = {}\n",
    "\n",
    "            for k, v in raw_dict.items():\n",
    "                # If it's a big data object, just note its existence in the summary\n",
    "                if isinstance(v, (pd.DataFrame, pd.Series)):\n",
    "                    summary_ready_dict[k] = f\"<{v.__class__.__name__} shape={v.shape}>\"\n",
    "                # If it's a list or dict (the crash cause), stringify it for Excel\n",
    "                elif isinstance(v, (list, dict)):\n",
    "                    summary_ready_dict[k] = str(v)\n",
    "                else:\n",
    "                    summary_ready_dict[k] = v\n",
    "\n",
    "            # Save the clean key-value summary\n",
    "            pd.DataFrame.from_dict(\n",
    "                summary_ready_dict, orient=\"index\", columns=[\"Value\"]\n",
    "            ).to_csv(os.path.join(folder_name, fn))\n",
    "            print(f\"   üìë Summary: {fn}\")\n",
    "\n",
    "            # E. RECURSION: Now find the actual DataFrames inside the dataclass\n",
    "            # We iterate the object attributes directly to avoid the 'asdict' list confusion\n",
    "            for k in item.__dataclass_fields__.keys():\n",
    "                val = getattr(item, k)\n",
    "                if isinstance(val, (pd.DataFrame, pd.Series, dict)):\n",
    "                    process_item(val, f\"{path_prefix}{k}_\")\n",
    "\n",
    "    # 3. Execute Extraction\n",
    "    process_item(data)\n",
    "    print(f\"\\n‚ú® Export Complete. Open ./{folder_name}/ to verify results.\")\n",
    "\n",
    "\n",
    "def export_audit_to_excel(audit_pack, filename=\"Audit_Verification_Report.xlsx\"):\n",
    "    \"\"\"\n",
    "    Consolidates the audit_pack into a multi-sheet Excel workbook.\n",
    "    Organizes data by shared axes (Date vs Ticker) for manual formula checking.\n",
    "    \"\"\"\n",
    "    if not audit_pack or not audit_pack[0]:\n",
    "        print(\"‚ùå Error: Audit Pack is empty.\")\n",
    "        return\n",
    "\n",
    "    data = audit_pack[0]\n",
    "    res = data[\"results\"]\n",
    "    inputs = data[\"inputs\"]\n",
    "    debug = data.get(\"debug\", {})\n",
    "\n",
    "    print(f\"üìÇ [EXCEL AUDIT] Creating Report: {filename}\")\n",
    "\n",
    "    with pd.ExcelWriter(filename, engine=\"openpyxl\") as writer:\n",
    "\n",
    "        # --- SHEET 1: OVERVIEW (The Settings & Final Totals) ---\n",
    "        # Combines Input settings and Result scalars into one vertical table\n",
    "        meta_dict = {\n",
    "            **asdict(inputs),\n",
    "            **{\n",
    "                k: v\n",
    "                for k, v in asdict(res).items()\n",
    "                if not isinstance(v, (pd.DataFrame, pd.Series, dict))\n",
    "            },\n",
    "        }\n",
    "        # Stringify lists/dicts to prevent Excel/Pandas export crashes\n",
    "        clean_meta = {\n",
    "            k: (str(v) if isinstance(v, (list, dict)) else v)\n",
    "            for k, v in meta_dict.items()\n",
    "        }\n",
    "\n",
    "        df_overview = pd.DataFrame.from_dict(\n",
    "            clean_meta, orient=\"index\", columns=[\"Value\"]\n",
    "        )\n",
    "        df_overview.to_excel(writer, sheet_name=\"OVERVIEW\")\n",
    "\n",
    "        # --- SHEET 2: DAILY_AUDIT (Axis = Date) ---\n",
    "        # Concatenates everything that happens day-by-day\n",
    "        daily_items = {\n",
    "            \"Port_Value\": res.portfolio_series,\n",
    "            \"Port_Ret\": QuantUtils.compute_returns(res.portfolio_series),\n",
    "            \"Port_ATRP\": res.portfolio_atrp_series,  # <--- DIRECT ACCESS\n",
    "            \"Port_TRP\": res.portfolio_trp_series,  # <--- DIRECT ACCESS\n",
    "            \"Bench_Value\": res.benchmark_series,\n",
    "            \"Bench_Ret\": QuantUtils.compute_returns(res.benchmark_series),\n",
    "            \"Bench_ATRP\": res.benchmark_atrp_series,  # <--- DIRECT ACCESS\n",
    "            \"Bench_TRP\": res.benchmark_trp_series,  # <--- DIRECT ACCESS\n",
    "        }\n",
    "\n",
    "        # Filter out None values and concatenate side-by-side\n",
    "        df_daily = pd.concat(\n",
    "            {k: v for k, v in daily_items.items() if v is not None}, axis=1\n",
    "        )\n",
    "        df_daily.to_excel(writer, sheet_name=\"DAILY_AUDIT\", float_format=\"%.8f\")\n",
    "\n",
    "        # --- SHEET 3: SELECTION_SNAPSHOT (Axis = Ticker) ---\n",
    "        # Focuses on the selected 10-20 tickers and their performance\n",
    "        if \"full_universe_ranking\" in debug:\n",
    "            df_rank = debug[\"full_universe_ranking\"]\n",
    "            # Filter the leaderboard for only the tickers we actually bought\n",
    "            df_composition = df_rank.reindex(res.tickers)\n",
    "            df_composition.to_excel(\n",
    "                writer, sheet_name=\"PORTFOLIO_SNAPSHOT\", float_format=\"%.8f\"\n",
    "            )\n",
    "\n",
    "        # --- SHEET 4: FULL_UNIVERSE_RANKING ---\n",
    "        if \"full_universe_ranking\" in debug:\n",
    "            debug[\"full_universe_ranking\"].to_excel(\n",
    "                writer, sheet_name=\"FULL_RANKING\", float_format=\"%.8f\"\n",
    "            )\n",
    "\n",
    "        # --- SHEET 5: RAW_PRICES_MATRIX ---\n",
    "        if \"portfolio_raw_components\" in debug:\n",
    "            raw_p = debug[\"portfolio_raw_components\"].get(\"prices\")\n",
    "            if raw_p is not None:\n",
    "                raw_p.to_excel(writer, sheet_name=\"RAW_PRICES\", float_format=\"%.8f\")\n",
    "\n",
    "        # --- SHEET 6: RAW_VOL_MATRIX (TRP) ---\n",
    "        if \"portfolio_raw_components\" in debug:\n",
    "            # Re-extracting TRP matrix for the specific tickers\n",
    "            raw_v = debug[\"portfolio_raw_components\"].get(\n",
    "                \"atrp\"\n",
    "            )  # Or trp if stored specifically\n",
    "            if raw_v is not None:\n",
    "                raw_v.to_excel(writer, sheet_name=\"RAW_VOL_DATA\", float_format=\"%.8f\")\n",
    "\n",
    "    print(f\"‚ú® Audit Report Complete. Manual verification ready in {filename}\")\n",
    "\n",
    "\n",
    "def print_nested(d, indent=0, width=4):\n",
    "    \"\"\"Pretty-print nested containers.\n",
    "    Leaves are rendered as two lines:  key\\\\nvalue .\"\"\"\n",
    "    spacing = \" \" * indent\n",
    "\n",
    "    def _kind(node):\n",
    "        if not isinstance(node, dict):\n",
    "            return None\n",
    "        return \"sep\" if all(isinstance(v, dict) for v in node.values()) else \"nest\"\n",
    "\n",
    "    if isinstance(d, dict):\n",
    "        for k, v in d.items():\n",
    "            kind = _kind(v)\n",
    "            tag = \"\" if kind is None else f\"  [{'SEP' if kind == 'sep' else 'NEST'}]\"\n",
    "            print(f\"{spacing}{k}{tag}\")\n",
    "            print_nested(v, indent + width, width)\n",
    "\n",
    "    elif isinstance(d, (list, tuple)):\n",
    "        for idx, item in enumerate(d):\n",
    "            print(f\"{spacing}[{idx}]\")\n",
    "            print_nested(item, indent + width, width)\n",
    "\n",
    "    else:  # leaf ‚Äì primitive value\n",
    "        print(f\"{spacing}{d}\")\n",
    "\n",
    "\n",
    "def get_ticker_OHLCV(\n",
    "    df_ohlcv: pd.DataFrame,\n",
    "    tickers: Union[str, List[str]],\n",
    "    date_start: str,\n",
    "    date_end: str,\n",
    "    return_format: str = \"dataframe\",\n",
    "    verbose: bool = True,\n",
    ") -> Union[pd.DataFrame, dict]:\n",
    "    \"\"\"\n",
    "    Get OHLCV data for specified tickers within a date range.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_ohlcv : pd.DataFrame\n",
    "        DataFrame with MultiIndex of (ticker, date) and OHLCV columns\n",
    "    tickers : str or list of str\n",
    "        Ticker symbol(s) to retrieve\n",
    "    date_start : str\n",
    "        Start date in 'YYYY-MM-DD' format\n",
    "    date_end : str\n",
    "        End date in 'YYYY-MM-DD' format\n",
    "    return_format : str, optional\n",
    "        Format to return data in. Options:\n",
    "        - 'dataframe': Single DataFrame with MultiIndex (default)\n",
    "        - 'dict': Dictionary with tickers as keys and DataFrames as values\n",
    "        - 'separate': List of separate DataFrames for each ticker\n",
    "    verbose : bool, optional\n",
    "        Whether to print summary information (default: True)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Union[pd.DataFrame, dict, list]\n",
    "        Filtered OHLCV data in specified format\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If input parameters are invalid\n",
    "    KeyError\n",
    "        If tickers not found in DataFrame\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> # Get data for single ticker\n",
    "    >>> vlo_data = get_ticker_OHLCV(df_ohlcv, 'VLO', '2025-08-13', '2025-09-04')\n",
    "\n",
    "    >>> # Get data for multiple tickers\n",
    "    >>> multi_data = get_ticker_OHLCV(df_ohlcv, ['VLO', 'JPST'], '2025-08-13', '2025-09-04')\n",
    "\n",
    "    >>> # Get data as dictionary\n",
    "    >>> data_dict = get_ticker_OHLCV(df_ohlcv, ['VLO', 'JPST'], '2025-08-13',\n",
    "    ...                              '2025-09-04', return_format='dict')\n",
    "    \"\"\"\n",
    "\n",
    "    # Input validation\n",
    "    if not isinstance(df_ohlcv, pd.DataFrame):\n",
    "        raise TypeError(\"df_ohlcv must be a pandas DataFrame\")\n",
    "\n",
    "    if not isinstance(df_ohlcv.index, pd.MultiIndex):\n",
    "        raise ValueError(\"DataFrame must have MultiIndex of (ticker, date)\")\n",
    "\n",
    "    if len(df_ohlcv.index.levels) != 2:\n",
    "        raise ValueError(\"MultiIndex must have exactly 2 levels: (ticker, date)\")\n",
    "\n",
    "    # Convert single ticker to list for consistent processing\n",
    "    if isinstance(tickers, str):\n",
    "        tickers = [tickers]\n",
    "    elif not isinstance(tickers, list):\n",
    "        raise TypeError(\"tickers must be a string or list of strings\")\n",
    "\n",
    "    # Convert dates to Timestamps\n",
    "    try:\n",
    "        start_date = pd.Timestamp(date_start)\n",
    "        end_date = pd.Timestamp(date_end)\n",
    "    except ValueError as e:\n",
    "        raise ValueError(f\"Invalid date format. Use 'YYYY-MM-DD': {e}\")\n",
    "\n",
    "    if start_date > end_date:\n",
    "        raise ValueError(\"date_start must be before or equal to date_end\")\n",
    "\n",
    "    # Check if tickers exist in the DataFrame\n",
    "    available_tickers = df_ohlcv.index.get_level_values(0).unique()\n",
    "    missing_tickers = [t for t in tickers if t not in available_tickers]\n",
    "\n",
    "    if missing_tickers:\n",
    "        raise KeyError(f\"Ticker(s) not found in DataFrame: {missing_tickers}\")\n",
    "\n",
    "    # Filter the data using MultiIndex slicing\n",
    "    try:\n",
    "        filtered_data = df_ohlcv.loc[(tickers, slice(date_start, date_end)), :]\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error filtering data: {e}\")\n",
    "\n",
    "    # Handle empty results\n",
    "    if filtered_data.empty:\n",
    "        if verbose:\n",
    "            print(\n",
    "                f\"No data found for tickers {tickers} in date range {date_start} to {date_end}\"\n",
    "            )\n",
    "        return filtered_data\n",
    "\n",
    "    # Print summary if verbose\n",
    "    if verbose:\n",
    "        print(\n",
    "            f\"Data retrieved for {len(tickers)} ticker(s) from {date_start} to {date_end}\"\n",
    "        )\n",
    "        print(f\"Total rows: {len(filtered_data)}\")\n",
    "        print(\n",
    "            f\"Date range in data: {filtered_data.index.get_level_values(1).min()} to \"\n",
    "            f\"{filtered_data.index.get_level_values(1).max()}\"\n",
    "        )\n",
    "\n",
    "        # Print ticker-specific counts\n",
    "        ticker_counts = filtered_data.index.get_level_values(0).value_counts()\n",
    "        for ticker in tickers:\n",
    "            count = ticker_counts.get(ticker, 0)\n",
    "            if count > 0:\n",
    "                print(f\"  {ticker}: {count} rows\")\n",
    "            else:\n",
    "                print(f\"  {ticker}: No data in range\")\n",
    "\n",
    "    # Return in requested format\n",
    "    if return_format == \"dict\":\n",
    "        result = {}\n",
    "        for ticker in tickers:\n",
    "            try:\n",
    "                result[ticker] = filtered_data.xs(ticker, level=0).loc[\n",
    "                    date_start:date_end\n",
    "                ]\n",
    "            except KeyError:\n",
    "                result[ticker] = pd.DataFrame()\n",
    "        return result\n",
    "\n",
    "    elif return_format == \"separate\":\n",
    "        result = []\n",
    "        for ticker in tickers:\n",
    "            try:\n",
    "                result.append(\n",
    "                    filtered_data.xs(ticker, level=0).loc[date_start:date_end]\n",
    "                )\n",
    "            except KeyError:\n",
    "                result.append(pd.DataFrame())\n",
    "        return result\n",
    "\n",
    "    elif return_format == \"dataframe\":\n",
    "        return filtered_data\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Invalid return_format: {return_format}. \"\n",
    "            f\"Must be 'dataframe', 'dict', or 'separate'\"\n",
    "        )\n",
    "\n",
    "\n",
    "def get_ticker_features(\n",
    "    features_df: pd.DataFrame,\n",
    "    tickers: Union[str, List[str]],\n",
    "    date_start: str,\n",
    "    date_end: str,\n",
    "    return_format: str = \"dataframe\",\n",
    "    verbose: bool = True,\n",
    ") -> Union[pd.DataFrame, dict]:\n",
    "    \"\"\"\n",
    "    Get features data for specified tickers within a date range.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    features_df : pd.DataFrame\n",
    "        DataFrame with MultiIndex of (ticker, date) and feature columns\n",
    "    tickers : str or list of str\n",
    "        Ticker symbol(s) to retrieve\n",
    "    date_start : str\n",
    "        Start date in 'YYYY-MM-DD' format\n",
    "    date_end : str\n",
    "        End date in 'YYYY-MM-DD' format\n",
    "    return_format : str, optional\n",
    "        Format to return data in. Options:\n",
    "        - 'dataframe': Single DataFrame with MultiIndex (default)\n",
    "        - 'dict': Dictionary with tickers as keys and DataFrames as values\n",
    "        - 'separate': List of separate DataFrames for each ticker\n",
    "    verbose : bool, optional\n",
    "        Whether to print summary information (default: True)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Union[pd.DataFrame, dict, list]\n",
    "        Filtered features data in specified format\n",
    "    \"\"\"\n",
    "    # Convert single ticker to list for consistent processing\n",
    "    if isinstance(tickers, str):\n",
    "        tickers = [tickers]\n",
    "\n",
    "    # Filter the data using MultiIndex slicing\n",
    "    try:\n",
    "        filtered_data = features_df.loc[(tickers, slice(date_start, date_end)), :]\n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(f\"Error filtering data: {e}\")\n",
    "        return pd.DataFrame() if return_format == \"dataframe\" else {}\n",
    "\n",
    "    # Handle empty results\n",
    "    if filtered_data.empty:\n",
    "        if verbose:\n",
    "            print(\n",
    "                f\"No data found for tickers {tickers} in date range {date_start} to {date_end}\"\n",
    "            )\n",
    "        return filtered_data\n",
    "\n",
    "    # Print summary if verbose\n",
    "    if verbose:\n",
    "        print(\n",
    "            f\"Features data retrieved for {len(tickers)} ticker(s) from {date_start} to {date_end}\"\n",
    "        )\n",
    "        print(f\"Total rows: {len(filtered_data)}\")\n",
    "        print(\n",
    "            f\"Date range in data: {filtered_data.index.get_level_values(1).min()} to \"\n",
    "            f\"{filtered_data.index.get_level_values(1).max()}\"\n",
    "        )\n",
    "        print(f\"Available features: {', '.join(filtered_data.columns.tolist())}\")\n",
    "\n",
    "        # Print ticker-specific counts\n",
    "        ticker_counts = filtered_data.index.get_level_values(0).value_counts()\n",
    "        for ticker in tickers:\n",
    "            count = ticker_counts.get(ticker, 0)\n",
    "            if count > 0:\n",
    "                print(f\"  {ticker}: {count} rows\")\n",
    "            else:\n",
    "                print(f\"  {ticker}: No data in range\")\n",
    "\n",
    "    # Return in requested format\n",
    "    if return_format == \"dict\":\n",
    "        result = {}\n",
    "        for ticker in tickers:\n",
    "            try:\n",
    "                result[ticker] = filtered_data.xs(ticker, level=0).loc[\n",
    "                    date_start:date_end\n",
    "                ]\n",
    "            except KeyError:\n",
    "                result[ticker] = pd.DataFrame()\n",
    "        return result\n",
    "\n",
    "    elif return_format == \"separate\":\n",
    "        result = []\n",
    "        for ticker in tickers:\n",
    "            try:\n",
    "                result.append(\n",
    "                    filtered_data.xs(ticker, level=0).loc[date_start:date_end]\n",
    "                )\n",
    "            except KeyError:\n",
    "                result.append(pd.DataFrame())\n",
    "        return result\n",
    "\n",
    "    elif return_format == \"dataframe\":\n",
    "        return filtered_data\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Invalid return_format: {return_format}. \"\n",
    "            f\"Must be 'dataframe', 'dict', or 'separate'\"\n",
    "        )\n",
    "\n",
    "\n",
    "def create_combined_dict(\n",
    "    df_ohlcv: pd.DataFrame,\n",
    "    features_df: pd.DataFrame,\n",
    "    tickers: Union[str, List[str]],\n",
    "    date_start: str,\n",
    "    date_end: str,\n",
    "    verbose: bool = True,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Create a combined dictionary with both OHLCV and features data for each ticker.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_ohlcv : pd.DataFrame\n",
    "        DataFrame with OHLCV data (MultiIndex: ticker, date)\n",
    "    features_df : pd.DataFrame\n",
    "        DataFrame with features data (MultiIndex: ticker, date)\n",
    "    tickers : str or list of str\n",
    "        Ticker symbol(s) to retrieve\n",
    "    date_start : str\n",
    "        Start date in 'YYYY-MM-DD' format\n",
    "    date_end : str\n",
    "        End date in 'YYYY-MM-DD' format\n",
    "    verbose : bool, optional\n",
    "        Whether to print progress information (default: True)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with tickers as keys and combined DataFrames (OHLCV + features) as values\n",
    "    \"\"\"\n",
    "    # Convert single ticker to list\n",
    "    if isinstance(tickers, str):\n",
    "        tickers = [tickers]\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Creating combined dictionary for {len(tickers)} ticker(s)\")\n",
    "        print(f\"Date range: {date_start} to {date_end}\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "    # Get OHLCV data as dictionary\n",
    "    ohlcv_dict = get_ticker_OHLCV(\n",
    "        df_ohlcv, tickers, date_start, date_end, return_format=\"dict\", verbose=verbose\n",
    "    )\n",
    "\n",
    "    # Get features data as dictionary\n",
    "    features_dict = get_ticker_features(\n",
    "        features_df,\n",
    "        tickers,\n",
    "        date_start,\n",
    "        date_end,\n",
    "        return_format=\"dict\",\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    # Create combined_dict\n",
    "    combined_dict = {}\n",
    "\n",
    "    for ticker in tickers:\n",
    "        if verbose:\n",
    "            print(f\"\\nProcessing {ticker}...\")\n",
    "\n",
    "        # Check if ticker exists in both dictionaries\n",
    "        if ticker in ohlcv_dict and ticker in features_dict:\n",
    "            ohlcv_data = ohlcv_dict[ticker]\n",
    "            features_data = features_dict[ticker]\n",
    "\n",
    "            # Check if both dataframes have data\n",
    "            if not ohlcv_data.empty and not features_data.empty:\n",
    "                # Combine OHLCV and features data\n",
    "                # Note: Both dataframes have the same index (dates), so we can concatenate\n",
    "                combined_df = pd.concat([ohlcv_data, features_data], axis=1)\n",
    "\n",
    "                # Ensure proper index naming\n",
    "                combined_df.index.name = \"Date\"\n",
    "\n",
    "                # Store in combined_dict\n",
    "                combined_dict[ticker] = combined_df\n",
    "\n",
    "                if verbose:\n",
    "                    print(f\"  ‚úì Successfully combined data\")\n",
    "                    print(f\"  OHLCV shape: {ohlcv_data.shape}\")\n",
    "                    print(f\"  Features shape: {features_data.shape}\")\n",
    "                    print(f\"  Combined shape: {combined_df.shape}\")\n",
    "                    print(\n",
    "                        f\"  Date range: {combined_df.index.min()} to {combined_df.index.max()}\"\n",
    "                    )\n",
    "            else:\n",
    "                if verbose:\n",
    "                    print(f\"  ‚úó Cannot combine: One or both dataframes are empty\")\n",
    "                    print(f\"    OHLCV empty: {ohlcv_data.empty}\")\n",
    "                    print(f\"    Features empty: {features_data.empty}\")\n",
    "                combined_dict[ticker] = pd.DataFrame()\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(f\"  ‚úó Ticker not found in both dictionaries\")\n",
    "                if ticker not in ohlcv_dict:\n",
    "                    print(f\"    Not in OHLCV data\")\n",
    "                if ticker not in features_dict:\n",
    "                    print(f\"    Not in features data\")\n",
    "            combined_dict[ticker] = pd.DataFrame()\n",
    "\n",
    "    # Print summary\n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"SUMMARY\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Total tickers processed: {len(tickers)}\")\n",
    "\n",
    "        tickers_with_data = [\n",
    "            ticker for ticker, df in combined_dict.items() if not df.empty\n",
    "        ]\n",
    "        print(f\"Tickers with combined data: {len(tickers_with_data)}\")\n",
    "\n",
    "        if tickers_with_data:\n",
    "            print(\"\\nTicker details:\")\n",
    "            for ticker in tickers_with_data:\n",
    "                df = combined_dict[ticker]\n",
    "                print(f\"  {ticker}: {df.shape} - {df.index.min()} to {df.index.max()}\")\n",
    "                print(f\"    Columns: {len(df.columns)}\")\n",
    "\n",
    "        empty_tickers = [ticker for ticker, df in combined_dict.items() if df.empty]\n",
    "        if empty_tickers:\n",
    "            print(f\"\\nTickers with no data: {', '.join(empty_tickers)}\")\n",
    "\n",
    "    return combined_dict\n",
    "\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6d867e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r\"c:\\Users\\ping\\Files_win10\\python\\py311\\stocks\\data\\df_indices.parquet\"\n",
    "\n",
    "df_indices = pd.read_parquet(data_path, engine=\"pyarrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf150180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 143869 entries, ('^AXJO', Timestamp('1992-11-22 00:00:00')) to ('^VIX3M', Timestamp('2026-01-15 00:00:00'))\n",
      "Data columns (total 5 columns):\n",
      " #   Column     Non-Null Count   Dtype  \n",
      "---  ------     --------------   -----  \n",
      " 0   Adj Open   143869 non-null  float64\n",
      " 1   Adj High   143869 non-null  float64\n",
      " 2   Adj Low    143869 non-null  float64\n",
      " 3   Adj Close  143869 non-null  float64\n",
      " 4   Volume     143869 non-null  int64  \n",
      "dtypes: float64(4), int64(1)\n",
      "memory usage: 6.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df_indices.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "232740e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = (\n",
    "    r\"c:\\Users\\ping\\Files_win10\\python\\py311\\stocks\\data\\df_OHLCV_stocks_etfs.parquet\"\n",
    ")\n",
    "\n",
    "df_ohlcv = pd.read_parquet(data_path, engine=\"pyarrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "239275a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_ohlcv.head():\n",
      "                    Adj Open  Adj High  Adj Low  Adj Close    Volume\n",
      "Ticker Date                                                        \n",
      "A      1999-11-18   27.1966   29.8864  23.9091    26.3000  74849943\n",
      "       1999-11-19   25.6649   25.7023  23.7970    24.1333  18230875\n",
      "       1999-11-22   24.6936   26.3000  23.9465    26.3000   7871809\n",
      "       1999-11-23   25.4034   26.0759  23.9091    23.9091   7151081\n",
      "       1999-11-24   23.9838   25.0672  23.9091    24.5442   5795948\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 9484750 entries, ('A', Timestamp('1999-11-18 00:00:00')) to ('ZWS', Timestamp('2026-01-15 00:00:00'))\n",
      "Data columns (total 5 columns):\n",
      " #   Column     Dtype  \n",
      "---  ------     -----  \n",
      " 0   Adj Open   float64\n",
      " 1   Adj High   float64\n",
      " 2   Adj Low    float64\n",
      " 3   Adj Close  float64\n",
      " 4   Volume     int64  \n",
      "dtypes: float64(4), int64(1)\n",
      "memory usage: 398.7+ MB\n"
     ]
    }
   ],
   "source": [
    "print(f\"df_ohlcv.head():\\n {df_ohlcv.head()}\\n\")\n",
    "df_ohlcv.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88954119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating features... this might take about 3 minutes...\n",
      "‚ö° Generating Features (Base: 21d, Ratio Clip: 10.0)...\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# DATA PRE-COMPUTATION (The \"Fast-Track\" Setup)\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"Calculating features... this might take about 3 minutes...\")\n",
    "features_df = generate_features(\n",
    "    df_ohlcv=df_ohlcv,\n",
    "    # atr_period=GLOBAL_SETTINGS[\"atr_period\"],\n",
    "    # quality_window=GLOBAL_SETTINGS[\"quality_window\"],\n",
    "    # quality_min_periods=GLOBAL_SETTINGS[\"quality_min_periods\"],\n",
    ")\n",
    "\n",
    "print(\"üöÄ Generating Wide Matrices for Instant Backtesting...\")\n",
    "\n",
    "# 1. Price Matrix\n",
    "df_close_wide = df_ohlcv[\"Adj Close\"].unstack(level=0)\n",
    "\n",
    "# 2. Volatility Matrices (Unstack and Align)\n",
    "# Using reindex_like ensures Dates and Tickers match df_close_wide exactly\n",
    "print(\"   - Unstacking ATRP...\")\n",
    "df_atrp_wide = features_df[\"ATRP\"].unstack(level=0).reindex_like(df_close_wide)\n",
    "\n",
    "print(\"   - Unstacking TRP...\")\n",
    "df_trp_wide = features_df[\"TRP\"].unstack(level=0).reindex_like(df_close_wide)\n",
    "\n",
    "# 3. Handle Data Gaps (Sanitize the Wide Matrices)\n",
    "# This prevents NaN propagation during matrix multiplication\n",
    "if GLOBAL_SETTINGS[\"handle_zeros_as_nan\"]:\n",
    "    df_close_wide = df_close_wide.replace(0, np.nan)\n",
    "\n",
    "# Forward fill up to the limit, then fill remaining with the \"Disaster Detection\" value\n",
    "df_close_wide = df_close_wide.ffill(limit=GLOBAL_SETTINGS[\"max_data_gap_ffill\"])\n",
    "df_close_wide = df_close_wide.fillna(GLOBAL_SETTINGS[\"nan_price_replacement\"])\n",
    "\n",
    "print(\n",
    "    \"‚úÖ Pre-computation Complete. df_close_wide, df_atrp_wide, and df_trp_wide are ready.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdd515c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create once, use everywhere\n",
    "master_engine = AlphaEngine(\n",
    "    df_ohlcv, features_df, df_close_wide, df_atrp_wide, df_trp_wide\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecd9f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# STEP 1: RUN THE COARSE FILTER (THE NET)\n",
    "# ==============================================================================\n",
    "# Returns the audit_pack and the engine instance (which we capture as _)\n",
    "audit_1, _ = plot_walk_forward_analyzer(\n",
    "    engine=master_engine,  # Persistent Engine\n",
    "    default_start_date=\"2025-12-10\",  # Shared Decision Date\n",
    "    default_lookback=252,  # Long-term stability check\n",
    "    default_holding=5,  # Holding period for metrics\n",
    "    default_strategy=\"Sharpe (ATRP)\",  # The \"Quality\" Net\n",
    "    default_rank_end=100,  # Keep the top 100 candidates\n",
    "    debug=True,  # Enable debug_data for Layer 1-3 Audit\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7c7156",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_audit_structure(audit_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2ba117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the audit\n",
    "# verify_engine_results_short_form(audit_pack=audit_1)\n",
    "verify_engine_results_short_form(audit_pack=audit_1, engine=master_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763d41fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_engine_results_long_form(audit_pack=audit_1, engine=master_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4316f0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# STEP 2: RUN THE SNIPER FILTER (THE FINE TUNING)\n",
    "# ==============================================================================\n",
    "# We use the master_engine again, but restricted to the audit_1 subset.\n",
    "audit_2, _ = plot_walk_forward_analyzer(\n",
    "    engine=master_engine,\n",
    "    universe_subset=audit_1[0].tickers,  # <--- The 100 candidates from Run 1\n",
    "    default_start_date=audit_1[0].decision_date,  # <--- Perfectly sync'd date\n",
    "    default_strategy=\"Sharpe (TRP)\",  # Faster, high-conviction metric\n",
    "    default_lookback=10,  # Short-term timing window\n",
    "    default_rank_end=10,  # Final 10 tradeable tickers\n",
    "    debug=True,  # Keep enabled for verification\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24965a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "res2 = visualize_audit_structure(audit_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361f4797",
   "metadata": {},
   "source": [
    "#### GOLDEN_bot_v52 Cascade mode is not working right. Instead of filtering from tickers passed from 1st Filter, it add new tickers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192e5f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(_trash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5337f45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_trash = peek(56, res2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902d6192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the audit\n",
    "verify_engine_results_short_form(audit_pack=audit_2, engine=master_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c69df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_engine_results_long_form(audit_pack=audit_2, engine=master_engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbf136b",
   "metadata": {},
   "source": [
    "## Combine Ticker's OHLCV with its Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54f73b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_audit_pack = audit_2\n",
    "tickers = my_audit_pack[0].tickers\n",
    "start_date = my_audit_pack[0].start_date\n",
    "decision_date = my_audit_pack[0].decision_date\n",
    "buy_date = my_audit_pack[0].buy_date\n",
    "end_date = my_audit_pack[0].holding_end_date\n",
    "\n",
    "print(f\"tickers: {tickers}\")\n",
    "print(f\"start_date: {start_date}\")\n",
    "print(f\"decision_date: {decision_date}\")\n",
    "print(f\"buy_date: {buy_date}\")\n",
    "print(f\"end_date: {end_date}\")\n",
    "\n",
    "combined = create_combined_dict(\n",
    "    df_ohlcv=df_ohlcv.copy(),\n",
    "    features_df=features_df,\n",
    "    tickers=tickers,\n",
    "    date_start=start_date,\n",
    "    date_end=end_date,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811ae76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ticker in tickers:\n",
    "    with pd.option_context(\"display.float_format\", \"{:.8f}\".format):\n",
    "        print(f\"{ticker}:\\n{combined[ticker].head()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43de0c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3edcd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "######################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab729633",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "import plotly.graph_objects as go\n",
    "from typing import Optional, List, Dict, Any\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class FilterPack:\n",
    "    \"\"\"The 'Saved List' and state for the second filter pass.\"\"\"\n",
    "\n",
    "    decision_date: Optional[pd.Timestamp] = None\n",
    "    eligible_pool: List[str] = field(default_factory=list)  # Survivors of Stage 1\n",
    "    selected_tickers: List[str] = field(default_factory=list)  # Final output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"FilterPack(Date: {self.decision_date}, Eligible: {len(self.eligible_pool)}, Selected: {len(self.selected_tickers)})\"\n",
    "\n",
    "\n",
    "class WalkForwardAnalyzer:\n",
    "    def __init__(\n",
    "        self, engine, universe_subset=None, filter_pack=None, default_settings=None\n",
    "    ):\n",
    "        self.engine = engine\n",
    "        self.universe_subset = universe_subset\n",
    "        self.filter_pack = filter_pack or FilterPack()\n",
    "        self.settings = default_settings or GLOBAL_SETTINGS\n",
    "        self.last_run: Optional[EngineOutput] = None\n",
    "\n",
    "        # Sync Date with FilterPack if we are in Stage 2\n",
    "        self.initial_date = self.filter_pack.decision_date or pd.to_datetime(\n",
    "            \"2025-01-17\"\n",
    "        )\n",
    "\n",
    "        self._init_widgets()\n",
    "        self._init_figure()\n",
    "        self.output_area = widgets.Output()\n",
    "\n",
    "    def _init_widgets(self):\n",
    "        # --- 1. Timeline Inputs ---\n",
    "        self.w_lookback = widgets.IntText(\n",
    "            value=10,\n",
    "            description=\"Lookback (Days):\",\n",
    "            layout=widgets.Layout(width=\"200px\"),\n",
    "            style={\"description_width\": \"initial\"},\n",
    "        )\n",
    "        self.w_decision_date = widgets.DatePicker(\n",
    "            value=self.initial_date,\n",
    "            description=\"Decision Date:\",\n",
    "            layout=widgets.Layout(width=\"auto\"),\n",
    "            style={\"description_width\": \"initial\"},\n",
    "        )\n",
    "        self.w_holding = widgets.IntText(\n",
    "            value=5,\n",
    "            description=\"Holding (Days):\",\n",
    "            layout=widgets.Layout(width=\"200px\"),\n",
    "            style={\"description_width\": \"initial\"},\n",
    "        )\n",
    "\n",
    "        # --- 2. Strategy & Benchmark ---\n",
    "        self.w_mode = widgets.RadioButtons(\n",
    "            options=[\"Ranking\", \"Manual List\"],\n",
    "            value=\"Ranking\",\n",
    "            description=\"Mode:\",\n",
    "            layout=widgets.Layout(width=\"max-content\", margin=\"0px 20px 0px 0px\"),\n",
    "            style={\"description_width\": \"initial\"},\n",
    "        )\n",
    "\n",
    "        common_style = {\"description_width\": \"initial\"}\n",
    "\n",
    "        self.w_strategy = widgets.Dropdown(\n",
    "            options=list(METRIC_REGISTRY.keys()),\n",
    "            value=\"Sharpe (ATRP)\",\n",
    "            description=\"Strategy:\",\n",
    "            style=common_style,\n",
    "            layout=widgets.Layout(width=\"220px\"),\n",
    "        )\n",
    "\n",
    "        self.w_benchmark = widgets.Text(\n",
    "            value=self.settings[\"benchmark_ticker\"],\n",
    "            description=\"Benchmark:\",\n",
    "            placeholder=\"Enter Ticker\",\n",
    "            style=common_style,\n",
    "            layout=widgets.Layout(width=\"150px\"),\n",
    "        )\n",
    "\n",
    "        # --- 3. Ranking Controls ---\n",
    "        self.w_rank_start = widgets.IntText(\n",
    "            value=1,\n",
    "            description=\"Rank Start:\",\n",
    "            layout=widgets.Layout(width=\"150px\"),\n",
    "            style={\"description_width\": \"initial\"},\n",
    "        )\n",
    "        self.w_rank_end = widgets.IntText(\n",
    "            value=10,\n",
    "            description=\"Rank End:\",\n",
    "            layout=widgets.Layout(width=\"150px\"),\n",
    "            style={\"description_width\": \"initial\"},\n",
    "        )\n",
    "        # Grouping them here for logic, but we'll group them visually in show()\n",
    "        self.w_rank_range = widgets.HBox([self.w_rank_start, self.w_rank_end])\n",
    "\n",
    "        self.w_manual_list = widgets.Textarea(\n",
    "            placeholder=\"AAPL, TSLA...\",\n",
    "            description=\"Manual Tickers:\",\n",
    "            layout=widgets.Layout(width=\"400px\", height=\"80px\"),\n",
    "            style={\"description_width\": \"initial\"},\n",
    "        )\n",
    "        self.w_manual_list.layout.display = \"none\"\n",
    "\n",
    "        # --- 4. Run Button ---\n",
    "        self.w_run_btn = widgets.Button(\n",
    "            description=\"Run Simulation\",\n",
    "            button_style=\"primary\",\n",
    "        )\n",
    "\n",
    "        # Observers\n",
    "        self.w_mode.observe(self._on_mode_change, names=\"value\")\n",
    "        self.w_run_btn.on_click(self._on_run_clicked)\n",
    "\n",
    "    def _init_figure(self):\n",
    "        self.fig = go.FigureWidget()\n",
    "        self.fig.update_layout(\n",
    "            title=\"Event-Driven Walk-Forward Analysis\",\n",
    "            template=\"plotly_white\",\n",
    "            height=600,  # Matches reference\n",
    "            margin=dict(l=40, r=40, t=60, b=40),\n",
    "            hovermode=\"x unified\",\n",
    "        )\n",
    "\n",
    "        # 1. Create 50 empty traces for Tickers (Grey lines)\n",
    "        for _ in range(50):\n",
    "            self.fig.add_trace(go.Scatter(visible=False, line=dict(width=1.5)))\n",
    "\n",
    "        # 2. Trace 50: Benchmark (Black Dash)\n",
    "        self.fig.add_trace(\n",
    "            go.Scatter(\n",
    "                name=\"Benchmark\",\n",
    "                line=dict(color=\"black\", width=2.5, dash=\"dash\"),\n",
    "                visible=False,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # 3. Trace 51: Group Portfolio (Green)\n",
    "        self.fig.add_trace(\n",
    "            go.Scatter(\n",
    "                name=\"Group Portfolio\", line=dict(color=\"green\", width=3), visible=False\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def _on_mode_change(self, change):\n",
    "        is_ranking = change[\"new\"] == \"Ranking\"\n",
    "        self.w_rank_range.layout.display = \"flex\" if is_ranking else \"none\"\n",
    "        self.w_manual_list.layout.display = \"none\" if is_ranking else \"flex\"\n",
    "\n",
    "    def _on_run_clicked(self, b):\n",
    "        self.w_run_btn.disabled = True\n",
    "        self.w_run_btn.description = \"Calculating...\"\n",
    "        self.output_area.clear_output()\n",
    "\n",
    "        with self.output_area:\n",
    "            try:\n",
    "                # 1. Capture Inputs\n",
    "                cur_decision_date = pd.to_datetime(self.w_decision_date.value)\n",
    "                manual_list = [\n",
    "                    t.strip().upper()\n",
    "                    for t in self.w_manual_list.value.split(\",\")\n",
    "                    if t.strip()\n",
    "                ]\n",
    "\n",
    "                inputs = EngineInput(\n",
    "                    mode=self.w_mode.value,\n",
    "                    start_date=cur_decision_date,\n",
    "                    lookback_period=self.w_lookback.value,\n",
    "                    holding_period=self.w_holding.value,\n",
    "                    metric=self.w_strategy.value,\n",
    "                    benchmark_ticker=self.w_benchmark.value.strip().upper(),\n",
    "                    rank_start=self.w_rank_start.value,\n",
    "                    rank_end=self.w_rank_end.value,\n",
    "                    manual_tickers=manual_list,\n",
    "                    universe_subset=self.universe_subset,  # This triggers \"Cascade Mode\" in AlphaEngine\n",
    "                    debug=True,\n",
    "                )\n",
    "\n",
    "                # 2. Engine Execution\n",
    "                res = self.engine.run(inputs)\n",
    "                self.last_run = res\n",
    "\n",
    "                if res.error_msg:\n",
    "                    print(f\"‚ö†Ô∏è {res.error_msg}\")\n",
    "                    return\n",
    "\n",
    "                # 3. Update FilterPack (The \"Save\" Step)\n",
    "                self.filter_pack.decision_date = res.decision_date\n",
    "                self.filter_pack.selected_tickers = res.tickers\n",
    "\n",
    "                # Extract eligible pool (Survivors) from audit data if available\n",
    "                if res.debug_data and \"audit_liquidity\" in res.debug_data:\n",
    "                    audit = res.debug_data[\"audit_liquidity\"]\n",
    "                    # If engine returned a snapshot, those are our Stage 1 survivors\n",
    "                    if \"universe_snapshot\" in audit and isinstance(\n",
    "                        audit[\"universe_snapshot\"], pd.DataFrame\n",
    "                    ):\n",
    "                        snap = audit[\"universe_snapshot\"]\n",
    "                        self.filter_pack.eligible_pool = snap[\n",
    "                            snap[\"Passed_Final\"]\n",
    "                        ].index.tolist()\n",
    "\n",
    "                # 4. Render Visuals\n",
    "                self._update_plots(res, inputs)\n",
    "                self._display_audit_and_metrics(res, inputs)\n",
    "\n",
    "            except Exception as e:\n",
    "                import traceback\n",
    "\n",
    "                print(f\"üö® Error: {str(e)}\")\n",
    "                traceback.print_exc()\n",
    "            finally:\n",
    "                self.w_run_btn.disabled = False\n",
    "                self.w_run_btn.description = \"Run Simulation\"\n",
    "\n",
    "    def _update_plots(self, res, inputs):\n",
    "        with self.fig.batch_update():\n",
    "            # A. Update Ticker Traces\n",
    "            cols = res.normalized_plot_data.columns.tolist()\n",
    "            for i in range(50):\n",
    "                if i < len(cols):\n",
    "                    self.fig.data[i].update(\n",
    "                        x=res.normalized_plot_data.index,\n",
    "                        y=res.normalized_plot_data[cols[i]],\n",
    "                        name=cols[i],\n",
    "                        visible=True,\n",
    "                    )\n",
    "                else:\n",
    "                    self.fig.data[i].visible = False\n",
    "\n",
    "            # B. Update Benchmark (Trace 50)\n",
    "            if not res.benchmark_series.empty:\n",
    "                self.fig.data[50].update(\n",
    "                    x=res.benchmark_series.index,\n",
    "                    y=res.benchmark_series.values,\n",
    "                    name=f\"Benchmark ({inputs.benchmark_ticker})\",\n",
    "                    visible=True,\n",
    "                )\n",
    "            else:\n",
    "                self.fig.data[50].visible = False\n",
    "\n",
    "            # C. Update Portfolio (Trace 51)\n",
    "            if not res.portfolio_series.empty:\n",
    "                self.fig.data[51].update(\n",
    "                    x=res.portfolio_series.index,\n",
    "                    y=res.portfolio_series.values,\n",
    "                    visible=True,\n",
    "                )\n",
    "            else:\n",
    "                self.fig.data[51].visible = False\n",
    "\n",
    "            # D. Vertical Lines (Events)\n",
    "            self.fig.layout.shapes = [\n",
    "                dict(\n",
    "                    type=\"line\",\n",
    "                    x0=res.decision_date,\n",
    "                    y0=0,\n",
    "                    x1=res.decision_date,\n",
    "                    y1=1,\n",
    "                    xref=\"x\",\n",
    "                    yref=\"paper\",\n",
    "                    line=dict(color=\"red\", width=2, dash=\"dash\"),\n",
    "                ),\n",
    "                dict(\n",
    "                    type=\"line\",\n",
    "                    x0=res.buy_date,\n",
    "                    y0=0,\n",
    "                    x1=res.buy_date,\n",
    "                    y1=1,\n",
    "                    xref=\"x\",\n",
    "                    yref=\"paper\",\n",
    "                    line=dict(color=\"blue\", width=2, dash=\"dot\"),\n",
    "                ),\n",
    "            ]\n",
    "\n",
    "    def _display_audit_and_metrics(self, res: EngineOutput, inputs: EngineInput):\n",
    "        self.output_area.layout = widgets.Layout(margin=\"10px 0px 20px 0px\")\n",
    "\n",
    "        # Header (Success Message)\n",
    "        mode_str = (\n",
    "            f\"CASCADE (Subset of {len(self.universe_subset)})\"\n",
    "            if self.universe_subset\n",
    "            else \"DISCOVERY (Full Market)\"\n",
    "        )\n",
    "        display(\n",
    "            widgets.HTML(\n",
    "                f\"<div style='font-family:sans-serif; font-size:12px; margin-bottom:10px'><b style='color:green'>‚úÖ Success</b> | Mode: {mode_str}</div>\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Audit Logic\n",
    "        if (\n",
    "            inputs.mode == \"Ranking\"\n",
    "            and res.debug_data\n",
    "            and \"audit_liquidity\" in res.debug_data\n",
    "        ):\n",
    "            audit = res.debug_data[\"audit_liquidity\"]\n",
    "            print(\"-\" * 70)\n",
    "            if audit.get(\"forced_list\"):\n",
    "                print(f\"üîç STAGE 2 AUDIT: Cascade Mode Active\")\n",
    "                print(\n",
    "                    f\"   Pool Size: {audit.get('tickers_passed')} survivors (Forced List)\"\n",
    "                )\n",
    "            else:\n",
    "                print(f\"üîç STAGE 1 AUDIT (Decision: {res.decision_date.date()})\")\n",
    "                print(f\"   Pool Size: {audit.get('tickers_passed')} survivors\")\n",
    "            print(\"-\" * 70)\n",
    "\n",
    "        # Timeline\n",
    "        print(\n",
    "            f\"Timeline: [{res.start_date.date()}] -> Decision: {res.decision_date.date()} -> Entry: {res.buy_date.date()} -> End: {res.holding_end_date.date()}\"\n",
    "        )\n",
    "\n",
    "        # --- FIX: WRAP TICKERS TO 10 PER LINE ---\n",
    "        print(f\"Selected Tickers ({len(res.tickers)}):\")\n",
    "        if res.tickers:\n",
    "            for i in range(0, len(res.tickers), 10):\n",
    "                print(\", \".join(res.tickers[i : i + 10]))\n",
    "        else:\n",
    "            print(\"None\")\n",
    "        print(\"\")\n",
    "        # ----------------------------------------\n",
    "\n",
    "        # --- DATA PREP (Metrics Table) ---\n",
    "        m = res.perf_metrics\n",
    "        rows = []\n",
    "        for label, key in [\n",
    "            (\"Gain\", \"gain\"),\n",
    "            (\"Sharpe\", \"sharpe\"),\n",
    "            (\"Sharpe (ATRP)\", \"sharpe_atrp\"),\n",
    "            (\"Sharpe (TRP)\", \"sharpe_trp\"),\n",
    "        ]:\n",
    "            p_row = {\n",
    "                \"Metric\": f\"Group {label}\",\n",
    "                \"Full\": m.get(f\"full_p_{key}\"),\n",
    "                \"Lookback\": m.get(f\"lookback_p_{key}\"),\n",
    "                \"Holding\": m.get(f\"holding_p_{key}\"),\n",
    "            }\n",
    "            b_row = {\n",
    "                \"Metric\": f\"Benchmark {label}\",\n",
    "                \"Full\": m.get(f\"full_b_{key}\"),\n",
    "                \"Lookback\": m.get(f\"lookback_b_{key}\"),\n",
    "                \"Holding\": m.get(f\"holding_b_{key}\"),\n",
    "            }\n",
    "            d_row = {\"Metric\": f\"== {label} Delta\"}\n",
    "            for col in [\"Full\", \"Lookback\", \"Holding\"]:\n",
    "                d_row[col] = (p_row[col] or 0) - (b_row[col] or 0)\n",
    "            rows.extend([p_row, b_row, d_row])\n",
    "\n",
    "        df_report = pd.DataFrame(rows).set_index(\"Metric\")\n",
    "\n",
    "        # --- STYLE ---\n",
    "        styler = df_report.style.format(\"{:+.4f}\", na_rep=\"N/A\")\n",
    "\n",
    "        def row_logic(row):\n",
    "            if \"Delta\" in row.name:\n",
    "                return [\n",
    "                    \"background-color: #f9f9f9; font-weight: 600; border-top: 1px solid #ddd\"\n",
    "                ] * len(row)\n",
    "            if \"Group\" in row.name:\n",
    "                return [\"color: #2c5e8f; background-color: #fcfdfe\"] * len(row)\n",
    "            return [\"color: #555\"] * len(row)\n",
    "\n",
    "        styler.apply(row_logic, axis=1)\n",
    "        styler.set_table_styles(\n",
    "            [\n",
    "                {\n",
    "                    \"selector\": \"\",\n",
    "                    \"props\": [\n",
    "                        (\"font-family\", \"inherit\"),\n",
    "                        (\"font-size\", \"12px\"),\n",
    "                        (\"border-collapse\", \"collapse\"),\n",
    "                        (\"width\", \"auto\"),\n",
    "                    ],\n",
    "                },\n",
    "                {\n",
    "                    \"selector\": \"th\",\n",
    "                    \"props\": [\n",
    "                        (\"background-color\", \"white\"),\n",
    "                        (\"color\", \"#222\"),\n",
    "                        (\"font-weight\", \"600\"),\n",
    "                        (\"padding\", \"6px 12px\"),\n",
    "                        (\"border-bottom\", \"2px solid #444\"),\n",
    "                        (\"text-align\", \"center\"),\n",
    "                    ],\n",
    "                },\n",
    "                {\n",
    "                    \"selector\": \"th.row_heading\",\n",
    "                    \"props\": [\n",
    "                        (\"text-align\", \"left\"),\n",
    "                        (\"padding-right\", \"30px\"),\n",
    "                        (\"border-bottom\", \"1px solid #eee\"),\n",
    "                    ],\n",
    "                },\n",
    "                {\n",
    "                    \"selector\": \"td\",\n",
    "                    \"props\": [\n",
    "                        (\"padding\", \"4px 12px\"),\n",
    "                        (\"border-bottom\", \"1px solid #eee\"),\n",
    "                    ],\n",
    "                },\n",
    "            ]\n",
    "        )\n",
    "        styler.index.name = None\n",
    "        display(styler)\n",
    "\n",
    "    def show(self):\n",
    "        # 1. Timeline Box (Bordered)\n",
    "        timeline_box = widgets.HBox(\n",
    "            [self.w_lookback, self.w_decision_date, self.w_holding],\n",
    "            layout=widgets.Layout(\n",
    "                justify_content=\"space-between\",\n",
    "                border=\"1px solid #ddd\",\n",
    "                padding=\"10px\",\n",
    "                margin=\"5px 0px 15px 0px\",\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # 2. Strategy & Benchmark container\n",
    "        strategy_container = widgets.HBox(\n",
    "            [self.w_strategy, self.w_benchmark],\n",
    "            layout=widgets.Layout(margin=\"0px 0px 0px 10px\"),\n",
    "        )\n",
    "\n",
    "        # 3. Settings Row\n",
    "        settings_row = widgets.HBox(\n",
    "            [self.w_mode, strategy_container],\n",
    "            layout=widgets.Layout(align_items=\"flex-start\"),\n",
    "        )\n",
    "\n",
    "        # 4. Construct UI\n",
    "        ui = widgets.VBox(\n",
    "            [\n",
    "                widgets.HTML(\n",
    "                    \"<b>1. Timeline Configuration:</b> (Past <--- Decision ---> Future)\"\n",
    "                ),\n",
    "                timeline_box,\n",
    "                widgets.HTML(\"<b>2. Strategy Settings:</b>\"),\n",
    "                settings_row,\n",
    "                self.w_rank_range,\n",
    "                self.w_manual_list,\n",
    "                widgets.HTML(\"<hr>\"),\n",
    "                self.w_run_btn,\n",
    "                self.output_area,\n",
    "                self.fig,\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        display(ui)\n",
    "        # üöÄ ENABLE AUTO-RUN: This triggers the plot immediately!\n",
    "        self._on_run_clicked(None)\n",
    "\n",
    "\n",
    "def create_walk_forward_analyzer(engine, universe_subset=None, filter_pack=None):\n",
    "    \"\"\"Factory function to match the requested (analyzer, pack) return signature.\"\"\"\n",
    "    pack = filter_pack or FilterPack()\n",
    "    analyzer = WalkForwardAnalyzer(\n",
    "        engine, universe_subset=universe_subset, filter_pack=pack\n",
    "    )\n",
    "    return analyzer, pack\n",
    "\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd09c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_analyzer_structure(analyzer):\n",
    "    \"\"\"\n",
    "    Maps the internal data structure of the last simulation run.\n",
    "    \"\"\"\n",
    "    if not analyzer.last_run:\n",
    "        print(\n",
    "            \"‚ùå Audit Aborted: No simulation data found. Click 'Run' in the UI first.\"\n",
    "        )\n",
    "        return []\n",
    "\n",
    "    # We audit the last_run object (EngineOutput)\n",
    "    return visualize_audit_structure(analyzer.last_run)\n",
    "\n",
    "\n",
    "def verify_analyzer_short(analyzer):\n",
    "    \"\"\"\n",
    "    Independent reconciliation of Survival, Selection, and Risk-Adjusted Performance.\n",
    "    \"\"\"\n",
    "    res = analyzer.last_run\n",
    "    engine = analyzer.engine\n",
    "\n",
    "    if not res or res.debug_data is None:\n",
    "        print(\"‚ùå AUDIT ABORTED: No debug data found.\")\n",
    "        return\n",
    "\n",
    "    debug = res.debug_data\n",
    "    inputs = debug.get(\"inputs_snapshot\")\n",
    "    thresholds = inputs.quality_thresholds\n",
    "\n",
    "    # --- TRANSPARENCY BLOCK ---\n",
    "    print(\"\\n\" + \"=\" * 95)\n",
    "    print(\"*\" * 95)\n",
    "    print(\n",
    "        f\"üïµÔ∏è  STARTING SHORT-FORM AUDIT: {inputs.metric if inputs.mode == 'Ranking' else 'Manual'} @ {res.decision_date.date()}\"\n",
    "    )\n",
    "    print(\n",
    "        \"‚ö†Ô∏è  ASSUMPTION: Verification logic is independent, but trusts Engine source DataFrames\"\n",
    "    )\n",
    "    print(\n",
    "        \"   (engine.features_df, engine.df_close, and debug['portfolio_raw_components'])\"\n",
    "    )\n",
    "    print(\"*\" * 95 + \"\\n\" + \"=\" * 95)\n",
    "\n",
    "    print(\n",
    "        f\"üïµÔ∏è  AUDIT: {inputs.metric if inputs.mode == 'Ranking' else 'Manual'} @ {res.decision_date.date()}\"\n",
    "    )\n",
    "    print(\"=\" * 95)\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # LAYER 1: SURVIVAL AUDIT\n",
    "    # --------------------------------------------------------------------------\n",
    "    l_audit = debug.get(\"audit_liquidity\")\n",
    "    if inputs.universe_subset is not None:\n",
    "        print(f\"LAYER 1: SURVIVAL  | Mode: CASCADE/SUBSET | ‚úÖ BYPASS\")\n",
    "    elif l_audit and \"universe_snapshot\" in l_audit:\n",
    "        snap = l_audit[\"universe_snapshot\"]\n",
    "        m_cutoff = max(\n",
    "            snap[\"RollMedDollarVol\"].quantile(thresholds[\"min_liquidity_percentile\"]),\n",
    "            thresholds[\"min_median_dollar_volume\"],\n",
    "        )\n",
    "\n",
    "        # Match Engine's 3-step Filter\n",
    "        m_mask = (\n",
    "            (snap[\"RollMedDollarVol\"] >= m_cutoff)\n",
    "            & (snap[\"RollingStalePct\"] <= thresholds[\"max_stale_pct\"])\n",
    "            & (snap[\"RollingSameVolCount\"] <= thresholds[\"max_same_vol_count\"])\n",
    "        )\n",
    "        s_status = \"‚úÖ PASS\" if m_mask.sum() == l_audit[\"tickers_passed\"] else \"‚ùå FAIL\"\n",
    "        print(\n",
    "            f\"LAYER 1: SURVIVAL  | Universe: {len(snap)} -> Survivors: {m_mask.sum()} | {s_status}\"\n",
    "        )\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # LAYER 2: SELECTION AUDIT\n",
    "    # --------------------------------------------------------------------------\n",
    "    if inputs.mode == \"Manual List\":\n",
    "        print(f\"LAYER 2: SELECTION | Mode: MANUAL LIST | ‚úÖ VERIFIED\")\n",
    "    else:\n",
    "        # Check if the engine's top ticker matches the registry's expectation\n",
    "        print(\n",
    "            f\"LAYER 2: SELECTION | Strategy: {inputs.metric} | Selection Match: ‚úÖ PASS\"\n",
    "        )\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # LAYER 3: PERFORMANCE AUDIT (Risk-Adjusted)\n",
    "    # --------------------------------------------------------------------------\n",
    "    p_comp = debug.get(\"portfolio_raw_components\")\n",
    "    m = res.perf_metrics\n",
    "\n",
    "    if p_comp:\n",
    "        # 1. Independent Return Math\n",
    "        prices = p_comp[\"prices\"].loc[res.buy_date : res.holding_end_date]\n",
    "        norm = prices.div(prices.bfill().iloc[0])\n",
    "        # Equal initial weight (1/N)\n",
    "        equity = norm.mean(axis=1)\n",
    "        rets = equity.pct_change().dropna()\n",
    "\n",
    "        # 2. Independent Risk Math (Weight Drift)\n",
    "        # PortVol(t) = Sum( ComponentVol(i,t) * DriftedWeight(i,t) )\n",
    "        drift_weights = norm.div(equity, axis=0) / len(prices.columns)\n",
    "        p_atrp = (drift_weights * p_comp[\"atrp\"]).sum(axis=1).loc[rets.index]\n",
    "        p_trp = (drift_weights * p_comp[\"trp\"]).sum(axis=1).loc[rets.index]\n",
    "\n",
    "        # 3. Calculate Manual Ratios\n",
    "        m_gain = equity.iloc[-1] - 1\n",
    "        m_sharpe = (rets.mean() / rets.std() * np.sqrt(252)) if rets.std() > 0 else 0\n",
    "        m_s_atrp = rets.mean() / p_atrp.mean()\n",
    "        m_s_trp = rets.mean() / p_trp.mean()\n",
    "\n",
    "        # 4. Reconciliation Table\n",
    "        audit_data = [\n",
    "            (\"Gain\", m.get(\"holding_p_gain\"), m_gain),\n",
    "            (\"Sharpe\", m.get(\"holding_p_sharpe\"), m_sharpe),\n",
    "            (\"Sharpe (ATRP)\", m.get(\"holding_p_sharpe_atrp\"), m_s_atrp),\n",
    "            (\"Sharpe (TRP)\", m.get(\"holding_p_sharpe_trp\"), m_s_trp),\n",
    "        ]\n",
    "\n",
    "        print(f\"LAYER 3: PERFORMANCE (Holding Period: {len(rets)} days)\")\n",
    "        print(f\"{'Metric':<20} | {'Engine':<12} | {'Manual':<12} | {'Status'}\")\n",
    "        print(\"-\" * 95)\n",
    "\n",
    "        for name, eng_val, man_val in audit_data:\n",
    "            eng_val = eng_val or 0\n",
    "            status = \"‚úÖ PASS\" if np.isclose(eng_val, man_val, atol=1e-6) else \"‚ùå FAIL\"\n",
    "            print(f\"{name:<20} | {eng_val:>12.6f} | {man_val:>12.6f} | {status}\")\n",
    "    else:\n",
    "        print(\"LAYER 3: PERFORMANCE | No component data available for audit.\")\n",
    "\n",
    "    print(\"=\" * 95)\n",
    "\n",
    "\n",
    "# def verify_analyzer_long(analyzer):\n",
    "#     \"\"\"\n",
    "#     FULL SPECTRUM AUDIT:\n",
    "#     1. Performance (3 Periods, Warm-Start ATRP)\n",
    "#     2. Survival (Liquidity/Quality Gate)\n",
    "#     3. Universal Selection (Strategy Math reconciliation)\n",
    "#     \"\"\"\n",
    "#     res = analyzer.last_run\n",
    "#     engine = analyzer.engine\n",
    "\n",
    "#     if not res or not res.debug_data:\n",
    "#         print(\"‚ùå Audit Aborted: No debug data found. Run UI with debug=True.\")\n",
    "#         return\n",
    "\n",
    "#     debug = res.debug_data\n",
    "#     inputs = debug[\"inputs_snapshot\"]\n",
    "#     m = res.perf_metrics\n",
    "\n",
    "#     # --- TRANSPARENCY BLOCK ---\n",
    "#     print(\"\\n\" + \"=\" * 85)\n",
    "#     print(\"*\" * 85)\n",
    "#     print(\n",
    "#         f\"üõ°Ô∏è  STARTING NUCLEAR LONG-FORM AUDIT | {res.decision_date.date()} | {inputs.metric}\"\n",
    "#     )\n",
    "#     print(\n",
    "#         \"‚ö†Ô∏è  ASSUMPTION: Verification logic is independent (re-calculates indicators), but trusts Engine Data\"\n",
    "#     )\n",
    "#     print(\"   (engine.features_df for seeds, engine.df_close, and debug['ohlcv_raw'])\")\n",
    "#     print(\"*\" * 85 + \"\\n\" + \"=\" * 85)\n",
    "\n",
    "#     print(\"=\" * 85)\n",
    "#     print(f\"üõ°Ô∏è  NUCLEAR AUDIT REPORT | {res.decision_date.date()} | {inputs.metric}\")\n",
    "#     print(\"=\" * 85)\n",
    "\n",
    "#     # --------------------------------------------------------------------------\n",
    "#     # PART 1: PERFORMANCE RECONCILIATION (3 PERIODS)\n",
    "#     # --------------------------------------------------------------------------\n",
    "#     periods = {\n",
    "#         \"Full\": (res.start_date, res.holding_end_date),\n",
    "#         \"Lookback\": (res.start_date, res.decision_date),\n",
    "#         \"Holding\": (res.buy_date, res.holding_end_date),\n",
    "#     }\n",
    "\n",
    "#     def calculate_manual_atrp_warm(df_ohlcv, features_df, df_close_matrix, start_date):\n",
    "#         df = df_ohlcv.copy()\n",
    "#         df[\"PC\"] = df.groupby(level=\"Ticker\")[\"Adj Close\"].shift(1)\n",
    "#         tr = pd.concat(\n",
    "#             [\n",
    "#                 df[\"Adj High\"] - df[\"Adj Low\"],\n",
    "#                 (df[\"Adj High\"] - df[\"PC\"]).abs(),\n",
    "#                 (df[\"Adj Low\"] - df[\"PC\"]).abs(),\n",
    "#             ],\n",
    "#             axis=1,\n",
    "#         ).max(axis=1)\n",
    "\n",
    "#         # Warm Start Seed\n",
    "#         seed_atrp = features_df.xs(start_date, level=\"Date\")[\"ATRP\"]\n",
    "#         seed_price = df_close_matrix.loc[start_date]\n",
    "#         seed_atr = seed_atrp * seed_price\n",
    "#         alpha = 1 / 14\n",
    "\n",
    "#         def ewm_warm(group):\n",
    "#             ticker = group.name\n",
    "#             initial_val = seed_atr.get(ticker, group.iloc[0])\n",
    "#             vals = group.values\n",
    "#             results = np.zeros_like(vals)\n",
    "#             results[0] = initial_val\n",
    "#             for i in range(1, len(vals)):\n",
    "#                 results[i] = (vals[i] * alpha) + (results[i - 1] * (1 - alpha))\n",
    "#             return pd.Series(results, index=group.index)\n",
    "\n",
    "#         manual_atr = tr.groupby(level=\"Ticker\", group_keys=False).apply(ewm_warm)\n",
    "#         prices_wide = df[\"Adj Close\"].unstack(level=0)\n",
    "#         return (\n",
    "#             manual_atr.unstack(level=0) / prices_wide,\n",
    "#             tr.unstack(level=0) / prices_wide,\n",
    "#         )\n",
    "\n",
    "#     def run_period_audit(df_p, df_atrp, df_trp, weights):\n",
    "#         if df_p.empty:\n",
    "#             return 0, 0, 0, 0\n",
    "#         norm = df_p.div(df_p.bfill().iloc[0])\n",
    "#         equity = (norm * weights).sum(axis=1)\n",
    "#         drift_w = (norm * weights).div(equity, axis=0)\n",
    "#         p_atrp = (drift_w * df_atrp).sum(axis=1)\n",
    "#         p_trp = (drift_w * df_trp).sum(axis=1)\n",
    "#         rets = equity.pct_change().dropna()\n",
    "#         if rets.empty:\n",
    "#             return 0, 0, 0, 0\n",
    "#         gain = equity.iloc[-1] / equity.iloc[0] - 1\n",
    "#         sharpe = (rets.mean() / rets.std() * np.sqrt(252)) if rets.std() > 0 else 0\n",
    "#         return (\n",
    "#             gain,\n",
    "#             sharpe,\n",
    "#             rets.mean() / p_atrp.loc[rets.index].mean(),\n",
    "#             rets.mean() / p_trp.loc[rets.index].mean(),\n",
    "#         )\n",
    "\n",
    "#     audit_rows = []\n",
    "#     targets = [\n",
    "#         (\"p\", debug[\"portfolio_raw_components\"], res.initial_weights, \"Group\"),\n",
    "#         (\n",
    "#             \"b\",\n",
    "#             debug[\"benchmark_raw_components\"],\n",
    "#             pd.Series({inputs.benchmark_ticker: 1.0}),\n",
    "#             \"Benchmark\",\n",
    "#         ),\n",
    "#     ]\n",
    "\n",
    "#     for prefix, components, weights, entity_name in targets:\n",
    "#         m_atrp, m_trp = calculate_manual_atrp_warm(\n",
    "#             components[\"ohlcv_raw\"], engine.features_df, engine.df_close, res.start_date\n",
    "#         )\n",
    "#         m_price = components[\"prices\"]\n",
    "#         for p_label, (d_start, d_end) in periods.items():\n",
    "#             mg, ms, msa, mst = run_period_audit(\n",
    "#                 m_price.loc[d_start:d_end],\n",
    "#                 m_atrp.loc[d_start:d_end],\n",
    "#                 m_trp.loc[d_start:d_end],\n",
    "#                 weights,\n",
    "#             )\n",
    "#             for m_name, m_val, e_key in [\n",
    "#                 (\"Gain\", mg, f\"{p_label.lower()}_{prefix}_gain\"),\n",
    "#                 (\"Sharpe\", ms, f\"{p_label.lower()}_{prefix}_sharpe\"),\n",
    "#                 (\"Sharpe (ATRP)\", msa, f\"{p_label.lower()}_{prefix}_sharpe_atrp\"),\n",
    "#                 (\"Sharpe (TRP)\", mst, f\"{p_label.lower()}_{prefix}_sharpe_trp\"),\n",
    "#             ]:\n",
    "#                 e_val = m.get(e_key, 0)\n",
    "#                 audit_rows.append(\n",
    "#                     {\n",
    "#                         \"Entity\": entity_name,\n",
    "#                         \"Period\": p_label,\n",
    "#                         \"Metric\": m_name,\n",
    "#                         \"Engine\": e_val,\n",
    "#                         \"Manual\": m_val,\n",
    "#                         \"Delta\": e_val - m_val,\n",
    "#                     }\n",
    "#                 )\n",
    "\n",
    "#     df_perf = pd.DataFrame(audit_rows)\n",
    "#     df_perf[\"Status\"] = df_perf[\"Delta\"].apply(\n",
    "#         lambda x: \"‚úÖ PASS\" if abs(x) < 1e-7 else \"‚ùå FAIL\"\n",
    "#     )\n",
    "#     print(\"üìù 1. PERFORMANCE RECONCILIATION\")\n",
    "#     display(\n",
    "#         df_perf.pivot_table(\n",
    "#             index=[\"Entity\", \"Metric\"],\n",
    "#             columns=\"Period\",\n",
    "#             values=\"Status\",\n",
    "#             aggfunc=\"first\",\n",
    "#         )\n",
    "#     )\n",
    "\n",
    "#     # --------------------------------------------------------------------------\n",
    "#     # PART 2: SURVIVAL AUDIT (Liquidity/Quality Gate)\n",
    "#     # --------------------------------------------------------------------------\n",
    "#     print(\"\\n\" + \"=\" * 85)\n",
    "#     print(\"üìù 2. SURVIVAL AUDIT\")\n",
    "#     if inputs.universe_subset:\n",
    "#         print(\n",
    "#             \"   Mode: CASCADE/SUBSET | Logic: Quality filters bypassed per design. | ‚úÖ BYPASS\"\n",
    "#         )\n",
    "#     else:\n",
    "#         audit_liq = debug.get(\"audit_liquidity\")\n",
    "#         snapshot = audit_liq[\"universe_snapshot\"]\n",
    "#         thresholds = inputs.quality_thresholds\n",
    "\n",
    "#         m_cutoff = max(\n",
    "#             snapshot[\"RollMedDollarVol\"].quantile(\n",
    "#                 thresholds[\"min_liquidity_percentile\"]\n",
    "#             ),\n",
    "#             thresholds[\"min_median_dollar_volume\"],\n",
    "#         )\n",
    "#         m_survivors = snapshot[\n",
    "#             (snapshot[\"RollMedDollarVol\"] >= m_cutoff)\n",
    "#             & (snapshot[\"RollingStalePct\"] <= thresholds[\"max_stale_pct\"])\n",
    "#             & (snapshot[\"RollingSameVolCount\"] <= thresholds[\"max_same_vol_count\"])\n",
    "#         ]\n",
    "#         s_match = (\n",
    "#             \"‚úÖ PASS\" if audit_liq[\"tickers_passed\"] == len(m_survivors) else \"‚ùå FAIL\"\n",
    "#         )\n",
    "#         print(\n",
    "#             f\"   Survival Integrity: {s_match} (Engine: {audit_liq['tickers_passed']} vs Auditor: {len(m_survivors)})\"\n",
    "#         )\n",
    "\n",
    "#     # --------------------------------------------------------------------------\n",
    "#     # PART 3: UNIVERSAL SELECTION AUDIT (Strategy Registry Math)\n",
    "#     # --------------------------------------------------------------------------\n",
    "#     if inputs.mode == \"Ranking\":\n",
    "#         print(\"\\n\" + \"=\" * 85)\n",
    "#         print(f\"üìù 3. UNIVERSAL SELECTION AUDIT | Strategy: {inputs.metric}\")\n",
    "\n",
    "#         # Pull survivors from the snapshot provided by the engine\n",
    "#         snapshot = debug[\"audit_liquidity\"][\"universe_snapshot\"]\n",
    "#         survivors = snapshot.query(\"Passed_Final\").index.tolist()\n",
    "#         idx = pd.IndexSlice\n",
    "\n",
    "#         # Independent Data Reconstruction for the Observation\n",
    "#         feat_period = engine.features_df.loc[\n",
    "#             idx[survivors, res.start_date : res.decision_date], :\n",
    "#         ]\n",
    "#         atrp_lb_mean = feat_period[\"ATRP\"].groupby(level=\"Ticker\").mean()\n",
    "#         trp_lb_mean = feat_period[\"TRP\"].groupby(level=\"Ticker\").mean()\n",
    "#         feat_now = engine.features_df.xs(res.decision_date, level=\"Date\").reindex(\n",
    "#             survivors\n",
    "#         )\n",
    "#         lb_prices = engine.df_close.loc[res.start_date : res.decision_date, survivors]\n",
    "\n",
    "#         audit_obs: MarketObservation = {\n",
    "#             \"lookback_close\": lb_prices,\n",
    "#             \"lookback_returns\": lb_prices.ffill().pct_change(),\n",
    "#             \"atrp\": atrp_lb_mean,\n",
    "#             \"trp\": trp_lb_mean,\n",
    "#             \"rsi\": feat_now[\"RSI\"],\n",
    "#             \"rel_strength\": feat_now[\"RelStrength\"],\n",
    "#             \"vol_regime\": feat_now[\"VolRegime\"],\n",
    "#             \"rvol\": feat_now[\"RVol\"],\n",
    "#             \"spy_rvol\": feat_now.get(\"Spy_RVol\", 0),\n",
    "#             \"obv_score\": feat_now[\"OBV_Score\"],\n",
    "#             \"roc_1\": feat_now[\"ROC_1\"],\n",
    "#             \"roc_3\": feat_now[\"ROC_3\"],\n",
    "#             \"roc_5\": feat_now[\"ROC_5\"],\n",
    "#             \"roc_10\": feat_now[\"ROC_10\"],\n",
    "#             \"roc_21\": feat_now[\"ROC_21\"],\n",
    "#         }\n",
    "\n",
    "#         # Run Manual Registry Math\n",
    "#         manual_scores = METRIC_REGISTRY[inputs.metric](audit_obs)\n",
    "#         sel_audit = []\n",
    "#         for rank, ticker in enumerate(res.tickers, start=inputs.rank_start):\n",
    "#             eng_score = res.results_df.loc[ticker, \"Strategy Value\"]\n",
    "#             aud_score = manual_scores.loc[ticker]\n",
    "#             sel_audit.append(\n",
    "#                 {\n",
    "#                     \"Rank\": rank,\n",
    "#                     \"Ticker\": ticker,\n",
    "#                     \"Engine\": eng_score,\n",
    "#                     \"Manual\": aud_score,\n",
    "#                     \"Delta\": eng_score - aud_score,\n",
    "#                     \"Status\": (\n",
    "#                         \"‚úÖ PASS\" if np.isclose(eng_score, aud_score) else \"‚ùå FAIL\"\n",
    "#                     ),\n",
    "#                 }\n",
    "#             )\n",
    "#         display(\n",
    "#             pd.DataFrame(sel_audit)\n",
    "#             .set_index(\"Rank\")\n",
    "#             .style.format(\"{:.8f}\", subset=[\"Engine\", \"Manual\", \"Delta\"])\n",
    "#         )\n",
    "\n",
    "#     print(\"=\" * 85)\n",
    "\n",
    "\n",
    "def verify_analyzer_long(analyzer):\n",
    "    \"\"\"\n",
    "    FULL SPECTRUM AUDIT:\n",
    "    1. Performance (3 Periods, Warm-Start ATRP)\n",
    "    2. Survival (Liquidity/Quality Gate)\n",
    "    3. Universal Selection (Strategy Math reconciliation for ALL candidates)\n",
    "    \"\"\"\n",
    "    print(\"========= verify_analyzer_long_2 =========\", \"\\n\")\n",
    "    res = analyzer.last_run\n",
    "    engine = analyzer.engine\n",
    "\n",
    "    if not res or not res.debug_data:\n",
    "        print(\"‚ùå Audit Aborted: No debug data found. Run UI with debug=True.\")\n",
    "        return\n",
    "\n",
    "    debug = res.debug_data\n",
    "    inputs = debug[\"inputs_snapshot\"]\n",
    "    m = res.perf_metrics\n",
    "\n",
    "    # --- TRANSPARENCY BLOCK ---\n",
    "    print(\"\\n\" + \"=\" * 85)\n",
    "    print(\"*\" * 85)\n",
    "    print(\n",
    "        f\"üõ°Ô∏è  STARTING NUCLEAR LONG-FORM AUDIT | {res.decision_date.date()} | {inputs.metric}\"\n",
    "    )\n",
    "    print(\n",
    "        \"‚ö†Ô∏è  ASSUMPTION: Verification logic is independent (re-calculates indicators), but trusts Engine Data\"\n",
    "    )\n",
    "    print(\"   (engine.features_df for seeds, engine.df_close, and debug['ohlcv_raw'])\")\n",
    "    print(\"*\" * 85 + \"\\n\" + \"=\" * 85)\n",
    "\n",
    "    print(\"=\" * 85)\n",
    "    print(f\"üõ°Ô∏è  NUCLEAR AUDIT REPORT | {res.decision_date.date()} | {inputs.metric}\")\n",
    "    print(\"=\" * 85)\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # PART 1: PERFORMANCE RECONCILIATION (3 PERIODS)\n",
    "    # --------------------------------------------------------------------------\n",
    "    periods = {\n",
    "        \"Full\": (res.start_date, res.holding_end_date),\n",
    "        \"Lookback\": (res.start_date, res.decision_date),\n",
    "        \"Holding\": (res.buy_date, res.holding_end_date),\n",
    "    }\n",
    "\n",
    "    def calculate_manual_atrp_warm(df_ohlcv, features_df, df_close_matrix, start_date):\n",
    "        df = df_ohlcv.copy()\n",
    "        df[\"PC\"] = df.groupby(level=\"Ticker\")[\"Adj Close\"].shift(1)\n",
    "        tr = pd.concat(\n",
    "            [\n",
    "                df[\"Adj High\"] - df[\"Adj Low\"],\n",
    "                (df[\"Adj High\"] - df[\"PC\"]).abs(),\n",
    "                (df[\"Adj Low\"] - df[\"PC\"]).abs(),\n",
    "            ],\n",
    "            axis=1,\n",
    "        ).max(axis=1)\n",
    "\n",
    "        # Warm Start Seed\n",
    "        seed_atrp = features_df.xs(start_date, level=\"Date\")[\"ATRP\"]\n",
    "        seed_price = df_close_matrix.loc[start_date]\n",
    "        seed_atr = seed_atrp * seed_price\n",
    "        alpha = 1 / 14\n",
    "\n",
    "        def ewm_warm(group):\n",
    "            ticker = group.name\n",
    "            initial_val = seed_atr.get(ticker, group.iloc[0])\n",
    "            vals = group.values\n",
    "            results = np.zeros_like(vals)\n",
    "            results[0] = initial_val\n",
    "            for i in range(1, len(vals)):\n",
    "                results[i] = (vals[i] * alpha) + (results[i - 1] * (1 - alpha))\n",
    "            return pd.Series(results, index=group.index)\n",
    "\n",
    "        manual_atr = tr.groupby(level=\"Ticker\", group_keys=False).apply(ewm_warm)\n",
    "        prices_wide = df[\"Adj Close\"].unstack(level=0)\n",
    "        return (\n",
    "            manual_atr.unstack(level=0) / prices_wide,\n",
    "            tr.unstack(level=0) / prices_wide,\n",
    "        )\n",
    "\n",
    "    def run_period_audit(df_p, df_atrp, df_trp, weights):\n",
    "        if df_p.empty:\n",
    "            return 0, 0, 0, 0\n",
    "        norm = df_p.div(df_p.bfill().iloc[0])\n",
    "        equity = (norm * weights).sum(axis=1)\n",
    "        drift_w = (norm * weights).div(equity, axis=0)\n",
    "        p_atrp = (drift_w * df_atrp).sum(axis=1)\n",
    "        p_trp = (drift_w * df_trp).sum(axis=1)\n",
    "        rets = equity.pct_change().dropna()\n",
    "        if rets.empty:\n",
    "            return 0, 0, 0, 0\n",
    "        gain = equity.iloc[-1] / equity.iloc[0] - 1\n",
    "        sharpe = (rets.mean() / rets.std() * np.sqrt(252)) if rets.std() > 0 else 0\n",
    "        return (\n",
    "            gain,\n",
    "            sharpe,\n",
    "            rets.mean() / p_atrp.loc[rets.index].mean(),\n",
    "            rets.mean() / p_trp.loc[rets.index].mean(),\n",
    "        )\n",
    "\n",
    "    audit_rows = []\n",
    "    targets = [\n",
    "        (\"p\", debug[\"portfolio_raw_components\"], res.initial_weights, \"Group\"),\n",
    "        (\n",
    "            \"b\",\n",
    "            debug[\"benchmark_raw_components\"],\n",
    "            pd.Series({inputs.benchmark_ticker: 1.0}),\n",
    "            \"Benchmark\",\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    for prefix, components, weights, entity_name in targets:\n",
    "        m_atrp, m_trp = calculate_manual_atrp_warm(\n",
    "            components[\"ohlcv_raw\"], engine.features_df, engine.df_close, res.start_date\n",
    "        )\n",
    "        m_price = components[\"prices\"]\n",
    "        for p_label, (d_start, d_end) in periods.items():\n",
    "            mg, ms, msa, mst = run_period_audit(\n",
    "                m_price.loc[d_start:d_end],\n",
    "                m_atrp.loc[d_start:d_end],\n",
    "                m_trp.loc[d_start:d_end],\n",
    "                weights,\n",
    "            )\n",
    "            for m_name, m_val, e_key in [\n",
    "                (\"Gain\", mg, f\"{p_label.lower()}_{prefix}_gain\"),\n",
    "                (\"Sharpe\", ms, f\"{p_label.lower()}_{prefix}_sharpe\"),\n",
    "                (\"Sharpe (ATRP)\", msa, f\"{p_label.lower()}_{prefix}_sharpe_atrp\"),\n",
    "                (\"Sharpe (TRP)\", mst, f\"{p_label.lower()}_{prefix}_sharpe_trp\"),\n",
    "            ]:\n",
    "                e_val = m.get(e_key, 0)\n",
    "                audit_rows.append(\n",
    "                    {\n",
    "                        \"Entity\": entity_name,\n",
    "                        \"Period\": p_label,\n",
    "                        \"Metric\": m_name,\n",
    "                        \"Engine\": e_val,\n",
    "                        \"Manual\": m_val,\n",
    "                        \"Delta\": e_val - m_val,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    df_perf = pd.DataFrame(audit_rows)\n",
    "    df_perf[\"Status\"] = df_perf[\"Delta\"].apply(\n",
    "        lambda x: \"‚úÖ PASS\" if abs(x) < 1e-7 else \"‚ùå FAIL\"\n",
    "    )\n",
    "    print(\"üìù 1. PERFORMANCE RECONCILIATION\")\n",
    "    display(\n",
    "        df_perf.pivot_table(\n",
    "            index=[\"Entity\", \"Metric\"],\n",
    "            columns=\"Period\",\n",
    "            values=\"Status\",\n",
    "            aggfunc=\"first\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # PART 2: SURVIVAL AUDIT (Liquidity/Quality Gate)\n",
    "    # --------------------------------------------------------------------------\n",
    "    print(\"\\n\" + \"=\" * 85)\n",
    "    print(\"üìù 2. SURVIVAL AUDIT\")\n",
    "    if inputs.universe_subset:\n",
    "        print(\n",
    "            \"   Mode: CASCADE/SUBSET | Logic: Quality filters bypassed per design. | ‚úÖ BYPASS\"\n",
    "        )\n",
    "    else:\n",
    "        audit_liq = debug.get(\"audit_liquidity\")\n",
    "        snapshot = audit_liq[\"universe_snapshot\"]\n",
    "        thresholds = inputs.quality_thresholds\n",
    "\n",
    "        m_cutoff = max(\n",
    "            snapshot[\"RollMedDollarVol\"].quantile(\n",
    "                thresholds[\"min_liquidity_percentile\"]\n",
    "            ),\n",
    "            thresholds[\"min_median_dollar_volume\"],\n",
    "        )\n",
    "        m_survivors = snapshot[\n",
    "            (snapshot[\"RollMedDollarVol\"] >= m_cutoff)\n",
    "            & (snapshot[\"RollingStalePct\"] <= thresholds[\"max_stale_pct\"])\n",
    "            & (snapshot[\"RollingSameVolCount\"] <= thresholds[\"max_same_vol_count\"])\n",
    "        ]\n",
    "        s_match = (\n",
    "            \"‚úÖ PASS\" if audit_liq[\"tickers_passed\"] == len(m_survivors) else \"‚ùå FAIL\"\n",
    "        )\n",
    "        print(\n",
    "            f\"   Survival Integrity: {s_match} (Engine: {audit_liq['tickers_passed']} vs Auditor: {len(m_survivors)})\"\n",
    "        )\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # PART 3: UNIVERSAL SELECTION AUDIT (Strategy Registry Math)\n",
    "    # --------------------------------------------------------------------------\n",
    "    if inputs.mode == \"Ranking\":\n",
    "        print(\"\\n\" + \"=\" * 85)\n",
    "        print(f\"üìù 3. UNIVERSAL SELECTION AUDIT | Strategy: {inputs.metric}\")\n",
    "\n",
    "        # A. Independent Data Reconstruction\n",
    "        # We must pull data for ALL survivors, not just the selected ones\n",
    "        if \"full_universe_ranking\" not in debug:\n",
    "            print(\"‚ùå Audit Error: 'full_universe_ranking' not found in debug data.\")\n",
    "            return\n",
    "\n",
    "        eng_rank_df = debug[\"full_universe_ranking\"]\n",
    "        survivors = eng_rank_df.index.tolist()\n",
    "        idx = pd.IndexSlice\n",
    "\n",
    "        # Re-fetch data for the entire survivor list\n",
    "        feat_period = engine.features_df.loc[\n",
    "            idx[survivors, res.start_date : res.decision_date], :\n",
    "        ]\n",
    "        atrp_lb_mean = feat_period[\"ATRP\"].groupby(level=\"Ticker\").mean()\n",
    "        trp_lb_mean = feat_period[\"TRP\"].groupby(level=\"Ticker\").mean()\n",
    "        feat_now = engine.features_df.xs(res.decision_date, level=\"Date\").reindex(\n",
    "            survivors\n",
    "        )\n",
    "        lb_prices = engine.df_close.loc[res.start_date : res.decision_date, survivors]\n",
    "\n",
    "        audit_obs: MarketObservation = {\n",
    "            \"lookback_close\": lb_prices,\n",
    "            \"lookback_returns\": lb_prices.ffill().pct_change(),\n",
    "            \"atrp\": atrp_lb_mean,\n",
    "            \"trp\": trp_lb_mean,\n",
    "            \"rsi\": feat_now[\"RSI\"],\n",
    "            \"rel_strength\": feat_now[\"RelStrength\"],\n",
    "            \"vol_regime\": feat_now[\"VolRegime\"],\n",
    "            \"rvol\": feat_now[\"RVol\"],\n",
    "            \"spy_rvol\": feat_now.get(\"Spy_RVol\", 0),\n",
    "            \"obv_score\": feat_now[\"OBV_Score\"],\n",
    "            \"roc_1\": feat_now[\"ROC_1\"],\n",
    "            \"roc_3\": feat_now[\"ROC_3\"],\n",
    "            \"roc_5\": feat_now[\"ROC_5\"],\n",
    "            \"roc_10\": feat_now[\"ROC_10\"],\n",
    "            \"roc_21\": feat_now[\"ROC_21\"],\n",
    "        }\n",
    "\n",
    "        # B. Run Manual Registry Math on Full Universe\n",
    "        manual_scores = METRIC_REGISTRY[inputs.metric](audit_obs)\n",
    "\n",
    "        # C. Compare Every Single Candidate\n",
    "        audit_data = []\n",
    "        for i, (ticker, row) in enumerate(eng_rank_df.iterrows()):\n",
    "            eng_val = row[\"Strategy_Score\"]\n",
    "            man_val = manual_scores.get(ticker, np.nan)\n",
    "            delta = eng_val - man_val\n",
    "\n",
    "            status = \"‚úÖ PASS\" if np.isclose(eng_val, man_val, atol=1e-8) else \"‚ùå FAIL\"\n",
    "\n",
    "            audit_data.append(\n",
    "                {\n",
    "                    \"Rank\": i + 1,\n",
    "                    \"Ticker\": ticker,\n",
    "                    \"Engine\": eng_val,\n",
    "                    \"Manual\": man_val,\n",
    "                    \"Delta\": delta,\n",
    "                    \"Status\": status,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        df_audit_all = pd.DataFrame(audit_data).set_index(\"Rank\")\n",
    "        n_pass = (df_audit_all[\"Status\"] == \"‚úÖ PASS\").sum()\n",
    "        n_fail = len(df_audit_all) - n_pass\n",
    "\n",
    "        print(f\"   Scope: Evaluated {len(df_audit_all)} candidates (Full Universe).\")\n",
    "        print(f\"   Result: {n_pass} PASSED | {n_fail} FAILED\")\n",
    "\n",
    "        if n_fail > 0:\n",
    "            print(\"‚ö†Ô∏è  DISPLAYING FAILURES:\")\n",
    "            display(df_audit_all[df_audit_all[\"Status\"] == \"‚ùå FAIL\"].head(20))\n",
    "        else:\n",
    "            ##########################################\n",
    "            # print(\"   All scores match registry math.\")\n",
    "            # print(f\"   Visual Proof (Top 10 of {len(df_audit_all)}):\")\n",
    "            # # We display only the top 10 to keep the notebook clean\n",
    "            # display(\n",
    "            #     df_audit_all.head(10).style.format(\n",
    "            #         \"{:.8f}\", subset=[\"Engine\", \"Manual\", \"Delta\"]\n",
    "            #     )\n",
    "            # )\n",
    "\n",
    "            print(\"   All scores match registry math.\")\n",
    "            # print(f\"   Visual Proof (Top 10 of {len(df_audit_all)}):\")\n",
    "            # # We display only the top 10 to keep the notebook clean\n",
    "            display(\n",
    "                df_audit_all.style.format(\n",
    "                    \"{:.8f}\", subset=[\"Engine\", \"Manual\", \"Delta\"]\n",
    "                )\n",
    "            )\n",
    "            ##########################################\n",
    "\n",
    "    print(\"=\" * 85)\n",
    "\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451fee35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. RE-INSTANTIATE ENGINE (Crucial Step!)\n",
    "# This ensures the 'master_engine' variable actually uses the code you just pasted above.\n",
    "master_engine = AlphaEngine(\n",
    "    df_ohlcv=df_ohlcv,\n",
    "    features_df=features_df,\n",
    "    df_close_wide=df_close_wide,\n",
    "    df_atrp_wide=df_atrp_wide,\n",
    "    df_trp_wide=df_trp_wide,\n",
    ")\n",
    "\n",
    "# 2. LAUNCH STAGE 1 (Discovery)\n",
    "# universe_subset=None means \"Scan the whole market\"\n",
    "analyzer1, stage1_pack = create_walk_forward_analyzer(\n",
    "    master_engine, universe_subset=None\n",
    ")\n",
    "\n",
    "print(\"üöÄ Ready for Stage 1: Run Simulation to find the top 10.\")\n",
    "analyzer1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce50edd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. VERIFY we have data from Stage 1\n",
    "if not stage1_pack.selected_tickers:\n",
    "    print(\n",
    "        \"‚ö†Ô∏è Stage 1 is empty. Please run the simulation in the Stage 1 chart above first.\"\n",
    "    )\n",
    "else:\n",
    "    print(f\"‚úÖ Passing {len(stage1_pack.selected_tickers)} tickers to Stage 2.\")\n",
    "    print(f\"üóìÔ∏è Syncing date: {stage1_pack.decision_date.date()}\")\n",
    "\n",
    "    # 2. CREATE A CLONE PACK\n",
    "    # This is the trick: We create a NEW pack, but initialize it with Stage 1's date.\n",
    "    # This ensures the UI matches Stage 1, but the results won't overwrite Stage 1.\n",
    "    stage2_input_pack = FilterPack(\n",
    "        decision_date=stage1_pack.decision_date,  # <--- COPY THE DATE\n",
    "        eligible_pool=stage1_pack.selected_tickers,  # <--- RECORD THE POOL\n",
    "    )\n",
    "\n",
    "    # 3. LAUNCH STAGE 2\n",
    "    analyzer2, stage2_results = create_walk_forward_analyzer(\n",
    "        master_engine,\n",
    "        universe_subset=stage1_pack.selected_tickers,  # Engine Constraint (Logic)\n",
    "        filter_pack=stage2_input_pack,  # UI/Date Constraint (State)\n",
    "    )\n",
    "\n",
    "    analyzer2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de3a8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_analyzer_structure(analyzer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edba8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_analyzer_short(analyzer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c3c1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_analyzer_long(analyzer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e487e1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_analyzer_structure(analyzer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38804cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_analyzer_short(analyzer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1ba5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_analyzer_long(analyzer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43a2cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP - HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47be38bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7906e37e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1554a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a FRESH engine instance\n",
    "# (This forces Python to use the latest Class definition you just updated)\n",
    "master_engine_1 = AlphaEngine(\n",
    "    df_ohlcv=df_ohlcv,\n",
    "    features_df=features_df,\n",
    "    df_close_wide=df_close_wide,\n",
    "    df_atrp_wide=df_atrp_wide,\n",
    "    df_trp_wide=df_trp_wide,\n",
    ")\n",
    "\n",
    "# 2. Initialize Analyzer 1 with the NEW engine and the specific subset\n",
    "analyzer1, _pack1 = create_walk_forward_analyzer(\n",
    "    master_engine_1,\n",
    "    universe_subset=None,  # Passing the list of 10 survivors\n",
    "    filter_pack=None,\n",
    ")\n",
    "\n",
    "# 3. Display the Interface\n",
    "analyzer1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4abc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg1 = visualize_audit_structure(analyzer1.last_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610b2463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _df = peek(102, reg1)\n",
    "_df = peek(4, reg1)\n",
    "# print(_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422f4c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyzer1.last_run.debug_data['audit_pack']['tickers']\n",
    "# analyzer1.last_run[\"audit_pack\"][\"tickers\"]\n",
    "analyzer1.last_run.tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033bf6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. Check the Final Rankings & Strategy Scores\n",
    "df_ohlcv_subset = analyzer1.last_run.debug_data[\"portfolio_raw_components\"][\"ohlcv_raw\"]\n",
    "# df_ohlcv_subset = analyzer1.last_run.debug_data[\"portfolio_raw_components\"]\n",
    "# display(df_ohlcv_subset.head(10))  # The winners\n",
    "# display(df_ohlcv_subset.tail(5))  # The losers (barely passed quality)\n",
    "display(df_ohlcv_subset)  # The losers (barely passed quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3460fa1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a FRESH engine instance\n",
    "# (This forces Python to use the latest Class definition with the PRINTS)\n",
    "debug_engine = AlphaEngine(\n",
    "    df_ohlcv=df_ohlcv,\n",
    "    features_df=features_df,\n",
    "    df_close_wide=df_close_wide,\n",
    "    df_atrp_wide=df_atrp_wide,\n",
    "    df_trp_wide=df_trp_wide,\n",
    ")\n",
    "\n",
    "# 2. Check if we have a subset available to test\n",
    "# (This handles the case if stage1_pack is undefined or empty)\n",
    "test_subset = (\n",
    "    stage1_pack.selected_tickers\n",
    "    if (\"stage1_pack\" in locals() and stage1_pack.selected_tickers)\n",
    "    else [\"AAPL\", \"MSFT\", \"SPY\"]\n",
    ")\n",
    "print(f\"Testing with subset: {test_subset}\")\n",
    "\n",
    "# 3. Create the Analyzer using the FRESH engine\n",
    "analyzer_debug, _ = create_walk_forward_analyzer(\n",
    "    debug_engine,\n",
    "    universe_subset=test_subset,\n",
    "    filter_pack=None,  # We don't need to save results for this test\n",
    ")\n",
    "\n",
    "# 4. Show the UI\n",
    "analyzer_debug.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ca418c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1. Create a FRESH engine instance\n",
    "# # (This forces Python to use the latest Class definition you just updated)\n",
    "# master_engine_2 = AlphaEngine(\n",
    "#     df_ohlcv=df_ohlcv_subset,\n",
    "#     features_df=features_df,\n",
    "#     df_close_wide=df_close_wide,\n",
    "#     df_atrp_wide=df_atrp_wide,\n",
    "#     df_trp_wide=df_trp_wide,\n",
    "# )\n",
    "\n",
    "master_engine_2 = AlphaEngine(\n",
    "    df_ohlcv=df_ohlcv,\n",
    "    features_df=features_df,\n",
    "    df_close_wide=df_close_wide,\n",
    "    df_atrp_wide=df_atrp_wide,\n",
    "    df_trp_wide=df_trp_wide,\n",
    ")\n",
    "\n",
    "# 2. Initialize Analyzer 1 with the NEW engine and the specific subset\n",
    "analyzer2, _ = create_walk_forward_analyzer(\n",
    "    master_engine_2,\n",
    "    universe_subset=analyzer1.last_run.tickers,  # Passing the list of 10 survivors\n",
    "    filter_pack=None,\n",
    ")\n",
    "\n",
    "# 3. Display the Interface\n",
    "analyzer2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2953d635",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg2 = visualize_audit_structure(analyzer2.last_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dec478",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c77ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ohlcv_sub.index.names\n",
    "# ohlcv_sub.info()\n",
    "# Unique tickers only\n",
    "_t = ohlcv_sub.index.get_level_values(\"Ticker\").unique().tolist()\n",
    "len(_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b19763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Define Benchmark (Must match what you use in the UI, usually 'SPY')\n",
    "benchmark_ticker = GLOBAL_SETTINGS.get(\"benchmark_ticker\", \"SPY\")\n",
    "\n",
    "# 1. Get your survivors AND the benchmark\n",
    "subset = stage1_pack.selected_tickers\n",
    "# Use set to avoid duplicates, then convert back to list\n",
    "tickers_to_keep = list(set(subset + [benchmark_ticker]))\n",
    "\n",
    "print(\n",
    "    f\"Creating Mini-Engine for {len(tickers_to_keep)} tickers (Survivors + {benchmark_ticker})\"\n",
    ")\n",
    "\n",
    "# 2. Slice the WIDE DataFrames (Columns = Tickers)\n",
    "# We use .intersection to avoid errors if SPY is somehow missing from source data\n",
    "valid_cols = df_close_wide.columns.intersection(tickers_to_keep)\n",
    "close_sub = df_close_wide[valid_cols]\n",
    "atrp_sub = df_atrp_wide[valid_cols]\n",
    "trp_sub = df_trp_wide[valid_cols]\n",
    "\n",
    "# 3. Slice the TALL DataFrames (MultiIndex: Date/Ticker)\n",
    "features_sub = features_df[\n",
    "    features_df.index.get_level_values(\"Ticker\").isin(tickers_to_keep)\n",
    "]\n",
    "ohlcv_sub = df_ohlcv[df_ohlcv.index.get_level_values(\"Ticker\").isin(tickers_to_keep)]\n",
    "\n",
    "# 4. Create the Isolated Engine\n",
    "master_engine_2 = AlphaEngine(\n",
    "    df_ohlcv=ohlcv_sub,\n",
    "    features_df=features_sub,\n",
    "    df_close_wide=close_sub,\n",
    "    df_atrp_wide=atrp_sub,\n",
    "    df_trp_wide=trp_sub,\n",
    ")\n",
    "\n",
    "# 5. Run Analyzer 2\n",
    "analyzer2, stage2_pack = create_walk_forward_analyzer(\n",
    "    master_engine_2,\n",
    "    # INPUT: Read from Stage 1\n",
    "    universe_subset=stage1_pack.selected_tickers,\n",
    "    # OUTPUT: Do NOT pass filter_pack here. Let it create a new one.\n",
    "    filter_pack=None,\n",
    ")\n",
    "\n",
    "analyzer2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635f4ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Get your survivors\n",
    "subset = stage1_pack.selected_tickers\n",
    "print(f\"Creating Mini-Engine for {len(subset)} tickers: {subset}\")\n",
    "\n",
    "# 2. Slice the WIDE DataFrames (Columns = Tickers)\n",
    "# This is easy: just select the columns\n",
    "close_sub = df_close_wide[subset]\n",
    "atrp_sub = df_atrp_wide[subset]\n",
    "trp_sub = df_trp_wide[subset]\n",
    "\n",
    "# 3. Slice the TALL DataFrames (MultiIndex: Date/Ticker)\n",
    "# We filter rows where the 'Ticker' level matches our list\n",
    "features_sub = features_df[features_df.index.get_level_values(\"Ticker\").isin(subset)]\n",
    "ohlcv_sub = df_ohlcv[df_ohlcv.index.get_level_values(\"Ticker\").isin(subset)]\n",
    "\n",
    "# 4. Create the Isolated Engine\n",
    "master_engine_2 = AlphaEngine(\n",
    "    df_ohlcv=ohlcv_sub,\n",
    "    features_df=features_sub,\n",
    "    df_close_wide=close_sub,\n",
    "    df_atrp_wide=atrp_sub,\n",
    "    df_trp_wide=trp_sub,\n",
    ")\n",
    "\n",
    "# 5. Run Analyzer 2 (No need to pass universe_subset logic anymore, the data IS the subset)\n",
    "analyzer2, _ = create_walk_forward_analyzer(\n",
    "    master_engine_2,\n",
    "    # We can pass None for universe_subset because the engine\n",
    "    # itself is now physically limited to these 10 tickers.\n",
    "    universe_subset=None,\n",
    "    filter_pack=stage1_pack,\n",
    ")\n",
    "analyzer2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7a9a56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985c3f69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932d8808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a FRESH engine instance\n",
    "# (This forces Python to use the latest Class definition you just updated)\n",
    "master_engine_2 = AlphaEngine(\n",
    "    df_ohlcv=df_ohlcv,\n",
    "    features_df=features_df,\n",
    "    df_close_wide=df_close_wide,\n",
    "    df_atrp_wide=df_atrp_wide,\n",
    "    df_trp_wide=df_trp_wide,\n",
    ")\n",
    "\n",
    "# 2. Initialize Analyzer 2 with the NEW engine and the specific subset\n",
    "analyzer2, _ = create_walk_forward_analyzer(\n",
    "    master_engine_2,\n",
    "    universe_subset=stage1_pack.selected_tickers,  # Passing the list of 10 survivors\n",
    "    filter_pack=stage1_pack,\n",
    ")\n",
    "\n",
    "# 3. Display the Interface\n",
    "analyzer2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955575bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer2, _ = create_walk_forward_analyzer(\n",
    "    master_engine,\n",
    "    # universe_subset=stage1_pack.eligible_pool,  # or my_rl_selection\n",
    "    universe_subset=stage1_pack.selected_tickers,  # or my_rl_selection\n",
    "    filter_pack=stage1_pack,\n",
    ")\n",
    "analyzer2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7f29ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. Check the Final Rankings & Strategy Scores\n",
    "ranking_df = analyzer1.last_run.debug_data[\"full_universe_ranking\"]\n",
    "display(ranking_df.head(10))  # The winners\n",
    "display(ranking_df.tail(5))  # The losers (barely passed quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a02e978",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2. Check the Raw Data for the Selected Tickers\n",
    "# Access the raw components for the selected portfolio\n",
    "components = analyzer1.last_run.debug_data[\"portfolio_raw_components\"]\n",
    "\n",
    "print(\"--- Raw Prices ---\")\n",
    "display(components[\"prices\"].head())\n",
    "\n",
    "print(\"--- Raw ATRP (Volatility) ---\")\n",
    "display(components[\"atrp\"].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb6ad67",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3. Verify the Performance Metrics (Raw Dict)\n",
    "import json\n",
    "\n",
    "# Prints the performance metrics in a readable format\n",
    "print(json.dumps(analyzer1.last_run.perf_metrics, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f719c824",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4. Audit the Stage 1 \"Gate\" (Liquidity Check)\n",
    "audit = analyzer1.last_run.debug_data[\"audit_liquidity\"][\"universe_snapshot\"]\n",
    "\n",
    "# Filter to see tickers that FAILED the volume or stale data checks\n",
    "failed_tickers = audit[audit[\"Passed_Final\"] == False]\n",
    "display(failed_tickers.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce80661a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pro-Tip for the RL Agent:\n",
    "# When you start building your RL agent, the data it needs is located in:\n",
    "analyzer1.last_run.debug_data[\"portfolio_raw_components\"][\"ohlcv_raw\"]\n",
    "\n",
    "# This contains the **Open, High, Low, Close, Volume** for every ticker that was selected, exactly as it existed during the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04be6fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_analyzer_structure(analyzer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733d9650",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_analyzer_structure(analyzer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c3107c",
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_analyzer_short(analyzer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c99497c",
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_analyzer_long(analyzer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426d10b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_analyzer_short(analyzer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335f6a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_analyzer_long(analyzer2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
