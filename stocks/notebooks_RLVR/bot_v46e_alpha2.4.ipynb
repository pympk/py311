{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1110f2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from dataclasses import dataclass, field, asdict, is_dataclass\n",
    "from typing import List, Dict, Optional, Any, Union, TypedDict\n",
    "from collections import Counter\n",
    "from datetime import datetime, date\n",
    "from pandas.testing import assert_series_equal\n",
    "\n",
    "\n",
    "# pd.set_option('display.max_rows', None)  display all rows\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", 1000)\n",
    "pd.set_option(\"display.max_colwidth\", 50)\n",
    "pd.set_option(\"display.precision\", 4)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# GLOBAL SETTINGS: The \"Control Panel\" for the Strategy\n",
    "# ==============================================================================\n",
    "\n",
    "GLOBAL_SETTINGS = {\n",
    "    # ENVIRONMENT (The \"Where\")\n",
    "    \"benchmark_ticker\": \"SPY\",\n",
    "    \"calendar_ticker\": \"SPY\",  # Used as the \"Master Clock\" for trading days\n",
    "    # DATA SANITIZER (The \"Glitches & Gaps\" Protector)\n",
    "    \"handle_zeros_as_nan\": True,  # Convert 0.0 prices to NaN to prevent math errors\n",
    "    \"max_data_gap_ffill\": 1,  # Max consecutive days to \"Forward Fill\" missing data\n",
    "    # IMPLICATION OF nan_price_replacement:\n",
    "    # - This defines what happens if the \"Forward Fill\" limit is exceeded.\n",
    "    # - If set to 0.0: A permanent data gap will look like a \"total loss\" (-100%).\n",
    "    #   The equity curve will plummet. Good for \"disaster detection.\"\n",
    "    #   Sharpe and Sharpe(ATR) drop because: return (gets smaller) / std (gets larger)\n",
    "    # - If set to np.nan: A permanent gap will cause portfolio calculations to return NaN.\n",
    "    #   The chart may break or show gaps. Good for \"math integrity.\"\n",
    "    \"nan_price_replacement\": 0.0,\n",
    "    # STRATEGY PARAMETERS (The \"How\")\n",
    "    \"atr_period\": 14,  # Used for volatility normalization\n",
    "    \"quality_window\": 252,  # 1 year lookback for liquidity/quality stats\n",
    "    \"quality_min_periods\": 126,  # Min history required to judge a stock\n",
    "    # QUALITY THRESHOLDS (The \"Rules\")\n",
    "    \"thresholds\": {\n",
    "        # HARD LIQUIDITY FLOOR\n",
    "        # Logic: Calculates (Adj Close * Volume) daily, then takes the ROLLING MEDIAN\n",
    "        # over the quality_window (252 days). Filters out stocks where the\n",
    "        # typical daily dollar turnover is below this absolute value.\n",
    "        \"min_median_dollar_volume\": 1_000_000,\n",
    "        # DYNAMIC LIQUIDITY CUTOFF (Relative to Universe)\n",
    "        # Logic: On the decision date, the engine calculates the X-quantile\n",
    "        # of 'RollMedDollarVol' across ALL available stocks.\n",
    "        # Setting this to 0.40 calculates the 60th percentile and requires\n",
    "        # stocks to be above it‚Äîeffectively keeping only the TOP 60% of the market.\n",
    "        \"min_liquidity_percentile\": 0.40,\n",
    "        # PRICE/VOLUME STALENESS\n",
    "        # Logic: Creates a binary flag (1 if Volume is 0 OR High equals Low).\n",
    "        # It then calculates the ROLLING MEAN of this flag.\n",
    "        # A value of 0.05 means the stock is rejected if it was \"stale\"\n",
    "        # for more than 5% of the trading days in the rolling window.\n",
    "        \"max_stale_pct\": 0.05,\n",
    "        # DATA INTEGRITY (FROZEN VOLUME)\n",
    "        # Logic: Checks if Volume is identical to the previous day (Volume.diff() == 0).\n",
    "        # It calculates the ROLLING SUM of these occurrences over the window.\n",
    "        # If the exact same volume is reported more than 10 times, the stock\n",
    "        # is rejected as having \"frozen\" or low-quality data.\n",
    "        \"max_same_vol_count\": 10,\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION A: CORE HELPER FUNCTIONS & FEATURE GENERATION\n",
    "# (Unchanged from previous version)\n",
    "# ==============================================================================\n",
    "# ... (Keep generate_features, calculate_gain, calculate_sharpe,\n",
    "#      calculate_sharpe_atr, calculate_buy_and_hold_performance as is) ...\n",
    "\n",
    "\n",
    "def generate_features(\n",
    "    df_ohlcv: pd.DataFrame,\n",
    "    atr_period: int = 14,\n",
    "    quality_window: int = 252,\n",
    "    quality_min_periods: int = 126,\n",
    ") -> pd.DataFrame:\n",
    "    # 1. Sort and Group\n",
    "    if not df_ohlcv.index.is_monotonic_increasing:\n",
    "        df_ohlcv = df_ohlcv.sort_index()\n",
    "    grouped = df_ohlcv.groupby(level=\"Ticker\")\n",
    "\n",
    "    # 2. ATR Calculation (Existing)\n",
    "    prev_close = grouped[\"Adj Close\"].shift(1)\n",
    "    tr = pd.concat(\n",
    "        [\n",
    "            df_ohlcv[\"Adj High\"] - df_ohlcv[\"Adj Low\"],\n",
    "            abs(df_ohlcv[\"Adj High\"] - prev_close),\n",
    "            abs(df_ohlcv[\"Adj Low\"] - prev_close),\n",
    "        ],\n",
    "        axis=1,\n",
    "    ).max(axis=1, skipna=False)\n",
    "\n",
    "    atr = tr.groupby(level=\"Ticker\").transform(\n",
    "        lambda x: x.ewm(alpha=1 / atr_period, adjust=False).mean()\n",
    "    )\n",
    "    atrp = (atr / df_ohlcv[\"Adj Close\"]).replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    # 3. --- NEW: MOMENTUM / RETURN FEATURES ---\n",
    "    # We calculate percentage change for specific windows useful for Pullbacks (3D, 5D) and Trends (21D)\n",
    "    # Note: We use grouped.pct_change to respect Ticker boundaries\n",
    "    roc_1 = grouped[\"Adj Close\"].pct_change(1)\n",
    "    roc_3 = grouped[\"Adj Close\"].pct_change(3)\n",
    "    roc_5 = grouped[\"Adj Close\"].pct_change(5)\n",
    "    roc_10 = grouped[\"Adj Close\"].pct_change(10)\n",
    "    roc_21 = grouped[\"Adj Close\"].pct_change(21)\n",
    "\n",
    "    indicator_df = pd.DataFrame(\n",
    "        {\n",
    "            \"ATR\": atr,\n",
    "            \"ATRP\": atrp,\n",
    "            \"ROC_1\": roc_1,\n",
    "            \"ROC_3\": roc_3,\n",
    "            \"ROC_5\": roc_5,\n",
    "            \"ROC_10\": roc_10,\n",
    "            \"ROC_21\": roc_21,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # 4. Quality/Liquidity Features (Existing)\n",
    "    quality_temp_df = pd.DataFrame(\n",
    "        {\n",
    "            \"IsStale\": np.where(\n",
    "                (df_ohlcv[\"Volume\"] == 0)\n",
    "                | (df_ohlcv[\"Adj High\"] == df_ohlcv[\"Adj Low\"]),\n",
    "                1,\n",
    "                0,\n",
    "            ),\n",
    "            \"DollarVolume\": df_ohlcv[\"Adj Close\"] * df_ohlcv[\"Volume\"],\n",
    "            \"HasSameVolume\": (grouped[\"Volume\"].diff() == 0).astype(int),\n",
    "        },\n",
    "        index=df_ohlcv.index,\n",
    "    )\n",
    "\n",
    "    rolling_result = (\n",
    "        quality_temp_df.groupby(level=\"Ticker\")\n",
    "        .rolling(window=quality_window, min_periods=quality_min_periods)\n",
    "        .agg({\"IsStale\": \"mean\", \"DollarVolume\": \"median\", \"HasSameVolume\": \"sum\"})\n",
    "        .rename(\n",
    "            columns={\n",
    "                \"IsStale\": \"RollingStalePct\",\n",
    "                \"DollarVolume\": \"RollMedDollarVol\",\n",
    "                \"HasSameVolume\": \"RollingSameVolCount\",\n",
    "            }\n",
    "        )\n",
    "        .reset_index(level=0, drop=True)\n",
    "    )\n",
    "\n",
    "    # 5. Merge\n",
    "    return pd.concat([indicator_df, rolling_result], axis=1)\n",
    "\n",
    "\n",
    "def calculate_buy_and_hold_performance(\n",
    "    df_close, features_df, tickers, start_date, end_date\n",
    "):\n",
    "    if not tickers:\n",
    "        return pd.Series(dtype=float), pd.Series(dtype=float), pd.Series(dtype=float)\n",
    "    ticker_counts = Counter(tickers)\n",
    "    initial_weights = pd.Series({t: c / len(tickers) for t, c in ticker_counts.items()})\n",
    "    prices_raw = df_close[initial_weights.index.tolist()].loc[start_date:end_date]\n",
    "    if prices_raw.dropna(how=\"all\").empty:\n",
    "        return pd.Series(dtype=float), pd.Series(dtype=float), pd.Series(dtype=float)\n",
    "    prices_norm = prices_raw.div(prices_raw.bfill().iloc[0])\n",
    "    weighted_growth = prices_norm.mul(initial_weights, axis=\"columns\")\n",
    "    value_series = weighted_growth.sum(axis=1)\n",
    "    return_series = value_series.ffill().pct_change()\n",
    "    full_idx = pd.MultiIndex.from_product(\n",
    "        [initial_weights.index.tolist(), return_series.index], names=[\"Ticker\", \"Date\"]\n",
    "    )\n",
    "    feat_subset = features_df.reindex(full_idx)[\"ATRP\"].unstack(level=\"Ticker\")\n",
    "    atrp_series = (\n",
    "        weighted_growth.div(value_series, axis=\"index\").align(\n",
    "            feat_subset, join=\"inner\", axis=1\n",
    "        )[0]\n",
    "        * weighted_growth.div(value_series, axis=\"index\").align(\n",
    "            feat_subset, join=\"inner\", axis=1\n",
    "        )[1]\n",
    "    ).sum(axis=1)\n",
    "    return value_series, return_series, atrp_series\n",
    "\n",
    "\n",
    "def calculate_summary_gain(price_series: pd.Series) -> float:\n",
    "    \"\"\"REPORTING: Returns the total return of a single series.\"\"\"\n",
    "    if price_series.dropna().shape[0] < 2:\n",
    "        return 0.0\n",
    "    # (Final Price / Starting Price) - 1\n",
    "    res = (price_series.ffill().iloc[-1] / price_series.bfill().iloc[0]) - 1\n",
    "    return float(res) if np.isfinite(res) else 0.0\n",
    "\n",
    "\n",
    "def calculate_cross_sectional_gain(price_df: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"RANKING: Returns the total return for every ticker in the universe.\"\"\"\n",
    "    if price_df.empty:\n",
    "        return pd.Series(dtype=float)\n",
    "    # Vectorized calculation across all columns (tickers)\n",
    "    res = (price_df.ffill().iloc[-1] / price_df.bfill().iloc[0]) - 1\n",
    "    return res.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "\n",
    "\n",
    "def calculate_summary_sharpe(return_series: pd.Series) -> float:\n",
    "    \"\"\"REPORTING: Returns a single Reward value.\"\"\"\n",
    "    if return_series.dropna().shape[0] < 2:\n",
    "        return 0.0\n",
    "    mu, std = return_series.mean(), return_series.std()\n",
    "\n",
    "    # SENIOR FIX: Volatility floor to prevent 'Infinity' or 'Exploding' rewards\n",
    "    if std < 1e-6:\n",
    "        return 0.0\n",
    "\n",
    "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "        res = (mu / std) * np.sqrt(252)\n",
    "    return float(res) if np.isfinite(res) else 0.0\n",
    "\n",
    "\n",
    "def calculate_cross_sectional_sharpe(return_df: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"RANKING: Returns a Series of values for the whole universe.\"\"\"\n",
    "    if return_df.empty:\n",
    "        return pd.Series(dtype=float)\n",
    "    mu, std = return_df.mean(), return_df.std()\n",
    "\n",
    "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "        res = (mu / std) * np.sqrt(252)\n",
    "\n",
    "    # SENIOR FIX: Convert 'Broken' data (std=0) into 0.0 reward\n",
    "    return res.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "\n",
    "\n",
    "def calculate_summary_sharpe_atr(\n",
    "    return_series: pd.Series, atrp_input: Union[pd.Series, float]\n",
    ") -> float:\n",
    "    \"\"\"REPORTING: Returns a single Reward value normalized by Volatility.\"\"\"\n",
    "    if return_series.dropna().shape[0] < 2:\n",
    "        return 0.0\n",
    "    avg_atrp = atrp_input.mean() if hasattr(atrp_input, \"mean\") else atrp_input\n",
    "\n",
    "    if avg_atrp < 1e-6:\n",
    "        return 0.0  # Safety floor\n",
    "\n",
    "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "        res = return_series.mean() / avg_atrp\n",
    "    return float(res) if np.isfinite(res) else 0.0\n",
    "\n",
    "\n",
    "def calculate_cross_sectional_sharpe_atr(\n",
    "    return_df: pd.DataFrame, atrp_series: pd.Series\n",
    ") -> pd.Series:\n",
    "    \"\"\"RANKING: Returns a Series of Volatility-normalized values.\"\"\"\n",
    "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "        res = return_df.mean() / atrp_series\n",
    "    return res.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION B: METRIC REGISTRY (UPDATED VARIABLES)\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "class MarketObservation(TypedDict):\n",
    "    \"\"\"\n",
    "    The 'STATE' (Observation) in Reinforcement Learning.\n",
    "    This defines the context given to the agent to make a decision.\n",
    "    \"\"\"\n",
    "\n",
    "    lookback_returns: pd.DataFrame  # (Time x Tickers)\n",
    "    lookback_close: pd.DataFrame  # (Time x Tickers)\n",
    "    atrp: pd.Series  # (Tickers,) - The mean ATR% over lookback\n",
    "    roc_1: pd.Series  # (Tickers,) - Current 1D Momentum\n",
    "    roc_3: pd.Series  # ... etc\n",
    "    roc_5: pd.Series\n",
    "    roc_10: pd.Series\n",
    "    roc_21: pd.Series\n",
    "\n",
    "\n",
    "# Use the centralized helper functions for calculations\n",
    "\n",
    "\n",
    "def metric_price(obs: MarketObservation) -> pd.Series:\n",
    "    return calculate_cross_sectional_gain(obs[\"lookback_close\"])\n",
    "\n",
    "\n",
    "def metric_sharpe(obs: MarketObservation) -> pd.Series:\n",
    "    return calculate_cross_sectional_sharpe(obs[\"lookback_returns\"])\n",
    "\n",
    "\n",
    "def metric_sharpe_atr(obs: MarketObservation) -> pd.Series:\n",
    "    return calculate_cross_sectional_sharpe_atr(obs[\"lookback_returns\"], obs[\"atrp\"])\n",
    "\n",
    "\n",
    "METRIC_REGISTRY = {\n",
    "    \"Price\": metric_price,\n",
    "    \"Sharpe\": metric_sharpe,\n",
    "    \"Sharpe (ATR)\": metric_sharpe_atr,\n",
    "    \"Momentum 1D\": lambda obs: obs[\"roc_1\"],\n",
    "    \"Momentum 3D\": lambda obs: obs[\"roc_3\"],\n",
    "    \"Momentum 5D\": lambda obs: obs[\"roc_5\"],\n",
    "    \"Momentum 10D\": lambda obs: obs[\"roc_10\"],\n",
    "    \"Momentum 1M\": lambda obs: obs[\"roc_21\"],\n",
    "    \"Pullback 1D\": lambda obs: -obs[\"roc_1\"],\n",
    "    \"Pullback 3D\": lambda obs: -obs[\"roc_3\"],\n",
    "    \"Pullback 5D\": lambda obs: -obs[\"roc_5\"],\n",
    "    \"Pullback 10D\": lambda obs: -obs[\"roc_10\"],\n",
    "    \"Pullback 1M\": lambda obs: -obs[\"roc_21\"],\n",
    "}\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION C: DATA CONTRACTS (UPDATED v2.2 - Verification Ready)\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EngineInput:\n",
    "    mode: str\n",
    "    start_date: pd.Timestamp\n",
    "    lookback_period: int\n",
    "    holding_period: int\n",
    "    metric: str\n",
    "    benchmark_ticker: str\n",
    "    rank_start: int = 1\n",
    "    rank_end: int = 10\n",
    "    # Default factory pulls from Global thresholds\n",
    "    quality_thresholds: Dict[str, float] = field(\n",
    "        default_factory=lambda: GLOBAL_SETTINGS[\"thresholds\"].copy()\n",
    "    )\n",
    "    manual_tickers: List[str] = field(default_factory=list)\n",
    "    debug: bool = False\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EngineOutput:\n",
    "    portfolio_series: pd.Series\n",
    "    benchmark_series: pd.Series\n",
    "    normalized_plot_data: pd.DataFrame\n",
    "    tickers: List[str]\n",
    "    initial_weights: pd.Series\n",
    "    perf_metrics: Dict[str, float]\n",
    "    results_df: pd.DataFrame\n",
    "\n",
    "    # Dates\n",
    "    start_date: pd.Timestamp\n",
    "    decision_date: pd.Timestamp\n",
    "    buy_date: pd.Timestamp\n",
    "    holding_end_date: pd.Timestamp\n",
    "\n",
    "    error_msg: Optional[str] = None\n",
    "    debug_data: Optional[Dict[str, Any]] = None\n",
    "\n",
    "\n",
    "class AlphaEngine:\n",
    "    def __init__(\n",
    "        self,\n",
    "        df_ohlcv: pd.DataFrame,\n",
    "        features_df: pd.DataFrame = None,\n",
    "        df_close_wide: pd.DataFrame = None,\n",
    "        master_ticker: str = GLOBAL_SETTINGS[\"calendar_ticker\"],\n",
    "    ):\n",
    "        print(\"--- ‚öôÔ∏è Initializing AlphaEngine v2.2 (Sanitized) ---\")\n",
    "\n",
    "        # 1. SETUP PRICES (CLEAN-AT-ENTRY)\n",
    "        if df_close_wide is not None:\n",
    "            self.df_close = df_close_wide\n",
    "        else:\n",
    "            print(\"üê¢ Pivoting and Sanitizing Price Data...\")\n",
    "            self.df_close = df_ohlcv[\"Adj Close\"].unstack(level=0)\n",
    "\n",
    "        # APPLY DATA SANITIZER LOGIC\n",
    "        if GLOBAL_SETTINGS[\"handle_zeros_as_nan\"]:\n",
    "            # Replace 0.0 with NaN so math functions (mean/std) ignore them\n",
    "            self.df_close = self.df_close.replace(0, np.nan)\n",
    "\n",
    "        # Smooth over 1-2 day glitches (The \"FNV\" Fix)\n",
    "        self.df_close = self.df_close.ffill(limit=GLOBAL_SETTINGS[\"max_data_gap_ffill\"])\n",
    "\n",
    "        # Handle the remaining \"unfillable\" gaps\n",
    "        self.df_close = self.df_close.fillna(GLOBAL_SETTINGS[\"nan_price_replacement\"])\n",
    "\n",
    "        # 2. SETUP FEATURES\n",
    "        if features_df is not None:\n",
    "            self.features_df = features_df\n",
    "        else:\n",
    "            # We pass the cleaned price data if needed, or calculate from raw\n",
    "            self.features_df = generate_features(\n",
    "                df_ohlcv,\n",
    "                atr_period=GLOBAL_SETTINGS[\"atr_period\"],\n",
    "                quality_window=GLOBAL_SETTINGS[\"quality_window\"],\n",
    "                quality_min_periods=GLOBAL_SETTINGS[\"quality_min_periods\"],\n",
    "            )\n",
    "\n",
    "        # 3. Setup Calendar\n",
    "        if master_ticker not in self.df_close.columns:\n",
    "            master_ticker = self.df_close.columns[0]\n",
    "        self.trading_calendar = (\n",
    "            self.df_close[master_ticker].dropna().index.unique().sort_values()\n",
    "        )\n",
    "\n",
    "    # def run(self, inputs: EngineInput) -> EngineOutput:\n",
    "    #     # ... (Step 1 & 2 are unchanged) ...\n",
    "    #     dates, error = self._validate_timeline(inputs)\n",
    "    #     if error: return self._error_result(error)\n",
    "    #     (safe_start, safe_decision, safe_buy, safe_end) = dates\n",
    "\n",
    "    #     tickers_to_trade, results_table, debug_dict, error = self._select_tickers(\n",
    "    #         inputs, safe_start, safe_decision\n",
    "    #     )\n",
    "    #     if error: return self._error_result(error)\n",
    "\n",
    "    #     # --- Step 3: Generate Two Independent Performance Tracks ---\n",
    "\n",
    "    #     # TRACK A: Continuous Drift (For 'Full' and 'Lookback' metrics)\n",
    "    #     p_full_val, p_full_ret, p_full_atrp = calculate_buy_and_hold_performance(\n",
    "    #         self.df_close, self.features_df, tickers_to_trade, safe_start, safe_end\n",
    "    #     )\n",
    "    #     b_full_val, b_full_ret, b_full_atrp = calculate_buy_and_hold_performance(\n",
    "    #         self.df_close, self.features_df, [inputs.benchmark_ticker], safe_start, safe_end\n",
    "    #     )\n",
    "\n",
    "    #     # TRACK B: Independent Holding (RE-INITIALIZED Weights for 'Holding' metrics)\n",
    "    #     p_hd_val, p_hd_ret, p_hd_atrp = calculate_buy_and_hold_performance(\n",
    "    #         self.df_close, self.features_df, tickers_to_trade, safe_buy, safe_end\n",
    "    #     )\n",
    "    #     b_hd_val, b_hd_ret, b_hd_atrp = calculate_buy_and_hold_performance(\n",
    "    #         self.df_close, self.features_df, [inputs.benchmark_ticker], safe_buy, safe_end\n",
    "    #     )\n",
    "\n",
    "    #     # --- Step 4: Calculate Metrics (Directly from streams, no linking) ---\n",
    "    #     metrics = {}\n",
    "\n",
    "    #     p_metrics, p_slices = self._calculate_period_metrics(\n",
    "    #         p_full_val, p_full_ret, p_full_atrp, safe_decision,\n",
    "    #         p_hd_val, p_hd_ret, p_hd_atrp, prefix=\"p\"\n",
    "    #     )\n",
    "    #     metrics.update(p_metrics)\n",
    "\n",
    "    #     b_metrics, b_slices = self._calculate_period_metrics(\n",
    "    #         b_full_val, b_full_ret, b_full_atrp, safe_decision,\n",
    "    #         b_hd_val, b_hd_ret, b_hd_atrp, prefix=\"b\"\n",
    "    #     )\n",
    "    #     metrics.update(b_metrics)\n",
    "\n",
    "    #     # --- Step 5: Final Packaging ---\n",
    "    #     # The Plotly chart will show the continuous drifted path (Track A)\n",
    "    #     debug_dict[\"verification\"] = {\"portfolio\": p_slices, \"benchmark\": b_slices}\n",
    "\n",
    "    #     # For the Results Table, we show the re-initialized trade gain\n",
    "    #     results_table[\"Holding Gain\"] = (p_hd_val.iloc[-1] / p_hd_val.iloc[0]) - 1\n",
    "\n",
    "    #     return EngineOutput(\n",
    "    #         portfolio_series=p_full_val,\n",
    "    #         benchmark_series=b_full_val,\n",
    "    #         normalized_plot_data=self._get_normalized_plot_data(tickers_to_trade, safe_start, safe_end),\n",
    "    #         tickers=tickers_to_trade,\n",
    "    #         initial_weights=pd.Series({t: 1/len(tickers_to_trade) for t in tickers_to_trade}),\n",
    "    #         perf_metrics=metrics,\n",
    "    #         results_df=results_table,\n",
    "    #         start_date=safe_start,\n",
    "    #         decision_date=safe_decision,\n",
    "    #         buy_date=safe_buy,\n",
    "    #         holding_end_date=safe_end,\n",
    "    #         error_msg=None,\n",
    "    #         debug_data=debug_dict,\n",
    "    #     )\n",
    "\n",
    "    def run(self, inputs: EngineInput) -> EngineOutput:\n",
    "        dates, error = self._validate_timeline(inputs)\n",
    "        if error:\n",
    "            return self._error_result(error)\n",
    "        (safe_start, safe_decision, safe_buy, safe_end) = dates\n",
    "\n",
    "        tickers_to_trade, results_table, debug_dict, error = self._select_tickers(\n",
    "            inputs, safe_start, safe_decision\n",
    "        )\n",
    "        if error:\n",
    "            return self._error_result(error)\n",
    "\n",
    "        # GENERATE TRACKS\n",
    "        p_f_val, p_f_ret, p_f_atrp = calculate_buy_and_hold_performance(\n",
    "            self.df_close, self.features_df, tickers_to_trade, safe_start, safe_end\n",
    "        )\n",
    "        b_f_val, b_f_ret, b_f_atrp = calculate_buy_and_hold_performance(\n",
    "            self.df_close,\n",
    "            self.features_df,\n",
    "            [inputs.benchmark_ticker],\n",
    "            safe_start,\n",
    "            safe_end,\n",
    "        )\n",
    "\n",
    "        p_h_val, p_h_ret, p_h_atrp = calculate_buy_and_hold_performance(\n",
    "            self.df_close, self.features_df, tickers_to_trade, safe_buy, safe_end\n",
    "        )\n",
    "        b_h_val, b_h_ret, b_h_atrp = calculate_buy_and_hold_performance(\n",
    "            self.df_close,\n",
    "            self.features_df,\n",
    "            [inputs.benchmark_ticker],\n",
    "            safe_buy,\n",
    "            safe_end,\n",
    "        )\n",
    "\n",
    "        # CALCULATE METRICS\n",
    "        p_metrics, p_slices = self._calculate_period_metrics(\n",
    "            p_f_val,\n",
    "            p_f_ret,\n",
    "            p_f_atrp,\n",
    "            safe_decision,\n",
    "            p_h_val,\n",
    "            p_h_ret,\n",
    "            p_h_atrp,\n",
    "            prefix=\"p\",\n",
    "        )\n",
    "        b_metrics, b_slices = self._calculate_period_metrics(\n",
    "            b_f_val,\n",
    "            b_f_ret,\n",
    "            b_f_atrp,\n",
    "            safe_decision,\n",
    "            b_h_val,\n",
    "            b_h_ret,\n",
    "            b_h_atrp,\n",
    "            prefix=\"b\",\n",
    "        )\n",
    "\n",
    "        # CONSOLIDATE DEBUG DATA\n",
    "        debug_dict[\"verification\"] = {\"portfolio\": p_slices, \"benchmark\": b_slices}\n",
    "\n",
    "        # ADD RAW COMPONENT EXPORTS\n",
    "        debug_dict[\"portfolio_raw_components\"] = {\n",
    "            \"prices\": self.df_close[tickers_to_trade].loc[safe_start:safe_end],\n",
    "            \"atrp\": self.features_df.loc[\n",
    "                (tickers_to_trade, slice(safe_start, safe_end)), \"ATRP\"\n",
    "            ].unstack(level=0),\n",
    "        }\n",
    "        debug_dict[\"benchmark_raw_components\"] = {\n",
    "            \"prices\": self.df_close[[inputs.benchmark_ticker]].loc[safe_start:safe_end],\n",
    "            \"atrp\": self.features_df.loc[\n",
    "                ([inputs.benchmark_ticker], slice(safe_start, safe_end)), \"ATRP\"\n",
    "            ].unstack(level=0),\n",
    "        }\n",
    "\n",
    "        # FINAL OUTPUT\n",
    "        results_table[\"Holding Gain\"] = (p_h_val.iloc[-1] / p_h_val.iloc[0]) - 1\n",
    "        return EngineOutput(\n",
    "            portfolio_series=p_f_val,\n",
    "            benchmark_series=b_f_val,\n",
    "            normalized_plot_data=self._get_normalized_plot_data(\n",
    "                tickers_to_trade, safe_start, safe_end\n",
    "            ),\n",
    "            tickers=tickers_to_trade,\n",
    "            initial_weights=pd.Series(\n",
    "                {t: 1 / len(tickers_to_trade) for t in tickers_to_trade}\n",
    "            ),\n",
    "            perf_metrics={**p_metrics, **b_metrics},\n",
    "            results_df=results_table,\n",
    "            start_date=safe_start,\n",
    "            decision_date=safe_decision,\n",
    "            buy_date=safe_buy,\n",
    "            holding_end_date=safe_end,\n",
    "            debug_data=debug_dict,\n",
    "        )\n",
    "\n",
    "    # ==============================================================================\n",
    "    # INTERNAL LOGIC MODULES\n",
    "    # ==============================================================================\n",
    "\n",
    "    def _validate_timeline(self, inputs: EngineInput):\n",
    "        cal = self.trading_calendar\n",
    "        last_idx = len(cal) - 1\n",
    "\n",
    "        if len(cal) <= inputs.lookback_period:\n",
    "            return (\n",
    "                None,\n",
    "                f\"‚ùå Dataset too small.\\nNeed > {inputs.lookback_period} days of history.\",\n",
    "            )\n",
    "\n",
    "        # 2. Check \"Past\" Constraints (Lookback)\n",
    "        min_decision_date = cal[inputs.lookback_period]\n",
    "        if inputs.start_date < min_decision_date:\n",
    "            # Added \\n here\n",
    "            return None, (\n",
    "                f\"‚ùå Not enough history for a {inputs.lookback_period}-day lookback.\\n\"\n",
    "                f\"Earliest valid Decision Date: {min_decision_date.date()}\"\n",
    "            )\n",
    "\n",
    "        # 3. Check \"Future\" Constraints (Entry T+1 and Holding Period)\n",
    "        required_future_days = 1 + inputs.holding_period\n",
    "        latest_valid_idx = last_idx - required_future_days\n",
    "\n",
    "        if latest_valid_idx < 0:\n",
    "            return (\n",
    "                None,\n",
    "                f\"‚ùå Holding period too long.\\n{inputs.holding_period} days exceeds available data.\",\n",
    "            )\n",
    "\n",
    "        # If user picked a date beyond the available \"future\" runway\n",
    "        if inputs.start_date > cal[latest_valid_idx]:\n",
    "            latest_date = cal[latest_valid_idx].date()\n",
    "            # Added \\n here and shortened the text slightly to fit better\n",
    "            return None, (\n",
    "                f\"‚ùå Decision Date too late for a {inputs.holding_period}-day hold.\\n\"\n",
    "                f\"Latest valid date: {latest_date}. Please move picker back.\"\n",
    "            )\n",
    "\n",
    "        # 4. Map the safe indices\n",
    "        decision_idx = cal.searchsorted(inputs.start_date)\n",
    "        if decision_idx > latest_valid_idx:\n",
    "            decision_idx = latest_valid_idx\n",
    "\n",
    "        start_idx = decision_idx - inputs.lookback_period\n",
    "        entry_idx = decision_idx + 1\n",
    "        end_idx = entry_idx + inputs.holding_period\n",
    "\n",
    "        return (cal[start_idx], cal[decision_idx], cal[entry_idx], cal[end_idx]), None\n",
    "\n",
    "    def _select_tickers(self, inputs: EngineInput, start_date, decision_date):\n",
    "        debug_dict = {}\n",
    "        if inputs.mode == \"Manual List\":\n",
    "            validation_errors = []\n",
    "            valid_tickers = []\n",
    "            for t in inputs.manual_tickers:\n",
    "                if t not in self.df_close.columns:\n",
    "                    validation_errors.append(f\"‚ùå {t}: Not found.\")\n",
    "                    continue\n",
    "                if pd.isna(self.df_close.at[start_date, t]):\n",
    "                    validation_errors.append(f\"‚ö†Ô∏è {t}: No data on start date.\")\n",
    "                    continue\n",
    "                valid_tickers.append(t)\n",
    "\n",
    "            if validation_errors:\n",
    "                return [], pd.DataFrame(), {}, \"\\n\".join(validation_errors)\n",
    "            if not valid_tickers:\n",
    "                return [], pd.DataFrame(), {}, \"No valid tickers found.\"\n",
    "            return valid_tickers, pd.DataFrame(index=valid_tickers), {}, None\n",
    "\n",
    "        else:  # Ranking\n",
    "            audit_info = {}\n",
    "            eligible_tickers = self._filter_universe(\n",
    "                decision_date, inputs.quality_thresholds, audit_info\n",
    "            )\n",
    "            debug_dict[\"audit_liquidity\"] = audit_info\n",
    "\n",
    "            if not eligible_tickers:\n",
    "                return (\n",
    "                    [],\n",
    "                    pd.DataFrame(),\n",
    "                    debug_dict,\n",
    "                    \"No tickers passed quality filters.\",\n",
    "                )\n",
    "\n",
    "            lookback_close = self.df_close.loc[\n",
    "                start_date:decision_date, eligible_tickers\n",
    "            ]\n",
    "            idx_product = pd.MultiIndex.from_product(\n",
    "                [eligible_tickers, lookback_close.index], names=[\"Ticker\", \"Date\"]\n",
    "            )\n",
    "\n",
    "            feat_slice_current = self.features_df.xs(\n",
    "                decision_date, level=\"Date\"\n",
    "            ).reindex(eligible_tickers)\n",
    "            feat_slice_period = self.features_df.loc[\n",
    "                (slice(None), lookback_close.index), :\n",
    "            ].reindex(idx_product)\n",
    "            atrp_mean = feat_slice_period[\"ATRP\"].groupby(level=\"Ticker\").mean()\n",
    "\n",
    "            # 1. Package the Observation (The 'State')\n",
    "            observation: MarketObservation = {\n",
    "                \"lookback_close\": lookback_close,\n",
    "                \"lookback_returns\": lookback_close.ffill().pct_change(),\n",
    "                \"atrp\": atrp_mean,\n",
    "                \"roc_1\": feat_slice_current[\"ROC_1\"],\n",
    "                \"roc_3\": feat_slice_current[\"ROC_3\"],\n",
    "                \"roc_5\": feat_slice_current[\"ROC_5\"],\n",
    "                \"roc_10\": feat_slice_current[\"ROC_10\"],\n",
    "                \"roc_21\": feat_slice_current[\"ROC_21\"],\n",
    "            }\n",
    "\n",
    "            # 2. Run the Strategy (The 'Agent')\n",
    "            if inputs.metric not in METRIC_REGISTRY:\n",
    "                return [], pd.DataFrame(), {}, f\"Strategy '{inputs.metric}' not found.\"\n",
    "\n",
    "            metric_vals = METRIC_REGISTRY[inputs.metric](observation)\n",
    "            sorted_tickers = metric_vals.sort_values(ascending=False)\n",
    "            start_r = max(0, inputs.rank_start - 1)\n",
    "            end_r = inputs.rank_end\n",
    "            selected_tickers = sorted_tickers.iloc[start_r:end_r].index.tolist()\n",
    "\n",
    "            # --- VERIFICATION ADDITION: Ranking Audit (Bot Version) ---\n",
    "            debug_dict[\"full_universe_ranking\"] = pd.DataFrame(\n",
    "                {\n",
    "                    \"Strategy_Score\": metric_vals,\n",
    "                    \"Lookback_Return_Ann\": observation[\"lookback_returns\"].mean() * 252,\n",
    "                    \"Lookback_ATRP\": observation[\"atrp\"],\n",
    "                }\n",
    "            )\n",
    "\n",
    "            if not selected_tickers:\n",
    "                return (\n",
    "                    [],\n",
    "                    pd.DataFrame(),\n",
    "                    debug_dict,\n",
    "                    \"No tickers generated from ranking.\",\n",
    "                )\n",
    "\n",
    "            results_table = pd.DataFrame(\n",
    "                {\n",
    "                    \"Rank\": range(\n",
    "                        inputs.rank_start, inputs.rank_start + len(selected_tickers)\n",
    "                    ),\n",
    "                    \"Ticker\": selected_tickers,\n",
    "                    \"Strategy Value\": sorted_tickers.loc[selected_tickers].values,\n",
    "                }\n",
    "            ).set_index(\"Ticker\")\n",
    "\n",
    "            return selected_tickers, results_table, debug_dict, None\n",
    "\n",
    "    def _filter_universe(self, date_ts, thresholds, audit_container=None):\n",
    "        avail_dates = (\n",
    "            self.features_df.index.get_level_values(\"Date\").unique().sort_values()\n",
    "        )\n",
    "        valid_dates = avail_dates[avail_dates <= date_ts]\n",
    "        if valid_dates.empty:\n",
    "            return []\n",
    "        target_date = valid_dates[-1]\n",
    "        day_features = self.features_df.xs(target_date, level=\"Date\")\n",
    "\n",
    "        vol_cutoff = thresholds.get(\"min_median_dollar_volume\", 0)\n",
    "        percentile_used = \"N/A\"\n",
    "        if \"min_liquidity_percentile\" in thresholds:\n",
    "            percentile_used = thresholds[\"min_liquidity_percentile\"]\n",
    "            dynamic_val = day_features[\"RollMedDollarVol\"].quantile(percentile_used)\n",
    "            vol_cutoff = max(vol_cutoff, dynamic_val)\n",
    "\n",
    "        mask = (\n",
    "            (day_features[\"RollMedDollarVol\"] >= vol_cutoff)\n",
    "            & (day_features[\"RollingStalePct\"] <= thresholds[\"max_stale_pct\"])\n",
    "            & (day_features[\"RollingSameVolCount\"] <= thresholds[\"max_same_vol_count\"])\n",
    "        )\n",
    "\n",
    "        if audit_container is not None:\n",
    "            audit_container[\"date\"] = target_date\n",
    "            audit_container[\"total_tickers_available\"] = len(day_features)\n",
    "            audit_container[\"percentile_setting\"] = percentile_used\n",
    "            audit_container[\"final_cutoff_usd\"] = vol_cutoff\n",
    "            audit_container[\"tickers_passed\"] = mask.sum()\n",
    "            snapshot = day_features.copy()\n",
    "            snapshot[\"Calculated_Cutoff\"] = vol_cutoff\n",
    "            snapshot[\"Passed_Vol_Check\"] = snapshot[\"RollMedDollarVol\"] >= vol_cutoff\n",
    "            snapshot[\"Passed_Final\"] = mask\n",
    "            snapshot = snapshot.sort_values(\"RollMedDollarVol\", ascending=False)\n",
    "            audit_container[\"universe_snapshot\"] = snapshot\n",
    "\n",
    "        return day_features[mask].index.tolist()\n",
    "\n",
    "    # def _calculate_period_metrics(\n",
    "    #     self, f_val, f_ret, f_atrp, decision_date, h_val, h_ret, h_atrp, prefix\n",
    "    # ):\n",
    "    #     metrics = {}\n",
    "    #     slices = {}\n",
    "\n",
    "    #     # 1. Slice 'Full' stream to get the Lookback period\n",
    "    #     lb_val = f_val.loc[:decision_date]\n",
    "    #     lb_ret = f_ret.loc[:decision_date]\n",
    "    #     lb_atrp = f_atrp.loc[:decision_date]\n",
    "\n",
    "    #     # 2. FULL (Drifted Path)\n",
    "    #     metrics[f\"full_{prefix}_gain\"] = calculate_summary_gain(f_val)\n",
    "    #     metrics[f\"full_{prefix}_sharpe\"] = calculate_summary_sharpe(f_ret)\n",
    "    #     metrics[f\"full_{prefix}_sharpe_atr\"] = calculate_summary_sharpe_atr(f_ret, f_atrp)\n",
    "\n",
    "    #     # 3. LOOKBACK (Drifted Path)\n",
    "    #     metrics[f\"lookback_{prefix}_gain\"] = calculate_summary_gain(lb_val)\n",
    "    #     metrics[f\"lookback_{prefix}_sharpe\"] = calculate_summary_sharpe(lb_ret)\n",
    "    #     metrics[f\"lookback_{prefix}_sharpe_atr\"] = calculate_summary_sharpe_atr(lb_ret, lb_atrp)\n",
    "\n",
    "    #     # 4. HOLDING (Fresh Re-Initialized Path)\n",
    "    #     metrics[f\"holding_{prefix}_gain\"] = calculate_summary_gain(h_val)\n",
    "    #     metrics[f\"holding_{prefix}_sharpe\"] = calculate_summary_sharpe(h_ret)\n",
    "    #     metrics[f\"holding_{prefix}_sharpe_atr\"] = calculate_summary_sharpe_atr(h_ret, h_atrp)\n",
    "\n",
    "    #     # Store for audit\n",
    "    #     slices[\"lookback_val\"], slices[\"lookback_ret\"] = lb_val, lb_ret\n",
    "    #     slices[\"holding_val\"], slices[\"holding_ret\"], slices[\"holding_atrp\"] = h_val, h_ret, h_atrp\n",
    "\n",
    "    #     return metrics, slices\n",
    "\n",
    "    def _calculate_period_metrics(\n",
    "        self, f_val, f_ret, f_atrp, decision_date, h_val, h_ret, h_atrp, prefix\n",
    "    ):\n",
    "        metrics = {}\n",
    "        slices = {}\n",
    "\n",
    "        # Slices for Lookback (Derived from 'Full' track)\n",
    "        lb_val = f_val.loc[:decision_date]\n",
    "        lb_ret = f_ret.loc[:decision_date]\n",
    "        lb_atrp = f_atrp.loc[:decision_date]\n",
    "\n",
    "        # 1. GAIN\n",
    "        metrics[f\"full_{prefix}_gain\"] = calculate_summary_gain(f_val)\n",
    "        metrics[f\"lookback_{prefix}_gain\"] = calculate_summary_gain(lb_val)\n",
    "        metrics[f\"holding_{prefix}_gain\"] = calculate_summary_gain(h_val)\n",
    "\n",
    "        # 2. SHARPE\n",
    "        metrics[f\"full_{prefix}_sharpe\"] = calculate_summary_sharpe(f_ret)\n",
    "        metrics[f\"lookback_{prefix}_sharpe\"] = calculate_summary_sharpe(lb_ret)\n",
    "        metrics[f\"holding_{prefix}_sharpe\"] = calculate_summary_sharpe(h_ret)\n",
    "\n",
    "        # 3. SHARPE (ATR)\n",
    "        metrics[f\"full_{prefix}_sharpe_atr\"] = calculate_summary_sharpe_atr(\n",
    "            f_ret, f_atrp\n",
    "        )\n",
    "        metrics[f\"lookback_{prefix}_sharpe_atr\"] = calculate_summary_sharpe_atr(\n",
    "            lb_ret, lb_atrp\n",
    "        )\n",
    "        metrics[f\"holding_{prefix}_sharpe_atr\"] = calculate_summary_sharpe_atr(\n",
    "            h_ret, h_atrp\n",
    "        )\n",
    "\n",
    "        # 4. CAPTURE ALL SLICES FOR EXPORT (This was what was missing)\n",
    "        slices[f\"full_val\"] = f_val\n",
    "        slices[f\"full_ret\"] = f_ret\n",
    "        slices[f\"full_atrp\"] = f_atrp\n",
    "\n",
    "        slices[f\"lookback_val\"] = lb_val\n",
    "        slices[f\"lookback_ret\"] = lb_ret\n",
    "        slices[f\"lookback_atrp\"] = lb_atrp\n",
    "\n",
    "        slices[f\"holding_val\"] = h_val\n",
    "        slices[f\"holding_ret\"] = h_ret\n",
    "        slices[f\"holding_atrp\"] = h_atrp\n",
    "\n",
    "        return metrics, slices\n",
    "\n",
    "    def _get_normalized_plot_data(self, tickers, start_date, end_date):\n",
    "        if not tickers:\n",
    "            return pd.DataFrame()\n",
    "        data = self.df_close[list(set(tickers))].loc[start_date:end_date]\n",
    "        if data.empty:\n",
    "            return pd.DataFrame()\n",
    "        return data / data.bfill().iloc[0]\n",
    "\n",
    "    def _error_result(self, msg):\n",
    "        return EngineOutput(\n",
    "            portfolio_series=pd.Series(dtype=float),\n",
    "            benchmark_series=pd.Series(dtype=float),\n",
    "            normalized_plot_data=pd.DataFrame(),\n",
    "            tickers=[],\n",
    "            initial_weights=pd.Series(dtype=float),\n",
    "            perf_metrics={},\n",
    "            results_df=pd.DataFrame(),\n",
    "            start_date=pd.Timestamp.min,\n",
    "            decision_date=pd.Timestamp.min,\n",
    "            buy_date=pd.Timestamp.min,\n",
    "            holding_end_date=pd.Timestamp.min,\n",
    "            error_msg=msg,\n",
    "        )\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION E: THE UI (Visualization) - UPDATED v2.4 (Complete Timeline)\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "def plot_walk_forward_analyzer(\n",
    "    df_ohlcv,\n",
    "    precomputed_features=None,\n",
    "    precomputed_close=None,\n",
    "    default_start_date=\"2025-01-17\",\n",
    "    default_lookback=10,\n",
    "    default_holding=5,\n",
    "    default_strategy=\"Sharpe (ATR)\",\n",
    "    default_rank_start=1,\n",
    "    default_rank_end=10,\n",
    "    default_benchmark_ticker=GLOBAL_SETTINGS[\"benchmark_ticker\"],\n",
    "    master_calendar_ticker=GLOBAL_SETTINGS[\"calendar_ticker\"],\n",
    "    quality_thresholds=GLOBAL_SETTINGS[\"thresholds\"],\n",
    "    debug=False,\n",
    "):\n",
    "\n",
    "    engine = AlphaEngine(\n",
    "        df_ohlcv,\n",
    "        features_df=precomputed_features,\n",
    "        df_close_wide=precomputed_close,\n",
    "        master_ticker=master_calendar_ticker,\n",
    "    )\n",
    "\n",
    "    # Initialize containers\n",
    "    results_container = [None]\n",
    "    debug_container = [{}]\n",
    "\n",
    "    # If no thresholds passed, use the global Source of Truth\n",
    "    if quality_thresholds is None:\n",
    "        quality_thresholds = GLOBAL_SETTINGS[\"thresholds\"]\n",
    "\n",
    "    # --- Widgets ---\n",
    "    mode_selector = widgets.RadioButtons(\n",
    "        options=[\"Ranking\", \"Manual List\"],\n",
    "        value=\"Ranking\",\n",
    "        description=\"Mode:\",\n",
    "        layout={\"width\": \"max-content\"},\n",
    "        style={\"description_width\": \"initial\"},\n",
    "    )\n",
    "    lookback_input = widgets.IntText(\n",
    "        value=default_lookback,\n",
    "        description=\"Lookback (Days):\",\n",
    "        layout={\"width\": \"200px\"},\n",
    "        style={\"description_width\": \"initial\"},\n",
    "    )\n",
    "    decision_date_picker = widgets.DatePicker(\n",
    "        description=\"Decision Date:\",\n",
    "        value=pd.to_datetime(default_start_date),\n",
    "        layout={\"width\": \"auto\"},\n",
    "        style={\"description_width\": \"initial\"},\n",
    "    )\n",
    "    holding_input = widgets.IntText(\n",
    "        value=default_holding,\n",
    "        description=\"Holding (Days):\",\n",
    "        layout={\"width\": \"200px\"},\n",
    "        style={\"description_width\": \"initial\"},\n",
    "    )\n",
    "    strategy_dropdown = widgets.Dropdown(\n",
    "        options=list(METRIC_REGISTRY.keys()),\n",
    "        value=default_strategy,\n",
    "        description=\"Strategy:\",\n",
    "        layout={\"width\": \"220px\"},\n",
    "        style={\"description_width\": \"initial\"},\n",
    "    )\n",
    "    benchmark_input = widgets.Text(\n",
    "        value=default_benchmark_ticker,\n",
    "        description=\"Benchmark:\",\n",
    "        placeholder=\"Enter Ticker\",\n",
    "        layout={\"width\": \"180px\"},\n",
    "        style={\"description_width\": \"initial\"},\n",
    "    )\n",
    "    rank_start_input = widgets.IntText(\n",
    "        value=default_rank_start,\n",
    "        description=\"Rank Start:\",\n",
    "        layout={\"width\": \"150px\"},\n",
    "        style={\"description_width\": \"initial\"},\n",
    "    )\n",
    "    rank_end_input = widgets.IntText(\n",
    "        value=default_rank_end,\n",
    "        description=\"Rank End:\",\n",
    "        layout={\"width\": \"150px\"},\n",
    "        style={\"description_width\": \"initial\"},\n",
    "    )\n",
    "    manual_tickers_input = widgets.Textarea(\n",
    "        value=\"\",\n",
    "        placeholder=\"Enter tickers...\",\n",
    "        description=\"Manual Tickers:\",\n",
    "        layout={\"width\": \"400px\", \"height\": \"80px\"},\n",
    "        style={\"description_width\": \"initial\"},\n",
    "    )\n",
    "    update_button = widgets.Button(description=\"Run Simulation\", button_style=\"primary\")\n",
    "    ticker_list_output = widgets.Output()\n",
    "\n",
    "    # --- Layouts ---\n",
    "    timeline_box = widgets.HBox(\n",
    "        [lookback_input, decision_date_picker, holding_input],\n",
    "        layout=widgets.Layout(\n",
    "            justify_content=\"space-between\",\n",
    "            border=\"1px solid #ddd\",\n",
    "            padding=\"10px\",\n",
    "            margin=\"5px\",\n",
    "        ),\n",
    "    )\n",
    "    strategy_box = widgets.HBox([strategy_dropdown, benchmark_input])\n",
    "    ranking_box = widgets.HBox([rank_start_input, rank_end_input])\n",
    "\n",
    "    def on_mode_change(c):\n",
    "        ranking_box.layout.display = \"flex\" if c[\"new\"] == \"Ranking\" else \"none\"\n",
    "        manual_tickers_input.layout.display = (\n",
    "            \"none\" if c[\"new\"] == \"Ranking\" else \"flex\"\n",
    "        )\n",
    "        strategy_dropdown.disabled = c[\"new\"] == \"Manual List\"\n",
    "\n",
    "    mode_selector.observe(on_mode_change, names=\"value\")\n",
    "    on_mode_change({\"new\": mode_selector.value})\n",
    "\n",
    "    ui = widgets.VBox(\n",
    "        [\n",
    "            widgets.HTML(\n",
    "                \"<b>1. Timeline Configuration:</b> (Past <--- Decision ---> Future)\"\n",
    "            ),\n",
    "            timeline_box,\n",
    "            widgets.HTML(\"<b>2. Strategy Settings:</b>\"),\n",
    "            widgets.HBox([mode_selector, strategy_box]),\n",
    "            ranking_box,\n",
    "            manual_tickers_input,\n",
    "            widgets.HTML(\"<hr>\"),\n",
    "            update_button,\n",
    "            ticker_list_output,\n",
    "        ],\n",
    "        layout=widgets.Layout(margin=\"10px 0 20px 0\"),\n",
    "    )\n",
    "\n",
    "    fig = go.FigureWidget()\n",
    "    fig.update_layout(\n",
    "        title=\"Event-Driven Walk-Forward Analysis\",\n",
    "        height=600,\n",
    "        template=\"plotly_white\",\n",
    "        hovermode=\"x unified\",\n",
    "    )\n",
    "    for i in range(50):\n",
    "        fig.add_trace(go.Scatter(visible=False, line=dict(width=2)))\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            name=\"Benchmark\",\n",
    "            visible=True,\n",
    "            line=dict(color=\"black\", width=3, dash=\"dash\"),\n",
    "        )\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            name=\"Group Portfolio\", visible=True, line=dict(color=\"green\", width=3)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # --- Update Logic ---\n",
    "    def update_plot(b):\n",
    "        ticker_list_output.clear_output()\n",
    "        manual_list = [\n",
    "            t.strip().upper()\n",
    "            for t in manual_tickers_input.value.split(\",\")\n",
    "            if t.strip()\n",
    "        ]\n",
    "        decision_date_raw = pd.to_datetime(decision_date_picker.value)\n",
    "\n",
    "        inputs = EngineInput(\n",
    "            mode=mode_selector.value,\n",
    "            start_date=decision_date_raw,\n",
    "            lookback_period=lookback_input.value,\n",
    "            holding_period=holding_input.value,\n",
    "            metric=strategy_dropdown.value,\n",
    "            benchmark_ticker=benchmark_input.value.strip().upper(),\n",
    "            rank_start=rank_start_input.value,\n",
    "            rank_end=rank_end_input.value,\n",
    "            quality_thresholds=quality_thresholds,\n",
    "            manual_tickers=manual_list,\n",
    "            debug=debug,\n",
    "        )\n",
    "\n",
    "        # --- CAPTURE INPUTS FOR AUDIT ---\n",
    "        debug_container[0][\"inputs\"] = inputs\n",
    "\n",
    "        with ticker_list_output:\n",
    "            res = engine.run(inputs)\n",
    "            results_container[0] = res\n",
    "\n",
    "            # --- MERGE ENGINE DEBUG DATA ---\n",
    "            if res.debug_data:\n",
    "                debug_container[0].update(res.debug_data)\n",
    "\n",
    "            if res.error_msg:\n",
    "                print(f\"‚ö†Ô∏è Simulation Stopped: {res.error_msg}\")\n",
    "                return\n",
    "\n",
    "            # Plotting\n",
    "            with fig.batch_update():\n",
    "                cols = res.normalized_plot_data.columns.tolist()\n",
    "                for i in range(50):\n",
    "                    if i < len(cols):\n",
    "                        fig.data[i].update(\n",
    "                            x=res.normalized_plot_data.index,\n",
    "                            y=res.normalized_plot_data[cols[i]],\n",
    "                            name=cols[i],\n",
    "                            visible=True,\n",
    "                        )\n",
    "                    else:\n",
    "                        fig.data[i].visible = False\n",
    "\n",
    "                fig.data[50].update(\n",
    "                    x=res.benchmark_series.index,\n",
    "                    y=res.benchmark_series.values,\n",
    "                    name=f\"Benchmark ({inputs.benchmark_ticker})\",\n",
    "                    visible=not res.benchmark_series.empty,\n",
    "                )\n",
    "                fig.data[51].update(\n",
    "                    x=res.portfolio_series.index,\n",
    "                    y=res.portfolio_series.values,\n",
    "                    visible=True,\n",
    "                )\n",
    "\n",
    "                # Visual Lines\n",
    "                fig.layout.shapes = [\n",
    "                    dict(\n",
    "                        type=\"line\",\n",
    "                        x0=res.decision_date,\n",
    "                        y0=0,\n",
    "                        x1=res.decision_date,\n",
    "                        y1=1,\n",
    "                        xref=\"x\",\n",
    "                        yref=\"paper\",\n",
    "                        line=dict(color=\"red\", width=2, dash=\"dash\"),\n",
    "                    ),\n",
    "                    dict(\n",
    "                        type=\"line\",\n",
    "                        x0=res.buy_date,\n",
    "                        y0=0,\n",
    "                        x1=res.buy_date,\n",
    "                        y1=1,\n",
    "                        xref=\"x\",\n",
    "                        yref=\"paper\",\n",
    "                        line=dict(color=\"blue\", width=2, dash=\"dot\"),\n",
    "                    ),\n",
    "                ]\n",
    "\n",
    "                fig.layout.annotations = [\n",
    "                    dict(\n",
    "                        x=res.decision_date,\n",
    "                        y=0.05,\n",
    "                        xref=\"x\",\n",
    "                        yref=\"paper\",\n",
    "                        text=\"DECISION\",\n",
    "                        showarrow=False,\n",
    "                        bgcolor=\"red\",\n",
    "                        font=dict(color=\"white\"),\n",
    "                    ),\n",
    "                    dict(\n",
    "                        x=res.buy_date,\n",
    "                        y=1.0,\n",
    "                        xref=\"x\",\n",
    "                        yref=\"paper\",\n",
    "                        text=\"ENTRY (T+1)\",\n",
    "                        showarrow=False,\n",
    "                        bgcolor=\"blue\",\n",
    "                        font=dict(color=\"white\"),\n",
    "                    ),\n",
    "                ]\n",
    "\n",
    "            start_date = res.start_date.date()\n",
    "            act_date = res.decision_date.date()\n",
    "            entry_date = res.buy_date.date()\n",
    "\n",
    "            # Liquidity Audit Print\n",
    "            if (\n",
    "                inputs.mode == \"Ranking\"\n",
    "                and res.debug_data\n",
    "                and \"audit_liquidity\" in res.debug_data\n",
    "            ):\n",
    "                audit = res.debug_data[\"audit_liquidity\"]\n",
    "                if audit:\n",
    "                    raw_percentile = audit.get(\"percentile_setting\", 0)\n",
    "                    keep_pct = (\n",
    "                        1 - raw_percentile\n",
    "                    ) * 100  # Calculates the actual portion kept\n",
    "                    cut_val = audit.get(\"final_cutoff_usd\", 0)\n",
    "\n",
    "                    print(\"-\" * 60)\n",
    "                    print(f\"üîç LIQUIDITY CHECK (On Decision Date: {act_date})\")\n",
    "                    print(\n",
    "                        f\"   Universe Size: {audit.get('total_tickers_available')} tickers\"\n",
    "                    )\n",
    "                    print(\n",
    "                        f\"   Liquidity Threshold: {raw_percentile*100:.0f}th Percentile\"\n",
    "                    )\n",
    "                    print(f\"   Action: Keeping the Top {keep_pct:.0f}% of Market\")\n",
    "                    print(f\"   Calculated Cutoff: ${cut_val:,.0f} / day\")\n",
    "                    print(f\"   Tickers Remaining: {audit.get('tickers_passed')}\")\n",
    "                    print(\"-\" * 60)\n",
    "\n",
    "            # --- UPDATED TIMELINE PRINT ---\n",
    "            print(\n",
    "                f\"Timeline: Start [ {start_date} ] --> Decision [ {act_date} ] --> Cash (1d) --> Entry [ {entry_date} ] --> End [ {res.holding_end_date.date()} ]\"\n",
    "            )\n",
    "\n",
    "            if inputs.mode == \"Ranking\":\n",
    "                print(f\"Ranked Tickers ({len(res.tickers)}):\")\n",
    "                for i in range(0, len(res.tickers), 10):\n",
    "                    print(\", \".join(res.tickers[i : i + 10]))\n",
    "            else:\n",
    "                print(\"Manual Portfolio Tickers:\")\n",
    "                for i in range(0, len(res.tickers), 10):\n",
    "                    print(\", \".join(res.tickers[i : i + 10]))\n",
    "\n",
    "            m = res.perf_metrics\n",
    "\n",
    "            # --- DRY UI GENERATION ---\n",
    "            # 1. Define the metrics we want to display\n",
    "            metrics_to_show = [\n",
    "                (\"Gain\", \"gain\"),\n",
    "                (\"Sharpe\", \"sharpe\"),\n",
    "                (\"Sharpe (ATR)\", \"sharpe_atr\"),\n",
    "            ]\n",
    "\n",
    "            rows = []\n",
    "            for label, key in metrics_to_show:\n",
    "                p_row = {\n",
    "                    \"Metric\": f\"Group {label}\",\n",
    "                    \"Full\": m.get(f\"full_p_{key}\"),\n",
    "                    \"Lookback\": m.get(f\"lookback_p_{key}\"),\n",
    "                    \"Holding\": m.get(f\"holding_p_{key}\"),\n",
    "                }\n",
    "                b_row = {\n",
    "                    \"Metric\": f\"Benchmark {label}\",\n",
    "                    \"Full\": m.get(f\"full_b_{key}\"),\n",
    "                    \"Lookback\": m.get(f\"lookback_b_{key}\"),\n",
    "                    \"Holding\": m.get(f\"holding_b_{key}\"),\n",
    "                }\n",
    "\n",
    "                # Delta calculation\n",
    "                d_row = {\"Metric\": f\"== {label} Delta\"}\n",
    "                for col in [\"Full\", \"Lookback\", \"Holding\"]:\n",
    "                    d_row[col] = (p_row[col] or 0) - (b_row[col] or 0)\n",
    "\n",
    "                rows.extend([p_row, b_row, d_row])\n",
    "\n",
    "            df_report = pd.DataFrame(rows).set_index(\"Metric\")\n",
    "\n",
    "            # --- 2. STYLING (The \"Senior\" Design) ---\n",
    "            # --- 1. PREP DATA (Flattening the Index) ---\n",
    "            # We convert the index to a column so \"Metric\" sits on the same row as other headers\n",
    "            df_report = pd.DataFrame(rows)\n",
    "            df_report = df_report.set_index(\"Metric\")\n",
    "\n",
    "            # --- 2. THE STYLING (Sleek & Proportional) ---\n",
    "            def apply_sleek_style(styler):\n",
    "                # Match notebook font size (usually 13px)\n",
    "                styler.format(\"{:+.4f}\", na_rep=\"N/A\")\n",
    "\n",
    "                # Dynamic Row Highlighting\n",
    "                def row_logic(row):\n",
    "                    if \"Delta\" in row.name:\n",
    "                        return [\n",
    "                            \"background-color: #f9f9f9; font-weight: 600; border-top: 1px solid #ddd\"\n",
    "                        ] * len(row)\n",
    "                    if \"Group\" in row.name:\n",
    "                        return [\"color: #2c5e8f; background-color: #fcfdfe\"] * len(row)\n",
    "                    return [\"color: #555\"] * len(\n",
    "                        row\n",
    "                    )  # Benchmark rows are slightly muted\n",
    "\n",
    "                styler.apply(row_logic, axis=1)\n",
    "\n",
    "                styler.set_table_styles(\n",
    "                    [\n",
    "                        # Base Table Font - Scaling down to match standard text\n",
    "                        {\n",
    "                            \"selector\": \"\",\n",
    "                            \"props\": [\n",
    "                                (\"font-family\", \"inherit\"),\n",
    "                                (\"font-size\", \"12px\"),\n",
    "                                (\"border-collapse\", \"collapse\"),\n",
    "                                (\"width\", \"auto\"),\n",
    "                                (\"margin-left\", \"0\"),\n",
    "                            ],\n",
    "                        },\n",
    "                        # Header Row - Flattened and Muted\n",
    "                        {\n",
    "                            \"selector\": \"th\",\n",
    "                            \"props\": [\n",
    "                                (\"background-color\", \"white\"),\n",
    "                                (\"color\", \"#222\"),\n",
    "                                (\"font-weight\", \"600\"),\n",
    "                                (\"padding\", \"6px 12px\"),\n",
    "                                (\"border-bottom\", \"2px solid #444\"),\n",
    "                                (\"text-align\", \"center\"),\n",
    "                                (\n",
    "                                    \"vertical-align\",\n",
    "                                    \"bottom\",\n",
    "                                ),  # Aligns 'Metric' with others\n",
    "                            ],\n",
    "                        },\n",
    "                        # Index Column (The \"Metric\" labels)\n",
    "                        {\n",
    "                            \"selector\": \"th.row_heading\",\n",
    "                            \"props\": [\n",
    "                                (\"text-align\", \"left\"),\n",
    "                                (\"padding-right\", \"30px\"),\n",
    "                                (\"border-bottom\", \"1px solid #eee\"),\n",
    "                            ],\n",
    "                        },\n",
    "                        # Cell Data - Tighter padding\n",
    "                        {\n",
    "                            \"selector\": \"td\",\n",
    "                            \"props\": [\n",
    "                                (\"padding\", \"4px 12px\"),\n",
    "                                (\"border-bottom\", \"1px solid #eee\"),\n",
    "                            ],\n",
    "                        },\n",
    "                        # Remove the extra \"Index Name\" row completely\n",
    "                        {\n",
    "                            \"selector\": \"thead tr:nth-child(1) th\",\n",
    "                            \"props\": [(\"display\", \"table-cell\")],\n",
    "                        },\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "                # Hack to fix the 'Metric' alignment:\n",
    "                # We remove the index name and set it as the horizontal label for the index\n",
    "                styler.index.name = None\n",
    "\n",
    "                return styler\n",
    "\n",
    "            display(apply_sleek_style(df_report.style))\n",
    "\n",
    "    update_button.on_click(update_plot)\n",
    "    update_plot(None)\n",
    "    display(ui, fig)\n",
    "    return results_container, debug_container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a48017a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# SECTION F: UTILITIES\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "def export_debug_to_csv(container, source_label=\"Simulation\"):\n",
    "    \"\"\"\n",
    "    Flattens the debug_container and saves all components to high-precision CSVs.\n",
    "    \"\"\"\n",
    "    if not container or not container[0]:\n",
    "        print(\"‚ùå Error: Debug container is empty.\")\n",
    "        return\n",
    "\n",
    "    data = container[0]\n",
    "    inputs = data.get(\"inputs\")\n",
    "\n",
    "    # 1. Create a clean folder name\n",
    "    # e.g., Audit_Golden_20251211_SharpeATR\n",
    "    date_str = inputs.start_date.strftime(\"%Y-%m-%d\")\n",
    "    strategy_str = inputs.metric.replace(\" \", \"\").replace(\"(\", \"\").replace(\")\", \"\")\n",
    "    folder_name = f\"Audit_{source_label}_{date_str}_{strategy_str}\"\n",
    "\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "\n",
    "    print(f\"üìÇ Exporting audit data to: ./{folder_name}/\")\n",
    "\n",
    "    # 2. Recursive function to traverse the dictionary and save files\n",
    "    def process_item(item, path_prefix=\"\"):\n",
    "        # Handle Nested Dictionaries (like 'verification' or 'raw_components')\n",
    "        if isinstance(item, dict):\n",
    "            for key, value in item.items():\n",
    "                new_prefix = f\"{path_prefix}{key}_\" if path_prefix else f\"{key}_\"\n",
    "                process_item(value, new_prefix)\n",
    "\n",
    "        # Handle DataFrames\n",
    "        elif isinstance(item, pd.DataFrame):\n",
    "            filename = f\"{path_prefix.strip('_')}.csv\"\n",
    "            item.to_csv(os.path.join(folder_name, filename), float_format=\"%.8f\")\n",
    "            print(f\"   ‚úÖ Saved DataFrame: {filename}\")\n",
    "\n",
    "        # Handle Series (Convert to DataFrame for CSV preservation of Index)\n",
    "        elif isinstance(item, pd.Series):\n",
    "            filename = f\"{path_prefix.strip('_')}.csv\"\n",
    "            item.to_frame().to_csv(\n",
    "                os.path.join(folder_name, filename), float_format=\"%.8f\"\n",
    "            )\n",
    "            print(f\"   ‚úÖ Saved Series:    {filename}\")\n",
    "\n",
    "        # Handle Dataclasses (like 'inputs')\n",
    "        elif is_dataclass(item):\n",
    "            filename = f\"{path_prefix}Metadata.csv\"\n",
    "            # Convert to a vertical 2-column table for easy Excel reading\n",
    "            meta_df = pd.DataFrame.from_dict(\n",
    "                asdict(item), orient=\"index\", columns=[\"Value\"]\n",
    "            )\n",
    "            meta_df.to_csv(os.path.join(folder_name, filename))\n",
    "            print(f\"   ‚úÖ Saved Metadata:  {filename}\")\n",
    "\n",
    "    # 3. Start the extraction\n",
    "    process_item(data)\n",
    "    print(f\"\\n‚ú® Export Complete. All numbers saved with 8 decimal places.\")\n",
    "\n",
    "\n",
    "def print_nested(d, indent=0, width=4):\n",
    "    \"\"\"Pretty-print nested containers.\n",
    "    Leaves are rendered as two lines:  key\\\\nvalue .\"\"\"\n",
    "    spacing = \" \" * indent\n",
    "\n",
    "    def _kind(node):\n",
    "        if not isinstance(node, dict):\n",
    "            return None\n",
    "        return \"sep\" if all(isinstance(v, dict) for v in node.values()) else \"nest\"\n",
    "\n",
    "    if isinstance(d, dict):\n",
    "        for k, v in d.items():\n",
    "            kind = _kind(v)\n",
    "            tag = \"\" if kind is None else f\"  [{'SEP' if kind == 'sep' else 'NEST'}]\"\n",
    "            print(f\"{spacing}{k}{tag}\")\n",
    "            print_nested(v, indent + width, width)\n",
    "\n",
    "    elif isinstance(d, (list, tuple)):\n",
    "        for idx, item in enumerate(d):\n",
    "            print(f\"{spacing}[{idx}]\")\n",
    "            print_nested(item, indent + width, width)\n",
    "\n",
    "    else:  # leaf ‚Äì primitive value\n",
    "        print(f\"{spacing}{d}\")\n",
    "\n",
    "\n",
    "def get_ticker_OHLCV(\n",
    "    df_ohlcv: pd.DataFrame,\n",
    "    tickers: Union[str, List[str]],\n",
    "    date_start: str,\n",
    "    date_end: str,\n",
    "    return_format: str = \"dataframe\",\n",
    "    verbose: bool = True,\n",
    ") -> Union[pd.DataFrame, dict]:\n",
    "    \"\"\"\n",
    "    Get OHLCV data for specified tickers within a date range.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_ohlcv : pd.DataFrame\n",
    "        DataFrame with MultiIndex of (ticker, date) and OHLCV columns\n",
    "    tickers : str or list of str\n",
    "        Ticker symbol(s) to retrieve\n",
    "    date_start : str\n",
    "        Start date in 'YYYY-MM-DD' format\n",
    "    date_end : str\n",
    "        End date in 'YYYY-MM-DD' format\n",
    "    return_format : str, optional\n",
    "        Format to return data in. Options:\n",
    "        - 'dataframe': Single DataFrame with MultiIndex (default)\n",
    "        - 'dict': Dictionary with tickers as keys and DataFrames as values\n",
    "        - 'separate': List of separate DataFrames for each ticker\n",
    "    verbose : bool, optional\n",
    "        Whether to print summary information (default: True)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Union[pd.DataFrame, dict, list]\n",
    "        Filtered OHLCV data in specified format\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If input parameters are invalid\n",
    "    KeyError\n",
    "        If tickers not found in DataFrame\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> # Get data for single ticker\n",
    "    >>> vlo_data = get_ticker_OHLCV(df_ohlcv, 'VLO', '2025-08-13', '2025-09-04')\n",
    "\n",
    "    >>> # Get data for multiple tickers\n",
    "    >>> multi_data = get_ticker_OHLCV(df_ohlcv, ['VLO', 'JPST'], '2025-08-13', '2025-09-04')\n",
    "\n",
    "    >>> # Get data as dictionary\n",
    "    >>> data_dict = get_ticker_OHLCV(df_ohlcv, ['VLO', 'JPST'], '2025-08-13',\n",
    "    ...                              '2025-09-04', return_format='dict')\n",
    "    \"\"\"\n",
    "\n",
    "    # Input validation\n",
    "    if not isinstance(df_ohlcv, pd.DataFrame):\n",
    "        raise TypeError(\"df_ohlcv must be a pandas DataFrame\")\n",
    "\n",
    "    if not isinstance(df_ohlcv.index, pd.MultiIndex):\n",
    "        raise ValueError(\"DataFrame must have MultiIndex of (ticker, date)\")\n",
    "\n",
    "    if len(df_ohlcv.index.levels) != 2:\n",
    "        raise ValueError(\"MultiIndex must have exactly 2 levels: (ticker, date)\")\n",
    "\n",
    "    # Convert single ticker to list for consistent processing\n",
    "    if isinstance(tickers, str):\n",
    "        tickers = [tickers]\n",
    "    elif not isinstance(tickers, list):\n",
    "        raise TypeError(\"tickers must be a string or list of strings\")\n",
    "\n",
    "    # Convert dates to Timestamps\n",
    "    try:\n",
    "        start_date = pd.Timestamp(date_start)\n",
    "        end_date = pd.Timestamp(date_end)\n",
    "    except ValueError as e:\n",
    "        raise ValueError(f\"Invalid date format. Use 'YYYY-MM-DD': {e}\")\n",
    "\n",
    "    if start_date > end_date:\n",
    "        raise ValueError(\"date_start must be before or equal to date_end\")\n",
    "\n",
    "    # Check if tickers exist in the DataFrame\n",
    "    available_tickers = df_ohlcv.index.get_level_values(0).unique()\n",
    "    missing_tickers = [t for t in tickers if t not in available_tickers]\n",
    "\n",
    "    if missing_tickers:\n",
    "        raise KeyError(f\"Ticker(s) not found in DataFrame: {missing_tickers}\")\n",
    "\n",
    "    # Filter the data using MultiIndex slicing\n",
    "    try:\n",
    "        filtered_data = df_ohlcv.loc[(tickers, slice(date_start, date_end)), :]\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error filtering data: {e}\")\n",
    "\n",
    "    # Handle empty results\n",
    "    if filtered_data.empty:\n",
    "        if verbose:\n",
    "            print(\n",
    "                f\"No data found for tickers {tickers} in date range {date_start} to {date_end}\"\n",
    "            )\n",
    "        return filtered_data\n",
    "\n",
    "    # Print summary if verbose\n",
    "    if verbose:\n",
    "        print(\n",
    "            f\"Data retrieved for {len(tickers)} ticker(s) from {date_start} to {date_end}\"\n",
    "        )\n",
    "        print(f\"Total rows: {len(filtered_data)}\")\n",
    "        print(\n",
    "            f\"Date range in data: {filtered_data.index.get_level_values(1).min()} to \"\n",
    "            f\"{filtered_data.index.get_level_values(1).max()}\"\n",
    "        )\n",
    "\n",
    "        # Print ticker-specific counts\n",
    "        ticker_counts = filtered_data.index.get_level_values(0).value_counts()\n",
    "        for ticker in tickers:\n",
    "            count = ticker_counts.get(ticker, 0)\n",
    "            if count > 0:\n",
    "                print(f\"  {ticker}: {count} rows\")\n",
    "            else:\n",
    "                print(f\"  {ticker}: No data in range\")\n",
    "\n",
    "    # Return in requested format\n",
    "    if return_format == \"dict\":\n",
    "        result = {}\n",
    "        for ticker in tickers:\n",
    "            try:\n",
    "                result[ticker] = filtered_data.xs(ticker, level=0).loc[\n",
    "                    date_start:date_end\n",
    "                ]\n",
    "            except KeyError:\n",
    "                result[ticker] = pd.DataFrame()\n",
    "        return result\n",
    "\n",
    "    elif return_format == \"separate\":\n",
    "        result = []\n",
    "        for ticker in tickers:\n",
    "            try:\n",
    "                result.append(\n",
    "                    filtered_data.xs(ticker, level=0).loc[date_start:date_end]\n",
    "                )\n",
    "            except KeyError:\n",
    "                result.append(pd.DataFrame())\n",
    "        return result\n",
    "\n",
    "    elif return_format == \"dataframe\":\n",
    "        return filtered_data\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Invalid return_format: {return_format}. \"\n",
    "            f\"Must be 'dataframe', 'dict', or 'separate'\"\n",
    "        )\n",
    "\n",
    "\n",
    "def get_ticker_features(\n",
    "    features_df: pd.DataFrame,\n",
    "    tickers: Union[str, List[str]],\n",
    "    date_start: str,\n",
    "    date_end: str,\n",
    "    return_format: str = \"dataframe\",\n",
    "    verbose: bool = True,\n",
    ") -> Union[pd.DataFrame, dict]:\n",
    "    \"\"\"\n",
    "    Get features data for specified tickers within a date range.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    features_df : pd.DataFrame\n",
    "        DataFrame with MultiIndex of (ticker, date) and feature columns\n",
    "    tickers : str or list of str\n",
    "        Ticker symbol(s) to retrieve\n",
    "    date_start : str\n",
    "        Start date in 'YYYY-MM-DD' format\n",
    "    date_end : str\n",
    "        End date in 'YYYY-MM-DD' format\n",
    "    return_format : str, optional\n",
    "        Format to return data in. Options:\n",
    "        - 'dataframe': Single DataFrame with MultiIndex (default)\n",
    "        - 'dict': Dictionary with tickers as keys and DataFrames as values\n",
    "        - 'separate': List of separate DataFrames for each ticker\n",
    "    verbose : bool, optional\n",
    "        Whether to print summary information (default: True)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Union[pd.DataFrame, dict, list]\n",
    "        Filtered features data in specified format\n",
    "    \"\"\"\n",
    "    # Convert single ticker to list for consistent processing\n",
    "    if isinstance(tickers, str):\n",
    "        tickers = [tickers]\n",
    "\n",
    "    # Filter the data using MultiIndex slicing\n",
    "    try:\n",
    "        filtered_data = features_df.loc[(tickers, slice(date_start, date_end)), :]\n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(f\"Error filtering data: {e}\")\n",
    "        return pd.DataFrame() if return_format == \"dataframe\" else {}\n",
    "\n",
    "    # Handle empty results\n",
    "    if filtered_data.empty:\n",
    "        if verbose:\n",
    "            print(\n",
    "                f\"No data found for tickers {tickers} in date range {date_start} to {date_end}\"\n",
    "            )\n",
    "        return filtered_data\n",
    "\n",
    "    # Print summary if verbose\n",
    "    if verbose:\n",
    "        print(\n",
    "            f\"Features data retrieved for {len(tickers)} ticker(s) from {date_start} to {date_end}\"\n",
    "        )\n",
    "        print(f\"Total rows: {len(filtered_data)}\")\n",
    "        print(\n",
    "            f\"Date range in data: {filtered_data.index.get_level_values(1).min()} to \"\n",
    "            f\"{filtered_data.index.get_level_values(1).max()}\"\n",
    "        )\n",
    "        print(f\"Available features: {', '.join(filtered_data.columns.tolist())}\")\n",
    "\n",
    "        # Print ticker-specific counts\n",
    "        ticker_counts = filtered_data.index.get_level_values(0).value_counts()\n",
    "        for ticker in tickers:\n",
    "            count = ticker_counts.get(ticker, 0)\n",
    "            if count > 0:\n",
    "                print(f\"  {ticker}: {count} rows\")\n",
    "            else:\n",
    "                print(f\"  {ticker}: No data in range\")\n",
    "\n",
    "    # Return in requested format\n",
    "    if return_format == \"dict\":\n",
    "        result = {}\n",
    "        for ticker in tickers:\n",
    "            try:\n",
    "                result[ticker] = filtered_data.xs(ticker, level=0).loc[\n",
    "                    date_start:date_end\n",
    "                ]\n",
    "            except KeyError:\n",
    "                result[ticker] = pd.DataFrame()\n",
    "        return result\n",
    "\n",
    "    elif return_format == \"separate\":\n",
    "        result = []\n",
    "        for ticker in tickers:\n",
    "            try:\n",
    "                result.append(\n",
    "                    filtered_data.xs(ticker, level=0).loc[date_start:date_end]\n",
    "                )\n",
    "            except KeyError:\n",
    "                result.append(pd.DataFrame())\n",
    "        return result\n",
    "\n",
    "    elif return_format == \"dataframe\":\n",
    "        return filtered_data\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Invalid return_format: {return_format}. \"\n",
    "            f\"Must be 'dataframe', 'dict', or 'separate'\"\n",
    "        )\n",
    "\n",
    "\n",
    "def create_combined_dict(\n",
    "    df_ohlcv: pd.DataFrame,\n",
    "    features_df: pd.DataFrame,\n",
    "    tickers: Union[str, List[str]],\n",
    "    date_start: str,\n",
    "    date_end: str,\n",
    "    verbose: bool = True,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Create a combined dictionary with both OHLCV and features data for each ticker.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_ohlcv : pd.DataFrame\n",
    "        DataFrame with OHLCV data (MultiIndex: ticker, date)\n",
    "    features_df : pd.DataFrame\n",
    "        DataFrame with features data (MultiIndex: ticker, date)\n",
    "    tickers : str or list of str\n",
    "        Ticker symbol(s) to retrieve\n",
    "    date_start : str\n",
    "        Start date in 'YYYY-MM-DD' format\n",
    "    date_end : str\n",
    "        End date in 'YYYY-MM-DD' format\n",
    "    verbose : bool, optional\n",
    "        Whether to print progress information (default: True)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with tickers as keys and combined DataFrames (OHLCV + features) as values\n",
    "    \"\"\"\n",
    "    # Convert single ticker to list\n",
    "    if isinstance(tickers, str):\n",
    "        tickers = [tickers]\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Creating combined dictionary for {len(tickers)} ticker(s)\")\n",
    "        print(f\"Date range: {date_start} to {date_end}\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "    # Get OHLCV data as dictionary\n",
    "    ohlcv_dict = get_ticker_OHLCV(\n",
    "        df_ohlcv, tickers, date_start, date_end, return_format=\"dict\", verbose=verbose\n",
    "    )\n",
    "\n",
    "    # Get features data as dictionary\n",
    "    features_dict = get_ticker_features(\n",
    "        features_df,\n",
    "        tickers,\n",
    "        date_start,\n",
    "        date_end,\n",
    "        return_format=\"dict\",\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    # Create combined_dict\n",
    "    combined_dict = {}\n",
    "\n",
    "    for ticker in tickers:\n",
    "        if verbose:\n",
    "            print(f\"\\nProcessing {ticker}...\")\n",
    "\n",
    "        # Check if ticker exists in both dictionaries\n",
    "        if ticker in ohlcv_dict and ticker in features_dict:\n",
    "            ohlcv_data = ohlcv_dict[ticker]\n",
    "            features_data = features_dict[ticker]\n",
    "\n",
    "            # Check if both dataframes have data\n",
    "            if not ohlcv_data.empty and not features_data.empty:\n",
    "                # Combine OHLCV and features data\n",
    "                # Note: Both dataframes have the same index (dates), so we can concatenate\n",
    "                combined_df = pd.concat([ohlcv_data, features_data], axis=1)\n",
    "\n",
    "                # Ensure proper index naming\n",
    "                combined_df.index.name = \"Date\"\n",
    "\n",
    "                # Store in combined_dict\n",
    "                combined_dict[ticker] = combined_df\n",
    "\n",
    "                if verbose:\n",
    "                    print(f\"  ‚úì Successfully combined data\")\n",
    "                    print(f\"  OHLCV shape: {ohlcv_data.shape}\")\n",
    "                    print(f\"  Features shape: {features_data.shape}\")\n",
    "                    print(f\"  Combined shape: {combined_df.shape}\")\n",
    "                    print(\n",
    "                        f\"  Date range: {combined_df.index.min()} to {combined_df.index.max()}\"\n",
    "                    )\n",
    "            else:\n",
    "                if verbose:\n",
    "                    print(f\"  ‚úó Cannot combine: One or both dataframes are empty\")\n",
    "                    print(f\"    OHLCV empty: {ohlcv_data.empty}\")\n",
    "                    print(f\"    Features empty: {features_data.empty}\")\n",
    "                combined_dict[ticker] = pd.DataFrame()\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(f\"  ‚úó Ticker not found in both dictionaries\")\n",
    "                if ticker not in ohlcv_dict:\n",
    "                    print(f\"    Not in OHLCV data\")\n",
    "                if ticker not in features_dict:\n",
    "                    print(f\"    Not in features data\")\n",
    "            combined_dict[ticker] = pd.DataFrame()\n",
    "\n",
    "    # Print summary\n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"SUMMARY\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Total tickers processed: {len(tickers)}\")\n",
    "\n",
    "        tickers_with_data = [\n",
    "            ticker for ticker, df in combined_dict.items() if not df.empty\n",
    "        ]\n",
    "        print(f\"Tickers with combined data: {len(tickers_with_data)}\")\n",
    "\n",
    "        if tickers_with_data:\n",
    "            print(\"\\nTicker details:\")\n",
    "            for ticker in tickers_with_data:\n",
    "                df = combined_dict[ticker]\n",
    "                print(f\"  {ticker}: {df.shape} - {df.index.min()} to {df.index.max()}\")\n",
    "                print(f\"    Columns: {len(df.columns)}\")\n",
    "\n",
    "        empty_tickers = [ticker for ticker, df in combined_dict.items() if df.empty]\n",
    "        if empty_tickers:\n",
    "            print(f\"\\nTickers with no data: {', '.join(empty_tickers)}\")\n",
    "\n",
    "    return combined_dict\n",
    "\n",
    "\n",
    "def export_debug_to_csv(container, source_label=\"Simulation\"):\n",
    "    \"\"\"\n",
    "    Flattens the debug_container and saves all components to high-precision CSVs.\n",
    "    \"\"\"\n",
    "    if not container or not container[0]:\n",
    "        print(\"‚ùå Error: Debug container is empty.\")\n",
    "        return\n",
    "\n",
    "    data = container[0]\n",
    "    inputs = data.get(\"inputs\")\n",
    "\n",
    "    # 1. Create a clean folder name\n",
    "    # e.g., Audit_Golden_20251211_SharpeATR\n",
    "    date_str = inputs.start_date.strftime(\"%Y-%m-%d\")\n",
    "    strategy_str = inputs.metric.replace(\" \", \"\").replace(\"(\", \"\").replace(\")\", \"\")\n",
    "    folder_name = f\"Audit_{source_label}_{date_str}_{strategy_str}\"\n",
    "\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "\n",
    "    print(f\"üìÇ Exporting audit data to: ./{folder_name}/\")\n",
    "\n",
    "    # 2. Recursive function to traverse the dictionary and save files\n",
    "    def process_item(item, path_prefix=\"\"):\n",
    "        # Handle Nested Dictionaries (like 'verification' or 'raw_components')\n",
    "        if isinstance(item, dict):\n",
    "            for key, value in item.items():\n",
    "                new_prefix = f\"{path_prefix}{key}_\" if path_prefix else f\"{key}_\"\n",
    "                process_item(value, new_prefix)\n",
    "\n",
    "        # Handle DataFrames\n",
    "        elif isinstance(item, pd.DataFrame):\n",
    "            filename = f\"{path_prefix.strip('_')}.csv\"\n",
    "            item.to_csv(os.path.join(folder_name, filename), float_format=\"%.8f\")\n",
    "            print(f\"   ‚úÖ Saved DataFrame: {filename}\")\n",
    "\n",
    "        # Handle Series (Convert to DataFrame for CSV preservation of Index)\n",
    "        elif isinstance(item, pd.Series):\n",
    "            filename = f\"{path_prefix.strip('_')}.csv\"\n",
    "            item.to_frame().to_csv(\n",
    "                os.path.join(folder_name, filename), float_format=\"%.8f\"\n",
    "            )\n",
    "            print(f\"   ‚úÖ Saved Series:    {filename}\")\n",
    "\n",
    "        # Handle Dataclasses (like 'inputs')\n",
    "        elif is_dataclass(item):\n",
    "            filename = f\"{path_prefix}Metadata.csv\"\n",
    "            # Convert to a vertical 2-column table for easy Excel reading\n",
    "            meta_df = pd.DataFrame.from_dict(\n",
    "                asdict(item), orient=\"index\", columns=[\"Value\"]\n",
    "            )\n",
    "            meta_df.to_csv(os.path.join(folder_name, filename))\n",
    "            print(f\"   ‚úÖ Saved Metadata:  {filename}\")\n",
    "\n",
    "    # 3. Start the extraction\n",
    "    process_item(data)\n",
    "    print(f\"\\n‚ú® Export Complete. All numbers saved with 8 decimal places.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2989c8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# SECTION G: UNIT TEST FOR GENERATED FEATURES\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "def test_true_range_calculation():\n",
    "    \"\"\"Test TR = max(High-Low, |High-PrevClose|, |Low-PrevClose|) using robust pandas testing\"\"\"\n",
    "    print(\"Running test_true_range_calculation (Robust Version)...\")\n",
    "\n",
    "    # 1. SETUP: Create test data\n",
    "    test_data = {\n",
    "        \"Adj Open\": [100, 105, 95, 98, 102],\n",
    "        \"Adj High\": [105, 108, 97, 102, 105],\n",
    "        \"Adj Low\": [95, 103, 93, 100, 98],\n",
    "        \"Adj Close\": [100, 106, 96, 99, 103],\n",
    "        \"Volume\": [1000, 1200, 800, 900, 1100],\n",
    "    }\n",
    "\n",
    "    index = pd.MultiIndex.from_tuples(\n",
    "        [\n",
    "            (\"TEST\", pd.Timestamp(\"2024-01-01\")),\n",
    "            (\"TEST\", pd.Timestamp(\"2024-01-02\")),\n",
    "            (\"TEST\", pd.Timestamp(\"2024-01-03\")),\n",
    "            (\"TEST\", pd.Timestamp(\"2024-01-04\")),\n",
    "            (\"TEST\", pd.Timestamp(\"2024-01-05\")),\n",
    "        ],\n",
    "        names=[\"Ticker\", \"Date\"],\n",
    "    )\n",
    "\n",
    "    df_test = pd.DataFrame(test_data, index=index)\n",
    "    print(f\"test_true_range_df input:\\n{df_test}\\n\")\n",
    "\n",
    "    # 2. EXECUTION: Run function\n",
    "    result = generate_features(df_test, quality_window=5, quality_min_periods=2)\n",
    "\n",
    "    # 3. ASSERTION: Define EXACT expected series\n",
    "    # Day 1: NaN (No prev close)\n",
    "    # Day 2: 8.0 (PrevClose=100, High=108, Low=103. |108-100|=8)\n",
    "    # Day 3: 13.0 (PrevClose=106, High=97, Low=93. |93-106|=13)\n",
    "    # Day 4: 6.0 (PrevClose=96, High=102, Low=100. |102-96|=6)\n",
    "    # Day 5: 7.0 (PrevClose=99, High=105, Low=98. High-Low=7)\n",
    "    expected_tr_values = [np.nan, 8.0, 13.0, 6.0, 7.0]\n",
    "\n",
    "    expected_series = pd.Series(\n",
    "        expected_tr_values, index=result.index, name=\"TR\", dtype=\"float64\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Check Series Equality\n",
    "        # check_exact=False allows for minor floating point differences (rtol=1e-4)\n",
    "        assert_series_equal(result[\"TR\"], expected_series, check_exact=False, rtol=1e-4)\n",
    "\n",
    "        print(\"‚úÖ PD Testing Assertion Passed! All TR values match expected logic.\")\n",
    "        return True\n",
    "\n",
    "    except AssertionError as e:\n",
    "        print(f\"‚ùå PD Testing Assertion Failed: {e}\")\n",
    "\n",
    "        # Helper output to see what went wrong\n",
    "        print(\"\\nDetailed Comparison (Actual vs Expected):\")\n",
    "        comparison = pd.concat(\n",
    "            [result[\"TR\"], expected_series], axis=1, keys=[\"Actual_TR\", \"Expected_TR\"]\n",
    "        )\n",
    "        comparison[\"Diff\"] = comparison[\"Actual_TR\"] - comparison[\"Expected_TR\"]\n",
    "        print(comparison)\n",
    "\n",
    "        return False\n",
    "\n",
    "\n",
    "def test_atr_calculation():\n",
    "    \"\"\"Test ATR = EWMA of TR with alpha=1/period\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"Running test_atr_calculation...\")\n",
    "\n",
    "    # Test data with 5 days\n",
    "    test_data = {\n",
    "        \"Adj Open\": [100, 102, 103, 110, 108],\n",
    "        \"Adj High\": [101, 103, 103, 112, 110],\n",
    "        \"Adj Low\": [99, 101, 103, 108, 107],\n",
    "        \"Adj Close\": [100, 102, 103, 111, 109],\n",
    "        \"Volume\": [1000, 1000, 1000, 1000, 1000],  # All non-zero for simplicity\n",
    "    }\n",
    "\n",
    "    index = pd.MultiIndex.from_tuples(\n",
    "        [(\"TEST\", pd.Timestamp(f\"2024-01-{i:02d}\")) for i in range(1, 6)],\n",
    "        names=[\"Ticker\", \"Date\"],\n",
    "    )\n",
    "\n",
    "    df_test = pd.DataFrame(test_data, index=index)\n",
    "\n",
    "    print(f\"test_true_range_df:\\n{df_test}\\n\")\n",
    "\n",
    "    result = generate_features(df_test, atr_period=14)\n",
    "\n",
    "    print(\"\\nATR Calculation Results:\")\n",
    "    print(result[[\"TR\", \"ATR\", \"ATRP\"]])\n",
    "\n",
    "    # Manual calculation from our earlier example\n",
    "    # CORRECTED EXPECTED VALUES WITH MORE PRECISION\n",
    "    expected_atr = {\n",
    "        \"2024-01-02\": 3.0,\n",
    "        \"2024-01-03\": 40 / 14,  # ‚âà 2.857142857142857\n",
    "        \"2024-01-04\": 646 / 196,  # ‚âà 3.2959183673469388\n",
    "        \"2024-01-05\": 9182 / 2744,  # ‚âà 3.3462099125364433\n",
    "    }\n",
    "\n",
    "    all_passed = True\n",
    "    for date_str, expected in expected_atr.items():\n",
    "        actual = result.loc[(\"TEST\", pd.Timestamp(date_str)), \"ATR\"]\n",
    "        if abs(actual - expected) < 0.0001:\n",
    "            print(f\"‚úì {date_str} ATR: {actual:.6f} ‚âà {expected:.6f}\")\n",
    "        else:\n",
    "            print(f\"‚úó {date_str} ATR: {actual:.6f} != {expected:.6f}\")\n",
    "            all_passed = False\n",
    "\n",
    "    if all_passed:\n",
    "        print(\"\\n‚úÖ All ATR tests passed!\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå Some ATR tests failed!\")\n",
    "\n",
    "    return all_passed\n",
    "\n",
    "\n",
    "def test_is_stale_calculation():\n",
    "    \"\"\"Test IsStale = 1 when Volume=0 OR High=Low\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"Running test_is_stale_calculation...\")\n",
    "\n",
    "    test_data = {\n",
    "        \"Adj Open\": [100, 102, 103, 104],\n",
    "        \"Adj High\": [101, 103, 103, 105],  # Day 3: High=Low\n",
    "        \"Adj Low\": [99, 101, 103, 104],\n",
    "        \"Adj Close\": [100, 102, 103, 105],\n",
    "        \"Volume\": [1000, 0, 500, 1000],  # Day 2: Volume=0\n",
    "    }\n",
    "\n",
    "    index = pd.MultiIndex.from_tuples(\n",
    "        [(\"TEST\", pd.Timestamp(f\"2024-01-{i:02d}\")) for i in range(1, 5)],\n",
    "        names=[\"Ticker\", \"Date\"],\n",
    "    )\n",
    "\n",
    "    df_test = pd.DataFrame(test_data, index=index)\n",
    "\n",
    "    print(f\"test_is_stale_df:\\n{df_test}\\n\")\n",
    "\n",
    "    # Create IsStale manually to verify\n",
    "    is_stale_manual = np.where(\n",
    "        (df_test[\"Volume\"] == 0) | (df_test[\"Adj High\"] == df_test[\"Adj Low\"]), 1, 0\n",
    "    )\n",
    "\n",
    "    print(\"\\nüìä Manual IsStale Calculation:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"IsStale = 1 if EITHER condition is true:\")\n",
    "    print(\"  1. Volume == 0\")\n",
    "    print(\"  2. Adj High == Adj Low (no price movement)\")\n",
    "    print(\"Otherwise, IsStale = 0\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Create a temporary DataFrame to display the calculation clearly\n",
    "    manual_calc_df = df_test.copy()\n",
    "    manual_calc_df[\"IsStale_Manual\"] = is_stale_manual\n",
    "    manual_calc_df[\"Volume==0\"] = manual_calc_df[\"Volume\"] == 0\n",
    "    manual_calc_df[\"High==Low\"] = (\n",
    "        manual_calc_df[\"Adj High\"] == manual_calc_df[\"Adj Low\"]\n",
    "    )\n",
    "\n",
    "    print(\"\\nCalculation details:\")\n",
    "    for idx, row in manual_calc_df.iterrows():\n",
    "        ticker_date = f\"{idx[0]}, {idx[1].strftime('%Y-%m-%d')}\"\n",
    "        conditions = []\n",
    "        if row[\"Volume==0\"]:\n",
    "            conditions.append(\"Volume=0\")\n",
    "        if row[\"High==Low\"]:\n",
    "            conditions.append(\"High=Low\")\n",
    "\n",
    "        condition_str = \" OR \".join(conditions) if conditions else \"None (both False)\"\n",
    "        result = row[\"IsStale_Manual\"]\n",
    "\n",
    "        print(f\"  {ticker_date}:\")\n",
    "        print(\n",
    "            f\"    Volume={row['Volume']}, High={row['Adj High']}, Low={row['Adj Low']}\"\n",
    "        )\n",
    "        print(f\"    Conditions met: {condition_str}\")\n",
    "        print(f\"    ‚Üí IsStale = {result}\")\n",
    "        print()\n",
    "\n",
    "    expected = [\n",
    "        0,\n",
    "        1,\n",
    "        1,\n",
    "        0,\n",
    "    ]  # Day 1: normal, Day 2: vol=0, Day 3: high=low, Day 4: normal\n",
    "\n",
    "    print(f\"\\nManual IsStale calculation: {is_stale_manual}\")\n",
    "    print(f\"Expected: {expected}\")\n",
    "\n",
    "    if list(is_stale_manual) == expected:\n",
    "        print(\"‚úì IsStale calculation logic is correct\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\n",
    "            f\"‚úó IsStale calculation failed. Got {is_stale_manual}, expected {expected}\"\n",
    "        )\n",
    "        return False\n",
    "\n",
    "\n",
    "def test_multiple_tickers():\n",
    "    \"\"\"Test that calculations don't mix data between tickers\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"Running test_multiple_tickers...\")\n",
    "\n",
    "    test_data = {\n",
    "        \"Adj Open\": [100, 102, 50, 51],\n",
    "        \"Adj High\": [101, 103, 52, 53],\n",
    "        \"Adj Low\": [99, 101, 48, 49],\n",
    "        \"Adj Close\": [100, 102, 49, 52],\n",
    "        \"Volume\": [1000, 1000, 2000, 2000],\n",
    "    }\n",
    "\n",
    "    index = pd.MultiIndex.from_tuples(\n",
    "        [\n",
    "            (\"A\", pd.Timestamp(\"2024-01-01\")),\n",
    "            (\"A\", pd.Timestamp(\"2024-01-02\")),\n",
    "            (\"B\", pd.Timestamp(\"2024-01-01\")),\n",
    "            (\"B\", pd.Timestamp(\"2024-01-02\")),\n",
    "        ],\n",
    "        names=[\"Ticker\", \"Date\"],\n",
    "    )\n",
    "\n",
    "    df_test = pd.DataFrame(test_data, index=index)\n",
    "\n",
    "    print(f\"test_multiple_tickers_df:\\n{df_test}\\n\")\n",
    "\n",
    "    result = generate_features(df_test)\n",
    "\n",
    "    print(\"\\nMultiple Ticker Results:\")\n",
    "    print(result[[\"TR\", \"ATR\"]])\n",
    "\n",
    "    # Ticker A day 2 TR should use A day 1 close, not B day 1 close\n",
    "    tr_a2 = result.loc[(\"A\", \"2024-01-02\"), \"TR\"]\n",
    "    expected_a2 = 3.0  # max(103-101=2, |103-100|=3, |101-100|=1) = 3\n",
    "\n",
    "    tr_b2 = result.loc[(\"B\", \"2024-01-02\"), \"TR\"]\n",
    "    expected_b2 = 4.0  # max(53-49=4, |53-49|=4, |49-49|=0) = 4\n",
    "\n",
    "    tests_passed = 0\n",
    "    total_tests = 2\n",
    "\n",
    "    if abs(tr_a2 - expected_a2) < 0.0001:\n",
    "        print(f\"‚úì Ticker A TR: {tr_a2} (expected {expected_a2})\")\n",
    "        tests_passed += 1\n",
    "    else:\n",
    "        print(f\"‚úó Ticker A TR: {tr_a2} != {expected_a2}\")\n",
    "\n",
    "    if abs(tr_b2 - expected_b2) < 0.0001:\n",
    "        print(f\"‚úì Ticker B TR: {tr_b2} (expected {expected_b2})\")\n",
    "        tests_passed += 1\n",
    "    else:\n",
    "        print(f\"‚úó Ticker B TR: {tr_b2} != {expected_b2}\")\n",
    "\n",
    "    if tests_passed == total_tests:\n",
    "        print(\"‚úÖ Ticker separation test passed!\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"‚ùå Ticker separation test failed: {tests_passed}/{total_tests} passed\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def test_edge_cases():\n",
    "    \"\"\"Test edge cases like zero price, single row, etc.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"Running test_edge_cases...\")\n",
    "\n",
    "    all_passed = True\n",
    "\n",
    "    # Test 1: Very low price (penny stock)\n",
    "    print(\"\\n1. Testing penny stock with low price...\")\n",
    "    test_data = {\n",
    "        \"Adj Open\": [0.10, 0.11],\n",
    "        \"Adj High\": [0.10, 0.11],\n",
    "        \"Adj Low\": [0.10, 0.11],\n",
    "        \"Adj Close\": [0.10, 0.11],\n",
    "        \"Volume\": [1000, 1000],\n",
    "    }\n",
    "\n",
    "    index = pd.MultiIndex.from_tuples(\n",
    "        [\n",
    "            (\"PENNY\", pd.Timestamp(\"2024-01-01\")),\n",
    "            (\"PENNY\", pd.Timestamp(\"2024-01-02\")),\n",
    "        ],\n",
    "        names=[\"Ticker\", \"Date\"],\n",
    "    )\n",
    "\n",
    "    df_penny = pd.DataFrame(test_data, index=index)\n",
    "\n",
    "    print(f\"df_penny_stock:\\n{df_penny}\\n\")\n",
    "\n",
    "    result = generate_features(df_penny)\n",
    "\n",
    "    # Check ATRP is reasonable (not inf/nan)\n",
    "    atrp_val = result.loc[(\"PENNY\", \"2024-01-02\"), \"ATRP\"]\n",
    "    if pd.isna(atrp_val) or np.isinf(atrp_val):\n",
    "        print(f\"‚úó Penny stock ATRP is {atrp_val} (should be finite)\")\n",
    "        all_passed = False\n",
    "    else:\n",
    "        print(f\"‚úì Penny stock ATRP is {atrp_val:.4f}\")\n",
    "\n",
    "    # Test 2: Single row\n",
    "    print(\"\\n2. Testing single row data...\")\n",
    "    test_data_single = {\n",
    "        \"Adj Open\": [100],\n",
    "        \"Adj High\": [101],\n",
    "        \"Adj Low\": [99],\n",
    "        \"Adj Close\": [100],\n",
    "        \"Volume\": [1000],\n",
    "    }\n",
    "\n",
    "    index_single = pd.MultiIndex.from_tuples(\n",
    "        [(\"SINGLE\", pd.Timestamp(\"2024-01-01\"))], names=[\"Ticker\", \"Date\"]\n",
    "    )\n",
    "\n",
    "    df_single = pd.DataFrame(test_data_single, index=index_single)\n",
    "\n",
    "    print(f\"df_single:\\n{df_single}\\n\")\n",
    "\n",
    "    result_single = generate_features(\n",
    "        df_single, quality_window=3, quality_min_periods=2\n",
    "    )\n",
    "\n",
    "    # TR should be NaN (no previous close)\n",
    "    if pd.isna(result_single.loc[(\"SINGLE\", \"2024-01-01\"), \"TR\"]):\n",
    "        print(\"‚úì Single row TR is NaN (correct)\")\n",
    "    else:\n",
    "        print(\n",
    "            f\"‚úó Single row TR should be NaN but got {result_single.loc[('SINGLE', '2024-01-01'), 'TR']}\"\n",
    "        )\n",
    "        all_passed = False\n",
    "\n",
    "    # Rolling metrics should be NaN with min_periods=2\n",
    "    if pd.isna(result_single.loc[(\"SINGLE\", \"2024-01-01\"), \"RollingStalePct\"]):\n",
    "        print(\"‚úì Single row rolling metrics are NaN (correct - insufficient periods)\")\n",
    "    else:\n",
    "        print(\n",
    "            f\"‚úó Rolling metrics should be NaN but got {result_single.loc[('SINGLE', '2024-01-01'), 'RollingStalePct']}\"\n",
    "        )\n",
    "        all_passed = False\n",
    "\n",
    "    if all_passed:\n",
    "        print(\"\\n‚úÖ All edge case tests passed!\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå Some edge case tests failed!\")\n",
    "\n",
    "    return all_passed\n",
    "\n",
    "\n",
    "def test_zero_division_protection():\n",
    "    \"\"\"Test that Zero Price doesn't cause Inf values in ATRP\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"Running test_zero_division_protection...\")\n",
    "\n",
    "    test_data = {\n",
    "        \"Adj Open\": [10, 10, 10],\n",
    "        \"Adj High\": [12, 12, 12],\n",
    "        \"Adj Low\": [8, 8, 8],\n",
    "        \"Adj Close\": [10, 0, 10],  # Day 2 Price is ZERO\n",
    "        \"Volume\": [1000, 1000, 1000],\n",
    "    }\n",
    "    index = pd.MultiIndex.from_tuples(\n",
    "        [(\"ZERO\", pd.Timestamp(f\"2024-01-0{i}\")) for i in range(1, 4)],\n",
    "        names=[\"Ticker\", \"Date\"],\n",
    "    )\n",
    "    df_test = pd.DataFrame(test_data, index=index)\n",
    "\n",
    "    print(f\"test_zero_division_protection_df:\\n{df_test}\\n\")\n",
    "\n",
    "    # Run features\n",
    "    result = generate_features(df_test, atr_period=2)\n",
    "\n",
    "    # Check Day 2 ATRP\n",
    "    atrp_val = result.loc[(\"ZERO\", \"2024-01-02\"), \"ATRP\"]\n",
    "\n",
    "    if pd.isna(atrp_val):\n",
    "        print(\"‚úÖ Zero Division Test Passed: ATRP is NaN when Close is 0.\")\n",
    "        return True\n",
    "    elif np.isinf(atrp_val):\n",
    "        print(f\"‚ùå Zero Division Test Failed: ATRP is Infinite ({atrp_val}).\")\n",
    "        return False\n",
    "    else:\n",
    "        print(f\"‚ùå Zero Division Test Failed: Unexpected value {atrp_val}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def test_unsorted_input_handling():\n",
    "    \"\"\"Test that function handles unsorted dates correctly via sorting\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"Running test_unsorted_input_handling...\")\n",
    "\n",
    "    # Data is out of order: Day 2, Day 1, Day 3\n",
    "    index = pd.MultiIndex.from_tuples(\n",
    "        [\n",
    "            (\"A\", pd.Timestamp(\"2024-01-02\")),\n",
    "            (\"A\", pd.Timestamp(\"2024-01-01\")),\n",
    "            (\"A\", pd.Timestamp(\"2024-01-03\")),\n",
    "        ],\n",
    "        names=[\"Ticker\", \"Date\"],\n",
    "    )\n",
    "\n",
    "    # Prices: 100 -> 105 -> 110\n",
    "    # If processed in order given:\n",
    "    # 1. 105 (No prev)\n",
    "    # 2. 100 (Prev is 105) -> Change -5\n",
    "    # 3. 110 (Prev is 100) -> Change +10\n",
    "\n",
    "    # If sorted correctly:\n",
    "    # 1. 100 (No prev)\n",
    "    # 2. 105 (Prev is 100) -> Change +5\n",
    "    # 3. 110 (Prev is 105) -> Change +5\n",
    "\n",
    "    test_data = {\n",
    "        \"Adj Open\": [100, 100, 100],\n",
    "        \"Adj High\": [105, 100, 110],\n",
    "        \"Adj Low\": [105, 100, 110],\n",
    "        \"Adj Close\": [105, 100, 110],  # 105, 100, 110\n",
    "        \"Volume\": [100, 100, 100],\n",
    "    }\n",
    "\n",
    "    df_test = pd.DataFrame(test_data, index=index)\n",
    "\n",
    "    print(f\"test_unsorted_input_handling_df:\\n{df_test}\\n\")\n",
    "\n",
    "    # Run features\n",
    "    result = generate_features(df_test)\n",
    "\n",
    "    # Inspect 2024-01-02 (Should be Day 2 in sorted order)\n",
    "    # Prev close (Jan 1) was 100. Current High 105. TR should be roughly 5.\n",
    "    tr_day_2 = result.loc[(\"A\", \"2024-01-02\"), \"TR\"]\n",
    "\n",
    "    # If it wasn't sorted, Day 2 would be treated as the first row (TR=NaN)\n",
    "    # or compared against whatever came before it in memory.\n",
    "\n",
    "    if pd.isna(tr_day_2):\n",
    "        print(\"‚ùå Sorting Test Failed: Day 2 TR is NaN (likely treated as first row).\")\n",
    "        return False\n",
    "    elif abs(tr_day_2 - 5.0) < 0.1:\n",
    "        print(\"‚úÖ Sorting Test Passed: Logic applied in correct chronological order.\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"‚ùå Sorting Test Failed: Day 2 TR is {tr_day_2}, expected ~5.0\")\n",
    "        return False\n",
    "\n",
    "    \"\"\"Test TR = max(High-Low, |High-PrevClose|, |Low-PrevClose|) using robust pandas testing\"\"\"\n",
    "    print(\"Running test_true_range_calculation (Robust Version)...\")\n",
    "\n",
    "    # 1. SETUP: Create test data\n",
    "    test_data = {\n",
    "        \"Adj Open\": [100, 105, 95, 98, 102],\n",
    "        \"Adj High\": [105, 108, 97, 102, 105],\n",
    "        \"Adj Low\": [95, 103, 93, 100, 98],\n",
    "        \"Adj Close\": [100, 106, 96, 99, 103],\n",
    "        \"Volume\": [1000, 1200, 800, 900, 1100],\n",
    "    }\n",
    "\n",
    "    index = pd.MultiIndex.from_tuples(\n",
    "        [\n",
    "            (\"TEST\", pd.Timestamp(\"2024-01-01\")),\n",
    "            (\"TEST\", pd.Timestamp(\"2024-01-02\")),\n",
    "            (\"TEST\", pd.Timestamp(\"2024-01-03\")),\n",
    "            (\"TEST\", pd.Timestamp(\"2024-01-04\")),\n",
    "            (\"TEST\", pd.Timestamp(\"2024-01-05\")),\n",
    "        ],\n",
    "        names=[\"Ticker\", \"Date\"],\n",
    "    )\n",
    "\n",
    "    df_test = pd.DataFrame(test_data, index=index)\n",
    "    print(f\"test_true_range_df input:\\n{df_test}\\n\")\n",
    "\n",
    "    # 2. EXECUTION: Run function\n",
    "    result = generate_features(df_test, quality_window=5, quality_min_periods=2)\n",
    "\n",
    "    # 3. ASSERTION: Define EXACT expected series\n",
    "    # Day 1: NaN (No prev close)\n",
    "    # Day 2: 8.0 (PrevClose=100, High=108, Low=103. |108-100|=8)\n",
    "    # Day 3: 13.0 (PrevClose=106, High=97, Low=93. |93-106|=13)\n",
    "    # Day 4: 6.0 (PrevClose=96, High=102, Low=100. |102-96|=6)\n",
    "    # Day 5: 7.0 (PrevClose=99, High=105, Low=98. High-Low=7)\n",
    "    expected_tr_values = [np.nan, 8.0, 13.0, 6.0, 7.0]\n",
    "\n",
    "    expected_series = pd.Series(\n",
    "        expected_tr_values, index=result.index, name=\"TR\", dtype=\"float64\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Check Series Equality\n",
    "        # check_exact=False allows for minor floating point differences (rtol=1e-4)\n",
    "        assert_series_equal(result[\"TR\"], expected_series, check_exact=False, rtol=1e-4)\n",
    "\n",
    "        print(\"‚úÖ PD Testing Assertion Passed! All TR values match expected logic.\")\n",
    "        return True\n",
    "\n",
    "    except AssertionError as e:\n",
    "        print(f\"‚ùå PD Testing Assertion Failed: {e}\")\n",
    "\n",
    "        # Helper output to see what went wrong\n",
    "        print(\"\\nDetailed Comparison (Actual vs Expected):\")\n",
    "        comparison = pd.concat(\n",
    "            [result[\"TR\"], expected_series], axis=1, keys=[\"Actual_TR\", \"Expected_TR\"]\n",
    "        )\n",
    "        comparison[\"Diff\"] = comparison[\"Actual_TR\"] - comparison[\"Expected_TR\"]\n",
    "        print(comparison)\n",
    "\n",
    "        return False\n",
    "\n",
    "\n",
    "def test_quality_rolling_features():\n",
    "    \"\"\"\n",
    "    Test RollingStalePct, RollMedDollarVol, and RollingSameVolCount\n",
    "    verifying logic for Stale(Vol=0), Stale(H=L), SameVolume, and Median calculations.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"Running test_quality_rolling_features...\")\n",
    "\n",
    "    # 1. SETUP: Create specific test data\n",
    "    # We set up 5 days to test a window of 4\n",
    "    test_data = {\n",
    "        # Day 1: Normal Base Day. $Vol = 10*100 = 1000.\n",
    "        # Day 2: Same Volume as D1. $Vol = 10*100 = 1000.\n",
    "        # Day 3: Stale (Volume=0). $Vol = 20*0 = 0.\n",
    "        # Day 4: Stale (High=Low). $Vol = 20*50 = 1000.\n",
    "        # Day 5: Normal High Vol. $Vol = 30*200 = 6000.\n",
    "        \"Adj Open\": [10, 10, 20, 20, 30],\n",
    "        \"Adj High\": [12, 12, 22, 20, 35],  # Day 4 High=20\n",
    "        \"Adj Low\": [8, 8, 18, 20, 25],  # Day 4 Low=20 (H=L)\n",
    "        \"Adj Close\": [10, 10, 20, 20, 30],\n",
    "        \"Volume\": [100, 100, 0, 50, 200],  # Day 2 same as D1, Day 3 is 0\n",
    "    }\n",
    "\n",
    "    index = pd.MultiIndex.from_tuples(\n",
    "        [(\"TEST\", pd.Timestamp(f\"2024-01-0{i}\")) for i in range(1, 6)],\n",
    "        names=[\"Ticker\", \"Date\"],\n",
    "    )\n",
    "\n",
    "    df_test = pd.DataFrame(test_data, index=index)\n",
    "    print(f\"Input Data:\\n{df_test}\")\n",
    "\n",
    "    # 2. EXECUTION: Use window=4, min_periods=2 to capture partial rolling\n",
    "    # We expect Day 1 to be NaN (count=1 < min_periods=2)\n",
    "    result = generate_features(df_test, quality_window=4, quality_min_periods=2)\n",
    "\n",
    "    # 3. VERIFICATION\n",
    "\n",
    "    # --- A. Test RollingStalePct ---\n",
    "    print(\"\\n--- Testing RollingStalePct ---\")\n",
    "    # Logic:\n",
    "    # Day 1: IsStale=0. Result=NaN (min_periods)\n",
    "    # Day 2: IsStale=0. Window=[0,0]. Mean=0.0\n",
    "    # Day 3: IsStale=1 (Vol=0). Window=[0,0,1]. Mean=1/3 (~0.333)\n",
    "    # Day 4: IsStale=1 (High=Low). Window=[0,0,1,1]. Mean=2/4 = 0.5\n",
    "    # Day 5: IsStale=0. Window=[0,1,1,0] (Day 1 drops off). Mean=2/4 = 0.5\n",
    "\n",
    "    expected_stale = pd.Series(\n",
    "        [np.nan, 0.0, 1 / 3, 0.5, 0.5], index=result.index, name=\"RollingStalePct\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        assert_series_equal(\n",
    "            result[\"RollingStalePct\"], expected_stale, check_exact=False, rtol=1e-4\n",
    "        )\n",
    "        print(\"‚úÖ RollingStalePct Passed\")\n",
    "    except AssertionError as e:\n",
    "        print(f\"‚ùå RollingStalePct Failed: {e}\")\n",
    "        return False\n",
    "\n",
    "    # --- B. Test RollMedDollarVol ---\n",
    "    print(\"\\n--- Testing RollMedDollarVol ---\")\n",
    "    # Logic: $Vol = Close * Volume\n",
    "    # D1: 1000. Result=NaN\n",
    "    # D2: 1000. Window=[1000, 1000]. Median=1000\n",
    "    # D3: 0.    Window=[1000, 1000, 0]. Sorted=[0, 1000, 1000]. Median=1000\n",
    "    # D4: 1000. Window=[1000, 1000, 0, 1000]. Sorted=[0, 1000, 1000, 1000]. Median=(1000+1000)/2 = 1000\n",
    "    # D5: 6000. Window=[1000, 0, 1000, 6000]. Sorted=[0, 1000, 1000, 6000]. Median=1000\n",
    "\n",
    "    expected_dollar = pd.Series(\n",
    "        [np.nan, 1000.0, 1000.0, 1000.0, 1000.0],\n",
    "        index=result.index,\n",
    "        name=\"RollMedDollarVol\",\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        assert_series_equal(\n",
    "            result[\"RollMedDollarVol\"], expected_dollar, check_exact=False, rtol=1e-4\n",
    "        )\n",
    "        print(\"‚úÖ RollMedDollarVol Passed\")\n",
    "    except AssertionError as e:\n",
    "        print(f\"‚ùå RollMedDollarVol Failed: {e}\")\n",
    "        return False\n",
    "\n",
    "    # --- C. Test RollingSameVolCount ---\n",
    "    print(\"\\n--- Testing RollingSameVolCount ---\")\n",
    "    # Logic: HasSameVolume = (Volume == PrevVolume)\n",
    "    # D1: NaN/0 (First row diff is NaN, astype(int) -> 0). Result=NaN (min_periods)\n",
    "    # D2: 1 (100==100). Window=[0, 1]. Sum=1\n",
    "    # D3: 0 (0!=100).   Window=[0, 1, 0]. Sum=1\n",
    "    # D4: 0 (50!=0).    Window=[0, 1, 0, 0]. Sum=1\n",
    "    # D5: 0 (200!=50).  Window=[1, 0, 0, 0] (D1 drops). Sum=1\n",
    "\n",
    "    expected_same_vol = pd.Series(\n",
    "        [np.nan, 1.0, 1.0, 1.0, 1.0], index=result.index, name=\"RollingSameVolCount\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        assert_series_equal(\n",
    "            result[\"RollingSameVolCount\"],\n",
    "            expected_same_vol,\n",
    "            check_exact=False,\n",
    "            rtol=1e-4,\n",
    "        )\n",
    "        print(\"‚úÖ RollingSameVolCount Passed\")\n",
    "    except AssertionError as e:\n",
    "        print(f\"‚ùå RollingSameVolCount Failed: {e}\")\n",
    "        return False\n",
    "\n",
    "    print(\"\\nüéâ All Rolling Quality Feature Tests Passed!\")\n",
    "    return True\n",
    "\n",
    "\n",
    "def test_math_metrics():\n",
    "    \"\"\"Verifies calculate_gain and calculate_sharpe are mathematically precise.\"\"\"\n",
    "    print(\"Running test_math_metrics...\")\n",
    "    all_passed = True\n",
    "\n",
    "    # --- 1. Test calculate_gain ---\n",
    "    s_gain = pd.Series([100, 105, 110])\n",
    "    res_gain = calculate_gain(s_gain)\n",
    "    if abs(res_gain - 0.10) < 1e-9:\n",
    "        print(\"‚úÖ Gain Calc (Positive): Passed\")\n",
    "    else:\n",
    "        print(f\"‚ùå Gain Calc (Positive): Failed. Got {res_gain}\")\n",
    "        all_passed = False\n",
    "\n",
    "    s_loss = pd.Series([100, 95, 90])\n",
    "    res_loss = calculate_gain(s_loss)\n",
    "    if abs(res_loss - (-0.10)) < 1e-9:\n",
    "        print(\"‚úÖ Gain Calc (Negative): Passed\")\n",
    "    else:\n",
    "        print(f\"‚ùå Gain Calc (Negative): Failed. Got {res_loss}\")\n",
    "        all_passed = False\n",
    "\n",
    "    # --- 2. Test calculate_sharpe ---\n",
    "    s_flat = pd.Series([0.01, 0.01, 0.01])\n",
    "    res_sharpe_flat = calculate_sharpe(s_flat)\n",
    "    if res_sharpe_flat == 0.0:\n",
    "        print(\"‚úÖ Sharpe (Zero Volatility): Passed (Returns 0.0)\")\n",
    "    else:\n",
    "        print(f\"‚ùå Sharpe (Zero Volatility): Failed. Got {res_sharpe_flat}\")\n",
    "        all_passed = False\n",
    "\n",
    "    return all_passed\n",
    "\n",
    "\n",
    "def test_engine_lag_logic():\n",
    "    \"\"\"\n",
    "    CRITICAL TEST: Verifies the '1-Day Lag' logic using NEW Variable Names.\n",
    "    \"\"\"\n",
    "    print(\"\\nRunning test_engine_lag_logic...\")\n",
    "\n",
    "    # 1. Setup Mock Data\n",
    "    dates = pd.to_datetime([\"2024-01-01\", \"2024-01-02\", \"2024-01-03\", \"2024-01-04\"])\n",
    "    # Prices:\n",
    "    # Jan 1: 100\n",
    "    # Jan 2: 100 (Decision T0)\n",
    "    # Jan 3: 110 (Entry T1 - We buy HERE)\n",
    "    # Jan 4: 121 (Exit - We sell HERE)\n",
    "    prices = [100.0, 100.0, 110.0, 121.0]\n",
    "\n",
    "    df_ohlcv = pd.DataFrame(\n",
    "        {\n",
    "            \"Adj Close\": prices,\n",
    "            \"Adj High\": prices,\n",
    "            \"Adj Low\": prices,\n",
    "            \"Adj Open\": prices,\n",
    "            \"Volume\": [1000] * 4,\n",
    "        },\n",
    "        index=pd.MultiIndex.from_product([[\"MOCK\"], dates], names=[\"Ticker\", \"Date\"]),\n",
    "    )\n",
    "\n",
    "    # 2. Initialize Engine\n",
    "    engine = AlphaEngine(df_ohlcv)\n",
    "\n",
    "    # 3. Run Strategy\n",
    "    # Decision Date = 2024-01-02.\n",
    "    inputs = EngineInput(\n",
    "        mode=\"Manual List\",\n",
    "        start_date=pd.Timestamp(\"2024-01-02\"),\n",
    "        lookback_period=1,\n",
    "        holding_period=1,  # <--- FIXED: Set to 1 to fit the 4-day dataset\n",
    "        metric=\"Price\",\n",
    "        benchmark_ticker=\"MOCK\",\n",
    "        manual_tickers=[\"MOCK\"],\n",
    "    )\n",
    "\n",
    "    res = engine.run(inputs)\n",
    "\n",
    "    # --- SAFETY CHECK ---\n",
    "    if res.error_msg:\n",
    "        print(f\"‚ùå TEST FAILED: Engine returned error: {res.error_msg}\")\n",
    "        return False\n",
    "\n",
    "    # 4. Analyze Results\n",
    "    metrics = res.perf_metrics\n",
    "\n",
    "    print(f\"  Decision Date: {res.decision_date.date()}\")\n",
    "    print(f\"  Buy Date (T+1): {res.buy_date.date()}\")\n",
    "\n",
    "    # Use the NEW metric key\n",
    "    if \"holding_p_gain\" in metrics:\n",
    "        holding_gain = metrics[\"holding_p_gain\"]\n",
    "        print(f\"  Holding Gain: {holding_gain:.4f}\")\n",
    "    else:\n",
    "        print(\"‚ùå 'holding_p_gain' not found in metrics!\")\n",
    "        return False\n",
    "\n",
    "    # Verification Logic\n",
    "    # Buy @ Jan 3 Close (110) -> Sell @ Jan 4 Close (121)\n",
    "    # Math: (121 / 110) - 1 = 1.1 - 1 = 0.10\n",
    "    expected_gain = 0.10\n",
    "\n",
    "    if abs(holding_gain - expected_gain) < 1e-4:\n",
    "        print(f\"‚úÖ LAG VERIFIED: Holding Gain is {holding_gain:.2%}.\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"‚ùå LAG FAILURE: Holding Gain is {holding_gain:.2%}.\")\n",
    "        print(f\"   Expected {expected_gain:.2%} (based on 121/110).\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def run_all_tests():\n",
    "    \"\"\"Run all tests\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"STARTING UNIT TESTS\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # Run the math tests\n",
    "    results[\"Math Metrics\"] = test_math_metrics()\n",
    "\n",
    "    # Run the engine logic test (Requires updated Engine code)\n",
    "    try:\n",
    "        results[\"Engine Lag Logic\"] = test_engine_lag_logic()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Engine Logic crashed: {e}\")\n",
    "        results[\"Engine Lag Logic\"] = False\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"TEST SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    passed = sum(results.values())\n",
    "    total = len(results)\n",
    "\n",
    "    for test_name, result in results.items():\n",
    "        status = \"‚úÖ PASS\" if result else \"‚ùå FAIL\"\n",
    "        print(f\"{status}: {test_name}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "    return passed == total\n",
    "\n",
    "\n",
    "# run_all_tests()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba143293",
   "metadata": {},
   "source": [
    "#####  ========== VERIFICATION TEST START, DO NOT DELETE =========  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32907865",
   "metadata": {},
   "source": [
    "==== Test Input Meta Data ====  \n",
    "input_Metadata: 'C:\\Users\\ping\\Files_win10\\python\\py311\\stocks\\notebooks_RLVR\\bot_verification\\inputs_Metadata.csv'\n",
    ",Value  \n",
    "mode,Ranking  \n",
    "start_date,2025-01-17 00:00:00  \n",
    "lookback_period,10  \n",
    "holding_period,5  \n",
    "metric,Sharpe (ATR)  \n",
    "benchmark_ticker,SPY  \n",
    "rank_start,1  \n",
    "rank_end,10  \n",
    "quality_thresholds,\"{'min_median_dollar_volume': 1000000, 'min_liquidity_percentile': 0.4, 'max_stale_pct': 0.05, 'max_same_vol_count': 10}\"  \n",
    "manual_tickers,[]  \n",
    "debug,True  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17c79f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ========== DO NOT DELETE =========\n",
    "# # Use to verify bot calculation.\n",
    "# # For inputs in image \"C:\\Users\\ping\\Files_win10\\python\\py311\\stocks\\notebooks_RLVR\\bot_verification\\bot_calc_verification.PNG\"\n",
    "# # plot_walk_forward_analyzer plot should match results in image\n",
    "# #\n",
    "# data_path_test = r\"C:\\Users\\ping\\Files_win10\\python\\py311\\stocks\\notebooks_RLVR\\bot_verification\\df_OHLCV_stocks_etfs.parquet\"\n",
    "# df_ohlcv = pd.read_parquet(data_path_test, engine=\"pyarrow\")\n",
    "\n",
    "# print(f\"df_ohlcv.info():\\n{df_ohlcv.info()}\")\n",
    "# df_ohlcv\n",
    "# # ========== DO NOT DELETE ========="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53ec71d",
   "metadata": {},
   "source": [
    "#####  ================= VERIFICATION TEST END ================  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "232740e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = (\n",
    "    r\"c:\\Users\\ping\\Files_win10\\python\\py311\\stocks\\data\\df_OHLCV_stocks_etfs.parquet\"\n",
    ")\n",
    "\n",
    "df_ohlcv = pd.read_parquet(data_path, engine=\"pyarrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "957c5c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating features... this might take few minutes...\n",
      "1. Calculating Features...\n",
      "2. Pivoting Price Matrix...\n",
      "‚úÖ Optimization Complete. Ready for UI.\n"
     ]
    }
   ],
   "source": [
    "# Calculate features ONCE and store them in a variable\n",
    "print(\"Calculating features... this might take few minutes...\")\n",
    "print(\"1. Calculating Features...\")\n",
    "my_features = generate_features(\n",
    "    df_ohlcv=df_ohlcv, atr_period=14, quality_window=252, quality_min_periods=126\n",
    ")\n",
    "\n",
    "print(\"2. Pivoting Price Matrix...\")\n",
    "# This is the line that takes 12 seconds, but now we only run it ONCE.\n",
    "my_close_matrix = df_ohlcv[\"Adj Close\"].unstack(level=0)\n",
    "\n",
    "print(\"‚úÖ Optimization Complete. Ready for UI.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eaa8a33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_features:\n",
      "                      ATR    ATRP   ROC_1   ROC_3   ROC_5  ROC_10  ROC_21  RollingStalePct  RollMedDollarVol  RollingSameVolCount\n",
      "Ticker Date                                                                                                                      \n",
      "A      1999-11-18     NaN     NaN     NaN     NaN     NaN     NaN     NaN              NaN               NaN                  NaN\n",
      "       1999-11-19  2.5074  0.1037 -0.0824     NaN     NaN     NaN     NaN              NaN               NaN                  NaN\n",
      "       1999-11-22  2.4967  0.0948  0.0898     NaN     NaN     NaN     NaN              NaN               NaN                  NaN\n",
      "       1999-11-23  2.4895  0.1039 -0.0909 -0.0909     NaN     NaN     NaN              NaN               NaN                  NaN\n",
      "       1999-11-24  2.3945  0.0974  0.0266  0.0170     NaN     NaN     NaN              NaN               NaN                  NaN\n",
      "...                   ...     ...     ...     ...     ...     ...     ...              ...               ...                  ...\n",
      "ZWS    2025-12-16  1.0346  0.0219 -0.0055 -0.0090  0.0268 -0.0138  0.0259              0.0        3.2634e+07                  0.0\n",
      "       2025-12-17  1.0557  0.0226 -0.0110 -0.0106 -0.0047 -0.0171  0.0424              0.0        3.2650e+07                  0.0\n",
      "       2025-12-18  1.0568  0.0223  0.0154 -0.0013 -0.0048  0.0094  0.0527              0.0        3.2650e+07                  0.0\n",
      "       2025-12-19  1.0248  0.0216  0.0030  0.0072  0.0076  0.0137  0.0476              0.0        3.2650e+07                  0.0\n",
      "       2025-12-22  1.0213  0.0212  0.0141  0.0328  0.0158  0.0386  0.0608              0.0        3.2634e+07                  0.0\n",
      "\n",
      "[9476424 rows x 10 columns]\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 9476424 entries, ('A', Timestamp('1999-11-18 00:00:00')) to ('ZWS', Timestamp('2025-12-22 00:00:00'))\n",
      "Data columns (total 10 columns):\n",
      " #   Column               Dtype  \n",
      "---  ------               -----  \n",
      " 0   ATR                  float64\n",
      " 1   ATRP                 float64\n",
      " 2   ROC_1                float64\n",
      " 3   ROC_3                float64\n",
      " 4   ROC_5                float64\n",
      " 5   ROC_10               float64\n",
      " 6   ROC_21               float64\n",
      " 7   RollingStalePct      float64\n",
      " 8   RollMedDollarVol     float64\n",
      " 9   RollingSameVolCount  float64\n",
      "dtypes: float64(10)\n",
      "memory usage: 759.8+ MB\n"
     ]
    }
   ],
   "source": [
    "print(f\"my_features:\\n{my_features}\\n\")\n",
    "my_features.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96d82578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ‚öôÔ∏è Initializing AlphaEngine v2.2 (Sanitized) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dd1ad3a239949a5bd3e8d7aced07baf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<b>1. Timeline Configuration:</b> (Past <--- Decision ---> Future)'), HBox(children‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7eacdcbe7df4eeabfd17a1282f16b6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'line': {'width': 2},\n",
       "              'name': 'SNX',\n",
       "              'type': 'scatter',\n",
       "              'uid': 'e3ec8161-a5c4-4b20-b5fc-8554161fc72b',\n",
       "              'visible': True,\n",
       "              'x': array([datetime.datetime(2025, 1, 2, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 3, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 6, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 7, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 8, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 10, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 13, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 14, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 15, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 16, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 17, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 21, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 22, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 23, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 24, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 27, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 28, 0, 0)], dtype=object),\n",
       "              'y': array([1.        , 1.01245041, 1.01562609, 1.05794529, 1.04824424, 1.15050115,\n",
       "                          1.14045208, 1.16792824, 1.16981625, 1.17762929, 1.17702896, 1.20853344,\n",
       "                          1.2147978 , 1.22931022, 1.2271525 , 1.21587666, 1.22104476])},\n",
       "             {'line': {'width': 2},\n",
       "              'name': 'TRGP',\n",
       "              'type': 'scatter',\n",
       "              'uid': '88521089-aaaf-432d-b835-6c857228bbb7',\n",
       "              'visible': True,\n",
       "              'x': array([datetime.datetime(2025, 1, 2, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 3, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 6, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 7, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 8, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 10, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 13, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 14, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 15, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 16, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 17, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 21, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 22, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 23, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 24, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 27, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 28, 0, 0)], dtype=object),\n",
       "              'y': array([1.        , 1.01699117, 1.01693534, 1.02600325, 1.04872887, 1.055943  ,\n",
       "                          1.07871888, 1.10008208, 1.12258436, 1.16328388, 1.17841568, 1.18660696,\n",
       "                          1.15410429, 1.15476317, 1.14208819, 1.08838983, 1.10548709])},\n",
       "             {'line': {'width': 2},\n",
       "              'name': 'USFR',\n",
       "              'type': 'scatter',\n",
       "              'uid': '475e3c41-ef2a-4bef-b5c9-28932570c7de',\n",
       "              'visible': True,\n",
       "              'x': array([datetime.datetime(2025, 1, 2, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 3, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 6, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 7, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 8, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 10, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 13, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 14, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 15, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 16, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 17, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 21, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 22, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 23, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 24, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 27, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 28, 0, 0)], dtype=object),\n",
       "              'y': array([1.        , 1.00019819, 1.00039638, 1.00059663, 1.00079482, 1.00139145,\n",
       "                          1.00158964, 1.00198602, 1.00198602, 1.00198602, 1.00238446, 1.00258265,\n",
       "                          1.00238446, 1.00258265, 1.00298109, 1.00317928, 1.00341875])},\n",
       "             {'line': {'width': 2},\n",
       "              'name': 'BIL',\n",
       "              'type': 'scatter',\n",
       "              'uid': 'a79288e0-ea8f-466a-b3ef-6132d9427984',\n",
       "              'visible': True,\n",
       "              'x': array([datetime.datetime(2025, 1, 2, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 3, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 6, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 7, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 8, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 10, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 13, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 14, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 15, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 16, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 17, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 21, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 22, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 23, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 24, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 27, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 28, 0, 0)], dtype=object),\n",
       "              'y': array([1.        , 1.00010939, 1.00021878, 1.00043642, 1.00065519, 1.00098336,\n",
       "                          1.00109275, 1.00131152, 1.00142091, 1.00142091, 1.00196786, 1.00196786,\n",
       "                          1.00218663, 1.00218663, 1.00262419, 1.00273358, 1.00273358])},\n",
       "             {'line': {'width': 2},\n",
       "              'name': 'LNG',\n",
       "              'type': 'scatter',\n",
       "              'uid': 'ae0c3155-aa09-4d2f-90d9-dab9b75a530e',\n",
       "              'visible': True,\n",
       "              'x': array([datetime.datetime(2025, 1, 2, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 3, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 6, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 7, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 8, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 10, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 13, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 14, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 15, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 16, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 17, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 21, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 22, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 23, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 24, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 27, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 28, 0, 0)], dtype=object),\n",
       "              'y': array([1.        , 1.0085232 , 1.01500595, 1.01319425, 1.02080703, 1.02094428,\n",
       "                          1.04007228, 1.10194894, 1.11459877, 1.14986733, 1.14578644, 1.13159484,\n",
       "                          1.05784152, 1.06205966, 1.0510431 , 1.01749931, 1.02661268])},\n",
       "             {'line': {'width': 2},\n",
       "              'name': 'RPRX',\n",
       "              'type': 'scatter',\n",
       "              'uid': '942acbe5-3113-42b5-bc3c-59c7b04b41aa',\n",
       "              'visible': True,\n",
       "              'x': array([datetime.datetime(2025, 1, 2, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 3, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 6, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 7, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 8, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 10, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 13, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 14, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 15, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 16, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 17, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 21, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 22, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 23, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 24, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 27, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 28, 0, 0)], dtype=object),\n",
       "              'y': array([1.        , 1.00193989, 0.99844968, 1.00969546, 1.01589674, 1.14346818,\n",
       "                          1.16285513, 1.16440545, 1.19193754, 1.20007871, 1.19232313, 1.17487607,\n",
       "                          1.16634534, 1.18728261, 1.20434408, 1.22993628, 1.23846701])},\n",
       "             {'line': {'width': 2},\n",
       "              'name': 'MINT',\n",
       "              'type': 'scatter',\n",
       "              'uid': '47313e35-26cf-4f98-a5e9-4cfc977e44bf',\n",
       "              'visible': True,\n",
       "              'x': array([datetime.datetime(2025, 1, 2, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 3, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 6, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 7, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 8, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 10, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 13, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 14, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 15, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 16, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 17, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 21, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 22, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 23, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 24, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 27, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 28, 0, 0)], dtype=object),\n",
       "              'y': array([1.        , 1.00049935, 1.00069805, 1.00059818, 1.00099766, 1.00109753,\n",
       "                          1.00129623, 1.00149597, 1.00179454, 1.00209415, 1.00259246, 1.00259246,\n",
       "                          1.00279115, 1.00299089, 1.0034892 , 1.0034892 , 1.0034892 ])},\n",
       "             {'line': {'width': 2},\n",
       "              'name': 'SHV',\n",
       "              'type': 'scatter',\n",
       "              'uid': 'd173c3c7-75d6-4397-b9be-e1aa02b978ed',\n",
       "              'visible': True,\n",
       "              'x': array([datetime.datetime(2025, 1, 2, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 3, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 6, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 7, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 8, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 10, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 13, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 14, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 15, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 16, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 17, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 21, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 22, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 23, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 24, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 27, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 28, 0, 0)], dtype=object),\n",
       "              'y': array([1.        , 1.00027431, 1.00045403, 1.00063374, 1.00063374, 1.00090805,\n",
       "                          1.00108777, 1.00118236, 1.00136208, 1.00145666, 1.00191069, 1.00209041,\n",
       "                          1.00209041, 1.00227013, 1.00263902, 1.00281874, 1.00290387])},\n",
       "             {'line': {'width': 2},\n",
       "              'name': 'QRVO',\n",
       "              'type': 'scatter',\n",
       "              'uid': '41c2b46c-b8b0-41a5-81d4-461403996152',\n",
       "              'visible': True,\n",
       "              'x': array([datetime.datetime(2025, 1, 2, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 3, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 6, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 7, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 8, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 10, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 13, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 14, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 15, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 16, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 17, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 21, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 22, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 23, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 24, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 27, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 28, 0, 0)], dtype=object),\n",
       "              'y': array([1.        , 1.01247133, 1.03411697, 1.05547592, 1.04759174, 1.02766628,\n",
       "                          1.03081995, 1.04386468, 1.06307339, 1.05490252, 1.20713876, 1.25544725,\n",
       "                          1.24842317, 1.29830849, 1.27494266, 1.25616399, 1.2296445 ])},\n",
       "             {'line': {'width': 2},\n",
       "              'name': 'SGOV',\n",
       "              'type': 'scatter',\n",
       "              'uid': 'e8b1f435-9fed-4402-822d-3990392c19b0',\n",
       "              'visible': True,\n",
       "              'x': array([datetime.datetime(2025, 1, 2, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 3, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 6, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 7, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 8, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 10, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 13, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 14, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 15, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 16, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 17, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 21, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 22, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 23, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 24, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 27, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 28, 0, 0)], dtype=object),\n",
       "              'y': array([1.        , 1.00029902, 1.00029902, 1.00049836, 1.00069771, 1.0010964 ,\n",
       "                          1.00129575, 1.00139542, 1.00149509, 1.00149509, 1.00199346, 1.00209313,\n",
       "                          1.00229248, 1.00239215, 1.00269117, 1.00269117, 1.00299019])},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '7dcc0dce-dfdb-43cb-8f5b-91775a574516', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '8f10f387-256a-4222-a617-90c89805519c', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '49dc8c3e-aa47-44dc-819f-08e5c8ff3662', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '65a8b4a2-914a-441f-bd28-6b4a7e401e60', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '78cd61e8-5a74-4fdf-85ad-2e3b37a1b02c', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': 'de44865c-8fb5-47a9-9042-a87870438488', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': 'baa200d4-c1f2-4ded-89ce-14debe6f446a', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '59d15d4e-39e9-48bc-a0cb-970cdf5701b2', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '71f96a4d-faf0-4801-83aa-136bd6ab9850', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '0f8a70d7-e7b8-47ea-a378-91c03a9074dd', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '5af9b228-f337-4295-85a6-e04870624cd7', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '64f4bb20-ac25-4f83-a2ac-ffb13ffee86f', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '61bed1c7-dd5c-4da1-b3b7-a08559d973de', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '1e694fb4-6ebf-48bf-a718-8ea844d3497b', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': 'd4146b75-e484-4a85-8bd1-c28a9c3a7b42', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '4abaacde-7e90-404c-a4fc-b03f9893fc11', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': 'e885f400-c0fe-4497-b50f-4e142eeaef2b', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': 'cbdc4b2b-8641-4040-8a34-9a5737ab5cfd', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '527a52eb-5b12-4c4e-9425-d1ea5a553278', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '91de91ec-823b-4f15-8658-eab2e3bbbc24', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': 'aacf3f73-b070-477b-b007-1426f61f3c42', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '7c93d2b4-51c9-41a8-ab24-b03bb0d0907e', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '61b2d860-9dd2-4488-a5a5-b70d8d5f871b', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '7c6cfea5-11d1-427a-8ad3-97611de57c4d', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': 'a911881e-43fa-43e9-b9c0-f6a4f8469333', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '56535247-2344-4d21-952d-4a1cddcc5231', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': 'c4c8da6c-a6d8-405f-86ff-f4b33acf04fc', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '71d8a71e-7fdf-46aa-aae9-8d99ccaf1c4f', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '1fea9697-550a-4e4e-a877-8de8d9f8341f', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': 'a01f6354-f0df-4dd9-a9fb-ec5e2bcae1ac', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': 'c6c3ba42-19e4-441b-b607-1354bdf5c3fd', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '1d99cfa6-2eac-4c1b-adf9-bf0a32c0e067', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '4757086e-bb28-4e9f-b8ed-e7d335ea9b09', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '100771ed-cf98-46c4-bd2a-fc86df62dd23', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '05b048af-5fde-43a5-b611-cccaa16819f4', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '03ed526c-879b-464c-9f87-22c03481475e', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '256e937f-a2c1-418e-a0ff-fcb8f28f174b', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '112eaf70-9af4-4579-8d02-8ab773c8fa89', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': 'edddf850-68b1-4d4c-810f-e3d61d2b1f21', 'visible': False},\n",
       "             {'line': {'width': 2}, 'type': 'scatter', 'uid': '3029ecf6-2c0f-4331-a3f8-00c75865d487', 'visible': False},\n",
       "             {'line': {'color': 'black', 'dash': 'dash', 'width': 3},\n",
       "              'name': 'Benchmark (SPY)',\n",
       "              'type': 'scatter',\n",
       "              'uid': '485898d7-3325-46ba-81df-484b13afef8a',\n",
       "              'visible': True,\n",
       "              'x': array([datetime.datetime(2025, 1, 2, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 3, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 6, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 7, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 8, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 10, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 13, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 14, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 15, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 16, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 17, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 21, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 22, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 23, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 24, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 27, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 28, 0, 0)], dtype=object),\n",
       "              'y': array([1.        , 1.01250316, 1.01833681, 1.00682525, 1.00829621, 0.99290132,\n",
       "                          0.9944415 , 0.99581036, 1.01392393, 1.01197361, 1.02213362, 1.03148892,\n",
       "                          1.03728797, 1.0429503 , 1.03990454, 1.02519495, 1.0340034 ])},\n",
       "             {'line': {'color': 'green', 'width': 3},\n",
       "              'name': 'Group Portfolio',\n",
       "              'type': 'scatter',\n",
       "              'uid': '01e56dca-9bcf-4c49-b4e2-ae7ab879a16d',\n",
       "              'visible': True,\n",
       "              'x': array([datetime.datetime(2025, 1, 2, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 3, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 6, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 7, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 8, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 10, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 13, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 14, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 15, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 16, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 17, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 21, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 22, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 23, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 24, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 27, 0, 0),\n",
       "                          datetime.datetime(2025, 1, 28, 0, 0)], dtype=object),\n",
       "              'y': array([1.        , 1.00537562, 1.00822003, 1.01650775, 1.01850477, 1.04039997,\n",
       "                          1.04592805, 1.05856007, 1.06700689, 1.07542146, 1.09115419, 1.09683851,\n",
       "                          1.08532573, 1.09441466, 1.09139952, 1.0822778 , 1.08367916])}],\n",
       "    'layout': {'annotations': [{'bgcolor': 'red',\n",
       "                                'font': {'color': 'white'},\n",
       "                                'showarrow': False,\n",
       "                                'text': 'DECISION',\n",
       "                                'x': Timestamp('2025-01-17 00:00:00'),\n",
       "                                'xref': 'x',\n",
       "                                'y': 0.05,\n",
       "                                'yref': 'paper'},\n",
       "                               {'bgcolor': 'blue',\n",
       "                                'font': {'color': 'white'},\n",
       "                                'showarrow': False,\n",
       "                                'text': 'ENTRY (T+1)',\n",
       "                                'x': Timestamp('2025-01-21 00:00:00'),\n",
       "                                'xref': 'x',\n",
       "                                'y': 1.0,\n",
       "                                'yref': 'paper'}],\n",
       "               'height': 600,\n",
       "               'hovermode': 'x unified',\n",
       "               'shapes': [{'line': {'color': 'red', 'dash': 'dash', 'width': 2},\n",
       "                           'type': 'line',\n",
       "                           'x0': Timestamp('2025-01-17 00:00:00'),\n",
       "                           'x1': Timestamp('2025-01-17 00:00:00'),\n",
       "                           'xref': 'x',\n",
       "                           'y0': 0,\n",
       "                           'y1': 1,\n",
       "                           'yref': 'paper'},\n",
       "                          {'line': {'color': 'blue', 'dash': 'dot', 'width': 2},\n",
       "                           'type': 'line',\n",
       "                           'x0': Timestamp('2025-01-21 00:00:00'),\n",
       "                           'x1': Timestamp('2025-01-21 00:00:00'),\n",
       "                           'xref': 'x',\n",
       "                           'y0': 0,\n",
       "                           'y1': 1,\n",
       "                           'yref': 'paper'}],\n",
       "               'template': '...',\n",
       "               'title': {'text': 'Event-Driven Walk-Forward Analysis'}}\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_container, debug_container = plot_walk_forward_analyzer(\n",
    "    df_ohlcv=df_ohlcv,\n",
    "    precomputed_features=my_features,\n",
    "    precomputed_close=my_close_matrix,\n",
    "    default_start_date=\"2025-01-17\",\n",
    "    default_lookback=10,\n",
    "    default_holding=5,\n",
    "    default_strategy=\"Sharpe (ATR)\",\n",
    "    default_rank_start=1,\n",
    "    default_rank_end=10,\n",
    "    default_benchmark_ticker=GLOBAL_SETTINGS[\"benchmark_ticker\"],\n",
    "    master_calendar_ticker=GLOBAL_SETTINGS[\"calendar_ticker\"],\n",
    "    quality_thresholds=GLOBAL_SETTINGS[\"thresholds\"],\n",
    "    debug=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ab55888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Exporting audit data to: ./Audit_bot_v46_2025-01-17_SharpeATR/\n",
      "   ‚úÖ Saved Metadata:  inputs_Metadata.csv\n",
      "   ‚úÖ Saved DataFrame: audit_liquidity_universe_snapshot.csv\n",
      "   ‚úÖ Saved DataFrame: full_universe_ranking.csv\n",
      "   ‚úÖ Saved Series:    verification_portfolio_full_val.csv\n",
      "   ‚úÖ Saved Series:    verification_portfolio_full_ret.csv\n",
      "   ‚úÖ Saved Series:    verification_portfolio_full_atrp.csv\n",
      "   ‚úÖ Saved Series:    verification_portfolio_lookback_val.csv\n",
      "   ‚úÖ Saved Series:    verification_portfolio_lookback_ret.csv\n",
      "   ‚úÖ Saved Series:    verification_portfolio_lookback_atrp.csv\n",
      "   ‚úÖ Saved Series:    verification_portfolio_holding_val.csv\n",
      "   ‚úÖ Saved Series:    verification_portfolio_holding_ret.csv\n",
      "   ‚úÖ Saved Series:    verification_portfolio_holding_atrp.csv\n",
      "   ‚úÖ Saved Series:    verification_benchmark_full_val.csv\n",
      "   ‚úÖ Saved Series:    verification_benchmark_full_ret.csv\n",
      "   ‚úÖ Saved Series:    verification_benchmark_full_atrp.csv\n",
      "   ‚úÖ Saved Series:    verification_benchmark_lookback_val.csv\n",
      "   ‚úÖ Saved Series:    verification_benchmark_lookback_ret.csv\n",
      "   ‚úÖ Saved Series:    verification_benchmark_lookback_atrp.csv\n",
      "   ‚úÖ Saved Series:    verification_benchmark_holding_val.csv\n",
      "   ‚úÖ Saved Series:    verification_benchmark_holding_ret.csv\n",
      "   ‚úÖ Saved Series:    verification_benchmark_holding_atrp.csv\n",
      "   ‚úÖ Saved DataFrame: portfolio_raw_components_prices.csv\n",
      "   ‚úÖ Saved DataFrame: portfolio_raw_components_atrp.csv\n",
      "   ‚úÖ Saved DataFrame: benchmark_raw_components_prices.csv\n",
      "   ‚úÖ Saved DataFrame: benchmark_raw_components_atrp.csv\n",
      "\n",
      "‚ú® Export Complete. All numbers saved with 8 decimal places.\n"
     ]
    }
   ],
   "source": [
    "export_debug_to_csv(debug_container, source_label=\"bot_v46\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631da036",
   "metadata": {},
   "source": [
    "#####  Get Subset Data, Copy Cell Output and Paste into Excel with 'Import Wizard'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c34f99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date,Adj Open,Adj High,Adj Low,Adj Close,Volume\n",
      "2025-01-02,582.549,584.269,573.762,577.854,50793558\n",
      "2025-01-03,580.711,585.722,579.623,585.079,38333433\n",
      "2025-01-06,589.349,592.739,586.71,588.45,48239308\n",
      "2025-01-07,590.486,590.812,579.969,581.798,61102313\n",
      "2025-01-08,581.867,583.725,578.408,582.648,47860209\n",
      "2025-01-10,579.08,579.149,571.835,573.752,73963484\n",
      "2025-01-13,569.087,574.998,568.672,574.642,48472715\n",
      "2025-01-14,577.577,578.21,571.637,575.433,48989215\n",
      "2025-01-15,583.478,587.046,582.361,585.9,57568387\n",
      "2025-01-16,587.273,587.451,584.071,584.773,43828413\n",
      "2025-01-17,590.031,592.403,588.697,590.644,58752533\n",
      "2025-01-21,593.698,596.06,591.721,596.05,43032370\n",
      "2025-01-22,598.887,600.765,598.334,599.401,48761969\n",
      "2025-01-23,598.769,602.673,598.492,602.673,41635354\n",
      "2025-01-24,602.732,603.691,599.757,600.913,35011069\n",
      "2025-01-27,587.906,592.73,587.738,592.413,71187359\n",
      "2025-01-28,593.649,598.344,590.318,597.503,44955089\n",
      "\n",
      "Date,ATR,ATRP,ROC_1,ROC_3,ROC_5,ROC_10,ROC_21,RollingStalePct,RollMedDollarVol,RollingSameVolCount\n",
      "2025-01-02,6.971378751878971,0.012064256285980492,-0.002456510443190396,-0.017428890128276642,-0.02770724911285316,-0.029262204064353647,-0.028199427535476218,0.0,28448974215.475,0.0\n",
      "2025-01-03,7.035423126744755,0.01202474046538118,0.012503158237201717,0.006339945613380138,-0.015615115418265102,0.013069428307744602,-0.016505350497060145,0.0,28400995665.875,0.0\n",
      "2025-01-06,7.080035760548707,0.012031669233662515,0.005761615098132111,0.015835256707930734,0.000588331315549695,0.019218711570371028,-0.01694309312520037,0.0,28378186103.284,0.0\n",
      "2025-01-07,7.348818920509511,0.012631220665092542,-0.011304273940011988,0.006825253437719558,0.0006966028142754155,-0.00426330120881957,-0.02645384006546103,0.0,28378186103.284,0.0\n",
      "2025-01-08,7.203688997615975,0.012363706727931745,0.0014609881780274225,-0.004154994453740346,0.005819323052701941,-0.008742963012111526,-0.02687644052510274,0.0,28368298998.3385,0.0\n",
      "2025-01-10,7.4614969263576905,0.013004742338776494,-0.01526822369595382,-0.024977483218625363,-0.007098678905052336,-0.03460924315311098,-0.03677451956411171,0.0,28368298998.3385,0.0\n",
      "2025-01-13,7.380390003046428,0.012843457323074936,0.0015511928498725958,-0.012299801649369613,-0.01783861666544162,-0.033175180025573625,-0.0322715369523664,0.0,28336585077.9485,0.0\n",
      "2025-01-14,7.3227192885431185,0.012725581064247477,0.0013765092005109114,-0.01238311982534912,-0.022120825898547136,-0.021545508957599435,-0.03837264683027097,0.0,28303272760.1825,0.0\n",
      "2025-01-15,7.629167910790042,0.013021279929663837,0.018189780565243785,0.02117291094410123,0.007050557066198282,0.007752071318368081,-0.015810197005939775,0.0,28336585077.9485,0.0\n",
      "2025-01-16,7.325655917162182,0.012527349787288712,-0.0019235364396653631,0.017630107092763803,0.0036471420137029753,0.00948768896400165,-0.017508577035386663,0.0,28303272760.1825,0.0\n",
      "2025-01-17,7.347394780222026,0.012439633315875597,0.010039793218907134,0.026434007086837186,0.02944129170791565,0.022133618526478882,-0.011863116598492707,0.0,28303272760.1825,0.0\n",
      "2025-01-21,7.209438010206163,0.012095357789121991,0.009152721436262778,0.01732377538829155,0.03725449932305658,0.018751313925128077,0.0013052834581779305,0.0,28283334591.9635,0.0\n",
      "2025-01-22,7.0312638666200105,0.011730484044270881,0.0056220115762100065,0.02501483481624489,0.04165211240926392,0.018609907383804858,0.0378680971237908,0.0,28283334591.9635,0.0\n",
      "2025-01-23,6.82767359043287,0.01132898535430137,0.005458783018380098,0.020365905689383013,0.02862775217613933,0.03588015084273244,0.04385351101750401,0.0,28228329095.9225,0.0\n",
      "2025-01-24,6.620982619687671,0.011018204997541525,-0.0029203232930626877,0.008158711517490147,0.027600453509310396,0.03134825829660448,0.02845167374364399,0.0,28228329095.9225,0.0\n",
      "2025-01-27,7.089126718281405,0.011966527942974588,-0.014145142474867423,-0.011658305541698999,0.0029950359268866578,0.03252450536120155,0.007870197877802632,0.0,28228329095.9225,0.0\n",
      "2025-01-28,7.1560462384041665,0.01197658629061974,0.008591978906607345,-0.008578449673371735,0.0024377149567991196,0.03978303013006346,0.005354009028790907,0.0,28161285525.052498,0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_ticker = \"SPY\"\n",
    "_start_date = \"2025-01-02\"\n",
    "_end_date = \"2025-01-28\"\n",
    "\n",
    "_df = df_ohlcv.loc[_ticker][_start_date:_end_date]\n",
    "print(_df.to_csv())\n",
    "\n",
    "_df = my_features.loc[_ticker][_start_date:_end_date]\n",
    "print(_df.to_csv())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
