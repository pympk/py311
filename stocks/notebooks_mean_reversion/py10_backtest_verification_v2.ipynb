{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backtest Results Verification\n",
    "\n",
    "This notebook performs a manual, step-by-step calculation of portfolio returns for a **single, specific date** and compares them against the results generated by the automated backtesting engine (`py9`).\n",
    "\n",
    "Its purpose is to serve as a sanity check and a debugging tool to ensure the core logic of the backtester is correct.\n",
    "\n",
    "**Workflow:**\n",
    "1.  **Setup:** Define the single `VERIFICATION_DATE_STR` to be checked.\n",
    "2.  **Load Data:** Load the three required files: the portfolio selection for the target date, the historical price data, and the master backtest results file.\n",
    "3.  **Manual Calculation:** Manually identify the buy/sell dates and calculate the portfolio returns for each weighting scheme.\n",
    "4.  **Compare & Verify:** Extract the corresponding results from the master backtest file, display them side-by-side with the manual calculations, and assert that they are numerically equal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and Configuration\n",
    "\n",
    "**This is the only cell you need to edit.** Set the `VERIFICATION_DATE_STR` to the date of the selection run you want to verify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Notebook Dir: c:\\Users\\ping\\Files_win10\\python\\py311\\stocks\\notebooks_mean_reversion\n",
      "Calculated ROOT_DIR:   c:\\Users\\ping\\Files_win10\\python\\py311\\stocks\n",
      "Calculated SRC_DIR:    c:\\Users\\ping\\Files_win10\\python\\py311\\stocks\\src\n",
      "sys.path contains SRC_DIR: True\n",
      "Verifying backtest for selection date: 2025-08-07\n",
      "Selection File: c:\\Users\\ping\\Files_win10\\python\\py311\\stocks\\output\\selection_results\\2025-08-07_short_term_mean_reversion.parquet\n",
      "Backtest Results: c:\\Users\\ping\\Files_win10\\python\\py311\\stocks\\output\\backtest_results\\backtest_master_results.parquet\n",
      "Price Data: c:\\Users\\ping\\Files_win10\\python\\py311\\stocks\\data\\df_adj_close.parquet\n"
     ]
    }
   ],
   "source": [
    "# py10_backtest_verification.ipynb\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# --- THIS IS THE ONLY PARAMETER TO CHANGE ---\n",
    "VERIFICATION_DATE_STR = \"2025-08-08\" \n",
    "\n",
    "# --- Project Path Setup ---\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "\n",
    "# Corrected Logic:\n",
    "# The notebook is at ROOT/notebooks\n",
    "# To get to the ROOT directory, we need to go up one levels.\n",
    "# NOTEBOOK_DIR.parent -> .../stocks (This is the correct ROOT_DIR)\n",
    "ROOT_DIR = NOTEBOOK_DIR.parent\n",
    "if str(ROOT_DIR) not in sys.path:\n",
    "    sys.path.append(str(ROOT_DIR))\n",
    "\n",
    "SRC_DIR = ROOT_DIR / 'src'\n",
    "if str(SRC_DIR) not in sys.path:\n",
    "    sys.path.append(str(SRC_DIR))\n",
    "\n",
    "sys.path.append(str(ROOT_DIR / 'notebooks'))  # Add config.py to sys.path\n",
    "\n",
    "# --- Verification (Optional, but good for debugging) ---\n",
    "print(f\"Current Notebook Dir: {NOTEBOOK_DIR}\")\n",
    "print(f\"Calculated ROOT_DIR:   {ROOT_DIR}\")\n",
    "print(f\"Calculated SRC_DIR:    {SRC_DIR}\")\n",
    "print(f\"sys.path contains SRC_DIR: {str(SRC_DIR) in sys.path}\")\n",
    "\n",
    "# --- Local Imports ---\n",
    "import utils\n",
    "from config import DATE_STR, DAILY_RISK_FREE_RATE\n",
    "\n",
    "# --- File Path Construction (using our standard principles) ---\n",
    "# We derive all paths from the verification date\n",
    "SELECTION_DIR = ROOT_DIR / 'output' / 'selection_results'\n",
    "BACKTEST_DIR = ROOT_DIR / 'output' / 'backtest_results'\n",
    "DATA_DIR = ROOT_DIR / 'data' # Assuming data dir is at root/data\n",
    "\n",
    "\n",
    "# Construct the exact filenames we expect\n",
    "SELECTION_FILE_PATH = SELECTION_DIR / f\"{VERIFICATION_DATE_STR}_short_term_mean_reversion.parquet\"\n",
    "BACKTEST_RESULTS_PATH = BACKTEST_DIR / 'backtest_master_results.parquet'\n",
    "HISTORICAL_PRICES_PATH = DATA_DIR / 'df_adj_close.parquet'\n",
    "\n",
    "# --- Notebook Setup ---\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.float_format', '{:.6f}'.format)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# --- Verification ---\n",
    "print(f\"Verifying backtest for selection date: {VERIFICATION_DATE_STR}\")\n",
    "print(f\"Selection File: {SELECTION_FILE_PATH}\")\n",
    "print(f\"Backtest Results: {BACKTEST_RESULTS_PATH}\")\n",
    "print(f\"Price Data: {HISTORICAL_PRICES_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load All Required Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Loading all required data files ---\n",
      "✅ Successfully loaded selection file for 2025-08-07.\n",
      "✅ Successfully loaded master backtest results.\n",
      "✅ Successfully loaded historical price data.\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Step 1: Loading all required data files ---\")\n",
    "\n",
    "try:\n",
    "    # Load the specific portfolio selection for the verification date\n",
    "    df_selection = pd.read_parquet(SELECTION_FILE_PATH)\n",
    "    print(f\"✅ Successfully loaded selection file for {VERIFICATION_DATE_STR}.\")\n",
    "    \n",
    "    # Load the master backtest results file\n",
    "    df_backtest = pd.read_parquet(BACKTEST_RESULTS_PATH)\n",
    "    print(\"✅ Successfully loaded master backtest results.\")\n",
    "\n",
    "    # Load the historical price data\n",
    "    df_prices = pd.read_parquet(HISTORICAL_PRICES_PATH)\n",
    "    df_prices.index = pd.to_datetime(df_prices.index)\n",
    "    print(\"✅ Successfully loaded historical price data.\")\n",
    "    \n",
    "    data_loaded_successfully = True\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"❌ ERROR: Could not find a required file. {e}\")\n",
    "    data_loaded_successfully = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Manual Performance Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_prices.info()\n",
    "# VERIFICATION_DATE_STR = '2025-08-07'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 2: Manually calculating performance for 2025-08-07 ---\n",
      "Selection Date (actual used): 2025-08-07\n",
      "Buy Date (T+1): 2025-08-08\n",
      "Sell Date (T+2): 2025-08-11\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['CF', 'ACN'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSell Date (T+2): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msell_date.date()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Extract prices\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m buy_prices = \u001b[43mdf_prices\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbuy_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtickers\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     26\u001b[39m sell_prices = df_prices.loc[sell_date, tickers]\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Calculate returns and metrics\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ping\\Files_win10\\python\\py311\\.venv\\Lib\\site-packages\\pandas\\core\\indexing.py:1184\u001b[39m, in \u001b[36m_LocationIndexer.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   1182\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._is_scalar_access(key):\n\u001b[32m   1183\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.obj._get_value(*key, takeable=\u001b[38;5;28mself\u001b[39m._takeable)\n\u001b[32m-> \u001b[39m\u001b[32m1184\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getitem_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1185\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1186\u001b[39m     \u001b[38;5;66;03m# we by definition only have the 0th axis\u001b[39;00m\n\u001b[32m   1187\u001b[39m     axis = \u001b[38;5;28mself\u001b[39m.axis \u001b[38;5;129;01mor\u001b[39;00m \u001b[32m0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ping\\Files_win10\\python\\py311\\.venv\\Lib\\site-packages\\pandas\\core\\indexing.py:1368\u001b[39m, in \u001b[36m_LocIndexer._getitem_tuple\u001b[39m\u001b[34m(self, tup)\u001b[39m\n\u001b[32m   1366\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m suppress(IndexingError):\n\u001b[32m   1367\u001b[39m     tup = \u001b[38;5;28mself\u001b[39m._expand_ellipsis(tup)\n\u001b[32m-> \u001b[39m\u001b[32m1368\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getitem_lowerdim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1370\u001b[39m \u001b[38;5;66;03m# no multi-index, so validate all of the indexers\u001b[39;00m\n\u001b[32m   1371\u001b[39m tup = \u001b[38;5;28mself\u001b[39m._validate_tuple_indexer(tup)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ping\\Files_win10\\python\\py311\\.venv\\Lib\\site-packages\\pandas\\core\\indexing.py:1089\u001b[39m, in \u001b[36m_LocationIndexer._getitem_lowerdim\u001b[39m\u001b[34m(self, tup)\u001b[39m\n\u001b[32m   1087\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m section\n\u001b[32m   1088\u001b[39m         \u001b[38;5;66;03m# This is an elided recursive call to iloc/loc\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1089\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnew_key\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m   1091\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m IndexingError(\u001b[33m\"\u001b[39m\u001b[33mnot applicable\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ping\\Files_win10\\python\\py311\\.venv\\Lib\\site-packages\\pandas\\core\\indexing.py:1191\u001b[39m, in \u001b[36m_LocationIndexer.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   1189\u001b[39m maybe_callable = com.apply_if_callable(key, \u001b[38;5;28mself\u001b[39m.obj)\n\u001b[32m   1190\u001b[39m maybe_callable = \u001b[38;5;28mself\u001b[39m._check_deprecated_callable_usage(key, maybe_callable)\n\u001b[32m-> \u001b[39m\u001b[32m1191\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ping\\Files_win10\\python\\py311\\.venv\\Lib\\site-packages\\pandas\\core\\indexing.py:1420\u001b[39m, in \u001b[36m_LocIndexer._getitem_axis\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1417\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(key, \u001b[33m\"\u001b[39m\u001b[33mndim\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key.ndim > \u001b[32m1\u001b[39m:\n\u001b[32m   1418\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCannot index with multidimensional key\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1420\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getitem_iterable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1422\u001b[39m \u001b[38;5;66;03m# nested tuple slicing\u001b[39;00m\n\u001b[32m   1423\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_nested_tuple(key, labels):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ping\\Files_win10\\python\\py311\\.venv\\Lib\\site-packages\\pandas\\core\\indexing.py:1360\u001b[39m, in \u001b[36m_LocIndexer._getitem_iterable\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1357\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_key(key, axis)\n\u001b[32m   1359\u001b[39m \u001b[38;5;66;03m# A collection of keys\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1360\u001b[39m keyarr, indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_listlike_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1361\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.obj._reindex_with_indexers(\n\u001b[32m   1362\u001b[39m     {axis: [keyarr, indexer]}, copy=\u001b[38;5;28;01mTrue\u001b[39;00m, allow_dups=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1363\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ping\\Files_win10\\python\\py311\\.venv\\Lib\\site-packages\\pandas\\core\\indexing.py:1558\u001b[39m, in \u001b[36m_LocIndexer._get_listlike_indexer\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1555\u001b[39m ax = \u001b[38;5;28mself\u001b[39m.obj._get_axis(axis)\n\u001b[32m   1556\u001b[39m axis_name = \u001b[38;5;28mself\u001b[39m.obj._get_axis_name(axis)\n\u001b[32m-> \u001b[39m\u001b[32m1558\u001b[39m keyarr, indexer = \u001b[43max\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1560\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m keyarr, indexer\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ping\\Files_win10\\python\\py311\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6197\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6198\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6200\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6202\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n\u001b[32m   6203\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[32m   6204\u001b[39m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ping\\Files_win10\\python\\py311\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6252\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6249\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   6251\u001b[39m not_found = \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask.nonzero()[\u001b[32m0\u001b[39m]].unique())\n\u001b[32m-> \u001b[39m\u001b[32m6252\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in index\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: \"['CF', 'ACN'] not in index\""
     ]
    }
   ],
   "source": [
    "# In py10_backtest_verification.ipynb, replace the \"Step 2\" cell with this:\n",
    "\n",
    "if data_loaded_successfully:\n",
    "    print(f\"\\n--- Step 2: Manually calculating performance for {VERIFICATION_DATE_STR} ---\")\n",
    "    \n",
    "    # Isolate the tickers from our portfolio\n",
    "    tickers = df_selection.index.to_list()\n",
    "    \n",
    "    # Find the integer position of our selection date.\n",
    "    date_loc = df_prices.index.get_indexer([pd.to_datetime(VERIFICATION_DATE_STR)], method='ffill')[0]\n",
    "    \n",
    "    # --- NEW ROBUSTNESS CHECK ---\n",
    "    # Check if there are at least two more trading days in the price data.\n",
    "    if date_loc + 2 < len(df_prices.index):\n",
    "        \n",
    "        # --- Continue with calculation if data is sufficient ---\n",
    "        buy_date = df_prices.index[date_loc + 1]\n",
    "        sell_date = df_prices.index[date_loc + 2]\n",
    "        \n",
    "        print(f\"Selection Date (actual used): {df_prices.index[date_loc].date()}\")\n",
    "        print(f\"Buy Date (T+1): {buy_date.date()}\")\n",
    "        print(f\"Sell Date (T+2): {sell_date.date()}\")\n",
    "        \n",
    "        # Extract prices\n",
    "        buy_prices = df_prices.loc[buy_date, tickers]\n",
    "        sell_prices = df_prices.loc[sell_date, tickers]\n",
    "        \n",
    "        # Calculate returns and metrics\n",
    "        individual_returns = (sell_prices - buy_prices) / buy_prices\n",
    "        \n",
    "        weights_df = df_selection[['Weight_EW', 'Weight_IV', 'Weight_SW']]\n",
    "        weighted_returns = weights_df.multiply(individual_returns, axis=0)\n",
    "        manual_portfolio_results = weighted_returns.sum()\n",
    "        manual_portfolio_results.name = \"manual_return\"\n",
    "        manual_portfolio_results.index = manual_portfolio_results.index.str.split('_').str[-1]\n",
    "        \n",
    "        manual_std_dev = individual_returns.std()\n",
    "        manual_avg_return = individual_returns.mean()\n",
    "        \n",
    "        if manual_std_dev > 1e-9:\n",
    "            manual_sharpe = (manual_avg_return - DAILY_RISK_FREE_RATE) / manual_std_dev\n",
    "        else:\n",
    "            manual_sharpe = np.nan\n",
    "\n",
    "        # Display results\n",
    "        print(\"\\n--- Manual Calculation: Portfolio Returns ---\")\n",
    "        display(manual_portfolio_results.to_frame())\n",
    "        print(\"\\n--- Manual Calculation: Risk/Reward Metrics ---\")\n",
    "        print(f\"  - Std Dev of Returns: {manual_std_dev:.6f}\")\n",
    "        print(f\"  - Sharpe Ratio:       {manual_sharpe:.6f}\")\n",
    "\n",
    "    else:\n",
    "        # --- Halt and inform the user if data is insufficient ---\n",
    "        print(f\"\\n❌ VERIFICATION HALTED: Not enough future data to verify this date.\")\n",
    "        print(f\"   Selection Date '{VERIFICATION_DATE_STR}' requires prices for T+1 and T+2 trading days.\")\n",
    "        print(f\"   The latest date in your price data file ('{HISTORICAL_PRICES_PATH.name}') is {df_prices.index.max().date()}.\")\n",
    "        print(f\"   To fix, please update your price data or choose an older verification date.\")\n",
    "        # Clear the variables to prevent the next cell from running with stale data\n",
    "        if 'manual_portfolio_results' in locals(): del manual_portfolio_results\n",
    "        if 'manual_std_dev' in locals(): del manual_std_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Compare and Verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- NEW: Check if the manual calculation was successful before proceeding ---\n",
    "if 'manual_portfolio_results' in locals():\n",
    "\n",
    "    print(f\"\\n--- Step 3: Comparing manual results with automated backtest results ---\")\n",
    "    \n",
    "    # --- 1. Verify Weighted Portfolio Returns ---\n",
    "    print(\"\\n--- Verification 1: Portfolio Returns (Scheme-Dependent) ---\")\n",
    "    \n",
    "    automated_results_df = df_backtest[\n",
    "        (df_backtest['selection_date'] == VERIFICATION_DATE_STR)\n",
    "    ].set_index('scheme')\n",
    "\n",
    "    # Add a check to ensure we found automated results for this date\n",
    "    if automated_results_df.empty:\n",
    "        print(f\"❌ VERIFICATION SKIPPED: No automated backtest results found for {VERIFICATION_DATE_STR}.\")\n",
    "    else:\n",
    "        df_port_comparison = pd.concat([manual_portfolio_results, automated_results_df['portfolio_return']], axis=1)\n",
    "        df_port_comparison.columns = ['manual_return', 'backtest_return']\n",
    "        df_port_comparison['match'] = np.isclose(df_port_comparison['manual_return'], df_port_comparison['backtest_return'])\n",
    "\n",
    "        print(\"Comparison Table:\")\n",
    "        display(df_port_comparison)\n",
    "        \n",
    "        assert df_port_comparison['match'].all(), \"❌ VERIFICATION FAILED: Manual and backtest portfolio returns do not match!\"\n",
    "        print(\"✅ Portfolio Return Verification Successful.\")\n",
    "\n",
    "        # --- 2. Verify Scheme-Independent Metrics ---\n",
    "        print(\"\\n--- Verification 2: Risk/Reward Metrics (Scheme-Independent) ---\")\n",
    "        \n",
    "        automated_std_dev = automated_results_df['std_dev_return'].iloc[0]\n",
    "        automated_sharpe = automated_results_df['sharpe_ratio_period'].iloc[0]\n",
    "        \n",
    "        metrics_data = {\n",
    "            'Metric': ['std_dev_return', 'sharpe_ratio_period'],\n",
    "            'Manual_Value': [manual_std_dev, manual_sharpe],\n",
    "            'Backtest_Value': [automated_std_dev, automated_sharpe]\n",
    "        }\n",
    "        df_metrics_comparison = pd.DataFrame(metrics_data).set_index('Metric')\n",
    "        df_metrics_comparison['match'] = np.isclose(df_metrics_comparison['Manual_Value'], df_metrics_comparison['Backtest_Value'], equal_nan=True)\n",
    "\n",
    "        print(\"Comparison Table:\")\n",
    "        display(df_metrics_comparison)\n",
    "        \n",
    "        assert df_metrics_comparison['match'].all(), \"❌ VERIFICATION FAILED: Manual and backtest risk metrics do not match!\"\n",
    "        print(\"✅ Risk/Reward Metrics Verification Successful.\")\n",
    "        \n",
    "        print(\"\\n\\n✅✅✅ OVERALL VERIFICATION SUCCESSFUL ✅✅✅\")\n",
    "\n",
    "else:\n",
    "    # This block will run if Step 2 was halted\n",
    "    print(\"\\n--- Step 3: Skipped ---\")\n",
    "    print(\"Skipping comparison because manual calculation was not performed in the previous step.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'df_backtest:\\n{df_backtest.head(10)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
