{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OHLCV Data Cleaning Pipeline\n",
    "\n",
    "This notebook cleans the consolidated OHLCV data by ensuring data integrity and temporal alignment across all tickers.\n",
    "\n",
    "**Workflow:**\n",
    "\n",
    "1.  **Prerequisite:** A consolidated OHLCV Parquet file (e.g., `df_OHLCV_stocks_etfs.parquet`) must exist.\n",
    "2.  **Load Data:** The raw, consolidated OHLCV data is loaded from the Parquet file.\n",
    "3.  **Clean & Filter:** The data undergoes a two-step cleaning process:\n",
    "    *   **Date Alignment:** All tickers are aligned to a common date index based on a stable reference symbol (e.g., 'VOO'). Tickers with mismatched dates are removed.\n",
    "    *   **Completeness Check:** Any remaining tickers with `NaN` values or an incomplete date range are removed.\n",
    "4.  **Save Data:** The final, clean DataFrame is saved to a new Parquet file.\n",
    "5.  **Summarize:** A final report is generated detailing the number of tickers at each stage and listing those that were filtered out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and Configuration\n",
    "\n",
    "**This is the only cell you need to modify.** Adjust the filenames and reference symbol as needed for your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T02:27:45.895911Z",
     "iopub.status.busy": "2025-06-30T02:27:45.895911Z",
     "iopub.status.idle": "2025-06-30T02:27:53.166488Z",
     "shell.execute_reply": "2025-06-30T02:27:53.165481Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Root Directory: C:\\Users\\ping\\Files_win10\\python\\py311\\stocks_v0_works\n",
      "Source file: C:\\Users\\ping\\Files_win10\\python\\py311\\stocks_v0_works\\data\\df_OHLCV_stocks_etfs.parquet\n",
      "Destination file: C:\\Users\\ping\\Files_win10\\python\\py311\\stocks_v0_works\\data\\df_OHLCV_clean_stocks_etfs.parquet\n",
      "Reference symbol for date alignment: 'VOO'\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# --- Project Path Setup ---\n",
    "# Autodetect the project's root directory.\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "ROOT_DIR = NOTEBOOK_DIR.parent if NOTEBOOK_DIR.name == 'notebooks' else NOTEBOOK_DIR\n",
    "DATA_DIR = ROOT_DIR / 'data'\n",
    "SRC_DIR = ROOT_DIR / 'src'\n",
    "if str(SRC_DIR) not in sys.path:\n",
    "    sys.path.append(str(SRC_DIR))\n",
    "\n",
    "# Import custom utility functions now that the path is set\n",
    "import utils\n",
    "\n",
    "# --- File and Cleaning Configuration ---\n",
    "SOURCE_FILENAME = 'df_OHLCV_stocks_etfs.parquet'\n",
    "DEST_FILENAME = 'df_OHLCV_clean_stocks_etfs.parquet'\n",
    "REFERENCE_SYMBOL = 'VOO'  # Use this ticker's date index as the gold standard\n",
    "\n",
    "# --- Construct Full Paths ---\n",
    "SOURCE_PATH = DATA_DIR / SOURCE_FILENAME\n",
    "DEST_PATH = DATA_DIR / DEST_FILENAME\n",
    "\n",
    "# --- Notebook Setup ---\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 2000)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# --- Verification ---\n",
    "print(f\"Project Root Directory: {ROOT_DIR}\")\n",
    "print(f\"Source file: {SOURCE_PATH}\")\n",
    "print(f\"Destination file: {DEST_PATH}\")\n",
    "print(f\"Reference symbol for date alignment: '{REFERENCE_SYMBOL}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load Raw OHLCV Data\n",
    "\n",
    "Load the consolidated OHLCV data and perform an initial validation to ensure the reference symbol exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T02:27:53.218961Z",
     "iopub.status.busy": "2025-06-30T02:27:53.218961Z",
     "iopub.status.idle": "2025-06-30T02:27:54.041068Z",
     "shell.execute_reply": "2025-06-30T02:27:54.041068Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Loading raw data from df_OHLCV_stocks_etfs.parquet ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded data with 1640 unique tickers.\n",
      "Validation successful: Reference symbol 'VOO' is present.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 569121 entries, ('A', Timestamp('2025-06-26 00:00:00')) to ('ZWS', Timestamp('2024-02-01 00:00:00'))\n",
      "Data columns (total 5 columns):\n",
      " #   Column     Non-Null Count   Dtype  \n",
      "---  ------     --------------   -----  \n",
      " 0   Adj Open   569121 non-null  float64\n",
      " 1   Adj High   569121 non-null  float64\n",
      " 2   Adj Low    569121 non-null  float64\n",
      " 3   Adj Close  569121 non-null  float64\n",
      " 4   Volume     569121 non-null  int64  \n",
      "dtypes: float64(4), int64(1)\n",
      "memory usage: 24.0+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Adj Open</th>\n",
       "      <th>Adj High</th>\n",
       "      <th>Adj Low</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ticker</th>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">A</th>\n",
       "      <th>2025-06-26</th>\n",
       "      <td>119.18</td>\n",
       "      <td>120.54</td>\n",
       "      <td>118.04</td>\n",
       "      <td>119.42</td>\n",
       "      <td>1486310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-25</th>\n",
       "      <td>117.11</td>\n",
       "      <td>119.78</td>\n",
       "      <td>116.97</td>\n",
       "      <td>118.62</td>\n",
       "      <td>1690000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-24</th>\n",
       "      <td>117.33</td>\n",
       "      <td>118.00</td>\n",
       "      <td>116.57</td>\n",
       "      <td>117.64</td>\n",
       "      <td>1945300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Adj Open  Adj High  Adj Low  Adj Close   Volume\n",
       "Ticker Date                                                       \n",
       "A      2025-06-26    119.18    120.54   118.04     119.42  1486310\n",
       "       2025-06-25    117.11    119.78   116.97     118.62  1690000\n",
       "       2025-06-24    117.33    118.00   116.57     117.64  1945300"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"--- Step 1: Loading raw data from {SOURCE_PATH.name} ---\")\n",
    "\n",
    "try:\n",
    "    df_raw = pd.read_parquet(SOURCE_PATH, engine='pyarrow')\n",
    "    raw_ticker_count = len(df_raw.index.get_level_values('Ticker').unique())\n",
    "    print(f\"Successfully loaded data with {raw_ticker_count} unique tickers.\")\n",
    "    \n",
    "    # --- CRITICAL VALIDATION ---\n",
    "    # Check if the reference symbol exists before proceeding.\n",
    "    if REFERENCE_SYMBOL not in df_raw.index.get_level_values('Ticker'):\n",
    "        raise ValueError(f\"Reference symbol '{REFERENCE_SYMBOL}' not found in the raw data. Halting execution.\")\n",
    "    else:\n",
    "        print(f\"Validation successful: Reference symbol '{REFERENCE_SYMBOL}' is present.\")\n",
    "\n",
    "    df_raw.info()\n",
    "    display(df_raw.head(3))\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Source file not found at {SOURCE_PATH}\")\n",
    "    df_raw = None\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    df_raw = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Clean and Filter Data\n",
    "\n",
    "Apply the two-stage cleaning process using the custom utility functions. After each utility call, we restore the index names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T02:27:54.045102Z",
     "iopub.status.busy": "2025-06-30T02:27:54.045102Z",
     "iopub.status.idle": "2025-06-30T02:28:00.025483Z",
     "shell.execute_reply": "2025-06-30T02:28:00.025483Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 2: Starting data cleaning and filtering ---\n",
      "\n",
      "Part A: Filtering dates to align with the reference symbol...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 'Ticker' as the symbol identifier.\n",
      "Original number of Tickers: 1640\n",
      "Number of Tickers after filtering: 1559\n",
      "Number of Tickers filtered out: 81\n",
      "\n",
      "First 10 Tickers that were filtered out:\n",
      "['CRWV', 'CTRE', 'CRCL', 'ACIW', 'SAIC', 'LB', 'VG', 'OTF', 'EWU', 'EMLP']\n",
      "\n",
      "Example of dates for first filtered out Ticker:\n",
      "\n",
      "Dates for CRWV:\n",
      "DatetimeIndex(['2025-06-26', '2025-06-25', '2025-06-24', '2025-06-23', '2025-06-20', '2025-06-18', '2025-06-17', '2025-06-16', '2025-06-13', '2025-06-12', '2025-06-11', '2025-06-10', '2025-06-09', '2025-06-06', '2025-06-05', '2025-06-04', '2025-06-03', '2025-06-02', '2025-05-30', '2025-05-29', '2025-05-28', '2025-05-27', '2025-05-23', '2025-05-22', '2025-05-21', '2025-05-20', '2025-05-19', '2025-05-16', '2025-05-15', '2025-05-14', '2025-05-13', '2025-05-12', '2025-05-09', '2025-05-08', '2025-05-07', '2025-05-06', '2025-05-05', '2025-05-02', '2025-05-01', '2025-04-30', '2025-04-29', '2025-04-28', '2025-04-25', '2025-04-24', '2025-04-23', '2025-04-22', '2025-04-21', '2025-04-17', '2025-04-16', '2025-04-15', '2025-04-14', '2025-04-11', '2025-04-10', '2025-04-09', '2025-04-08', '2025-04-07', '2025-04-04', '2025-04-03', '2025-04-02', '2025-04-01', '2025-03-31', '2025-03-28'], dtype='datetime64[ns]', name='Date', freq=None)\n",
      "\n",
      "Filtered DataFrame info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 547209 entries, ('A', Timestamp('2025-06-26 00:00:00')) to ('ZWS', Timestamp('2024-02-01 00:00:00'))\n",
      "Data columns (total 5 columns):\n",
      " #   Column     Non-Null Count   Dtype  \n",
      "---  ------     --------------   -----  \n",
      " 0   Adj Open   547209 non-null  float64\n",
      " 1   Adj High   547209 non-null  float64\n",
      " 2   Adj Low    547209 non-null  float64\n",
      " 3   Adj Close  547209 non-null  float64\n",
      " 4   Volume     547209 non-null  int64  \n",
      "dtypes: float64(4), int64(1)\n",
      "memory usage: 23.0+ MB\n",
      "None\n",
      "Removed 81 tickers with non-matching date ranges.\n",
      "Tickers remaining after date alignment: 1559\n",
      "\n",
      "Part B: Filtering symbols with missing values (NaNs) or incomplete data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 0 tickers with missing data points.\n",
      "Tickers remaining after final cleaning: 1559\n"
     ]
    }
   ],
   "source": [
    "if df_raw is not None:\n",
    "    print(\"\\n--- Step 2: Starting data cleaning and filtering ---\")\n",
    "    \n",
    "    # --- Part A: Align dates to the reference symbol ---\n",
    "    print(\"\\nPart A: Filtering dates to align with the reference symbol...\")\n",
    "    df_filtered, filtered_out_symbols = utils.filter_df_dates_to_reference_symbol(\n",
    "        df=df_raw,\n",
    "        reference_symbol=REFERENCE_SYMBOL\n",
    "    )\n",
    "    \n",
    "    # FIX: Restore index names, as they can be lost by the utility function.\n",
    "    df_filtered.index.names = ['Ticker', 'Date']\n",
    "    \n",
    "    filter_ticker_count = len(df_filtered.index.get_level_values('Ticker').unique())\n",
    "    print(f\"Removed {len(filtered_out_symbols)} tickers with non-matching date ranges.\")\n",
    "    print(f\"Tickers remaining after date alignment: {filter_ticker_count}\")\n",
    "    \n",
    "    # --- Part B: Remove symbols with missing values or incomplete date ranges ---\n",
    "    print(\"\\nPart B: Filtering symbols with missing values (NaNs) or incomplete data...\")\n",
    "    df_clean, missing_values_symbols = utils.filter_symbols_with_missing_values(\n",
    "        df=df_filtered\n",
    "    )\n",
    "\n",
    "    # FIX: Restore index names again for the final clean DataFrame.\n",
    "    df_clean.index.names = ['Ticker', 'Date']\n",
    "\n",
    "    clean_ticker_count = len(df_clean.index.get_level_values('Ticker').unique())\n",
    "    print(f\"Removed {len(missing_values_symbols)} tickers with missing data points.\")\n",
    "    print(f\"Tickers remaining after final cleaning: {clean_ticker_count}\")\n",
    "\n",
    "else:\n",
    "    print(\"Skipping cleaning step because raw data failed to load.\")\n",
    "    df_clean = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Save Cleaned Data\n",
    "\n",
    "Save the fully cleaned DataFrame to a new Parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T02:28:00.025483Z",
     "iopub.status.busy": "2025-06-30T02:28:00.025483Z",
     "iopub.status.idle": "2025-06-30T02:28:00.830504Z",
     "shell.execute_reply": "2025-06-30T02:28:00.829987Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 3: Saving cleaned data ---"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved cleaned data to: C:\\Users\\ping\\Files_win10\\python\\py311\\stocks_v0_works\\data\\df_OHLCV_clean_stocks_etfs.parquet\n"
     ]
    }
   ],
   "source": [
    "if df_clean is not None and not df_clean.empty:\n",
    "    print(f\"\\n--- Step 3: Saving cleaned data ---\")\n",
    "    try:\n",
    "        # Ensure the destination directory exists\n",
    "        DEST_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        df_clean.to_parquet(DEST_PATH, engine='pyarrow', compression='zstd')\n",
    "        print(f\"Successfully saved cleaned data to: {DEST_PATH}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: Failed to save Parquet file. Details: {e}\")\n",
    "else:\n",
    "    print(\"\\nSkipping save step because the cleaned DataFrame is empty or does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Final Summary\n",
    "\n",
    "Provide a complete summary of the cleaning process, showing the number of tickers at each stage and listing those that were removed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T02:28:00.830504Z",
     "iopub.status.busy": "2025-06-30T02:28:00.830504Z",
     "iopub.status.idle": "2025-06-30T02:28:01.086638Z",
     "shell.execute_reply": "2025-06-30T02:28:01.085643Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 4: Cleaning Process Summary ---\n",
      "Initial ticker count:           1640\n",
      "Tickers after date alignment:   1559\n",
      "Final clean ticker count:       1559\n",
      "----------------------------------------\n",
      "Total tickers removed:          81\n",
      "\n",
      "81 symbols removed due to non-matching date index:\n",
      "['ACIW', 'ADMA', 'ALAB', 'BMA', 'BTC', 'BULL', 'CACC', 'CADE', 'CAI', 'CHH', 'CHYM', 'CRCL', 'CROX', 'CRVL', 'CRWV', 'CTRE', 'DFS', 'EMLP', 'ETHA', 'ETOR', 'EWU', 'FCN', 'FNGA', 'GEV', 'GLBE', 'GLOB', 'GOLD', 'GPI', 'HOMB', 'JBS', 'JGLO', 'JHG', 'KRE', 'KRMN', 'LB', 'LINE', 'LNC', 'LNTH', 'LOAR', 'LOPE', 'LTM', 'MMSI', 'MNSO', 'MOG-A', 'MSTY', 'MTN', 'NBIS', 'NEAR', 'ONTO', 'OS', 'OTF', 'PTLC', 'QFIN', 'QTWO', 'RBRK', 'RDDT', 'SAIC', 'SAIL', 'SARO', 'SFD', 'SLGN', 'SMBS', 'SNDK', 'SOLV', 'SPHD', 'SPSC', 'SQQQ', 'SW', 'TEM', 'TFI', 'TFX', 'TGTX', 'TLX', 'TTAN', 'UCON', 'ULS', 'VG', 'VIK', 'WAY', 'X', 'ZK']\n",
      "\n",
      "0 symbols removed due to missing values or incomplete data:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "if 'df_raw' in locals() and df_raw is not None:\n",
    "    print(\"\\n--- Step 4: Cleaning Process Summary ---\")\n",
    "    \n",
    "    print(f\"Initial ticker count:           {raw_ticker_count}\")\n",
    "    print(f\"Tickers after date alignment:   {filter_ticker_count}\")\n",
    "    print(f\"Final clean ticker count:       {clean_ticker_count}\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Total tickers removed:          {raw_ticker_count - clean_ticker_count}\")\n",
    "    \n",
    "    print(f\"\\n{len(filtered_out_symbols)} symbols removed due to non-matching date index:\")\n",
    "    print(sorted(filtered_out_symbols))\n",
    "    \n",
    "    print(f\"\\n{len(missing_values_symbols)} symbols removed due to missing values or incomplete data:\")\n",
    "    print(sorted(missing_values_symbols))\n",
    "else:\n",
    "    print(\"\\nSummary could not be generated as raw data was not loaded.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
