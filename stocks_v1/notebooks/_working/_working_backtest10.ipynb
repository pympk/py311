{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Python will look in these locations:\n",
      "['C:\\\\Users\\\\ping\\\\.pyenv\\\\pyenv-win\\\\versions\\\\3.11.9\\\\python311.zip', 'C:\\\\Users\\\\ping\\\\.pyenv\\\\pyenv-win\\\\versions\\\\3.11.9\\\\DLLs', 'C:\\\\Users\\\\ping\\\\.pyenv\\\\pyenv-win\\\\versions\\\\3.11.9\\\\Lib', 'C:\\\\Users\\\\ping\\\\.pyenv\\\\pyenv-win\\\\versions\\\\3.11.9', 'c:\\\\Users\\\\ping\\\\Files_win10\\\\python\\\\py311\\\\.venv', '', 'c:\\\\Users\\\\ping\\\\Files_win10\\\\python\\\\py311\\\\.venv\\\\Lib\\\\site-packages', 'c:\\\\Users\\\\ping\\\\Files_win10\\\\python\\\\py311\\\\.venv\\\\Lib\\\\site-packages\\\\win32', 'c:\\\\Users\\\\ping\\\\Files_win10\\\\python\\\\py311\\\\.venv\\\\Lib\\\\site-packages\\\\win32\\\\lib', 'c:\\\\Users\\\\ping\\\\Files_win10\\\\python\\\\py311\\\\.venv\\\\Lib\\\\site-packages\\\\Pythonwin', 'c:\\\\Users\\\\ping\\\\Files_win10\\\\python\\\\py311\\\\stocks\\\\src', 'c:\\\\Users\\\\ping\\\\Files_win10\\\\python\\\\py311\\\\.venv\\\\Lib\\\\site-packages\\\\setuptools\\\\_vendor', 'c:\\\\Users\\\\ping\\\\Files_win10\\\\python\\\\py311\\\\stocks\\\\src']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import os\n",
    "from IPython.display import display, Markdown  # Assuming you use these for display\n",
    "\n",
    "\n",
    "# Set pandas display options to show more columns and rows\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "# pd.set_option('display.max_rows', 10)       # Limit to 10 rows for readability\n",
    "pd.set_option('display.width', 1500)        # Let the display adjust to the window\n",
    "\n",
    "\n",
    "# Notebook cell\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Get root directory (assuming notebook is in root/notebooks/)\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "ROOT_DIR = NOTEBOOK_DIR.parent if NOTEBOOK_DIR.name == 'notebooks' else NOTEBOOK_DIR\n",
    "\n",
    "# Add src directory to Python path\n",
    "sys.path.append(str(ROOT_DIR / 'src'))\n",
    "\n",
    "# Verify path\n",
    "print(f\"Python will look in these locations:\\n{sys.path}\")\n",
    "\n",
    "\n",
    "# --- Execute the processor ---\n",
    "import utils\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Available 'starting with 'df_OHLCV' and containing 'clean'' files:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "- (1) `[DATA]` `df_OHLCV_clean_stocks_etfs.parquet` <span style='color:#00ffff'>(12.85 MB, 2025-05-09 15:18)</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input a number to select file (1-1)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "    **Selected paths:**\n",
       "    - Source: `c:\\Users\\ping\\Files_win10\\python\\py311\\stocks\\data\\df_OHLCV_clean_stocks_etfs.parquet`\n",
       "    - Destination: `c:\\Users\\ping\\Files_win10\\python\\py311\\stocks\\data\\df_OHLCV_clean_stocks_etfs_clean.parquet`\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path_OHLCV: c:\\Users\\ping\\Files_win10\\python\\py311\\stocks\\data\\df_OHLCV_clean_stocks_etfs.parquet\n",
      "df_OHLCV:\n",
      "                   Adj Open  Adj High  Adj Low  Adj Close   Volume\n",
      "Symbol Date                                                       \n",
      "A      2025-05-09    108.96    109.86   106.80     106.93  1124390\n",
      "       2025-05-08    108.00    110.65   106.55     108.70  2093300\n",
      "       2025-05-07    106.69    107.60   104.79     107.52  2143700\n",
      "       2025-05-06    107.25    108.21   104.36     105.24  1960600\n",
      "       2025-05-05    108.10    109.25   107.46     108.37  1385500\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 492536 entries, ('A', Timestamp('2025-05-09 00:00:00')) to ('ZWS', Timestamp('2024-02-01 00:00:00'))\n",
      "Data columns (total 5 columns):\n",
      " #   Column     Non-Null Count   Dtype  \n",
      "---  ------     --------------   -----  \n",
      " 0   Adj Open   492536 non-null  float64\n",
      " 1   Adj High   492536 non-null  float64\n",
      " 2   Adj Low    492536 non-null  float64\n",
      " 3   Adj Close  492536 non-null  float64\n",
      " 4   Volume     492536 non-null  int64  \n",
      "dtypes: float64(4), int64(1)\n",
      "memory usage: 20.7+ MB\n",
      "df_OHLCV.info():\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "path_OHLCV, _, _ = utils.main_processor(\n",
    "    data_dir='..\\data',  \n",
    "    # data_dir='output\\selection_results',  # search project ..\\data\n",
    "    downloads_dir='',  # None searchs Downloads dir, '' omits search1\n",
    "    downloads_limit=60,  # search the first 10 files\n",
    "    clean_name_override=None,  # override filename\n",
    "    start_file_pattern='df_OHLCV', # search for files starting with 'df_'\n",
    "    contains_pattern='clean' # search for files containing 'df_'\n",
    ")\n",
    "\n",
    "print(f'path_OHLCV: {path_OHLCV}')\n",
    "df_OHLCV = pd.read_parquet(path_OHLCV)\n",
    "print(f'df_OHLCV:\\n{df_OHLCV.head()}\\n')\n",
    "print(f'df_OHLCV.info():\\n{df_OHLCV.info()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "import pprint # For cleaner dictionary printing\n",
    "import numpy as np\n",
    "import traceback # Added for detailed error logging\n",
    "import json\n",
    "import os # For creating directories and checking file existence\n",
    "import logging # For logging instead of print\n",
    "import datetime # For timestamping runs\n",
    "import sys # Added for interpreter/path logging\n",
    "\n",
    "from typing import List, Dict, Tuple, Set, Any, Callable, Optional # Import types for hinting\n",
    "\n",
    "\n",
    "# --- Make sure 'utils' exists and has 'extract_date_from_string' ---\n",
    "try:\n",
    "    import utils # Assuming this is your utility module for date extraction\n",
    "    # Check if the function exists (optional but good practice)\n",
    "    if not hasattr(utils, 'extract_date_from_string'):\n",
    "        logging.error(\"ERROR: 'utils' module imported but 'extract_date_from_string' function not found!\")\n",
    "        sys.exit(\"Critical function missing from utils module.\")\n",
    "except ImportError:\n",
    "    # This will be caught if logging isn't set up yet, so print is a fallback here.\n",
    "    print(\"ERROR: Failed to import the 'utils' module. Make sure utils.py exists and is in the Python path.\")\n",
    "    # If logging is set up, this will also go to the log.\n",
    "    if logging.getLogger().hasHandlers():\n",
    "        logging.error(\"ERROR: Failed to import the 'utils' module.\", exc_info=True)\n",
    "    sys.exit(\"Missing required 'utils' module.\") # Exit if utils can't be imported\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: An unexpected error occurred during 'utils' import: {e}\")\n",
    "    if logging.getLogger().hasHandlers():\n",
    "        logging.error(f\"ERROR: An unexpected error occurred during 'utils' import: {e}\", exc_info=True)\n",
    "    sys.exit(\"Error during module import.\")\n",
    "\n",
    "\n",
    "# --- Constants ---\n",
    "RISK_FREE_RATE_DAILY = 0.04 / 365\n",
    "LOG_DIR = 'logs'\n",
    "RESULTS_DIR = 'output/backtest_results'\n",
    "RESULTS_CSV_PATH = os.path.join(RESULTS_DIR, 'backtest_parameter_performance.csv') # Path to the CSV file\n",
    "RESULTS_DF_PATH = os.path.join(RESULTS_DIR, 'df_backtest_parameter_performance.parquet') # Path to dataframe file\n",
    "\n",
    "# Parameters that, along with selection_date and scheme, define a unique run for overwriting purposes\n",
    "PARAMS_TO_TRACK = [\n",
    "    'n_select_requested',\n",
    "    'inv_vol_col_name',\n",
    "    'filter_min_price',\n",
    "    'filter_min_avg_volume_m',\n",
    "    'filter_min_roe_pct',\n",
    "    'filter_max_debt_eq',\n",
    "    'score_weight_rsi',\n",
    "    'score_weight_change',\n",
    "    'score_weight_rel_volume',\n",
    "    'score_weight_volatility',\n",
    "    # 'weight' (scheme) is handled separately by using 'scheme' column\n",
    "]\n",
    "\n",
    "# Define column order for the CSV file. This ensures consistency.\n",
    "# This list should include all fields from PARAMS_TO_TRACK, plus others.\n",
    "CSV_COLUMN_ORDER = [\n",
    "    'run_timestamp', 'log_file', 'selection_date', 'actual_selection_date_used', 'scheme',\n",
    "    # Parameters from PARAMS_TO_TRACK\n",
    "    'n_select_requested', 'inv_vol_col_name', 'filter_min_price',\n",
    "    'filter_min_avg_volume_m', 'filter_min_roe_pct', 'filter_max_debt_eq',\n",
    "    'score_weight_rsi', 'score_weight_change', 'score_weight_rel_volume', 'score_weight_volatility',\n",
    "    # Other parameters/results\n",
    "    'n_select_actual',\n",
    "    'portfolio_return', 'portfolio_return_normalized',\n",
    "    'num_attempted_trades',\n",
    "    'num_successful_trades', 'num_failed_or_skipped_trades',\n",
    "    'total_weight_traded',\n",
    "    'win_rate', 'average_return', 'std_dev_return', 'sharpe_ratio_period'\n",
    "]\n",
    "\n",
    "# Check at startup that all PARAMS_TO_TRACK are in CSV_COLUMN_ORDER\n",
    "if not set(PARAMS_TO_TRACK).issubset(set(CSV_COLUMN_ORDER)):\n",
    "    logging.error(\"CRITICAL: Not all PARAMS_TO_TRACK are present in CSV_COLUMN_ORDER!\")\n",
    "    sys.exit(\"Configuration error: PARAMS_TO_TRACK mismatch with CSV_COLUMN_ORDER.\")\n",
    "\n",
    "\n",
    "# Columns used to uniquely identify a row for the purpose of overwriting.\n",
    "# This combines 'selection_date', 'scheme', and all parameters defined in PARAMS_TO_TRACK.\n",
    "UNIQUE_KEY_COLUMNS_FOR_CSV = ['selection_date', 'scheme'] + PARAMS_TO_TRACK\n",
    "\n",
    "# Path to adjusted close prices\n",
    "ADJ_CLOSE_PATH = '../data/df_adj_close.parquet'\n",
    "\n",
    "# Path to selection results\n",
    "OUTPUT_DIR = 'output/selection_results/'\n",
    "\n",
    "\n",
    "\n",
    "# --- 1. Setup Logging ---\n",
    "def setup_logging(log_dir: str = LOG_DIR):\n",
    "    \"\"\"Configures logging to write to a file.\"\"\"\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    log_filename = datetime.datetime.now().strftime(\"backtest_run_%Y%m%d_%H%M%S.log\")\n",
    "    log_filepath = os.path.join(log_dir, log_filename)\n",
    "\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO) \n",
    "\n",
    "    for handler in logger.handlers[:]:\n",
    "        handler.close()\n",
    "        logger.removeHandler(handler)\n",
    "\n",
    "    file_handler_instance = logging.FileHandler(log_filepath)\n",
    "    file_handler_instance.setLevel(logging.INFO) \n",
    "\n",
    "    stream_handler = logging.StreamHandler()\n",
    "    stream_handler.setLevel(logging.INFO) \n",
    "\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    file_handler_instance.setFormatter(formatter)\n",
    "    stream_handler.setFormatter(formatter)\n",
    "\n",
    "    logger.addHandler(file_handler_instance)\n",
    "    logger.addHandler(stream_handler)\n",
    "\n",
    "    logging.info(f\"Logging initialized. Log file: {log_filepath}\")\n",
    "    return log_filepath \n",
    "\n",
    "\n",
    "def extract_backtest_setups(\n",
    "    dataframe: pd.DataFrame,\n",
    "    weight_column_names: List[str],\n",
    "    date_str: str, \n",
    "    scheme_separator: str = '_'\n",
    "    ) -> Dict[str, Dict[str, Dict[str, float]]]: \n",
    "    \"\"\"\n",
    "    Extracts Ticker-Weight pairs from specified columns in a DataFrame.\n",
    "    \"\"\"\n",
    "    if not date_str: \n",
    "        logging.error(\"The 'date_str' argument cannot be None or empty.\")\n",
    "        raise ValueError(\"The 'date_str' argument cannot be None or empty.\")\n",
    "\n",
    "    if dataframe is None or dataframe.empty:\n",
    "        logging.warning(\"Input DataFrame is None or empty. Cannot extract setups.\")\n",
    "        return {} \n",
    "\n",
    "    scheme_setups: Dict[str, Dict[str, float]] = {}\n",
    "\n",
    "    for col_name in weight_column_names:\n",
    "        if col_name in dataframe.columns:\n",
    "            try:\n",
    "                parts = col_name.split(scheme_separator)\n",
    "                scheme_name = parts[-1] if len(parts) > 1 else col_name\n",
    "                numeric_col = pd.to_numeric(dataframe[col_name], errors='coerce')\n",
    "                ticker_weights = numeric_col.dropna().astype(float).to_dict()\n",
    "\n",
    "                if ticker_weights:\n",
    "                    if scheme_name in scheme_setups:\n",
    "                        logging.warning(f\"Duplicate scheme name '{scheme_name}' derived. \"\n",
    "                                        f\"Weights from column '{col_name}' might overwrite previous ones.\")\n",
    "                    scheme_setups[scheme_name] = ticker_weights\n",
    "                    logging.info(f\"Successfully extracted weights for scheme: {scheme_name} \"\n",
    "                                f\"({len(ticker_weights)} tickers) for date {date_str}\")\n",
    "                else:\n",
    "                    logging.warning(f\"No valid (non-NaN, numeric) weights found for column '{col_name}'. \"\n",
    "                                  f\"Skipping scheme '{scheme_name}' for date '{date_str}'.\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing column '{col_name}': {e}\", exc_info=True) \n",
    "        else:\n",
    "            logging.warning(f\"Column '{col_name}' not found in the DataFrame.\")\n",
    "\n",
    "    final_output = {date_str: scheme_setups}\n",
    "\n",
    "    if not scheme_setups:\n",
    "      logging.warning(f\"No valid backtest setups generated for date {date_str}.\")\n",
    "    return final_output\n",
    "\n",
    "\n",
    "def run_single_backtest(\n",
    "    selection_date: str,\n",
    "    scheme_name: str,\n",
    "    ticker_weights: Dict[str, float],\n",
    "    df_adj_close: pd.DataFrame,\n",
    "    risk_free_rate_daily: float = RISK_FREE_RATE_DAILY,\n",
    "    ) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Runs a simple backtest for a given selection date and ticker weights.\n",
    "    \"\"\"\n",
    "    logging.info(\"-\" * 30)\n",
    "    logging.info(f\"Initiating Backtest Run...\")\n",
    "    logging.info(f\"  Date          : {selection_date}\")\n",
    "    logging.info(f\"  Scheme        : {scheme_name}\")\n",
    "    logging.info(f\"  Num Tickers   : {len(ticker_weights)}\")\n",
    "    sample_weights_str = io.StringIO()\n",
    "    pprint.pprint(dict(list(ticker_weights.items())[:3]), stream=sample_weights_str)\n",
    "    if len(ticker_weights) > 3: sample_weights_str.write(\"    ...\\n\")\n",
    "    logging.debug(f\"  Sample Weights:\\n{sample_weights_str.getvalue()}\") \n",
    "\n",
    "    try:\n",
    "        df_prices = df_adj_close.copy()\n",
    "        if not isinstance(df_prices.index, pd.DatetimeIndex):\n",
    "            try:\n",
    "                df_prices.index = pd.to_datetime(df_prices.index)\n",
    "                logging.info(\"  Info: Converted DataFrame index to DatetimeIndex.\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"  Error: Failed to convert DataFrame index to DatetimeIndex: {e}\", exc_info=True)\n",
    "                logging.info(\"-\" * 30)\n",
    "                return None\n",
    "\n",
    "        if not df_prices.index.is_monotonic_increasing:\n",
    "            logging.info(\"  Info: Sorting DataFrame index by date...\")\n",
    "            df_prices = df_prices.sort_index()\n",
    "            logging.info(\"  Info: DataFrame index sorted.\")\n",
    "\n",
    "        all_trading_dates = df_prices.index\n",
    "        selection_timestamp = pd.Timestamp(selection_date)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"  Error during initial data preparation: {e}\", exc_info=True)\n",
    "        logging.info(\"-\" * 30)\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        try:\n",
    "            selection_idx = all_trading_dates.get_loc(selection_timestamp)\n",
    "        except KeyError:\n",
    "            indexer = all_trading_dates.get_indexer([selection_timestamp], method='ffill') \n",
    "            if indexer[0] == -1: \n",
    "                 indexer_bfill = all_trading_dates.get_indexer([selection_timestamp], method='bfill')\n",
    "                 if indexer_bfill[0] == -1:\n",
    "                    logging.error(f\"  Error: Selection date {selection_date} or a nearby trading date not found in price data index.\")\n",
    "                    logging.info(\"-\" * 30)\n",
    "                    return None\n",
    "                 else:\n",
    "                     selection_idx = indexer_bfill[0]\n",
    "                     actual_selection_date_used = all_trading_dates[selection_idx]\n",
    "                     logging.warning(f\"  Warning: Exact selection date {selection_date} not found. Using next available date: {actual_selection_date_used.strftime('%Y-%m-%d')}\")\n",
    "            else:\n",
    "                selection_idx = indexer[0]\n",
    "                actual_selection_date_used = all_trading_dates[selection_idx]\n",
    "                if actual_selection_date_used != selection_timestamp:\n",
    "                     logging.warning(f\"  Warning: Exact selection date {selection_date} not found. Using previous available date: {actual_selection_date_used.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "        if selection_idx + 1 >= len(all_trading_dates):\n",
    "            logging.error(f\"  Error: No trading date found after selection date index {selection_idx} ({all_trading_dates[selection_idx].strftime('%Y-%m-%d')}).\")\n",
    "            logging.info(\"-\" * 30)\n",
    "            return None\n",
    "        buy_date = all_trading_dates[selection_idx + 1]\n",
    "\n",
    "        if selection_idx + 2 >= len(all_trading_dates):\n",
    "            logging.error(f\"  Error: No trading date found after buy date {buy_date.strftime('%Y-%m-%d')}.\")\n",
    "            logging.info(\"-\" * 30)\n",
    "            return None\n",
    "        sell_date = all_trading_dates[selection_idx + 2]\n",
    "\n",
    "        logging.info(f\"  Selection Date Used: {all_trading_dates[selection_idx].strftime('%Y-%m-%d')}\")\n",
    "        logging.info(f\"  Buy Date           : {buy_date.strftime('%Y-%m-%d')}\")\n",
    "        logging.info(f\"  Sell Date          : {sell_date.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "        trades = []\n",
    "        returns = []\n",
    "        portfolio_return = 0.0\n",
    "        total_weight_traded = 0.0\n",
    "        valid_tickers_count = 0\n",
    "        missing_price_count = 0\n",
    "\n",
    "        relevant_tickers = [t for t in ticker_weights.keys() if t in df_prices.columns]\n",
    "        relevant_dates = [buy_date, sell_date]\n",
    "        try:\n",
    "            price_subset = df_prices.loc[relevant_dates, relevant_tickers]\n",
    "        except KeyError as e:\n",
    "            logging.error(f\"  Error selecting price subset for dates {relevant_dates} and tickers. Missing columns?: {e}\", exc_info=True)\n",
    "            return None \n",
    "\n",
    "        for ticker in ticker_weights.keys():\n",
    "            if ticker not in price_subset.columns: \n",
    "                logging.warning(f\"    Warning: Ticker {ticker} not found in price data columns. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            valid_tickers_count += 1\n",
    "            trade_info = { \"ticker\": ticker, \"weight\": ticker_weights[ticker],\n",
    "                          \"buy_date\": buy_date.strftime('%Y-%m-%d'), \"sell_date\": sell_date.strftime('%Y-%m-%d'),\n",
    "                          \"buy_price\": None, \"sell_price\": None, \"return\": None, \"status\": \"Pending\" }\n",
    "\n",
    "            try:\n",
    "                buy_price = price_subset.at[buy_date, ticker]\n",
    "                if pd.isna(buy_price) or buy_price <= 0: raise ValueError(f\"Invalid buy price ({buy_price})\")\n",
    "                sell_price = price_subset.at[sell_date, ticker]\n",
    "                if pd.isna(sell_price): raise ValueError(f\"Invalid sell price ({sell_price})\") \n",
    "\n",
    "                trade_return = (sell_price - buy_price) / buy_price\n",
    "                trade_info.update({\"buy_price\": buy_price, \"sell_price\": sell_price, \"return\": trade_return, \"status\": \"Success\"})\n",
    "                trades.append(trade_info)\n",
    "                returns.append(trade_return)\n",
    "\n",
    "                current_weight = ticker_weights[ticker]\n",
    "                portfolio_return += trade_return * current_weight\n",
    "                total_weight_traded += current_weight\n",
    "            except KeyError as e:\n",
    "                logging.warning(f\"    Error accessing price for {ticker} on {e}. Skipping trade.\")\n",
    "                trade_info[\"status\"] = f\"Error: Price data missing ({e})\"\n",
    "                trades.append(trade_info)\n",
    "                missing_price_count += 1\n",
    "            except ValueError as e:\n",
    "                logging.warning(f\"    Warning: Invalid price data for {ticker} between {buy_date.strftime('%Y-%m-%d')} and {sell_date.strftime('%Y-%m-%d')} ({e}). Skipping trade.\")\n",
    "                trade_info[\"status\"] = f\"Skipped: Invalid price ({e})\"\n",
    "                try: trade_info[\"buy_price\"] = price_subset.at[buy_date, ticker]\n",
    "                except: pass\n",
    "                try: trade_info[\"sell_price\"] = price_subset.at[sell_date, ticker]\n",
    "                except: pass\n",
    "                trades.append(trade_info)\n",
    "                missing_price_count += 1\n",
    "            except Exception as e:\n",
    "                logging.error(f\"    Unexpected error processing trade for {ticker}: {e}\", exc_info=True)\n",
    "                trade_info[\"status\"] = f\"Error: Unexpected ({type(e).__name__})\"\n",
    "                trades.append(trade_info)\n",
    "                missing_price_count += 1\n",
    "\n",
    "        num_attempted_trades = valid_tickers_count\n",
    "        num_successful_trades = len(returns)\n",
    "        metrics = {\n",
    "            'num_selected_tickers': len(ticker_weights),\n",
    "            'num_valid_tickers_in_data': valid_tickers_count,\n",
    "            'num_attempted_trades': num_attempted_trades,\n",
    "            'num_successful_trades': num_successful_trades,\n",
    "            'num_failed_or_skipped_trades': num_attempted_trades - num_successful_trades,\n",
    "            'portfolio_return': portfolio_return if num_successful_trades > 0 and abs(total_weight_traded) > 1e-9 else 0.0,\n",
    "            'total_weight_traded': total_weight_traded,\n",
    "            'win_rate': None, 'average_return': None, 'std_dev_return': None, 'sharpe_ratio_period': None,\n",
    "        }\n",
    "\n",
    "        if num_successful_trades > 0:\n",
    "            returns_array = np.array(returns)\n",
    "            metrics['win_rate'] = np.sum(returns_array > 0) / num_successful_trades\n",
    "            metrics['average_return'] = np.mean(returns_array)\n",
    "            metrics['std_dev_return'] = np.std(returns_array, ddof=1) if num_successful_trades > 1 else 0.0\n",
    "            std_dev = metrics['std_dev_return']\n",
    "            avg_ret = metrics['average_return'] \n",
    "\n",
    "            if std_dev is not None and std_dev > 1e-9: \n",
    "                excess_return = avg_ret - risk_free_rate_daily\n",
    "                metrics['sharpe_ratio_period'] = excess_return / std_dev\n",
    "            elif avg_ret is not None: \n",
    "                excess_return = avg_ret - risk_free_rate_daily\n",
    "                if abs(excess_return) < 1e-9: \n",
    "                    metrics['sharpe_ratio_period'] = 0.0\n",
    "                else: \n",
    "                    metrics['sharpe_ratio_period'] = np.inf * np.sign(excess_return)\n",
    "            else: \n",
    "                metrics['sharpe_ratio_period'] = np.nan\n",
    "\n",
    "            logging.info(f\"  Trades Executed: {num_successful_trades}/{num_attempted_trades}\")\n",
    "            if abs(total_weight_traded - 1.0) > 1e-6 and abs(total_weight_traded) > 1e-9:\n",
    "                normalized_portfolio_return = portfolio_return / total_weight_traded\n",
    "                logging.info(f\"  Portfolio Return (Raw)    : {portfolio_return:.4f} (Based on Weight Sum: {total_weight_traded:.4f})\")\n",
    "                logging.info(f\"  Portfolio Return (Norm'd) : {normalized_portfolio_return:.4f}\")\n",
    "                metrics['portfolio_return_normalized'] = normalized_portfolio_return \n",
    "            else:\n",
    "                 logging.info(f\"  Portfolio Return          : {portfolio_return:.4f} (Based on Weight Sum: {total_weight_traded:.4f})\")\n",
    "\n",
    "            logging.info(f\"  Win Rate (Individual)   : {metrics['win_rate']:.2%}\" if metrics['win_rate'] is not None else \"N/A\")\n",
    "            logging.info(f\"  Avg Ticker Return       : {metrics['average_return']:.4f}\" if metrics['average_return'] is not None else \"N/A\")\n",
    "            logging.info(f\"  Std Dev Ticker Return   : {metrics['std_dev_return']:.4f}\" if metrics['std_dev_return'] is not None else \"N/A\")\n",
    "            logging.info(f\"  Period Sharpe (Indiv)   : {metrics['sharpe_ratio_period']:.4f}\" if metrics['sharpe_ratio_period'] is not None else \"N/A\")\n",
    "        else:\n",
    "            logging.warning(f\"  No successful trades executed out of {num_attempted_trades} attempted.\")\n",
    "            logging.info(f\"  Portfolio Return          : {metrics['portfolio_return']:.4f}\")\n",
    "\n",
    "        backtest_results = {\n",
    "            \"run_inputs\": {\n",
    "                \"selection_date\": selection_date,\n",
    "                \"actual_selection_date_used\": all_trading_dates[selection_idx].strftime('%Y-%m-%d'), \n",
    "                \"scheme_name\": scheme_name,\n",
    "                \"num_tickers_input\": len(ticker_weights),\n",
    "                \"risk_free_rate_daily\": risk_free_rate_daily,\n",
    "                \"buy_date\": buy_date.strftime('%Y-%m-%d'),\n",
    "                \"sell_date\": sell_date.strftime('%Y-%m-%d'),\n",
    "            },\n",
    "            \"metrics\": metrics,\n",
    "            \"trades\": trades \n",
    "        }\n",
    "        logging.info(f\"Backtest simulation for '{scheme_name}' on {selection_date} completed.\")\n",
    "        logging.info(\"-\" * 30)\n",
    "        return backtest_results\n",
    "    except Exception as e:\n",
    "        logging.critical(f\"  FATAL ERROR during backtest run for {selection_date}, {scheme_name}: {e}\", exc_info=True)\n",
    "        logging.info(\"-\" * 30)\n",
    "        return None\n",
    "\n",
    "\n",
    "def process_all_backtests(\n",
    "    nested_setups: Dict[str, Dict[str, Dict[str, float]]],\n",
    "    df_adj_close: pd.DataFrame \n",
    "    ) -> Dict[str, Dict[str, Optional[Dict[str, Any]]]]:\n",
    "    \"\"\"\n",
    "    Iterates through the nested setup dictionary and runs individual backtests.\n",
    "    \"\"\"  \n",
    "    all_results: Dict[str, Dict[str, Optional[Dict[str, Any]]]] = {}\n",
    "\n",
    "    if not nested_setups:\n",
    "        logging.warning(\"Received empty setup dictionary. No backtests to run.\")\n",
    "        return all_results\n",
    "\n",
    "    logging.info(\"\\n===== Starting Batch Backtest Processing =====\")\n",
    "    try:\n",
    "        df_prices_global = df_adj_close.copy()\n",
    "        if not isinstance(df_prices_global.index, pd.DatetimeIndex):\n",
    "            df_prices_global.index = pd.to_datetime(df_prices_global.index)\n",
    "        if not df_prices_global.index.is_monotonic_increasing:\n",
    "            df_prices_global = df_prices_global.sort_index()\n",
    "        logging.info(\"Prepared global price data copy for backtests.\")\n",
    "    except Exception as e:\n",
    "        logging.critical(f\"Failed to prepare global price data copy: {e}\", exc_info=True)\n",
    "        return all_results \n",
    "\n",
    "    for date_str, schemes_for_date in nested_setups.items():\n",
    "        logging.info(f\"\\nProcessing date: {date_str}\")\n",
    "        if not schemes_for_date:\n",
    "            logging.warning(f\"  No schemes found for this date. Skipping.\")\n",
    "            all_results[date_str] = {} \n",
    "            continue\n",
    "\n",
    "        results_for_date: Dict[str, Optional[Dict[str, Any]]] = {}\n",
    "        for scheme_name, ticker_weights in schemes_for_date.items():\n",
    "            if not ticker_weights:\n",
    "                logging.warning(f\"  Skipping scheme '{scheme_name}': No ticker weights provided.\")\n",
    "                results_for_date[scheme_name] = None \n",
    "                continue\n",
    "            try:\n",
    "                backtest_result = run_single_backtest(\n",
    "                    selection_date=date_str,\n",
    "                    scheme_name=scheme_name,\n",
    "                    ticker_weights=ticker_weights,\n",
    "                    df_adj_close=df_prices_global, \n",
    "                    risk_free_rate_daily = RISK_FREE_RATE_DAILY,\n",
    "                )\n",
    "                results_for_date[scheme_name] = backtest_result\n",
    "            except Exception as e:\n",
    "                logging.error(f\"!! UNEXPECTED Error running backtest for {scheme_name} on {date_str} in outer loop: {e}\", exc_info=True)\n",
    "                results_for_date[scheme_name] = None \n",
    "\n",
    "        all_results[date_str] = results_for_date\n",
    "\n",
    "    logging.info(\"\\n===== Batch Backtest Processing Finished =====\")\n",
    "    return all_results\n",
    "\n",
    "# --- 3. Function to Extract Parameters and Results for Storage ---\n",
    "def extract_params_and_results(\n",
    "    params: Dict[str, Any],\n",
    "    backtest_results_summary: Dict[str, Dict[str, Optional[Dict[str, Any]]]],\n",
    "    run_timestamp: str,\n",
    "    log_filepath: str\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Extracts relevant parameters and portfolio returns from results.\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    if not log_filepath: \n",
    "        log_filename = \"unknown_log.log\"\n",
    "        logging.error(\"Log filepath was not set correctly during parameter extraction.\")\n",
    "    else:\n",
    "        log_filename = os.path.basename(log_filepath)\n",
    "\n",
    "    for date_str, scheme_results in backtest_results_summary.items():\n",
    "        if not scheme_results: \n",
    "             logging.warning(f\"No scheme results found for date {date_str} during param extraction.\")\n",
    "             continue\n",
    "        for scheme_name, result in scheme_results.items():\n",
    "            record = {\n",
    "                'run_timestamp': run_timestamp,\n",
    "                'log_file': log_filename,\n",
    "                'selection_date': date_str,\n",
    "                'scheme': scheme_name,\n",
    "            }\n",
    "            for p_key in PARAMS_TO_TRACK:\n",
    "                record[p_key] = params.get(p_key, None) \n",
    "            record['n_select_actual'] = params.get('n_select_actual', np.nan) \n",
    "\n",
    "            if result and isinstance(result, dict) and 'metrics' in result and isinstance(result['metrics'], dict):\n",
    "                metrics = result['metrics']\n",
    "                record['portfolio_return'] = metrics.get('portfolio_return', np.nan)\n",
    "                record['portfolio_return_normalized'] = metrics.get('portfolio_return_normalized', np.nan) \n",
    "                record['num_successful_trades'] = metrics.get('num_successful_trades', 0)\n",
    "                record['total_weight_traded'] = metrics.get('total_weight_traded', 0.0)\n",
    "                record['win_rate'] = metrics.get('win_rate', np.nan)\n",
    "                record['average_return'] = metrics.get('average_return', np.nan)\n",
    "                record['std_dev_return'] = metrics.get('std_dev_return', np.nan) \n",
    "                record['sharpe_ratio_period'] = metrics.get('sharpe_ratio_period', np.nan) \n",
    "                record['num_attempted_trades'] = metrics.get('num_attempted_trades', 0)\n",
    "                record['num_failed_or_skipped_trades'] = metrics.get('num_failed_or_skipped_trades', 0)\n",
    "                record['actual_selection_date_used'] = result.get('run_inputs', {}).get('actual_selection_date_used', None)\n",
    "            else:\n",
    "                record['portfolio_return'] = np.nan\n",
    "                record['portfolio_return_normalized'] = np.nan\n",
    "                record['num_successful_trades'] = 0\n",
    "                record['total_weight_traded'] = 0.0\n",
    "                record['win_rate'] = np.nan\n",
    "                record['average_return'] = np.nan\n",
    "                record['std_dev_return'] = np.nan\n",
    "                record['sharpe_ratio_period'] = np.nan\n",
    "                record['num_attempted_trades'] = np.nan \n",
    "                record['num_failed_or_skipped_trades'] = np.nan\n",
    "                record['actual_selection_date_used'] = None\n",
    "                if result is None:\n",
    "                     logging.warning(f\"Extracting params: Backtest result was None for {date_str} / {scheme_name}.\")\n",
    "                else:\n",
    "                     logging.warning(f\"Extracting params: Unexpected result format for {date_str} / {scheme_name}: {type(result)}\")\n",
    "            records.append(record)\n",
    "    return records\n",
    "\n",
    "# --- 4. MODIFIED Function to Write Results to CSV (handles overwriting) ---\n",
    "def write_results_to_csv(records: List[Dict[str, Any]], filepath: str = RESULTS_CSV_PATH):\n",
    "    \"\"\"\n",
    "    Writes a list of result records to a CSV file.\n",
    "    If a record with the same selection_date, scheme, and all parameters\n",
    "    defined in PARAMS_TO_TRACK already exists, it's overwritten by the new record.\n",
    "    Only the latest result (from the current batch of `records`) for that specific\n",
    "    combination is kept. The file is rewritten entirely.\n",
    "    \"\"\"\n",
    "    if not records:\n",
    "        logging.info(\"No records to write to CSV.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        df_new_records = pd.DataFrame(records)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to create DataFrame from new records: {e}\", exc_info=True)\n",
    "        return\n",
    "\n",
    "    # Ensure df_new_records has all columns from CSV_COLUMN_ORDER, adding NaNs for missing ones\n",
    "    for col in CSV_COLUMN_ORDER:\n",
    "        if col not in df_new_records.columns:\n",
    "            df_new_records[col] = np.nan\n",
    "    # Select and reorder columns to match CSV_COLUMN_ORDER\n",
    "    df_new_records = df_new_records[CSV_COLUMN_ORDER]\n",
    "\n",
    "    # Ensure directory for the CSV file exists\n",
    "    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "\n",
    "    df_existing_records = pd.DataFrame(columns=CSV_COLUMN_ORDER) # Default to empty DF with correct columns\n",
    "    if os.path.exists(filepath) and os.path.getsize(filepath) > 0:\n",
    "        try:\n",
    "            df_existing_records = pd.read_csv(filepath, na_filter=True, keep_default_na=True)\n",
    "\n",
    "            # Align columns of df_existing_records with CSV_COLUMN_ORDER\n",
    "            for col in CSV_COLUMN_ORDER:\n",
    "                if col not in df_existing_records.columns:\n",
    "                    logging.warning(f\"Column '{col}' from CSV_COLUMN_ORDER not found in existing CSV '{filepath}'. Adding it with NaNs.\")\n",
    "                    df_existing_records[col] = np.nan\n",
    "            df_existing_records = df_existing_records[CSV_COLUMN_ORDER]\n",
    "\n",
    "        except pd.errors.EmptyDataError:\n",
    "            logging.info(f\"Existing CSV file '{filepath}' is empty. Will create a new one.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error reading existing CSV file '{filepath}': {e}. \"\n",
    "                          \"Proceeding as if it were empty or to create a new file.\", exc_info=True)\n",
    "            df_existing_records = pd.DataFrame(columns=CSV_COLUMN_ORDER) # Reset to empty on error\n",
    "\n",
    "    # Combine existing data with the new records.\n",
    "    # New records are placed after existing ones.\n",
    "    df_combined = pd.concat([df_existing_records, df_new_records], ignore_index=True)\n",
    "\n",
    "    # Deduplicate based on the unique key columns. 'keep=\"last\"' ensures that\n",
    "    # if duplicates exist (i.e., an old record and a new record for the same key set),\n",
    "    # the one from df_new_records (which is last in df_combined) is kept.\n",
    "    df_final = df_combined.drop_duplicates(subset=UNIQUE_KEY_COLUMNS_FOR_CSV, keep='last').copy()\n",
    "\n",
    "    # Optional: Sort the final DataFrame for consistent ordering in the CSV.\n",
    "    if 'run_timestamp' in df_final.columns: # Should always be true\n",
    "        df_final['run_timestamp'] = pd.to_datetime(df_final['run_timestamp'], errors='coerce')\n",
    "    \n",
    "    if 'selection_date' in df_final.columns: # Should always be true\n",
    "        # Create a temporary column for sorting by datetime version of selection_date\n",
    "        # Convert selection_date to string first to handle potential mixed types before to_datetime\n",
    "        # This assumes selection_date might be like 'YYYYMMDD' or 'YYYY-MM-DD'\n",
    "        # A try-except block can make parsing more robust if formats vary\n",
    "        try:\n",
    "            # Attempt parsing with a common format like YYYYMMDD if applicable, e.g., format='%Y%m%d'\n",
    "            # For general case, let pandas infer or handle multiple formats.\n",
    "            # Convert to string first to ensure consistent input to to_datetime\n",
    "            df_final['selection_date_dt_sort'] = pd.to_datetime(df_final['selection_date'].astype(str), errors='coerce')\n",
    "        except Exception as e: # Broad exception for parsing issues\n",
    "            logging.warning(f\"Could not reliably parse 'selection_date' for sorting: {e}. Sorting may be string-based for this column.\")\n",
    "            df_final['selection_date_dt_sort'] = df_final['selection_date'] # Fallback to original for sort\n",
    "\n",
    "        df_final = df_final.sort_values(\n",
    "            by=['run_timestamp', 'selection_date_dt_sort', 'scheme'],\n",
    "            ascending=[False, False, True] # Latest runs, latest selection dates, then by scheme\n",
    "        ).drop(columns=['selection_date_dt_sort'], errors='ignore') # Drop temporary sort column\n",
    "\n",
    "    try:\n",
    "        # Write the consolidated DataFrame to the CSV file, overwriting it.\n",
    "        df_final.to_csv(filepath, mode='w', header=True, index=False, float_format='%.8f')\n",
    "\n",
    "        num_input_records = len(df_new_records)\n",
    "        num_final_records = len(df_final)\n",
    "        num_existing_before_op = len(df_existing_records) # Count before concat\n",
    "        net_change = num_final_records - num_existing_before_op\n",
    "\n",
    "        logging.info(f\"Processed {num_input_records} new records. \"\n",
    "                     f\"CSV '{filepath}' now contains {num_final_records} records (net change: {net_change:+}).\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error writing consolidated data to CSV file '{filepath}': {e}\", exc_info=True)\n",
    "\n",
    "def setup_script_logging():\n",
    "    \"\"\"\n",
    "    Sets up logging for the script and logs initial information.\n",
    "    Returns the log file path.\n",
    "    \"\"\"\n",
    "    log_filepath = setup_logging()\n",
    "    run_timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    logging.info(f\"Script execution started at: {run_timestamp}\")\n",
    "\n",
    "    logging.info(f\"Python Interpreter: {sys.executable}\")\n",
    "    logging.info(f\"Current Working Directory: {os.getcwd()}\")\n",
    "    logging.info(f\"Pandas Version: {pd.__version__}\")\n",
    "    logging.info(f\"Numpy Version: {np.__version__}\")\n",
    "\n",
    "    return log_filepath, run_timestamp # Return both log_filepath and run_timestamp\n",
    "\n",
    "# --- New Function for Loading and Preparing Price Data ---\n",
    "def load_and_prepare_price_data(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads price data from a parquet file, validates and prepares its index.\n",
    "\n",
    "    Args:\n",
    "        file_path: The path to the price data parquet file.\n",
    "\n",
    "    Returns:\n",
    "        A pandas DataFrame with a DatetimeIndex, sorted chronologically.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the specified file_path does not exist.\n",
    "        Exception: For other errors during loading or processing.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Attempting to load price data from: {os.path.abspath(file_path)}\")\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"Price data file not found: {os.path.abspath(file_path)}\")\n",
    "\n",
    "    try:\n",
    "        df_adj_close = pd.read_parquet(file_path)\n",
    "\n",
    "        if not isinstance(df_adj_close.index, pd.DatetimeIndex):\n",
    "            logging.info(\"Converting price data index to DatetimeIndex...\")\n",
    "            # Attempt common conversions, handle potential errors\n",
    "            try:\n",
    "                 df_adj_close.index = pd.to_datetime(df_adj_close.index)\n",
    "            except Exception as e:\n",
    "                 logging.error(f\"Error converting index to DatetimeIndex: {e}\", exc_info=True)\n",
    "                 # Decide how to handle: raise, return None, etc.\n",
    "                 # For this example, we'll let the outer exception handler catch it if it fails.\n",
    "                 raise # Re-raise the conversion error\n",
    "\n",
    "        if not df_adj_close.index.is_monotonic_increasing:\n",
    "            logging.info(\"Sorting price data index...\")\n",
    "            df_adj_close = df_adj_close.sort_index()\n",
    "\n",
    "        logging.info(f\"Successfully loaded and prepared price data from {file_path}\")\n",
    "        logging.info(f\"Price data shape: {df_adj_close.shape}, Date range: {df_adj_close.index.min()} to {df_adj_close.index.max()}\")\n",
    "        logging.info(\"Price data loading completed.\")\n",
    "\n",
    "        return df_adj_close\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred while loading/preparing price data from {file_path}: {e}\", exc_info=True)\n",
    "        raise # Re-raise the exception so the main try/except block can catch it\n",
    "\n",
    "# --- New Function for Finding and Mapping Files ---\n",
    "def find_and_map_param_files(directory_path: str) -> Tuple[List[str], List[str], Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Discovers .parquet and .json files in a directory that start with '20', and creates a map\n",
    "    of parameter files by extracted date.\n",
    "\n",
    "    Args:\n",
    "        directory_path: The path to the directory containing the files.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "        - A list of discovered selection file names (.parquet).\n",
    "        - A list of discovered parameter file names (.json).\n",
    "        - A dictionary mapping extracted date strings to parameter file names.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the specified directory_path does not exist.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Attempting to find files in: {os.path.abspath(directory_path)}\")\n",
    "\n",
    "    if not os.path.isdir(directory_path):\n",
    "        raise FileNotFoundError(f\"Data directory not found: {os.path.abspath(directory_path)}\")\n",
    "\n",
    "    all_files = os.listdir(directory_path)\n",
    "    logging.debug(f\"Files found in directory: {all_files}\")\n",
    "\n",
    "    # Assuming files starting with '20' and ending with specific extensions are relevant\n",
    "    selection_files = sorted([f for f in all_files if f.startswith('20') and f.endswith('.parquet')])\n",
    "    param_files = sorted([f for f in all_files if f.startswith('20') and f.endswith('.json')])\n",
    "    logging.info(f\"Found {len(selection_files)} potential selection files (.parquet)\")\n",
    "    logging.info(f\"Found {len(param_files)} potential parameter files (.json)\")\n",
    "\n",
    "    param_map = {}\n",
    "    # Extracted dates sets are useful for internal function validation/logging\n",
    "    # but not strictly necessary to return if only used for mismatch reporting later.\n",
    "    # We'll keep them for logging within this function.\n",
    "    extracted_dates_params = set()\n",
    "\n",
    "\n",
    "    for pf in param_files:\n",
    "        try:\n",
    "            # Assume utils.extract_date_from_string is available globally or imported\n",
    "            date_key = utils.extract_date_from_string(pf)\n",
    "            if date_key:\n",
    "                if date_key in param_map:\n",
    "                    logging.warning(f\"Duplicate date key '{date_key}' found for param file '{pf}'. Overwriting mapping with previous file '{param_map[date_key]}'.\")\n",
    "                param_map[date_key] = pf\n",
    "                extracted_dates_params.add(date_key)\n",
    "            else:\n",
    "                logging.warning(f\"Could not extract date from param file: {pf}. Skipping.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error extracting date from param file '{pf}': {e}\", exc_info=True)\n",
    "\n",
    "    logging.debug(f\"Parameter map created: {param_map}\")\n",
    "    logging.info(\"File discovery and parameter mapping completed.\")\n",
    "\n",
    "    # Return the lists of files and the parameter map\n",
    "    # return selection_files, param_files, param_map\n",
    "    return selection_files, param_files, param_map, extracted_dates_params # Return the set of extracted dates for params\n",
    "\n",
    "# --- New Function for Pairing Files ---\n",
    "def pair_data_and_param_files(\n",
    "    selection_files: List[str],\n",
    "    param_files: List[str],\n",
    "    param_map: Dict[str, str],\n",
    "    extracted_dates_params: Set[str], # Now takes this as input\n",
    "    utils_module # Pass utility module\n",
    ") -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Pairs selection files with parameter files based on date extraction.\n",
    "    Reports mismatch details.\n",
    "\n",
    "    Args:\n",
    "        selection_files: List of discovered selection file names.\n",
    "        param_files: List of discovered parameter file names (for mismatch reporting).\n",
    "        param_map: Dictionary mapping date strings to parameter file names.\n",
    "        extracted_dates_params: Set of dates extracted from parameter files.\n",
    "        utils_module: Module containing necessary utility functions.\n",
    "\n",
    "    Returns:\n",
    "        A list of successfully paired (selection_file, param_file) tuples.\n",
    "    \"\"\"\n",
    "    file_pairs = []\n",
    "    extracted_dates_select = set() # Keep this inside, only used for mismatch reporting here\n",
    "\n",
    "    logging.info(\"Attempting to pair selection files with parameter files...\")\n",
    "\n",
    "    for sf in selection_files:\n",
    "        date_str_key = None\n",
    "        try:\n",
    "            date_str_key = utils_module.extract_date_from_string(sf)\n",
    "            if date_str_key:\n",
    "                extracted_dates_select.add(date_str_key)\n",
    "                logging.debug(f\"Extracted date '{date_str_key}' from selection file: {sf}\")\n",
    "                if date_str_key in param_map:\n",
    "                    file_pairs.append((sf, param_map[date_str_key]))\n",
    "                    logging.debug(f\"  Matched pair: ({sf}, {param_map[date_str_key]})\")\n",
    "                else:\n",
    "                    logging.warning(f\"Could not find matching param file for data file: {sf} (extracted date: {date_str_key})\")\n",
    "            else:\n",
    "                logging.warning(f\"Could not extract valid date from selection file: {sf}. Skipping.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error extracting date from selection file '{sf}': {e}\", exc_info=True)\n",
    "\n",
    "    logging.info(f\"\\n--- Found {len(file_pairs)} Paired Data and Parameter Files ---\")\n",
    "\n",
    "    # Mismatch details reporting using the sets and original file counts\n",
    "    if len(selection_files) > len(file_pairs) or len(param_files) > len(file_pairs):\n",
    "          logging.warning(f\"Mismatch details: Selection dates={extracted_dates_select}, Param dates={extracted_dates_params}\")\n",
    "\n",
    "    logging.info(\"File pairing completed.\")\n",
    "\n",
    "    # IMPORTANT: Do NOT include the 'if not file_pairs:' check here.\n",
    "    # That's logic for the caller (the main script) to decide what to do with the result.\n",
    "\n",
    "    return file_pairs\n",
    "\n",
    "# --- New Function to Process a Single Pair ---\n",
    "def process_single_pair(\n",
    "    data_file: str,\n",
    "    param_file_name: str,\n",
    "    output_dir: str, # Use output_dir consistently\n",
    "    df_adj_close: pd.DataFrame,\n",
    "    run_timestamp: str,\n",
    "    log_filepath: str,\n",
    "    utils_module: Any,\n",
    "    extract_backtest_setups_func: Callable,\n",
    "    process_all_backtests_func: Callable,\n",
    "    extract_params_and_results_func: Callable\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Processes a single data/parameter file pair, runs backtests, and extracts results.\n",
    "\n",
    "    Args:\n",
    "        data_file: Name of the selection data file (.parquet).\n",
    "        param_file_name: Name of the parameter file (.json).\n",
    "        output_dir: Directory containing the data and parameter files.\n",
    "        df_adj_close: DataFrame containing the main price data.\n",
    "        run_timestamp: Timestamp for the current run.\n",
    "        log_filepath: Path to the log file for the current run.\n",
    "        utils_module: Module containing utility functions like extract_date_from_string.\n",
    "        extract_backtest_setups_func: The function to extract backtest setups.\n",
    "        process_all_backtests_func: The function to run backtests.\n",
    "        extract_params_and_results_func: The function to extract performance records.\n",
    "\n",
    "    Returns:\n",
    "        A list of performance records (dictionaries) for this pair, or an empty list if\n",
    "        processing failed or no valid setups were found.\n",
    "    \"\"\"\n",
    "    current_date_str = None\n",
    "    try:\n",
    "        # Date re-extraction (kept here as in original logic, though could be passed in file_pairs)\n",
    "        current_date_str = utils_module.extract_date_from_string(data_file)\n",
    "        if not current_date_str:\n",
    "            logging.error(f\"Skipping pair - Failed to re-extract valid date from {data_file}\")\n",
    "            return [] # Return empty list on failure\n",
    "\n",
    "        logging.info(f\"Processing for extracted date: {current_date_str}\")\n",
    "\n",
    "        # Ensure price data is available before proceeding\n",
    "        if df_adj_close is None or df_adj_close.empty:\n",
    "             logging.critical(f\"Price data (df_adj_close) is not loaded or is empty. Skipping pair for {current_date_str}.\")\n",
    "             return [] # Return empty list if price data is missing\n",
    "\n",
    "        param_path = os.path.join(OUTPUT_DIR, param_file_name)\n",
    "        logging.debug(f\"Reading parameters from: {param_path}\")\n",
    "        with open(param_path, 'r', encoding='utf-8') as f:\n",
    "            params = json.load(f)\n",
    "            logging.info(f\"Parameters loaded from {param_file_name}:\")\n",
    "            # Log parameters neatly\n",
    "            params_str_io = io.StringIO()\n",
    "            pprint.pprint(params, stream=params_str_io, width=100)\n",
    "            logging.info(\"\\n\" + params_str_io.getvalue())\n",
    "\n",
    "        selection_path = os.path.join(OUTPUT_DIR, data_file)\n",
    "        logging.debug(f\"Reading selection data from: {selection_path}\")\n",
    "        selection_df = pd.read_parquet(selection_path)\n",
    "        logging.debug(f'Loaded selection_df. Shape: {selection_df.shape}, Index type: {type(selection_df.index)}, Columns: {selection_df.columns.tolist()[:10]}...')\n",
    "\n",
    "        logging.debug(f\"Extracting backtest setups for date: {current_date_str}\")\n",
    "        backtest_setups = extract_backtest_setups_func(\n",
    "            dataframe=selection_df,\n",
    "            weight_column_names=['Weight_EW', 'Weight_IV', 'Weight_SW'],\n",
    "            date_str=current_date_str,\n",
    "        )\n",
    "\n",
    "        if not backtest_setups or not backtest_setups.get(current_date_str):\n",
    "            logging.warning(f\"No valid backtest setups extracted for {current_date_str}. Skipping backtest run for this pair.\")\n",
    "            return [] # Return empty list if no setups\n",
    "\n",
    "        # Log extracted setups neatly\n",
    "        logging.info(f\"Successfully extracted {len(backtest_setups.get(current_date_str, {}))} setup(s) for {current_date_str}.\")\n",
    "        setups_str_io = io.StringIO()\n",
    "        pprint.pprint(backtest_setups, stream=setups_str_io, width=120, depth=3)\n",
    "        logging.debug(\"Extracted Backtest Setups (preview):\\n\" + setups_str_io.getvalue())\n",
    "\n",
    "        logging.info(f\"Running backtests for date: {current_date_str}\")\n",
    "        backtest_results_summary = process_all_backtests_func(backtest_setups, df_adj_close)\n",
    "\n",
    "        # Log backtest summary neatly\n",
    "        summary_str_io = io.StringIO()\n",
    "        for res_date, res_schemes in backtest_results_summary.items():\n",
    "            pprint.pprint({res_date: list(res_schemes.keys())}, stream=summary_str_io)\n",
    "        logging.debug(\"\\n--- Backtest Results Summary (Schemes Processed) ---\\n\" + summary_str_io.getvalue())\n",
    "\n",
    "\n",
    "        logging.debug(f\"Extracting parameters and results for date: {current_date_str}\")\n",
    "        run_records = extract_params_and_results_func(\n",
    "            params=params,\n",
    "            backtest_results_summary=backtest_results_summary,\n",
    "            run_timestamp=run_timestamp,\n",
    "            log_filepath=log_filepath\n",
    "        )\n",
    "        logging.info(f\"Extracted {len(run_records)} performance records for this pair.\")\n",
    "\n",
    "        logging.info(f\"--- Finished processing pair for {current_date_str}. ---\")\n",
    "\n",
    "        return run_records # Return the list of records on success\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        logging.error(f\"FILE NOT FOUND Error processing pair ({data_file}, {param_file_name}): {e}\", exc_info=True)\n",
    "        return [] # Return empty list on specific error\n",
    "    except KeyError as e:\n",
    "         logging.error(f\"KEY Error processing pair ({data_file}, {param_file_name}) - often related to missing columns/dates: {e}\", exc_info=True)\n",
    "         return [] # Return empty list on specific error\n",
    "    except Exception as e:\n",
    "        logging.error(f\"UNHANDLED Error processing pair ({data_file}, {param_file_name}) for date {current_date_str}: {e}\", exc_info=True)\n",
    "        logging.error(traceback.format_exc())\n",
    "        return [] # Return empty list on any other unhandled error\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 19:13:34,185 - INFO - Logging initialized. Log file: logs\\backtest_run_20250513_191334.log\n",
      "2025-05-13 19:13:34,186 - INFO - Script execution started at: 2025-05-13 19:13:34\n",
      "2025-05-13 19:13:34,190 - INFO - Python Interpreter: c:\\Users\\ping\\Files_win10\\python\\py311\\.venv\\Scripts\\python.exe\n",
      "2025-05-13 19:13:34,193 - INFO - Current Working Directory: c:\\Users\\ping\\Files_win10\\python\\py311\\stocks\\notebooks\n",
      "2025-05-13 19:13:34,194 - INFO - Pandas Version: 2.2.3\n",
      "2025-05-13 19:13:34,197 - INFO - Numpy Version: 1.26.4\n",
      "2025-05-13 19:13:34,199 - INFO - Attempting to load price data from: c:\\Users\\ping\\Files_win10\\python\\py311\\stocks\\data\\df_adj_close.parquet\n",
      "2025-05-13 19:13:34,525 - INFO - Successfully loaded and prepared price data from ../data/df_adj_close.parquet\n",
      "2025-05-13 19:13:34,525 - INFO - Price data shape: (319, 1544), Date range: 2024-02-01 00:00:00 to 2025-05-09 00:00:00\n",
      "2025-05-13 19:13:34,542 - INFO - Price data loading completed.\n",
      "2025-05-13 19:13:34,542 - INFO - Attempting to find files in: c:\\Users\\ping\\Files_win10\\python\\py311\\stocks\\notebooks\\output\\selection_results\n",
      "2025-05-13 19:13:34,550 - INFO - Found 1 potential selection files (.parquet)\n",
      "2025-05-13 19:13:34,550 - INFO - Found 1 potential parameter files (.json)\n",
      "2025-05-13 19:13:34,550 - INFO - File discovery and parameter mapping completed.\n",
      "2025-05-13 19:13:34,559 - INFO - Attempting to pair selection files with parameter files...\n",
      "2025-05-13 19:13:34,562 - INFO - \n",
      "--- Found 1 Paired Data and Parameter Files ---\n",
      "2025-05-13 19:13:34,564 - INFO - File pairing completed.\n",
      "2025-05-13 19:13:34,567 - INFO - Starting processing for 1 file pairs...\n",
      "2025-05-13 19:13:34,567 - INFO - \n",
      "--- Processing Pair 1/1: Data='2025-04-25_my_selection_run_1.parquet', Params='2025-04-25_my_selection_run_1_params.json' ---\n",
      "2025-05-13 19:13:34,576 - INFO - Processing for extracted date: 2025-04-25\n",
      "2025-05-13 19:13:34,576 - INFO - Parameters loaded from 2025-04-25_my_selection_run_1_params.json:\n",
      "2025-05-13 19:13:34,576 - INFO - \n",
      "{'filter_max_debt_eq': 1.5,\n",
      " 'filter_min_avg_volume_m': 2.0,\n",
      " 'filter_min_price': 10.0,\n",
      " 'filter_min_roe_pct': 5.0,\n",
      " 'inv_vol_col_name': 'ATR/Price %',\n",
      " 'n_select_actual': 10,\n",
      " 'n_select_requested': 10,\n",
      " 'score_weight_change': 0.35,\n",
      " 'score_weight_rel_volume': 0.2,\n",
      " 'score_weight_rsi': 0.35,\n",
      " 'score_weight_volatility': 0.1}\n",
      "\n",
      "2025-05-13 19:13:34,592 - INFO - Successfully extracted weights for scheme: EW (10 tickers) for date 2025-04-25\n",
      "2025-05-13 19:13:34,592 - INFO - Successfully extracted weights for scheme: IV (10 tickers) for date 2025-04-25\n",
      "2025-05-13 19:13:34,606 - INFO - Successfully extracted weights for scheme: SW (10 tickers) for date 2025-04-25\n",
      "2025-05-13 19:13:34,609 - INFO - Successfully extracted 3 setup(s) for 2025-04-25.\n",
      "2025-05-13 19:13:34,609 - INFO - Running backtests for date: 2025-04-25\n",
      "2025-05-13 19:13:34,609 - INFO - \n",
      "===== Starting Batch Backtest Processing =====\n",
      "2025-05-13 19:13:34,619 - INFO - Prepared global price data copy for backtests.\n",
      "2025-05-13 19:13:34,626 - INFO - \n",
      "Processing date: 2025-04-25\n",
      "2025-05-13 19:13:34,626 - INFO - ------------------------------\n",
      "2025-05-13 19:13:34,626 - INFO - Initiating Backtest Run...\n",
      "2025-05-13 19:13:34,626 - INFO -   Date          : 2025-04-25\n",
      "2025-05-13 19:13:34,626 - INFO -   Scheme        : EW\n",
      "2025-05-13 19:13:34,626 - INFO -   Num Tickers   : 10\n",
      "2025-05-13 19:13:34,643 - INFO -   Selection Date Used: 2025-04-25\n",
      "2025-05-13 19:13:34,643 - INFO -   Buy Date           : 2025-04-28\n",
      "2025-05-13 19:13:34,643 - INFO -   Sell Date          : 2025-04-29\n",
      "2025-05-13 19:13:34,660 - INFO -   Trades Executed: 10/10\n",
      "2025-05-13 19:13:34,660 - INFO -   Portfolio Return          : 0.0085 (Based on Weight Sum: 1.0000)\n",
      "2025-05-13 19:13:34,660 - INFO -   Win Rate (Individual)   : 70.00%\n",
      "2025-05-13 19:13:34,673 - INFO -   Avg Ticker Return       : 0.0085\n",
      "2025-05-13 19:13:34,675 - INFO -   Std Dev Ticker Return   : 0.0203\n",
      "2025-05-13 19:13:34,675 - INFO -   Period Sharpe (Indiv)   : 0.4154\n",
      "2025-05-13 19:13:34,675 - INFO - Backtest simulation for 'EW' on 2025-04-25 completed.\n",
      "2025-05-13 19:13:34,675 - INFO - ------------------------------\n",
      "2025-05-13 19:13:34,675 - INFO - ------------------------------\n",
      "2025-05-13 19:13:34,675 - INFO - Initiating Backtest Run...\n",
      "2025-05-13 19:13:34,692 - INFO -   Date          : 2025-04-25\n",
      "2025-05-13 19:13:34,694 - INFO -   Scheme        : IV\n",
      "2025-05-13 19:13:34,696 - INFO -   Num Tickers   : 10\n",
      "2025-05-13 19:13:34,700 - INFO -   Selection Date Used: 2025-04-25\n",
      "2025-05-13 19:13:34,701 - INFO -   Buy Date           : 2025-04-28\n",
      "2025-05-13 19:13:34,701 - INFO -   Sell Date          : 2025-04-29\n",
      "2025-05-13 19:13:34,717 - INFO -   Trades Executed: 10/10\n",
      "2025-05-13 19:13:34,717 - INFO -   Portfolio Return          : 0.0075 (Based on Weight Sum: 1.0000)\n",
      "2025-05-13 19:13:34,725 - INFO -   Win Rate (Individual)   : 70.00%\n",
      "2025-05-13 19:13:34,726 - INFO -   Avg Ticker Return       : 0.0085\n",
      "2025-05-13 19:13:34,729 - INFO -   Std Dev Ticker Return   : 0.0203\n",
      "2025-05-13 19:13:34,729 - INFO -   Period Sharpe (Indiv)   : 0.4154\n",
      "2025-05-13 19:13:34,729 - INFO - Backtest simulation for 'IV' on 2025-04-25 completed.\n",
      "2025-05-13 19:13:34,729 - INFO - ------------------------------\n",
      "2025-05-13 19:13:34,729 - INFO - ------------------------------\n",
      "2025-05-13 19:13:34,729 - INFO - Initiating Backtest Run...\n",
      "2025-05-13 19:13:34,729 - INFO -   Date          : 2025-04-25\n",
      "2025-05-13 19:13:34,729 - INFO -   Scheme        : SW\n",
      "2025-05-13 19:13:34,745 - INFO -   Num Tickers   : 10\n",
      "2025-05-13 19:13:34,745 - INFO -   Selection Date Used: 2025-04-25\n",
      "2025-05-13 19:13:34,745 - INFO -   Buy Date           : 2025-04-28\n",
      "2025-05-13 19:13:34,757 - INFO -   Sell Date          : 2025-04-29\n",
      "2025-05-13 19:13:34,763 - INFO -   Trades Executed: 10/10\n",
      "2025-05-13 19:13:34,775 - INFO -   Portfolio Return          : 0.0096 (Based on Weight Sum: 1.0000)\n",
      "2025-05-13 19:13:34,779 - INFO -   Win Rate (Individual)   : 70.00%\n",
      "2025-05-13 19:13:34,779 - INFO -   Avg Ticker Return       : 0.0085\n",
      "2025-05-13 19:13:34,779 - INFO -   Std Dev Ticker Return   : 0.0203\n",
      "2025-05-13 19:13:34,779 - INFO -   Period Sharpe (Indiv)   : 0.4154\n",
      "2025-05-13 19:13:34,779 - INFO - Backtest simulation for 'SW' on 2025-04-25 completed.\n",
      "2025-05-13 19:13:34,779 - INFO - ------------------------------\n",
      "2025-05-13 19:13:34,779 - INFO - \n",
      "===== Batch Backtest Processing Finished =====\n",
      "2025-05-13 19:13:34,796 - INFO - Extracted 3 performance records for this pair.\n",
      "2025-05-13 19:13:34,796 - INFO - --- Finished processing pair for 2025-04-25. ---\n",
      "2025-05-13 19:13:34,796 - INFO - --- File processing loop finished. ---\n",
      "2025-05-13 19:13:34,796 - INFO - \n",
      "--- Attempting to Save/Update 3 Performance Records to CSV ---\n",
      "C:\\Users\\ping\\AppData\\Local\\Temp\\ipykernel_17788\\456981485.py:553: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_combined = pd.concat([df_existing_records, df_new_records], ignore_index=True)\n",
      "2025-05-13 19:13:34,848 - INFO - Processed 3 new records. CSV 'output/backtest_results\\backtest_parameter_performance.csv' now contains 3 records (net change: +3).\n",
      "2025-05-13 19:13:34,851 - INFO - === Script Execution Finished (with errors if reported above) ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Script Execution Finished (with errors if reported above) ===\n",
      "Logging shutdown complete.\n"
     ]
    }
   ],
   "source": [
    "# --- Main Execution Block (Updated with process_single_pair) ---\n",
    "\n",
    "log_filepath = None\n",
    "run_timestamp = None\n",
    "df_adj_close = None\n",
    "all_performance_records = []\n",
    "file_pairs = []\n",
    "\n",
    "try:\n",
    "    log_filepath, run_timestamp = setup_script_logging()\n",
    "\n",
    "    # --- Step 1: Load and Prepare Price Data ---\n",
    "    df_adj_close = load_and_prepare_price_data(ADJ_CLOSE_PATH)\n",
    "\n",
    "    # --- Step 2: Discover and Map Input Files ---\n",
    "    # Use 'output' directory for demonstration consistency\n",
    "    selection_files, param_files, param_map, extracted_dates_params = find_and_map_param_files(OUTPUT_DIR)\n",
    "\n",
    "    # --- Step 3: Pair Selection Files with Parameter Files ---\n",
    "    file_pairs = pair_data_and_param_files(\n",
    "        selection_files=selection_files,\n",
    "        param_files=param_files,\n",
    "        param_map=param_map,\n",
    "        extracted_dates_params=extracted_dates_params,\n",
    "        utils_module=utils\n",
    "    )\n",
    "\n",
    "    # --- Step 4: Check if pairs were found and log before proceeding to loop ---\n",
    "    if not file_pairs:\n",
    "        logging.warning(\"No file pairs found to process. Skipping backtest loop.\")\n",
    "    else:\n",
    "        logging.info(f\"Starting processing for {len(file_pairs)} file pairs...\")\n",
    "\n",
    "        # --- Step 5: Process Paired Files (Loop calls process_single_pair) ---\n",
    "        processed_pair_count = 0\n",
    "        for data_file, param_file_name in file_pairs:\n",
    "            processed_pair_count += 1\n",
    "\n",
    "            # Log the start of processing for this specific pair (kept in the loop for context)\n",
    "            logging.info(f\"\\n--- Processing Pair {processed_pair_count}/{len(file_pairs)}: Data='{data_file}', Params='{param_file_name}' ---\")\n",
    "\n",
    "            # Call the function to process this single pair\n",
    "            # Pass necessary data and function references\n",
    "            pair_records = process_single_pair(\n",
    "                data_file=data_file,\n",
    "                param_file_name=param_file_name,\n",
    "                output_dir=OUTPUT_DIR, # Directory where the pair files live\n",
    "                df_adj_close=df_adj_close, # Pass the main price data\n",
    "                run_timestamp=run_timestamp, # Pass run metadata\n",
    "                log_filepath=log_filepath,\n",
    "                utils_module=utils, # Pass utilities module\n",
    "                extract_backtest_setups_func=extract_backtest_setups, # Pass function references\n",
    "                process_all_backtests_func=process_all_backtests,\n",
    "                extract_params_and_results_func=extract_params_and_results\n",
    "            )\n",
    "\n",
    "            # Extend the main results list with the records returned by the function\n",
    "            # This handles cases where the function returned [] due to an error/skip\n",
    "            all_performance_records.extend(pair_records)\n",
    "\n",
    "        logging.info(\"--- File processing loop finished. ---\")\n",
    "\n",
    "    # --- Step 6: Save Accumulated Results ---\n",
    "    logging.info(f\"\\n--- Attempting to Save/Update {len(all_performance_records)} Performance Records to CSV ---\")\n",
    "    if all_performance_records:\n",
    "        write_results_to_csv(all_performance_records, RESULTS_CSV_PATH)\n",
    "    else:\n",
    "        logging.info(\"No performance records to save.\")\n",
    "\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"FATAL ERROR: Required file or directory not found: {e}\")\n",
    "    if log_filepath and logging.getLogger().hasHandlers():\n",
    "          logging.critical(f\"FATAL FileNotFoundError: {e}\", exc_info=True)\n",
    "    else:\n",
    "          print(f\"Logging not initialized. Error: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"FATAL ERROR in main execution block: {e}\")\n",
    "    if log_filepath and logging.getLogger().hasHandlers():\n",
    "          logging.critical(f\"CRITICAL ERROR in main execution block: {e}\", exc_info=True)\n",
    "    else:\n",
    "          print(f\"Logging not initialized. Error: {e}\")\n",
    "          traceback.print_exc()\n",
    "finally:\n",
    "    final_message = \"=== Script Execution Finished (with errors if reported above) ===\"\n",
    "    print(final_message)\n",
    "    if log_filepath and logging.getLogger().hasHandlers():\n",
    "          logging.info(final_message)\n",
    "          logging.shutdown()\n",
    "          print(\"Logging shutdown complete.\")\n",
    "    else:\n",
    "          print(\"Logging was not fully initialized or already shut down.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
