{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Notebook cell\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Get root directory (assuming notebook is in root/notebooks/)\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "ROOT_DIR = NOTEBOOK_DIR.parent if NOTEBOOK_DIR.name == 'notebooks' else NOTEBOOK_DIR\n",
    "\n",
    "# Add src directory to Python path\n",
    "sys.path.append(str(ROOT_DIR / 'src'))\n",
    "\n",
    "# Verify path\n",
    "print(f\"Python will look in these locations:\\n{sys.path}\")\n",
    "\n",
    "\n",
    "# # --- Execute the processor ---\n",
    "# import utils\n",
    "# from config import date_str, DOWNLOAD_DIR, DEST_DIR\n",
    "\n",
    "date_str = '2025-04-01'\n",
    "print(f'\\nSTOCK SELECTION DATE: {date_str}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Set pandas display options to show more columns and rows\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "# pd.set_option('display.max_rows', 10)       # Limit to 10 rows for readability\n",
    "pd.set_option('display.width', 1000)        # Let the display adjust to the window\n",
    "# pd.set_option('display.max_colwidth', None) # Show full content of each cell\n",
    "pd.set_option('display.max_rows', 200)\n",
    "# pd.set_option('display.width', 120)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zscore_df = pd.read_parquet(f'..\\data\\{date_str}_zscore_df.parquet', engine='pyarrow')\n",
    "cluster_stats_df = pd.read_parquet(f'..\\data\\{date_str}_cluster_stats_df.parquet', engine='pyarrow')\n",
    "detailed_clusters_df = pd.read_parquet(f'..\\data\\{date_str}_detailed_clusters_df.parquet', engine='pyarrow')\n",
    "# df_finviz_merged = pd.read_parquet(f'..\\data\\{date_str}_df_finviz_merged.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detailed_clusters_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_finviz_merged = pd.read_parquet(f'..\\data\\{date_str}_df_finviz_merged.parquet', engine='pyarrow')\n",
    "df_OHLCV = pd.read_parquet(r'..\\data\\2025-04-17_df_OHLCV_clean.parquet', engine='pyarrow')\n",
    "df_OHLCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # Assuming pandas is used\n",
    "\n",
    "def select_stocks_from_clusters(cluster_stats_df, detailed_clusters_df,\n",
    "                                select_top_n_clusters=3, max_selection_per_cluster=5,\n",
    "                                min_cluster_size=5, penalty_IntraCluster_Corr=0.3,\n",
    "                                date_str=date_str,\n",
    "                                min_raw_score=None, # <-- Added argument\n",
    "                                min_risk_adj_score=None): # <-- Added argument\n",
    "    \"\"\"\n",
    "    Pipeline to select stocks from better performing clusters, with optional score thresholds.\n",
    "\n",
    "    Parameters:\n",
    "    - cluster_stats_df: DataFrame with cluster statistics.\n",
    "    - detailed_clusters_df: DataFrame with detailed cluster information including\n",
    "                            'Ticker', 'Cluster_ID', 'Raw_Score', 'Risk_Adj_Score', etc.\n",
    "    - select_top_n_clusters: int, Number of top clusters to select (default=3).\n",
    "    - max_selection_per_cluster: int, Max number of stocks to select from each cluster (default=5).\n",
    "    - min_cluster_size: int, Minimum size for a cluster to be considered (default=5).\n",
    "    - penalty_IntraCluster_Corr: float, Penalty weight for intra-cluster correlation in\n",
    "                                     composite score (default=0.3).\n",
    "    - date_str: str, Date string for tracking/parameter storage.\n",
    "    - min_raw_score: float, optional (default=None)\n",
    "        Minimum Raw_Score required for a stock to be considered for selection.\n",
    "        If None, no threshold is applied based on Raw_Score.\n",
    "    - min_risk_adj_score: float, optional (default=None)\n",
    "        Minimum Risk_Adj_Score required for a stock to be considered for selection.\n",
    "        If None, no threshold is applied based on Risk_Adj_Score.\n",
    "\n",
    "    Returns:\n",
    "    - dict: A dictionary containing:\n",
    "        - 'selected_top_n_cluster_ids': List of top selected cluster IDs.\n",
    "        - 'selected_stocks': DataFrame of selected stocks.\n",
    "        - 'cluster_performance': DataFrame of selected cluster metrics.\n",
    "        - 'parameters': Dictionary of the input parameters used.\n",
    "    \"\"\"\n",
    "\n",
    "    # Store input parameters\n",
    "    parameters = {\n",
    "        'date_str': date_str,\n",
    "        'select_top_n_clusters': select_top_n_clusters,\n",
    "        'max_selection_per_cluster': max_selection_per_cluster,\n",
    "        'min_cluster_size': min_cluster_size,\n",
    "        'min_raw_score': min_raw_score,         # <-- Stored parameter\n",
    "        'min_risk_adj_score': min_risk_adj_score, # <-- Stored parameter\n",
    "        'penalty_IntraCluster_Corr': penalty_IntraCluster_Corr,\n",
    "    }\n",
    "    \n",
    "    # ===== 1. Filter and Rank Clusters =====\n",
    "    qualified_clusters = cluster_stats_df[cluster_stats_df['Size'] >= min_cluster_size].copy()\n",
    "    if qualified_clusters.empty:\n",
    "        print(f\"Warning: No clusters met the minimum size criteria ({min_cluster_size}).\")\n",
    "        return {\n",
    "            'selected_stocks': pd.DataFrame(),\n",
    "            'cluster_performance': pd.DataFrame(),\n",
    "            'parameters': parameters\n",
    "        }\n",
    "\n",
    "    qualified_clusters['Composite_Cluster_Score'] = (\n",
    "        (1 - penalty_IntraCluster_Corr) * qualified_clusters['Avg_Raw_Score'] +\n",
    "        penalty_IntraCluster_Corr * (1 - qualified_clusters['Avg_IntraCluster_Corr'])\n",
    "    )\n",
    "    ranked_clusters = qualified_clusters.sort_values('Composite_Cluster_Score', ascending=False)\n",
    "    selected_clusters = ranked_clusters.head(select_top_n_clusters)\n",
    "    cluster_ids = selected_clusters['Cluster_ID'].tolist()\n",
    "\n",
    "    if not cluster_ids:\n",
    "        print(\"Warning: No clusters were selected based on ranking.\")\n",
    "        return {\n",
    "            'selected_stocks': pd.DataFrame(),\n",
    "            'cluster_performance': selected_clusters, # Return empty selected clusters df\n",
    "            'parameters': parameters\n",
    "        }\n",
    "\n",
    "\n",
    "    # ===== 2. Select Stocks from Each Cluster =====\n",
    "    selected_stocks_list = []\n",
    "    for cluster_id in cluster_ids:\n",
    "        # Get all stocks for the current cluster\n",
    "        cluster_stocks = detailed_clusters_df[detailed_clusters_df['Cluster_ID'] == cluster_id].copy()\n",
    "\n",
    "        # ===> Apply Threshold Filters <===\n",
    "        if min_raw_score is not None:\n",
    "            cluster_stocks = cluster_stocks[cluster_stocks['Raw_Score'] >= min_raw_score]\n",
    "        if min_risk_adj_score is not None:\n",
    "            cluster_stocks = cluster_stocks[cluster_stocks['Risk_Adj_Score'] >= min_risk_adj_score]\n",
    "        # ===> End of Added Filters <===\n",
    "\n",
    "        # Proceed only if stocks remain after filtering\n",
    "        if len(cluster_stocks) > 0:\n",
    "            # Sort remaining stocks by Risk_Adj_Score and select top N\n",
    "            top_stocks = cluster_stocks.sort_values('Risk_Adj_Score', ascending=False).head(max_selection_per_cluster)\n",
    "\n",
    "            # Add cluster-level metrics to the selected stock rows\n",
    "            cluster_metrics = selected_clusters[selected_clusters['Cluster_ID'] == cluster_id].iloc[0]\n",
    "            for col in ['Composite_Cluster_Score', 'Avg_IntraCluster_Corr', 'Avg_Volatility',\n",
    "                      'Avg_Raw_Score', 'Avg_Risk_Adj_Score', 'Size']: # Added Size for context\n",
    "                # Use .get() for safety if a column might be missing\n",
    "                top_stocks[f'Cluster_{col}'] = cluster_metrics.get(col, None)\n",
    "            selected_stocks_list.append(top_stocks)\n",
    "\n",
    "    # Consolidate selected stocks\n",
    "    if selected_stocks_list:\n",
    "        selected_stocks = pd.concat(selected_stocks_list)\n",
    "        # Recalculate weights based on the final selection\n",
    "        if selected_stocks['Risk_Adj_Score'].sum() != 0:\n",
    "             selected_stocks['Weight'] = (selected_stocks['Risk_Adj_Score'] /\n",
    "                                          selected_stocks['Risk_Adj_Score'].sum())\n",
    "        else:\n",
    "             # Handle case where all selected scores are zero (unlikely but possible)\n",
    "             selected_stocks['Weight'] = 1 / len(selected_stocks) if len(selected_stocks) > 0 else 0\n",
    "\n",
    "        selected_stocks = selected_stocks.sort_values(['Cluster_ID', 'Risk_Adj_Score'],\n",
    "                                                    ascending=[True, False])\n",
    "    else:\n",
    "        selected_stocks = pd.DataFrame()\n",
    "        print(\"Warning: No stocks met selection criteria (including score thresholds if applied).\")\n",
    "\n",
    "\n",
    "    # ===== 3. Prepare Enhanced Output Reports =====\n",
    "    cluster_performance = selected_clusters.copy()\n",
    "    # Calculate how many stocks were actually selected per cluster after filtering\n",
    "    cluster_performance['Stocks_Selected'] = cluster_performance['Cluster_ID'].apply(\n",
    "        lambda x: len(selected_stocks[selected_stocks['Cluster_ID'] == x]) if not selected_stocks.empty else 0)\n",
    "\n",
    "    if not selected_stocks.empty:\n",
    "         # Ensure Avg_IntraCluster_Corr exists before calculating diversification\n",
    "        if 'Avg_IntraCluster_Corr' in cluster_performance.columns:\n",
    "             cluster_performance['Intra_Cluster_Diversification'] = 1 - cluster_performance['Avg_IntraCluster_Corr']\n",
    "        else:\n",
    "             cluster_performance['Intra_Cluster_Diversification'] = pd.NA # Or None\n",
    "    else:\n",
    "      # Handle case where selected_stocks is empty\n",
    "        cluster_performance['Intra_Cluster_Diversification'] = pd.NA # Or None\n",
    "\n",
    "    # ===> Package results and parameters\n",
    "    results_bundle = {\n",
    "        'selected_top_n_cluster_ids': cluster_ids,\n",
    "        'selected_stocks': selected_stocks,\n",
    "        'cluster_performance': cluster_performance,\n",
    "        'parameters': parameters\n",
    "    }\n",
    "\n",
    "    return results_bundle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import Dict, Any\n",
    "\n",
    "def print_stock_selection_report(output: Dict[str, Any]) -> None:\n",
    "    \"\"\"\n",
    "    Prints a detailed report summarizing the results of the stock selection process,\n",
    "    extracting all necessary information from the output dictionary.\n",
    "\n",
    "    Args:\n",
    "        output (Dict[str, Any]): The dictionary returned by the\n",
    "                                 select_stocks_from_clusters function, containing:\n",
    "                                 - 'selected_stocks': DataFrame of selected stocks.\n",
    "                                 - 'cluster_performance': DataFrame of selected cluster metrics.\n",
    "                                #  - 'parameters': Dictionary of the input parameters used.\n",
    "                                #  - 'cluster_stats_df': Original cluster stats DataFrame.\n",
    "                                #  - 'detailed_clusters_df': Original detailed clusters DataFrame.\n",
    "    Returns:\n",
    "        None: This function prints output to the console.\n",
    "    \"\"\"\n",
    "    # Extract data from the output dictionary using .get() for safety\n",
    "    selected_stocks = output.get('selected_stocks', pd.DataFrame())\n",
    "    cluster_performance = output.get('cluster_performance', pd.DataFrame())\n",
    "    used_params = output.get('parameters', {})\n",
    "    # Extract the input DataFrames needed for the report\n",
    "    # cluster_stats_df = output.get('input_cluster_stats_df') # Might be None\n",
    "    cluster_stats_df = output.get('cluster_stats_df') # Might be None\n",
    "    # detailed_clusters_df = output.get('input_detailed_clusters_df') # Might be None\n",
    "    detailed_clusters_df = output.get('detailed_clusters_df') # Might be None\n",
    "\n",
    "    # --- Start of Original Code Block (adapted) ---\n",
    "\n",
    "    print(\"\\n=== CLUSTER SELECTION CRITERIA ===\")\n",
    "    print(\"* Using Composite_Cluster_Score (balancing Raw Score and diversification) for cluster ranking.\")\n",
    "    print(\"* Using Risk_Adj_Score for stock selection within clusters.\")\n",
    "\n",
    "    num_selected_clusters = len(cluster_performance) if not cluster_performance.empty else 0\n",
    "    # Use the extracted cluster_stats_df\n",
    "    total_clusters = len(cluster_stats_df) if cluster_stats_df is not None and not cluster_stats_df.empty else 'N/A'\n",
    "\n",
    "    print(f\"* Selected top {num_selected_clusters} clusters from {total_clusters} total initial clusters.\") # Adjusted wording slightly\n",
    "    print(f\"* Selection Criteria:\")\n",
    "    if used_params:\n",
    "        for key, value in used_params.items():\n",
    "            # Avoid printing the large input dataframes stored in parameters if they were added there too\n",
    "            if not isinstance(value, pd.DataFrame):\n",
    "                 print(f\"    {key}: {value}\")\n",
    "    else:\n",
    "        print(\"    Parameters not available.\")\n",
    "\n",
    "\n",
    "    if not cluster_performance.empty:\n",
    "        print(\"\\n=== SELECTED CLUSTERS (RANKED BY COMPOSITE SCORE) ===\")\n",
    "        display_cols_exist = [col for col in [\n",
    "                                'Cluster_ID', 'Size', 'Avg_Raw_Score', 'Avg_Risk_Adj_Score',\n",
    "                                'Avg_IntraCluster_Corr', 'Avg_Volatility', 'Composite_Cluster_Score',\n",
    "                                'Stocks_Selected', 'Intra_Cluster_Diversification']\n",
    "                              if col in cluster_performance.columns]\n",
    "        print(cluster_performance[display_cols_exist].sort_values('Composite_Cluster_Score', ascending=False).to_string(index=False))\n",
    "\n",
    "        # Print top 8 stocks by Raw_Score for each selected cluster\n",
    "        # Check if detailed_clusters_df was successfully extracted\n",
    "        if detailed_clusters_df is not None and not detailed_clusters_df.empty:\n",
    "            print(\"\\n=== TOP STOCKS BY RAW SCORE PER SELECTED CLUSTER ===\")\n",
    "            print(\"\"\"* Volatility is the standard deviation of daily returns over the past 250 trading days (example context).\n",
    "* Note: The stocks below are shown ranked by Raw_Score for analysis,\n",
    "*       but actual selection within the cluster was based on Risk_Adj_Score.\"\"\")\n",
    "\n",
    "            for cluster_id in cluster_performance['Cluster_ID']:\n",
    "                 cluster_stocks = detailed_clusters_df[detailed_clusters_df['Cluster_ID'] == cluster_id]\n",
    "                 if not cluster_stocks.empty:\n",
    "                    required_cols = ['Ticker', 'Raw_Score', 'Risk_Adj_Score', 'Volatility']\n",
    "                    if all(col in cluster_stocks.columns for col in required_cols):\n",
    "                        top_raw = cluster_stocks.nlargest(8, 'Raw_Score')[required_cols]\n",
    "\n",
    "                        print(f\"\\nCluster {cluster_id} - Top 8 by Raw Score:\")\n",
    "                        print(top_raw.to_string(index=False))\n",
    "                        cluster_avg_raw = cluster_performance.loc[cluster_performance['Cluster_ID'] == cluster_id, 'Avg_Raw_Score'].values\n",
    "                        cluster_avg_risk = cluster_performance.loc[cluster_performance['Cluster_ID'] == cluster_id, 'Avg_Risk_Adj_Score'].values\n",
    "                        if len(cluster_avg_raw) > 0: print(f\"Cluster Avg Raw Score: {cluster_avg_raw[0]:.2f}\")\n",
    "                        if len(cluster_avg_risk) > 0: print(f\"Cluster Avg Risk Adj Score: {cluster_avg_risk[0]:.2f}\")\n",
    "                    else:\n",
    "                        print(f\"\\nCluster {cluster_id} - Missing required columns in detailed_clusters_df to show top stocks.\")\n",
    "                 else:\n",
    "                     print(f\"\\nCluster {cluster_id} - No stocks found in detailed_clusters_df for this cluster.\")\n",
    "        else:\n",
    "            print(\"\\n=== TOP STOCKS BY RAW SCORE PER SELECTED CLUSTER ===\")\n",
    "            print(\"Skipping - Detailed cluster information ('input_detailed_clusters_df') not found in the output dictionary.\")\n",
    "\n",
    "    else:\n",
    "        print(\"\\n=== SELECTED CLUSTERS ===\")\n",
    "        print(\"No clusters were selected based on the criteria.\")\n",
    "\n",
    "\n",
    "    print(f\"\\n=== FINAL SELECTED STOCKS (FILTERED & WEIGHTED) ===\")\n",
    "    if not selected_stocks.empty:\n",
    "        print(\"* Stocks actually selected based on Risk_Adj_Score (and optional thresholds) within each cluster.\")\n",
    "        print(\"* Position weights assigned based on Risk_Adj_Score within the final selected portfolio.\")\n",
    "\n",
    "        desired_cols = ['Cluster_ID', 'Ticker', 'Raw_Score', 'Risk_Adj_Score',\n",
    "                        'Volatility', 'Weight',\n",
    "                        'Cluster_Avg_Raw_Score', 'Cluster_Avg_Risk_Adj_Score']\n",
    "        available_cols = [col for col in desired_cols if col in selected_stocks.columns]\n",
    "        print(selected_stocks[available_cols].sort_values(['Cluster_ID', 'Risk_Adj_Score'],\n",
    "                                                        ascending=[True, False]).to_string(index=False))\n",
    "\n",
    "        print(\"\\n=== PORTFOLIO SUMMARY ===\")\n",
    "        print(f\"Total Stocks Selected: {len(selected_stocks)}\")\n",
    "        print(f\"Average Raw Score: {selected_stocks.get('Raw_Score', pd.Series(dtype=float)).mean():.2f}\")\n",
    "        print(f\"Average Risk-Adjusted Score: {selected_stocks.get('Risk_Adj_Score', pd.Series(dtype=float)).mean():.2f}\")\n",
    "        print(f\"Average Volatility: {selected_stocks.get('Volatility', pd.Series(dtype=float)).mean():.2f}\")\n",
    "        print(f\"Total Weight (should be close to 1.0): {selected_stocks.get('Weight', pd.Series(dtype=float)).sum():.4f}\")\n",
    "        print(\"\\nCluster Distribution:\")\n",
    "        print(selected_stocks['Cluster_ID'].value_counts().to_string())\n",
    "    else:\n",
    "        print(\"No stocks were selected after applying all filters and criteria.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools # Import the itertools module\n",
    "\n",
    "# --- Define Factor Ranges ---\n",
    "# Generate the factors using numpy.arange for float steps\n",
    "# Add a small epsilon to the end value to ensure the endpoint is included due to float precision\n",
    "raw_score_factors = np.arange(0.5, 1.2 + 0.01, 0.1)\n",
    "risk_adj_score_factors = np.arange(0.5, 1.2 + 0.01, 0.1)\n",
    "penalty_factors = np.arange(0, 0.4 + 0.01, 0.1) # New factor range\n",
    "\n",
    "print(\"--- Parameter Ranges ---\")\n",
    "print(f\"Raw Score Factors: {np.round(raw_score_factors,1)}\")\n",
    "print(f\"Risk Adj Score Factors: {np.round(risk_adj_score_factors,1)}\")\n",
    "print(f\"Penalty Factors: {np.round(penalty_factors,1)}\")\n",
    "\n",
    "\n",
    "# --- Generate All Combinations ---\n",
    "# Use itertools.product to create an iterator of all combinations\n",
    "parameter_combinations = list(itertools.product(raw_score_factors, risk_adj_score_factors, penalty_factors))\n",
    "total_combinations = len(parameter_combinations)\n",
    "print(f\"\\nTotal parameter combinations to iterate: {total_combinations}\")\n",
    "\n",
    "\n",
    "# --- Store results ---\n",
    "all_portfolios = {} # Dictionary to store portfolios by name\n",
    "\n",
    "# --- Fixed Parameters (that don't vary in this loop) ---\n",
    "select_top_n_clusters = 60\n",
    "max_selection_per_cluster = 2\n",
    "min_cluster_size = 3  # prevent extreme high risk adj scores\n",
    "# You might want to get the date dynamically\n",
    "portf_date_base = date_str # Example date, adjust as needed (Ensure date_str is defined before this)\n",
    "\n",
    "# --- Iteration Loop (Single Loop over Combinations) ---\n",
    "print(\"\\nStarting portfolio generation loop...\")\n",
    "for i, (raw_scale, risk_adj_scale, penalty) in enumerate(parameter_combinations):\n",
    "\n",
    "    # --- Calculate dynamic parameters based on current factors ---\n",
    "    # Round factors slightly to avoid potential floating point representation issues in calculations/names\n",
    "    raw_scale = round(raw_scale, 1)\n",
    "    risk_adj_scale = round(risk_adj_scale, 1)\n",
    "    penalty = round(penalty, 1) # Round the new penalty factor as well\n",
    "\n",
    "    min_raw_score = 2.0 * raw_scale\n",
    "    min_risk_adj_score = 100.0 * risk_adj_scale\n",
    "    # penalty_IntraCluster_Corr is now the 'penalty' variable from the combination\n",
    "\n",
    "    print(f\"\\nRunning combination {i+1}/{total_combinations}: \"\n",
    "          f\"raw_F={raw_scale:.1f}, riskAdj_F={risk_adj_scale:.1f}, penalty={penalty:.1f}\")\n",
    "    print(f\"Resulting thresholds: min_raw_score={min_raw_score:.2f}, min_risk_adj_score={min_risk_adj_score:.1f}\")\n",
    "\n",
    "    # --- Run the selection pipeline ---\n",
    "    try:\n",
    "        output = select_stocks_from_clusters(\n",
    "            cluster_stats_df=cluster_stats_df,      # Ensure this DataFrame is loaded/defined\n",
    "            detailed_clusters_df=detailed_clusters_df, # Ensure this DataFrame is loaded/defined\n",
    "            select_top_n_clusters=select_top_n_clusters,\n",
    "            max_selection_per_cluster=max_selection_per_cluster,\n",
    "            min_cluster_size=min_cluster_size,\n",
    "            penalty_IntraCluster_Corr=penalty, # Use the penalty from the current combination\n",
    "            min_raw_score=min_raw_score,\n",
    "            min_risk_adj_score=min_risk_adj_score,\n",
    "            date_str=portf_date_base # Pass the date\n",
    "        )\n",
    "\n",
    "        # --- Process and Store Results ---\n",
    "        portf_date = output['parameters']['date_str']\n",
    "        portf_raw_score_val = output['parameters']['min_raw_score'] # Use the value returned by the function\n",
    "        portf_risk_adj_score_val = output['parameters']['min_risk_adj_score'] # Use the value returned by the function\n",
    "        portf_penalty_val = output['parameters']['penalty_IntraCluster_Corr'] # Get penalty used\n",
    "        _selected_stocks = output['selected_stocks']\n",
    "\n",
    "        if _selected_stocks is not None and not _selected_stocks.empty:\n",
    "            portf_selected_stocks = _selected_stocks.set_index('Ticker')[['Weight']] # Select only Weight column after setting index\n",
    "\n",
    "            # Format the name using the factors - now includes penalty\n",
    "            portf_name = f'{portf_date}_portf_rawF_{raw_scale:.1f}_riskAdjF_{risk_adj_scale:.1f}_pen_{penalty:.1f}'\n",
    "            # Alternative using resulting thresholds and penalty:\n",
    "            # portf_name = f'{portf_date}_portf_raw_{portf_raw_score_val:.2f}_riskadj_{portf_risk_adj_score_val:.1f}_pen_{portf_penalty_val:.1f}'\n",
    "\n",
    "            print(f'Generated Portfolio:')\n",
    "            print(f'  Name: {portf_name}')\n",
    "            print(f'  Number of stocks: {len(portf_selected_stocks)}')\n",
    "            # print(f'port_selected_stocks:\\n{portf_selected_stocks}') # Can be verbose\n",
    "\n",
    "            # Store the results - using the portfolio name as the key\n",
    "            all_portfolios[portf_name] = {\n",
    "                'parameters': {\n",
    "                    'raw_score_scale_factor': raw_scale,\n",
    "                    'risk_adj_score_scale_factor': risk_adj_scale,\n",
    "                    'penalty_IntraCluster_Corr': penalty, # Store the penalty factor used\n",
    "                    'min_raw_score': portf_raw_score_val,\n",
    "                    'min_risk_adj_score': portf_risk_adj_score_val,\n",
    "                    'select_top_n_clusters': select_top_n_clusters,\n",
    "                    'max_selection_per_cluster': max_selection_per_cluster,\n",
    "                    'min_cluster_size': min_cluster_size,\n",
    "                    'date': portf_date\n",
    "                },\n",
    "                'selected_stocks': portf_selected_stocks\n",
    "            }\n",
    "        else:\n",
    "             print(f\"No stocks selected for raw_F={raw_scale:.1f}, riskAdj_F={risk_adj_scale:.1f}, penalty={penalty:.1f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR processing combination raw_F={raw_scale:.1f}, riskAdj_F={risk_adj_scale:.1f}, penalty={penalty:.1f}: {e}\")\n",
    "        # Decide if you want to continue or stop on error - currently continues\n",
    "\n",
    "print(f\"\\n--- Portfolio Generation Complete ---\")\n",
    "print(f\"Generated {len(all_portfolios)} portfolios out of {total_combinations} combinations attempted.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_portfolios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AI Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys # Needed for sys.exit\n",
    "import traceback # Import traceback for detailed error printing\n",
    "import os\n",
    "\n",
    "# --- Parameters ---\n",
    "ADJ_CLOSE_COL = 'Adj Close'\n",
    "POTENTIAL_TICKER_COLS = ['Symbol', 'Ticker']\n",
    "POTENTIAL_DATE_COL = 'Date'\n",
    "OUTPUT_RETURNS_CSV = 'portfolio_returns.csv'\n",
    "OUTPUT_SUMMARY_CSV = 'portfolio_factor_performance.csv'\n",
    "\n",
    "print(\"--- Portfolio Performance Calculation Script ---\")\n",
    "\n",
    "# =============================================================================\n",
    "# 1. Data and Index Preparation\n",
    "# =============================================================================\n",
    "print(\"\\nChecking and preparing df_OHLCV index structure...\")\n",
    "ticker_level_name = None\n",
    "date_level_name = None\n",
    "try:\n",
    "    # Check if df_OHLCV already has a suitable MultiIndex\n",
    "    if isinstance(df_OHLCV.index, pd.MultiIndex) and len(df_OHLCV.index.levels) >= 2:\n",
    "        actual_names = list(df_OHLCV.index.names)\n",
    "        print(f\"Detected MultiIndex with names: {actual_names}\")\n",
    "        ticker_level_name = actual_names[0] if actual_names[0] is not None else 0\n",
    "        date_level_name = actual_names[1] if actual_names[1] is not None else 1\n",
    "        if isinstance(ticker_level_name, int) or isinstance(date_level_name, int):\n",
    "             print(f\"Warning: Using positional index levels ({ticker_level_name}, {date_level_name}) as names were missing.\")\n",
    "    else:\n",
    "        print(\"Index is not a MultiIndex or has too few levels. Checking columns...\")\n",
    "        ticker_col_to_use = None\n",
    "        for col in POTENTIAL_TICKER_COLS:\n",
    "            if col in df_OHLCV.columns:\n",
    "                ticker_col_to_use = col\n",
    "                break\n",
    "        date_col_to_use = POTENTIAL_DATE_COL if POTENTIAL_DATE_COL in df_OHLCV.columns else None\n",
    "        if ticker_col_to_use and date_col_to_use:\n",
    "            required_cols = [ticker_col_to_use, date_col_to_use]\n",
    "            print(f\"Attempting to set index using columns: {required_cols}...\")\n",
    "            # Ensure df_OHLCV exists and is a DataFrame before modification\n",
    "            if 'df_OHLCV' in locals() and isinstance(df_OHLCV, pd.DataFrame):\n",
    "                 df_OHLCV = df_OHLCV.set_index(required_cols)\n",
    "                 ticker_level_name = required_cols[0]\n",
    "                 date_level_name = required_cols[1]\n",
    "                 print(f\"MultiIndex set successfully.\")\n",
    "            else:\n",
    "                 print(\"ERROR: df_OHLCV not defined or not a DataFrame before setting index.\")\n",
    "                 sys.exit(\"Exiting due to data structure issue.\")\n",
    "\n",
    "        else:\n",
    "            missing = []\n",
    "            if not ticker_col_to_use: missing.extend(POTENTIAL_TICKER_COLS)\n",
    "            if not date_col_to_use: missing.append(POTENTIAL_DATE_COL)\n",
    "            print(f\"ERROR: Cannot set index. Required columns for Ticker ({POTENTIAL_TICKER_COLS}) or Date ({POTENTIAL_DATE_COL}) not found.\")\n",
    "            if 'df_OHLCV' in locals() and isinstance(df_OHLCV, pd.DataFrame):\n",
    "                 print(\"df_OHLCV columns:\", df_OHLCV.columns)\n",
    "            else:\n",
    "                 print(\"df_OHLCV is not defined or not a DataFrame.\")\n",
    "            sys.exit(\"Exiting due to incorrect df_OHLCV structure.\")\n",
    "\n",
    "    print(f\"Using '{ticker_level_name}' for ticker level and '{date_level_name}' for date level.\")\n",
    "\n",
    "    if 'df_OHLCV' not in locals() or not isinstance(df_OHLCV, pd.DataFrame):\n",
    "        print(\"ERROR: df_OHLCV is not defined or not a DataFrame.\")\n",
    "        sys.exit(\"Exiting due to missing data.\")\n",
    "\n",
    "    if ADJ_CLOSE_COL not in df_OHLCV.columns:\n",
    "        print(f\"ERROR: Required price column '{ADJ_CLOSE_COL}' not found in df_OHLCV columns: {df_OHLCV.columns}\")\n",
    "        sys.exit(\"Exiting due to missing price column.\")\n",
    "\n",
    "    # Convert date level to datetime if it's not already\n",
    "    date_level_values = df_OHLCV.index.get_level_values(date_level_name)\n",
    "    if not pd.api.types.is_datetime64_any_dtype(date_level_values):\n",
    "        print(f\"Converting date level '{date_level_name}' to datetime objects...\")\n",
    "        # Create a new index with the converted dates\n",
    "        new_levels = list(df_OHLCV.index.levels)\n",
    "        date_level_idx = df_OHLCV.index.names.index(date_level_name) # Find position of date level\n",
    "        new_levels[date_level_idx] = pd.to_datetime(date_level_values.unique()) # Convert unique dates\n",
    "        # Need to reconstruct the index carefully, preserving levels and codes\n",
    "        new_codes = list(df_OHLCV.index.codes)\n",
    "        new_index = pd.MultiIndex(levels=new_levels, codes=new_codes, names=df_OHLCV.index.names)\n",
    "        df_OHLCV.index = new_index\n",
    "        print(\"Date level converted.\")\n",
    "\n",
    "    if not df_OHLCV.index.is_monotonic_increasing:\n",
    "         print(\"Sorting df_OHLCV index...\")\n",
    "         df_OHLCV = df_OHLCV.sort_index()\n",
    "         print(\"Index sorted.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: An unexpected error occurred during index check/setup: {e}\")\n",
    "    traceback.print_exc()\n",
    "    sys.exit(\"Exiting due to index setup error.\")\n",
    "\n",
    "# --- ADD THIS: Get all unique tickers from df_OHLCV ---\n",
    "all_known_tickers_set = set()\n",
    "if 'df_OHLCV' in locals() and isinstance(df_OHLCV, pd.DataFrame) and ticker_level_name is not None:\n",
    "    try:\n",
    "        all_known_tickers = df_OHLCV.index.get_level_values(ticker_level_name).unique().tolist()\n",
    "        all_known_tickers_set = set(all_known_tickers)\n",
    "        print(f\"\\nFound {len(all_known_tickers_set):,} unique tickers in df_OHLCV data.\")\n",
    "        if not all_known_tickers_set:\n",
    "             print(\"Warning: No tickers found in df_OHLCV index. Price fetching will likely fail.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Could not extract unique tickers from df_OHLCV index level '{ticker_level_name}': {e}\")\n",
    "        sys.exit(\"Exiting due to error extracting ticker list.\")\n",
    "else:\n",
    "     print(\"Warning: df_OHLCV not available or ticker level name not set during initialization. Cannot pre-check ticker existence.\")\n",
    "# --- END OF ADDITION ---\n",
    "\n",
    "try:\n",
    "    all_dates_in_data = df_OHLCV.index.get_level_values(date_level_name).unique()\n",
    "    trading_dates = pd.DatetimeIndex(pd.to_datetime(all_dates_in_data)).sort_values()\n",
    "    if not trading_dates.empty:\n",
    "         print(f\"Trading dates extracted using level '{date_level_name}'. Found {len(trading_dates)} unique dates (e.g., {trading_dates[0].date()} to {trading_dates[-1].date()}).\")\n",
    "    else:\n",
    "         print(f\"ERROR: No dates found in the index level '{date_level_name}'. Cannot proceed.\")\n",
    "         sys.exit(\"Exiting - Cannot determine trading dates.\")\n",
    "except (KeyError, IndexError) as e:\n",
    "    print(f\"ERROR: Could not find or access level '{date_level_name}' to extract trading dates: {e}\")\n",
    "    sys.exit(\"Exiting - Cannot determine trading dates.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: An unexpected error occurred extracting trading dates: {e}\")\n",
    "    traceback.print_exc()\n",
    "    sys.exit(\"Exiting - Cannot determine trading dates.\")\n",
    "\n",
    "# =============================================================================\n",
    "# 2. Helper Function for Trading Dates (NO CHANGE NEEDED HERE)\n",
    "# =============================================================================\n",
    "# ... (get_next_trading_date function remains the same) ...\n",
    "def get_next_trading_date(current_date, sorted_trading_dates):\n",
    "    if not isinstance(sorted_trading_dates, pd.DatetimeIndex) or not sorted_trading_dates.is_monotonic_increasing:\n",
    "        try:\n",
    "            sorted_trading_dates = pd.DatetimeIndex(sorted_trading_dates).sort_values()\n",
    "        except Exception: return None\n",
    "    if sorted_trading_dates.empty: return None\n",
    "    current_date = pd.Timestamp(current_date)\n",
    "    try:\n",
    "        loc = sorted_trading_dates.searchsorted(current_date, side='right')\n",
    "        return sorted_trading_dates[loc] if loc < len(sorted_trading_dates) else None\n",
    "    except Exception: return None\n",
    "\n",
    "# =============================================================================\n",
    "# 3. Portfolio Return Calculation Loop\n",
    "# =============================================================================\n",
    "portfolio_returns_data = []\n",
    "print(\"\\nCalculating portfolio returns for the current run...\")\n",
    "index_mismatch_warning_shown = False\n",
    "\n",
    "if 'all_portfolios' not in locals() or not isinstance(all_portfolios, dict):\n",
    "    print(\"ERROR: 'all_portfolios' dictionary is not defined.\")\n",
    "    all_portfolios = {}\n",
    "\n",
    "if not all_portfolios:\n",
    "    print(\"Warning: 'all_portfolios' dictionary is empty. No returns to calculate.\")\n",
    "else:\n",
    "    processed_count = 0\n",
    "    skipped_count = 0\n",
    "    error_count = 0\n",
    "    for portf_name, portf_data in list(all_portfolios.items()):\n",
    "        try:\n",
    "            # --- 3.1 Extract Parameters and Holdings ---\n",
    "            params = portf_data.get('parameters')\n",
    "            selected_stocks_df = portf_data.get('selected_stocks') # DataFrame with Ticker/Symbol as index, 'Weight' column\n",
    "\n",
    "            # Basic validation\n",
    "            if not isinstance(params, dict) or not isinstance(selected_stocks_df, pd.DataFrame) or 'Weight' not in selected_stocks_df.columns:\n",
    "                print(f\"Warning: Skipping {portf_name} - Invalid structure or missing 'Weight' column.\")\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "            # Ensure index has a name before comparison\n",
    "            if selected_stocks_df.index.name is None:\n",
    "                # Try to guess based on potential names or default to 'Ticker'/'Symbol'\n",
    "                potential_index_names = [name for name in POTENTIAL_TICKER_COLS if name in selected_stocks_df.index.name or selected_stocks_df.index.name is None]\n",
    "                if potential_index_names:\n",
    "                    selected_stocks_df.index.name = potential_index_names[0] # Use the first match\n",
    "                else:\n",
    "                    selected_stocks_df.index.name = 'Ticker' # Default if none match\n",
    "                # print(f\"Debug: Assigned index name '{selected_stocks_df.index.name}' to selected_stocks_df for {portf_name}\")\n",
    "\n",
    "\n",
    "            portf_date_str = params.get('date')\n",
    "            raw_factor = params.get('raw_score_scale_factor')\n",
    "            risk_adj_factor = params.get('risk_adj_score_scale_factor')\n",
    "            penalty_factor = params.get('penalty_IntraCluster_Corr')\n",
    "\n",
    "            if portf_date_str is None or raw_factor is None or risk_adj_factor is None or penalty_factor is None:\n",
    "                print(f\"Warning: Skipping {portf_name} - Missing essential parameter.\")\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "\n",
    "            portf_date = pd.Timestamp(portf_date_str)\n",
    "\n",
    "            if selected_stocks_df.empty:\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "\n",
    "            # --- 3.2 Ensure Portfolio Index Matches Data Index ---\n",
    "            if selected_stocks_df.index.name != ticker_level_name:\n",
    "                if not index_mismatch_warning_shown:\n",
    "                    print(f\"Warning: Portfolio index name ('{selected_stocks_df.index.name}') mismatches data index ('{ticker_level_name}'). Renaming will occur.\")\n",
    "                    index_mismatch_warning_shown = True\n",
    "                try:\n",
    "                    selected_stocks_df = selected_stocks_df.copy() # Avoid SettingWithCopyWarning\n",
    "                    selected_stocks_df.index.name = ticker_level_name\n",
    "                except Exception as rename_e:\n",
    "                    print(f\"Error: Failed to rename portfolio index for {portf_name}: {rename_e}. Skipping.\")\n",
    "                    skipped_count += 1\n",
    "                    continue\n",
    "\n",
    "            original_portfolio_tickers = selected_stocks_df.index.tolist()\n",
    "            if not original_portfolio_tickers: # Double check after potential renaming/copying\n",
    "                 print(f\"Info: Skipping {portf_name} - Portfolio has no tickers after index handling.\")\n",
    "                 skipped_count += 1\n",
    "                 continue\n",
    "\n",
    "            # --- ADD THIS CHECK: Verify tickers exist in df_OHLCV ---\n",
    "            tickers_in_portfolio_set = set(original_portfolio_tickers)\n",
    "            missing_from_ohlcv = tickers_in_portfolio_set - all_known_tickers_set # Find tickers in portfolio but not in OHLCV data\n",
    "\n",
    "            if missing_from_ohlcv:\n",
    "                print(f\"Warning for {portf_name}: The following tickers selected for the portfolio are completely missing from the historical price data (df_OHLCV) and will be dropped: {sorted(list(missing_from_ohlcv))}\")\n",
    "                # Filter the list of tickers to proceed with\n",
    "                tickers_to_process = [t for t in original_portfolio_tickers if t in all_known_tickers_set]\n",
    "\n",
    "                if not tickers_to_process:\n",
    "                    print(f\"Info: Skipping {portf_name} - No tickers remaining after removing those completely missing from price data.\")\n",
    "                    skipped_count += 1\n",
    "                    continue # Skip to the next portfolio\n",
    "\n",
    "                # IMPORTANT: Filter the selected_stocks_df as well to keep weights consistent\n",
    "                selected_stocks_df = selected_stocks_df.loc[tickers_to_process]\n",
    "                original_portfolio_tickers = tickers_to_process # Update the list for downstream use\n",
    "            # --- END OF ADDED CHECK ---\n",
    "\n",
    "\n",
    "            # --- 3.3 Determine Buy and Sell Dates ---\n",
    "            buy_date = get_next_trading_date(portf_date, trading_dates)\n",
    "            if buy_date is None:\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "            sell_date = get_next_trading_date(buy_date, trading_dates)\n",
    "            if sell_date is None:\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "            required_dates = [buy_date, sell_date]\n",
    "\n",
    "            # --- 3.4 Fetch Price Data & Filter Available Tickers ---\n",
    "            # Use the potentially filtered 'original_portfolio_tickers'\n",
    "            idx = pd.IndexSlice\n",
    "            try:\n",
    "                 # Attempt to slice; use reindex().dropna() for robustness if direct slice fails often\n",
    "                 potential_data = df_OHLCV.loc[idx[original_portfolio_tickers, required_dates], :]\n",
    "            except KeyError:\n",
    "                 # Handle cases where some ticker/date combinations might be missing even if ticker exists overall\n",
    "                 # Use reindex for safer access\n",
    "                 multi_idx = pd.MultiIndex.from_product([original_portfolio_tickers, required_dates], names=[ticker_level_name, date_level_name])\n",
    "                 potential_data = df_OHLCV.reindex(multi_idx) # This will have NaNs where data is missing\n",
    "\n",
    "            # Identify tickers that have *valid* data for BOTH required dates\n",
    "            # Group by ticker and check count of non-NA Adj Close prices for the required dates\n",
    "            ticker_date_counts = potential_data.dropna(subset=[ADJ_CLOSE_COL])\\\n",
    "                                               .index.get_level_values(ticker_level_name)\\\n",
    "                                               .value_counts()\n",
    "\n",
    "            available_tickers = ticker_date_counts[ticker_date_counts == len(required_dates)].index.tolist()\n",
    "\n",
    "            if not available_tickers:\n",
    "                # print(f\"Info: Skipping {portf_name} - No stocks found with data for BOTH Buy({buy_date.date()}) and Sell({sell_date.date()}) dates.\")\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "\n",
    "            # --- 3.5 Process Prices for Available Tickers ---\n",
    "            try:\n",
    "                # Select 'Adj Close' for the available tickers and dates from potential_data\n",
    "                idx_slice = pd.IndexSlice[available_tickers, required_dates]\n",
    "                prices = potential_data.loc[idx_slice, ADJ_CLOSE_COL]\n",
    "\n",
    "                prices_unstacked = prices.unstack(level=date_level_name)\n",
    "                prices_unstacked = prices_unstacked.rename(columns={buy_date: 'Buy Price', sell_date: 'Sell Price'})\n",
    "                prices_unstacked = prices_unstacked.dropna() # Drop rows with NaN in Buy or Sell Price\n",
    "\n",
    "                if prices_unstacked.empty:\n",
    "                    skipped_count += 1\n",
    "                    continue\n",
    "\n",
    "                final_available_tickers = prices_unstacked.index.tolist()\n",
    "\n",
    "            except KeyError as e:\n",
    "                 print(f\"Error: KeyError during price processing for {portf_name}: {e}. Check index slicing or column '{ADJ_CLOSE_COL}'. Skipping.\")\n",
    "                 skipped_count += 1\n",
    "                 continue\n",
    "            except Exception as e:\n",
    "                 print(f\"Error: Unexpected error during price processing for {portf_name}: {e}. Skipping.\")\n",
    "                 traceback.print_exc()\n",
    "                 skipped_count += 1\n",
    "                 continue\n",
    "\n",
    "            # --- 3.6 Calculate Portfolio Return ---\n",
    "            # Align weights using the *original* selected_stocks_df (which was filtered earlier if tickers were missing from OHLCV)\n",
    "            # but index it with the tickers that *actually have price data* for the calculation period.\n",
    "            aligned_weights = selected_stocks_df.loc[final_available_tickers, 'Weight']\n",
    "\n",
    "            # ---> Weight Handling happens HERE <---\n",
    "            weight_sum = aligned_weights.sum()\n",
    "            if weight_sum <= 1e-9:\n",
    "                 skipped_count += 1\n",
    "                 continue\n",
    "            normalized_weights = aligned_weights / weight_sum\n",
    "            # ---> End of Weight Handling <---\n",
    "\n",
    "            buy_prices = prices_unstacked.loc[final_available_tickers, 'Buy Price']\n",
    "            sell_prices = prices_unstacked.loc[final_available_tickers, 'Sell Price']\n",
    "\n",
    "            # ... (rest of calculation: handling zero prices, calculating return) ...\n",
    "            zero_price_mask = (buy_prices.abs() < 1e-9)\n",
    "            if zero_price_mask.any():\n",
    "                # ... (handle zero prices and re-normalize weights if needed) ...\n",
    "                valid_price_mask = ~zero_price_mask\n",
    "                buy_prices = buy_prices[valid_price_mask]\n",
    "                sell_prices = sell_prices[valid_price_mask]\n",
    "                normalized_weights = normalized_weights.loc[valid_price_mask] # Align weights again\n",
    "                if buy_prices.empty:\n",
    "                    skipped_count += 1\n",
    "                    continue\n",
    "                # Re-normalize weights after removing zero-price stocks\n",
    "                weight_sum = normalized_weights.sum()\n",
    "                if weight_sum <= 1e-9:\n",
    "                    skipped_count += 1\n",
    "                    continue\n",
    "                normalized_weights = normalized_weights / weight_sum # Renormalize again\n",
    "\n",
    "            if buy_prices.isnull().any() or sell_prices.isnull().any() or normalized_weights.isnull().any() or \\\n",
    "               np.isinf(buy_prices).any() or np.isinf(sell_prices).any() or np.isinf(normalized_weights).any():\n",
    "                print(f\"Warning: Skipping {portf_name} - Found NaN or Inf in final arrays before calculation.\")\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "\n",
    "            individual_returns = (sell_prices / (buy_prices + 1e-12)) - 1\n",
    "            portfolio_return = (individual_returns * normalized_weights).sum()\n",
    "\n",
    "            # --- 3.7 Store Results ---\n",
    "            portfolio_returns_data.append({\n",
    "                'portfolio_name': portf_name,\n",
    "                'generation_date': portf_date,\n",
    "                'buy_date': buy_date,\n",
    "                'sell_date': sell_date,\n",
    "                'raw_factor': raw_factor,\n",
    "                'risk_adj_factor': risk_adj_factor,\n",
    "                'penalty_factor': penalty_factor,\n",
    "                'portfolio_return': portfolio_return,\n",
    "                # Use len(selected_stocks_df.index) *before* aligning with final_available_tickers\n",
    "                # if you want to capture the count *after* removing OHLCV-missing tickers but *before* date filtering.\n",
    "                # Or keep the absolute original count if needed. Let's store both potentially:\n",
    "                'num_stocks_selected': len(tickers_in_portfolio_set), # Count from original selection\n",
    "                'num_stocks_in_ohlcv': len(original_portfolio_tickers), # Count after removing those missing from OHLCV\n",
    "                'num_stocks_calc': len(final_available_tickers) # Count actually used in calculation (passed all filters)\n",
    "            })\n",
    "            processed_count += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"FATAL Error processing portfolio {portf_name}: {e}\")\n",
    "            traceback.print_exc()\n",
    "            error_count += 1\n",
    "            # Decide whether to continue or stop\n",
    "            # continue\n",
    "\n",
    "    # ... (print summary counts: processed, skipped, error) ...\n",
    "    print(f\"\\nCurrent run portfolio return calculation finished.\")\n",
    "    print(f\"Successfully processed: {processed_count}\")\n",
    "    print(f\"Skipped (missing data/criteria/tickers): {skipped_count}\")\n",
    "    print(f\"Errors during processing: {error_count}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 4. Load Historical Data, Combine, and Analyze\n",
    "# =============================================================================\n",
    "print(\"\\n--- Loading Historical Data and Performing Combined Analysis ---\")\n",
    "\n",
    "all_returns_df = pd.DataFrame() # Initialize an empty DataFrame\n",
    "\n",
    "# --- Load existing historical data ---\n",
    "if os.path.exists(OUTPUT_RETURNS_CSV):\n",
    "    try:\n",
    "        print(f\"Loading historical data from {OUTPUT_RETURNS_CSV}...\")\n",
    "        historical_returns_df = pd.read_csv(OUTPUT_RETURNS_CSV, parse_dates=['generation_date', 'buy_date', 'sell_date'])\n",
    "        # Optional: Add checks for expected columns\n",
    "        expected_cols = ['generation_date', 'buy_date', 'sell_date', 'raw_factor', 'risk_adj_factor', 'penalty_factor', 'portfolio_return']\n",
    "        if all(col in historical_returns_df.columns for col in expected_cols):\n",
    "            all_returns_df = historical_returns_df\n",
    "            print(f\"Loaded {len(all_returns_df)} historical records.\")\n",
    "        else:\n",
    "            print(f\"Warning: Historical data file {OUTPUT_RETURNS_CSV} is missing expected columns. Ignoring historical data.\")\n",
    "            # Decide if you want to rename/backup the bad file\n",
    "            # os.rename(OUTPUT_RETURNS_CSV, OUTPUT_RETURNS_CSV + \".bad_format\")\n",
    "\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(f\"Historical data file {OUTPUT_RETURNS_CSV} is empty. Starting fresh.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading historical data from {OUTPUT_RETURNS_CSV}: {e}\")\n",
    "        print(\"Warning: Proceeding without historical data.\")\n",
    "else:\n",
    "    print(f\"Historical data file {OUTPUT_RETURNS_CSV} not found. Starting fresh.\")\n",
    "\n",
    "# --- Combine with current run's data ---\n",
    "if portfolio_returns_data:\n",
    "    current_returns_df = pd.DataFrame(portfolio_returns_data)\n",
    "    print(f\"Combining {len(current_returns_df)} new records with historical data.\")\n",
    "\n",
    "    # Ensure date columns are datetime objects in the new data\n",
    "    for col in ['generation_date', 'buy_date', 'sell_date']:\n",
    "         if col in current_returns_df.columns:\n",
    "              current_returns_df[col] = pd.to_datetime(current_returns_df[col])\n",
    "\n",
    "    # Concatenate old and new data\n",
    "    if not all_returns_df.empty:\n",
    "        all_returns_df = pd.concat([all_returns_df, current_returns_df], ignore_index=True)\n",
    "    else:\n",
    "        all_returns_df = current_returns_df\n",
    "\n",
    "    # Optional: Remove potential duplicates based on key identifiers if runs might overlap accidentally\n",
    "    key_cols = ['generation_date', 'buy_date', 'sell_date', 'raw_factor', 'risk_adj_factor', 'penalty_factor']\n",
    "    initial_len = len(all_returns_df)\n",
    "    all_returns_df = all_returns_df.drop_duplicates(subset=key_cols, keep='last')\n",
    "    if len(all_returns_df) < initial_len:\n",
    "        print(f\"Removed {initial_len - len(all_returns_df)} duplicate records based on key columns.\")\n",
    "\n",
    "else:\n",
    "    print(\"No new portfolio returns were calculated in this run.\")\n",
    "\n",
    "# --- Perform analysis ONLY if there's data ---\n",
    "if not all_returns_df.empty:\n",
    "    # Ensure correct data types before grouping (especially factors)\n",
    "    try:\n",
    "        all_returns_df['raw_factor'] = pd.to_numeric(all_returns_df['raw_factor'])\n",
    "        all_returns_df['risk_adj_factor'] = pd.to_numeric(all_returns_df['risk_adj_factor'])\n",
    "        all_returns_df['penalty_factor'] = pd.to_numeric(all_returns_df['penalty_factor'])\n",
    "        all_returns_df['portfolio_return'] = pd.to_numeric(all_returns_df['portfolio_return'])\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting factor/return columns to numeric: {e}\")\n",
    "        print(\"Analysis might be affected.\")\n",
    "\n",
    "\n",
    "    # Sort for easier viewing (optional, but good practice)\n",
    "    grouping_factors = ['raw_factor', 'risk_adj_factor', 'penalty_factor']\n",
    "    all_returns_df = all_returns_df.sort_values(by=['generation_date'] + grouping_factors)\n",
    "\n",
    "    print(\"\\n--- Combined Portfolio Return Analysis (All Historical Data) ---\")\n",
    "    print(f\"Total return instances analyzed: {len(all_returns_df)}\")\n",
    "    # print(\"Sample of combined data:\") # Optional: Print head if needed\n",
    "    # print(all_returns_df.head())\n",
    "\n",
    "    # --- Group by factor combinations to analyze performance ---\n",
    "    print(\"\\n--- Performance Analysis by Factor Combination (Based on All Data) ---\")\n",
    "\n",
    "    # Check if grouping factors exist\n",
    "    if not all(f in all_returns_df.columns for f in grouping_factors):\n",
    "         print(f\"Error: One or more grouping factors {grouping_factors} not found in the combined data columns: {all_returns_df.columns}\")\n",
    "         # Handle error - perhaps exit or skip analysis\n",
    "         performance_summary = pd.DataFrame() # Create empty df to avoid error later\n",
    "    else:\n",
    "        # Group by all three factors and aggregate portfolio returns\n",
    "        performance_summary = all_returns_df.groupby(grouping_factors)['portfolio_return'].agg(\n",
    "            mean='mean',\n",
    "            median='median',\n",
    "            std=lambda x: x.std(ddof=0) if pd.notna(x).sum() > 1 else np.nan,\n",
    "            count=lambda x: pd.notna(x).sum(), # Count only valid returns for this combo\n",
    "            sharpe=lambda x: (x.mean() / (x.std() + 1e-9)) * np.sqrt(252) if pd.notna(x).sum() > 1 and x.std() > 1e-9 else np.nan # Example: Annualized Sharpe (assuming daily returns, zero risk-free)\n",
    "        ).reset_index() # Reset index to make factors columns again\n",
    "\n",
    "        # Calculate additional metrics if desired, like total return, win rate etc.\n",
    "        # Example: Add win rate (percentage of positive return days)\n",
    "        win_rate = all_returns_df[all_returns_df['portfolio_return'] > 0].groupby(grouping_factors).size() / \\\n",
    "                   all_returns_df.groupby(grouping_factors).size()\n",
    "        performance_summary = pd.merge(performance_summary, win_rate.rename('win_rate').reset_index(), on=grouping_factors, how='left')\n",
    "        performance_summary['win_rate'] = performance_summary['win_rate'].fillna(0) # Fill NaN if no trades for a combo\n",
    "\n",
    "        # Sort by a chosen metric to find the 'best' combinations\n",
    "        # Sorting by Sharpe ratio is common, or mean return, or a combination\n",
    "        performance_summary = performance_summary.sort_values(by='mean', ascending=False) # Or sort by 'sharpe'\n",
    "        print(\"\\nPerformance per Factor Combination (sorted by mean return):\")\n",
    "        with pd.option_context('display.float_format', '{:.4f}'.format, 'display.max_rows', 200):\n",
    "            print(performance_summary)\n",
    "\n",
    "        # --- Analyze the number of stocks used ---\n",
    "        stock_count_summary = all_returns_df.groupby(grouping_factors)['num_stocks_calc'].agg(['mean', 'min', 'max'])\n",
    "        print(\"\\nAverage/Min/Max Stocks Used in Calculation per Factor Combination (All Data):\")\n",
    "        with pd.option_context('display.float_format', '{:.2f}'.format):\n",
    "            print(stock_count_summary.sort_values(by='mean', ascending=False))\n",
    "\n",
    "else:\n",
    "    print(\"\\nNo portfolio returns available (neither historical nor current) to analyze.\")\n",
    "    performance_summary = pd.DataFrame() # Ensure it exists as empty if no data\n",
    "\n",
    "# =============================================================================\n",
    "# 5. Save Results (Append Returns, Overwrite Summary)\n",
    "# =============================================================================\n",
    "# --- Save the individual returns (APPEND mode) ---\n",
    "if portfolio_returns_data: # Only append if there's new data from this run\n",
    "    current_to_save_df = pd.DataFrame(portfolio_returns_data) # Use the list directly from this run\n",
    "    try:\n",
    "        # Check if file exists to determine if header is needed\n",
    "        write_header = not os.path.exists(OUTPUT_RETURNS_CSV)\n",
    "        # Append new data\n",
    "        current_to_save_df.to_csv(OUTPUT_RETURNS_CSV, mode='a', header=write_header, index=False)\n",
    "        print(f\"\\nAppended {len(current_to_save_df)} new return records to {OUTPUT_RETURNS_CSV}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError appending returns to {OUTPUT_RETURNS_CSV}: {e}\")\n",
    "elif not os.path.exists(OUTPUT_RETURNS_CSV):\n",
    "     # If no new data AND the file doesn't exist, create an empty file with header\n",
    "     try:\n",
    "         pd.DataFrame(columns=portfolio_returns_data[0].keys() if portfolio_returns_data else [ # Get columns if possible\n",
    "            'portfolio_name', 'generation_date', 'buy_date', 'sell_date',\n",
    "            'raw_factor', 'risk_adj_factor', 'penalty_factor', 'portfolio_return',\n",
    "            'num_stocks_initial', 'num_stocks_calc']).to_csv(OUTPUT_RETURNS_CSV, index=False)\n",
    "         print(f\"\\nCreated empty returns file with header: {OUTPUT_RETURNS_CSV}\")\n",
    "     except Exception as e:\n",
    "         print(f\"\\nError creating empty returns file {OUTPUT_RETURNS_CSV}: {e}\")\n",
    "\n",
    "# --- Save the performance summary (OVERWRITE mode) ---\n",
    "# This summary is based on ALL data (historical + current), so we overwrite it each time\n",
    "if 'performance_summary' in locals() and not performance_summary.empty:\n",
    "    try:\n",
    "        # No need to reset index if already done after groupby\n",
    "        performance_summary.to_csv(OUTPUT_SUMMARY_CSV, index=False)\n",
    "        print(f\"Updated performance summary saved to {OUTPUT_SUMMARY_CSV}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError saving performance summary to {OUTPUT_SUMMARY_CSV}: {e}\")\n",
    "elif not all_returns_df.empty:\n",
    "     print(f\"Warning: Performance summary was calculated but is empty. Not saving {OUTPUT_SUMMARY_CSV}.\")\n",
    "else:\n",
    "    print(f\"No data to generate performance summary. {OUTPUT_SUMMARY_CSV} not saved or updated.\")\n",
    "\n",
    "print(\"\\n--- Script Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import sys # Needed for sys.exit\n",
    "# import traceback # Import traceback for detailed error printing\n",
    "# import os # <--- IMPORT OS MODULE\n",
    "\n",
    "# # --- Parameters ---\n",
    "# # Column name for adjusted close price in df_OHLCV\n",
    "# ADJ_CLOSE_COL = 'Adj Close'\n",
    "# # Potential column names for ticker/symbol if not in index initially\n",
    "# POTENTIAL_TICKER_COLS = ['Symbol', 'Ticker']\n",
    "# # Potential column name for date if not in index initially\n",
    "# POTENTIAL_DATE_COL = 'Date'\n",
    "# # --- Output Files ---\n",
    "# OUTPUT_RETURNS_CSV = 'portfolio_returns.csv'\n",
    "# OUTPUT_SUMMARY_CSV = 'portfolio_factor_performance.csv'\n",
    "\n",
    "\n",
    "# print(\"--- Portfolio Performance Calculation Script ---\")\n",
    "\n",
    "# # =============================================================================\n",
    "# # 1. Data and Index Preparation (NO CHANGES NEEDED HERE)\n",
    "# # =============================================================================\n",
    "# print(\"\\nChecking and preparing df_OHLCV index structure...\")\n",
    "# # ... (Keep your existing Section 1 code as is) ...\n",
    "# ticker_level_name = None\n",
    "# date_level_name = None\n",
    "# try:\n",
    "#     # Check if df_OHLCV already has a suitable MultiIndex\n",
    "#     if isinstance(df_OHLCV.index, pd.MultiIndex) and len(df_OHLCV.index.levels) >= 2:\n",
    "#         actual_names = list(df_OHLCV.index.names)\n",
    "#         print(f\"Detected MultiIndex with names: {actual_names}\")\n",
    "#         ticker_level_name = actual_names[0] if actual_names[0] is not None else 0\n",
    "#         date_level_name = actual_names[1] if actual_names[1] is not None else 1\n",
    "#         if isinstance(ticker_level_name, int) or isinstance(date_level_name, int):\n",
    "#              print(f\"Warning: Using positional index levels ({ticker_level_name}, {date_level_name}) as names were missing.\")\n",
    "#     else:\n",
    "#         print(\"Index is not a MultiIndex or has too few levels. Checking columns...\")\n",
    "#         ticker_col_to_use = None\n",
    "#         for col in POTENTIAL_TICKER_COLS:\n",
    "#             if col in df_OHLCV.columns:\n",
    "#                 ticker_col_to_use = col\n",
    "#                 break\n",
    "#         date_col_to_use = POTENTIAL_DATE_COL if POTENTIAL_DATE_COL in df_OHLCV.columns else None\n",
    "#         if ticker_col_to_use and date_col_to_use:\n",
    "#             required_cols = [ticker_col_to_use, date_col_to_use]\n",
    "#             print(f\"Attempting to set index using columns: {required_cols}...\")\n",
    "#             # Ensure df_OHLCV exists and is a DataFrame before modification\n",
    "#             if 'df_OHLCV' in locals() and isinstance(df_OHLCV, pd.DataFrame):\n",
    "#                  df_OHLCV = df_OHLCV.set_index(required_cols)\n",
    "#                  ticker_level_name = required_cols[0]\n",
    "#                  date_level_name = required_cols[1]\n",
    "#                  print(f\"MultiIndex set successfully.\")\n",
    "#             else:\n",
    "#                  print(\"ERROR: df_OHLCV not defined or not a DataFrame before setting index.\")\n",
    "#                  sys.exit(\"Exiting due to data structure issue.\")\n",
    "\n",
    "#         else:\n",
    "#             missing = []\n",
    "#             if not ticker_col_to_use: missing.extend(POTENTIAL_TICKER_COLS)\n",
    "#             if not date_col_to_use: missing.append(POTENTIAL_DATE_COL)\n",
    "#             print(f\"ERROR: Cannot set index. Required columns for Ticker ({POTENTIAL_TICKER_COLS}) or Date ({POTENTIAL_DATE_COL}) not found.\")\n",
    "#             if 'df_OHLCV' in locals() and isinstance(df_OHLCV, pd.DataFrame):\n",
    "#                  print(\"df_OHLCV columns:\", df_OHLCV.columns)\n",
    "#             else:\n",
    "#                  print(\"df_OHLCV is not defined or not a DataFrame.\")\n",
    "#             sys.exit(\"Exiting due to incorrect df_OHLCV structure.\")\n",
    "\n",
    "#     print(f\"Using '{ticker_level_name}' for ticker level and '{date_level_name}' for date level.\")\n",
    "\n",
    "#     if 'df_OHLCV' not in locals() or not isinstance(df_OHLCV, pd.DataFrame):\n",
    "#         print(\"ERROR: df_OHLCV is not defined or not a DataFrame.\")\n",
    "#         sys.exit(\"Exiting due to missing data.\")\n",
    "\n",
    "#     if ADJ_CLOSE_COL not in df_OHLCV.columns:\n",
    "#         print(f\"ERROR: Required price column '{ADJ_CLOSE_COL}' not found in df_OHLCV columns: {df_OHLCV.columns}\")\n",
    "#         sys.exit(\"Exiting due to missing price column.\")\n",
    "\n",
    "#     # Convert date level to datetime if it's not already\n",
    "#     date_level_values = df_OHLCV.index.get_level_values(date_level_name)\n",
    "#     if not pd.api.types.is_datetime64_any_dtype(date_level_values):\n",
    "#         print(f\"Converting date level '{date_level_name}' to datetime objects...\")\n",
    "#         # Create a new index with the converted dates\n",
    "#         new_levels = list(df_OHLCV.index.levels)\n",
    "#         date_level_idx = df_OHLCV.index.names.index(date_level_name) # Find position of date level\n",
    "#         new_levels[date_level_idx] = pd.to_datetime(date_level_values.unique()) # Convert unique dates\n",
    "#         # Need to reconstruct the index carefully, preserving levels and codes\n",
    "#         new_codes = list(df_OHLCV.index.codes)\n",
    "#         new_index = pd.MultiIndex(levels=new_levels, codes=new_codes, names=df_OHLCV.index.names)\n",
    "#         df_OHLCV.index = new_index\n",
    "#         print(\"Date level converted.\")\n",
    "\n",
    "\n",
    "#     if not df_OHLCV.index.is_monotonic_increasing:\n",
    "#          print(\"Sorting df_OHLCV index...\")\n",
    "#          df_OHLCV = df_OHLCV.sort_index()\n",
    "#          print(\"Index sorted.\")\n",
    "\n",
    "# except Exception as e:\n",
    "#     print(f\"ERROR: An unexpected error occurred during index check/setup: {e}\")\n",
    "#     traceback.print_exc()\n",
    "#     sys.exit(\"Exiting due to index setup error.\")\n",
    "\n",
    "# # --- Extract unique trading dates from the prepared index ---\n",
    "# try:\n",
    "#     all_dates_in_data = df_OHLCV.index.get_level_values(date_level_name).unique()\n",
    "#     # Ensure dates are Timestamps and sorted\n",
    "#     trading_dates = pd.DatetimeIndex(pd.to_datetime(all_dates_in_data)).sort_values()\n",
    "#     if not trading_dates.empty:\n",
    "#          print(f\"\\nTrading dates extracted using level '{date_level_name}'. Found {len(trading_dates)} unique dates (e.g., {trading_dates[0].date()} to {trading_dates[-1].date()}).\")\n",
    "#     else:\n",
    "#          print(f\"ERROR: No dates found in the index level '{date_level_name}'. Cannot proceed.\")\n",
    "#          sys.exit(\"Exiting - Cannot determine trading dates.\")\n",
    "# except (KeyError, IndexError) as e:\n",
    "#     print(f\"ERROR: Could not find or access level '{date_level_name}' to extract trading dates: {e}\")\n",
    "#     sys.exit(\"Exiting - Cannot determine trading dates.\")\n",
    "# except Exception as e:\n",
    "#     print(f\"ERROR: An unexpected error occurred extracting trading dates: {e}\")\n",
    "#     traceback.print_exc()\n",
    "#     sys.exit(\"Exiting - Cannot determine trading dates.\")\n",
    "\n",
    "\n",
    "# # =============================================================================\n",
    "# # 2. Helper Function for Trading Dates (NO CHANGES NEEDED HERE)\n",
    "# # =============================================================================\n",
    "# def get_next_trading_date(current_date, sorted_trading_dates):\n",
    "#     \"\"\"\n",
    "#     Finds the first trading date strictly after current_date in a sorted DatetimeIndex.\n",
    "#     Args:\n",
    "#         current_date (str, datetime, Timestamp): The date after which to find the next trading date.\n",
    "#         sorted_trading_dates (pd.DatetimeIndex): A sorted index of available trading dates.\n",
    "#     Returns:\n",
    "#         pd.Timestamp or None: The next trading date, or None if no date is found after current_date.\n",
    "#     \"\"\"\n",
    "#     if not isinstance(sorted_trading_dates, pd.DatetimeIndex) or not sorted_trading_dates.is_monotonic_increasing:\n",
    "#         print(\"Warning: sorted_trading_dates is not a sorted DatetimeIndex in get_next_trading_date.\")\n",
    "#         # Attempt to sort if possible, otherwise return None\n",
    "#         try:\n",
    "#             sorted_trading_dates = pd.DatetimeIndex(sorted_trading_dates).sort_values()\n",
    "#         except Exception:\n",
    "#             return None # Cannot proceed if dates aren't sortable Timestamps\n",
    "\n",
    "#     if sorted_trading_dates.empty:\n",
    "#         return None\n",
    "\n",
    "#     current_date = pd.Timestamp(current_date)\n",
    "#     try:\n",
    "#         loc = sorted_trading_dates.searchsorted(current_date, side='right')\n",
    "#         if loc < len(sorted_trading_dates):\n",
    "#             return sorted_trading_dates[loc]\n",
    "#         else:\n",
    "#             return None\n",
    "#     except Exception as e:\n",
    "#         print(f\"Warning: Error in get_next_trading_date for {current_date}: {e}\")\n",
    "#         return None\n",
    "\n",
    "\n",
    "# # =============================================================================\n",
    "# # 3. Portfolio Return Calculation Loop (NO CHANGES NEEDED HERE)\n",
    "# # =============================================================================\n",
    "# # Assume all_portfolios is populated correctly before this section\n",
    "# # Example initialization if needed for testing:\n",
    "# # all_portfolios = {} # Populate this dict from your portfolio generation logic\n",
    "\n",
    "# portfolio_returns_data = [] # List to store results for *this run*\n",
    "\n",
    "# print(\"\\nCalculating portfolio returns for the current run...\")\n",
    "\n",
    "# # Flag to track if the index mismatch warning has been shown\n",
    "# index_mismatch_warning_shown = False\n",
    "\n",
    "# # Check if all_portfolios is populated\n",
    "# if 'all_portfolios' not in locals() or not isinstance(all_portfolios, dict):\n",
    "#     print(\"ERROR: 'all_portfolios' dictionary is not defined or not a dictionary.\")\n",
    "#     all_portfolios = {} # Initialize to empty to prevent NameError later, though calculation won't run\n",
    "\n",
    "# if not all_portfolios:\n",
    "#     print(\"Warning: 'all_portfolios' dictionary is empty. No returns to calculate for this run.\")\n",
    "# else:\n",
    "#     processed_count = 0\n",
    "#     skipped_count = 0\n",
    "#     error_count = 0\n",
    "#     # ... (Keep your existing Section 3 loop code as is) ...\n",
    "#     for portf_name, portf_data in list(all_portfolios.items()):\n",
    "#         try:\n",
    "#             # --- 3.1 Extract Parameters and Holdings ---\n",
    "#             params = portf_data.get('parameters')\n",
    "#             selected_stocks_df = portf_data.get('selected_stocks')\n",
    "\n",
    "#             if not isinstance(params, dict) or not isinstance(selected_stocks_df, pd.DataFrame):\n",
    "#                 print(f\"Warning: Skipping {portf_name} - Invalid structure.\")\n",
    "#                 skipped_count += 1\n",
    "#                 continue\n",
    "\n",
    "#             portf_date_str = params.get('date')\n",
    "#             raw_factor = params.get('raw_score_scale_factor')\n",
    "#             risk_adj_factor = params.get('risk_adj_score_scale_factor')\n",
    "#             penalty_factor = params.get('penalty_IntraCluster_Corr')\n",
    "\n",
    "#             if portf_date_str is None or raw_factor is None or risk_adj_factor is None or penalty_factor is None:\n",
    "#                 print(f\"Warning: Skipping {portf_name} - Missing essential parameter.\")\n",
    "#                 skipped_count += 1\n",
    "#                 continue\n",
    "\n",
    "#             portf_date = pd.Timestamp(portf_date_str)\n",
    "\n",
    "#             if selected_stocks_df.empty:\n",
    "#                 skipped_count += 1\n",
    "#                 continue\n",
    "\n",
    "#             # --- 3.2 Ensure Portfolio Index Matches Data Index ---\n",
    "#             if selected_stocks_df.index.name != ticker_level_name:\n",
    "#                 if not index_mismatch_warning_shown:\n",
    "#                     print(f\"Warning: Portfolio index name ('{selected_stocks_df.index.name}') mismatches data index ('{ticker_level_name}'). Renaming.\")\n",
    "#                     index_mismatch_warning_shown = True\n",
    "#                 try:\n",
    "#                     selected_stocks_df = selected_stocks_df.copy()\n",
    "#                     selected_stocks_df.index.name = ticker_level_name\n",
    "#                 except Exception as rename_e:\n",
    "#                     print(f\"Error: Failed to rename portfolio index for {portf_name}: {rename_e}. Skipping.\")\n",
    "#                     skipped_count += 1\n",
    "#                     continue\n",
    "\n",
    "#             original_portfolio_tickers = selected_stocks_df.index.tolist()\n",
    "\n",
    "#             # --- 3.3 Determine Buy and Sell Dates ---\n",
    "#             buy_date = get_next_trading_date(portf_date, trading_dates)\n",
    "#             if buy_date is None:\n",
    "#                 skipped_count += 1\n",
    "#                 continue\n",
    "\n",
    "#             sell_date = get_next_trading_date(buy_date, trading_dates)\n",
    "#             if sell_date is None:\n",
    "#                 skipped_count += 1\n",
    "#                 continue\n",
    "\n",
    "#             required_dates = [buy_date, sell_date]\n",
    "\n",
    "#             # --- 3.4 Fetch Price Data & Filter Available Tickers ---\n",
    "#             idx = pd.IndexSlice\n",
    "#             potential_data = df_OHLCV.loc[\n",
    "#                 idx[original_portfolio_tickers, required_dates], : # Slice using index directly\n",
    "#             ]\n",
    "\n",
    "#             ticker_date_counts = potential_data.index.get_level_values(ticker_level_name).value_counts()\n",
    "#             available_tickers = ticker_date_counts[ticker_date_counts == len(required_dates)].index.tolist()\n",
    "\n",
    "#             if not available_tickers:\n",
    "#                 skipped_count += 1\n",
    "#                 continue\n",
    "\n",
    "#             # --- 3.5 Process Prices for Available Tickers ---\n",
    "#             try:\n",
    "#                 idx_slice = pd.IndexSlice[available_tickers, required_dates]\n",
    "#                 prices = potential_data.loc[idx_slice, ADJ_CLOSE_COL]\n",
    "#                 prices_unstacked = prices.unstack(level=date_level_name)\n",
    "#                 prices_unstacked = prices_unstacked.rename(columns={buy_date: 'Buy Price', sell_date: 'Sell Price'})\n",
    "#                 prices_unstacked = prices_unstacked.dropna()\n",
    "\n",
    "#                 if prices_unstacked.empty:\n",
    "#                     skipped_count += 1\n",
    "#                     continue\n",
    "\n",
    "#                 final_available_tickers = prices_unstacked.index.tolist()\n",
    "\n",
    "#             except KeyError as e:\n",
    "#                  print(f\"Error: KeyError during price processing for {portf_name}: {e}. Skipping.\")\n",
    "#                  skipped_count += 1\n",
    "#                  continue\n",
    "#             except Exception as e:\n",
    "#                  print(f\"Error: Unexpected error during price processing for {portf_name}: {e}. Skipping.\")\n",
    "#                  traceback.print_exc()\n",
    "#                  skipped_count += 1\n",
    "#                  continue\n",
    "\n",
    "#             # --- 3.6 Calculate Portfolio Return ---\n",
    "#             aligned_weights = selected_stocks_df.loc[final_available_tickers, 'Weight']\n",
    "#             weight_sum = aligned_weights.sum()\n",
    "#             if weight_sum <= 1e-9:\n",
    "#                  skipped_count += 1\n",
    "#                  continue\n",
    "#             normalized_weights = aligned_weights / weight_sum\n",
    "\n",
    "#             buy_prices = prices_unstacked.loc[final_available_tickers, 'Buy Price']\n",
    "#             sell_prices = prices_unstacked.loc[final_available_tickers, 'Sell Price']\n",
    "\n",
    "#             zero_price_mask = (buy_prices.abs() < 1e-9)\n",
    "#             if zero_price_mask.any():\n",
    "#                 num_zero = zero_price_mask.sum()\n",
    "#                 print(f\"Warning: Found {num_zero} stock(s) with near-zero buy price in {portf_name} on {buy_date.date()}. Excluding.\")\n",
    "#                 valid_price_mask = ~zero_price_mask\n",
    "#                 buy_prices = buy_prices[valid_price_mask]\n",
    "#                 sell_prices = sell_prices[valid_price_mask]\n",
    "#                 normalized_weights = normalized_weights.loc[valid_price_mask]\n",
    "\n",
    "#                 if buy_prices.empty:\n",
    "#                      skipped_count += 1\n",
    "#                      continue\n",
    "#                 weight_sum = normalized_weights.sum()\n",
    "#                 if weight_sum <= 1e-9:\n",
    "#                     skipped_count += 1\n",
    "#                     continue\n",
    "#                 normalized_weights = normalized_weights / weight_sum\n",
    "\n",
    "#             if buy_prices.isnull().any() or sell_prices.isnull().any() or normalized_weights.isnull().any() or \\\n",
    "#                np.isinf(buy_prices).any() or np.isinf(sell_prices).any() or np.isinf(normalized_weights).any():\n",
    "#                 print(f\"Warning: Skipping {portf_name} - Found NaN or Inf in final arrays.\")\n",
    "#                 skipped_count += 1\n",
    "#                 continue\n",
    "\n",
    "#             individual_returns = (sell_prices / (buy_prices + 1e-12)) - 1\n",
    "#             portfolio_return = (individual_returns * normalized_weights).sum()\n",
    "\n",
    "#             # --- 3.7 Store Results ---\n",
    "#             portfolio_returns_data.append({\n",
    "#                 'portfolio_name': portf_name,\n",
    "#                 'generation_date': portf_date,\n",
    "#                 'buy_date': buy_date,\n",
    "#                 'sell_date': sell_date,\n",
    "#                 'raw_factor': raw_factor,\n",
    "#                 'risk_adj_factor': risk_adj_factor,\n",
    "#                 'penalty_factor': penalty_factor,\n",
    "#                 'portfolio_return': portfolio_return,\n",
    "#                 'num_stocks_initial': len(original_portfolio_tickers),\n",
    "#                 'num_stocks_calc': len(final_available_tickers)\n",
    "#             })\n",
    "#             processed_count += 1\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(f\"FATAL Error processing portfolio {portf_name}: {e}\")\n",
    "#             traceback.print_exc()\n",
    "#             error_count += 1\n",
    "\n",
    "#     print(f\"\\nCurrent run portfolio return calculation finished.\")\n",
    "#     print(f\"Successfully processed: {processed_count}\")\n",
    "#     print(f\"Skipped (missing data/criteria): {skipped_count}\")\n",
    "#     print(f\"Errors during processing: {error_count}\")\n",
    "\n",
    "# # =============================================================================\n",
    "# # 4. Load Historical Data, Combine, and Analyze\n",
    "# # =============================================================================\n",
    "# print(\"\\n--- Loading Historical Data and Performing Combined Analysis ---\")\n",
    "\n",
    "# all_returns_df = pd.DataFrame() # Initialize an empty DataFrame\n",
    "\n",
    "# # --- Load existing historical data ---\n",
    "# if os.path.exists(OUTPUT_RETURNS_CSV):\n",
    "#     try:\n",
    "#         print(f\"Loading historical data from {OUTPUT_RETURNS_CSV}...\")\n",
    "#         historical_returns_df = pd.read_csv(OUTPUT_RETURNS_CSV, parse_dates=['generation_date', 'buy_date', 'sell_date'])\n",
    "#         # Optional: Add checks for expected columns\n",
    "#         expected_cols = ['generation_date', 'buy_date', 'sell_date', 'raw_factor', 'risk_adj_factor', 'penalty_factor', 'portfolio_return']\n",
    "#         if all(col in historical_returns_df.columns for col in expected_cols):\n",
    "#             all_returns_df = historical_returns_df\n",
    "#             print(f\"Loaded {len(all_returns_df)} historical records.\")\n",
    "#         else:\n",
    "#             print(f\"Warning: Historical data file {OUTPUT_RETURNS_CSV} is missing expected columns. Ignoring historical data.\")\n",
    "#             # Decide if you want to rename/backup the bad file\n",
    "#             # os.rename(OUTPUT_RETURNS_CSV, OUTPUT_RETURNS_CSV + \".bad_format\")\n",
    "\n",
    "#     except pd.errors.EmptyDataError:\n",
    "#         print(f\"Historical data file {OUTPUT_RETURNS_CSV} is empty. Starting fresh.\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error loading historical data from {OUTPUT_RETURNS_CSV}: {e}\")\n",
    "#         print(\"Warning: Proceeding without historical data.\")\n",
    "# else:\n",
    "#     print(f\"Historical data file {OUTPUT_RETURNS_CSV} not found. Starting fresh.\")\n",
    "\n",
    "# # --- Combine with current run's data ---\n",
    "# if portfolio_returns_data:\n",
    "#     current_returns_df = pd.DataFrame(portfolio_returns_data)\n",
    "#     print(f\"Combining {len(current_returns_df)} new records with historical data.\")\n",
    "\n",
    "#     # Ensure date columns are datetime objects in the new data\n",
    "#     for col in ['generation_date', 'buy_date', 'sell_date']:\n",
    "#          if col in current_returns_df.columns:\n",
    "#               current_returns_df[col] = pd.to_datetime(current_returns_df[col])\n",
    "\n",
    "#     # Concatenate old and new data\n",
    "#     if not all_returns_df.empty:\n",
    "#         all_returns_df = pd.concat([all_returns_df, current_returns_df], ignore_index=True)\n",
    "#     else:\n",
    "#         all_returns_df = current_returns_df\n",
    "\n",
    "#     # Optional: Remove potential duplicates based on key identifiers if runs might overlap accidentally\n",
    "#     key_cols = ['generation_date', 'buy_date', 'sell_date', 'raw_factor', 'risk_adj_factor', 'penalty_factor']\n",
    "#     initial_len = len(all_returns_df)\n",
    "#     all_returns_df = all_returns_df.drop_duplicates(subset=key_cols, keep='last')\n",
    "#     if len(all_returns_df) < initial_len:\n",
    "#         print(f\"Removed {initial_len - len(all_returns_df)} duplicate records based on key columns.\")\n",
    "\n",
    "# else:\n",
    "#     print(\"No new portfolio returns were calculated in this run.\")\n",
    "\n",
    "# # --- Perform analysis ONLY if there's data ---\n",
    "# if not all_returns_df.empty:\n",
    "#     # Ensure correct data types before grouping (especially factors)\n",
    "#     try:\n",
    "#         all_returns_df['raw_factor'] = pd.to_numeric(all_returns_df['raw_factor'])\n",
    "#         all_returns_df['risk_adj_factor'] = pd.to_numeric(all_returns_df['risk_adj_factor'])\n",
    "#         all_returns_df['penalty_factor'] = pd.to_numeric(all_returns_df['penalty_factor'])\n",
    "#         all_returns_df['portfolio_return'] = pd.to_numeric(all_returns_df['portfolio_return'])\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error converting factor/return columns to numeric: {e}\")\n",
    "#         print(\"Analysis might be affected.\")\n",
    "\n",
    "\n",
    "#     # Sort for easier viewing (optional, but good practice)\n",
    "#     grouping_factors = ['raw_factor', 'risk_adj_factor', 'penalty_factor']\n",
    "#     all_returns_df = all_returns_df.sort_values(by=['generation_date'] + grouping_factors)\n",
    "\n",
    "#     print(\"\\n--- Combined Portfolio Return Analysis (All Historical Data) ---\")\n",
    "#     print(f\"Total return instances analyzed: {len(all_returns_df)}\")\n",
    "#     # print(\"Sample of combined data:\") # Optional: Print head if needed\n",
    "#     # print(all_returns_df.head())\n",
    "\n",
    "#     # --- Group by factor combinations to analyze performance ---\n",
    "#     print(\"\\n--- Performance Analysis by Factor Combination (Based on All Data) ---\")\n",
    "\n",
    "#     # Check if grouping factors exist\n",
    "#     if not all(f in all_returns_df.columns for f in grouping_factors):\n",
    "#          print(f\"Error: One or more grouping factors {grouping_factors} not found in the combined data columns: {all_returns_df.columns}\")\n",
    "#          # Handle error - perhaps exit or skip analysis\n",
    "#          performance_summary = pd.DataFrame() # Create empty df to avoid error later\n",
    "#     else:\n",
    "#         # Group by all three factors and aggregate portfolio returns\n",
    "#         performance_summary = all_returns_df.groupby(grouping_factors)['portfolio_return'].agg(\n",
    "#             mean='mean',\n",
    "#             median='median',\n",
    "#             std=lambda x: x.std(ddof=0) if pd.notna(x).sum() > 1 else np.nan,\n",
    "#             count=lambda x: pd.notna(x).sum(), # Count only valid returns for this combo\n",
    "#             sharpe=lambda x: (x.mean() / (x.std() + 1e-9)) * np.sqrt(252) if pd.notna(x).sum() > 1 and x.std() > 1e-9 else np.nan # Example: Annualized Sharpe (assuming daily returns, zero risk-free)\n",
    "#         ).reset_index() # Reset index to make factors columns again\n",
    "\n",
    "#         # Calculate additional metrics if desired, like total return, win rate etc.\n",
    "#         # Example: Add win rate (percentage of positive return days)\n",
    "#         win_rate = all_returns_df[all_returns_df['portfolio_return'] > 0].groupby(grouping_factors).size() / \\\n",
    "#                    all_returns_df.groupby(grouping_factors).size()\n",
    "#         performance_summary = pd.merge(performance_summary, win_rate.rename('win_rate').reset_index(), on=grouping_factors, how='left')\n",
    "#         performance_summary['win_rate'] = performance_summary['win_rate'].fillna(0) # Fill NaN if no trades for a combo\n",
    "\n",
    "#         # Sort by a chosen metric to find the 'best' combinations\n",
    "#         # Sorting by Sharpe ratio is common, or mean return, or a combination\n",
    "#         performance_summary = performance_summary.sort_values(by='mean', ascending=False) # Or sort by 'sharpe'\n",
    "#         print(\"\\nPerformance per Factor Combination (sorted by mean return):\")\n",
    "#         with pd.option_context('display.float_format', '{:.4f}'.format, 'display.max_rows', 200):\n",
    "#             print(performance_summary)\n",
    "\n",
    "#         # --- Analyze the number of stocks used ---\n",
    "#         stock_count_summary = all_returns_df.groupby(grouping_factors)['num_stocks_calc'].agg(['mean', 'min', 'max'])\n",
    "#         print(\"\\nAverage/Min/Max Stocks Used in Calculation per Factor Combination (All Data):\")\n",
    "#         with pd.option_context('display.float_format', '{:.2f}'.format):\n",
    "#             print(stock_count_summary.sort_values(by='mean', ascending=False))\n",
    "\n",
    "# else:\n",
    "#     print(\"\\nNo portfolio returns available (neither historical nor current) to analyze.\")\n",
    "#     performance_summary = pd.DataFrame() # Ensure it exists as empty if no data\n",
    "\n",
    "# # =============================================================================\n",
    "# # 5. Save Results (Append Returns, Overwrite Summary)\n",
    "# # =============================================================================\n",
    "\n",
    "# # --- Save the individual returns (APPEND mode) ---\n",
    "# if portfolio_returns_data: # Only append if there's new data from this run\n",
    "#     current_to_save_df = pd.DataFrame(portfolio_returns_data) # Use the list directly from this run\n",
    "#     try:\n",
    "#         # Check if file exists to determine if header is needed\n",
    "#         write_header = not os.path.exists(OUTPUT_RETURNS_CSV)\n",
    "#         # Append new data\n",
    "#         current_to_save_df.to_csv(OUTPUT_RETURNS_CSV, mode='a', header=write_header, index=False)\n",
    "#         print(f\"\\nAppended {len(current_to_save_df)} new return records to {OUTPUT_RETURNS_CSV}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"\\nError appending returns to {OUTPUT_RETURNS_CSV}: {e}\")\n",
    "# elif not os.path.exists(OUTPUT_RETURNS_CSV):\n",
    "#      # If no new data AND the file doesn't exist, create an empty file with header\n",
    "#      try:\n",
    "#          pd.DataFrame(columns=portfolio_returns_data[0].keys() if portfolio_returns_data else [ # Get columns if possible\n",
    "#             'portfolio_name', 'generation_date', 'buy_date', 'sell_date',\n",
    "#             'raw_factor', 'risk_adj_factor', 'penalty_factor', 'portfolio_return',\n",
    "#             'num_stocks_initial', 'num_stocks_calc']).to_csv(OUTPUT_RETURNS_CSV, index=False)\n",
    "#          print(f\"\\nCreated empty returns file with header: {OUTPUT_RETURNS_CSV}\")\n",
    "#      except Exception as e:\n",
    "#          print(f\"\\nError creating empty returns file {OUTPUT_RETURNS_CSV}: {e}\")\n",
    "\n",
    "\n",
    "# # --- Save the performance summary (OVERWRITE mode) ---\n",
    "# # This summary is based on ALL data (historical + current), so we overwrite it each time\n",
    "# if 'performance_summary' in locals() and not performance_summary.empty:\n",
    "#     try:\n",
    "#         # No need to reset index if already done after groupby\n",
    "#         performance_summary.to_csv(OUTPUT_SUMMARY_CSV, index=False)\n",
    "#         print(f\"Updated performance summary saved to {OUTPUT_SUMMARY_CSV}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"\\nError saving performance summary to {OUTPUT_SUMMARY_CSV}: {e}\")\n",
    "# elif not all_returns_df.empty:\n",
    "#      print(f\"Warning: Performance summary was calculated but is empty. Not saving {OUTPUT_SUMMARY_CSV}.\")\n",
    "# else:\n",
    "#     print(f\"No data to generate performance summary. {OUTPUT_SUMMARY_CSV} not saved or updated.\")\n",
    "\n",
    "\n",
    "# print(\"\\n--- Script Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # =============================================================================\n",
    "# # 4b. Time-Based Performance Analysis (Example: By Year)\n",
    "# # =============================================================================\n",
    "# if not all_returns_df.empty:\n",
    "#     print(\"\\n--- Performance Analysis by Factor Combination AND Year ---\")\n",
    "\n",
    "#     # Ensure 'sell_date' is datetime and extract the year\n",
    "#     try:\n",
    "#         all_returns_df['sell_year'] = pd.to_datetime(all_returns_df['sell_date']).dt.year\n",
    "        \n",
    "#         # Define grouping factors including the year\n",
    "#         time_grouping_factors = ['sell_year'] + grouping_factors # grouping_factors was ['raw_factor', 'risk_adj_factor', 'penalty_factor']\n",
    "\n",
    "#         # Group by year and factors\n",
    "#         performance_by_year = all_returns_df.groupby(time_grouping_factors)['portfolio_return'].agg(\n",
    "#             mean='mean',\n",
    "#             median='median',\n",
    "#             std=lambda x: x.std(ddof=0) if pd.notna(x).sum() > 1 else np.nan,\n",
    "#             count=lambda x: pd.notna(x).sum()\n",
    "#             # Add other metrics like Sharpe, win rate per year if desired\n",
    "#         ).reset_index()\n",
    "\n",
    "#         # Sort for better readability (e.g., by year, then by mean return)\n",
    "#         performance_by_year = performance_by_year.sort_values(by=['sell_year', 'mean'], ascending=[True, False])\n",
    "\n",
    "#         print(\"\\nPerformance per Factor Combination per Year (sorted by year, then mean return):\")\n",
    "#         with pd.option_context('display.float_format', '{:.4f}'.format, 'display.max_rows', 500): # Show more rows\n",
    "#             print(performance_by_year)\n",
    "\n",
    "#         # You could save this to a separate CSV if needed\n",
    "#         # performance_by_year.to_csv('portfolio_factor_performance_by_year.csv', index=False)\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error during yearly performance analysis: {e}\")\n",
    "#         traceback.print_exc()\n",
    "# else:\n",
    "#     print(\"\\nNo data available for yearly performance analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
