{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HplHE-XnwHFz"
      },
      "source": [
        "===== TURN ON POWERTOY AWAKE to KEEP CONNECTION ALIVE =====\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KeSrHHAXwQzG",
        "outputId": "8621b890-b2e3-4c11-9efb-3023aaec77c2"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWX6sLNsmsCr",
        "outputId": "8b78eb18-70c3-4bf9-f08e-7045b239e267"
      },
      "outputs": [],
      "source": [
        "# symbols: The string (or sequence) to slice.\n",
        "# start_index: Optional. The starting index of the slice. If None, defaults to 0.\n",
        "# end_index: Optional. The ending index of the slice (exclusive). If None, defaults to the end of the string.\n",
        "# step_value: Optional. The step value for the slice. If None, defaults to 1.\n",
        "\n",
        "start_index = None\n",
        "\n",
        "# end_index = None\n",
        "end_index = 3\n",
        "\n",
        "step_value = None\n",
        "slice_obj = slice(start_index, end_index, step_value)  # Create a slice object\n",
        "\n",
        "print(f'slice of symbols: symbols[{slice_obj}]')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2xUTYT3wHF4",
        "outputId": "23185f48-cd7d-4752-f04f-a5d560a390ae"
      },
      "outputs": [],
      "source": [
        "! pip install fake-useragent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTCD6Lo1wHF4"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from fake_useragent import UserAgent\n",
        "\n",
        "selector = '.styled-table-new'\n",
        "ua = UserAgent()  # Initialize UserAgent\n",
        "\n",
        "def download_table(url, selector):\n",
        "    \"\"\"\n",
        "    Downloads table data from a Yahoo Finance page with rate limiting.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Add a User-Agent header to mimic a browser\n",
        "        # headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}  # Example User-Agent\n",
        "        new_user_agent = ua.random\n",
        "        headers = {\"User-Agent\": new_user_agent}\n",
        "        response = requests.get(url, headers=headers)\n",
        "        response.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        table_body = soup.select_one(selector)\n",
        "\n",
        "        if table_body is None:\n",
        "            print(f\"Error: Table body not found using selector: {selector}\")\n",
        "            return None\n",
        "\n",
        "        rows = table_body.find_all('tr')\n",
        "        if not rows:\n",
        "            print(\"Error: No rows found in the table.\")\n",
        "            return None\n",
        "\n",
        "        # Extract headers from the first row (th elements)\n",
        "        headers_list = [th.text.strip() for th in rows[0].find_all('th')]\n",
        "\n",
        "        data = []\n",
        "        for row in rows:\n",
        "            cells = row.find_all('td')\n",
        "            row_data = [cell.text.strip() for cell in cells]\n",
        "            if row_data:  # Only append if the row has data\n",
        "                data.append(row_data)\n",
        "\n",
        "        if not data:\n",
        "            print(\"Error: No data found in the table rows.\")\n",
        "            return None\n",
        "\n",
        "        df = pd.DataFrame(data, columns=headers_list)\n",
        "        return df\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error during request: {e}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D6T6kGjzwHF5"
      },
      "outputs": [],
      "source": [
        "columns_common_stock_etfs_metrics_sector_industry_category = 'c=0,1,2,3,4,6,14,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,125,126,59,61,63,64,67,65,66,103,100,109'\n",
        "columns_common_stock_etfs_metrics = 'c=0,1,2,6,14,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,125,126,59,61,63,64,67,65,66,100,109'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BU2FLNxcwHF5"
      },
      "outputs": [],
      "source": [
        "url_mktcap ='https://finviz.com/screener.ashx?v=152&ft=4&o=-marketcap&c='\n",
        "# url_columns is 'columns_common_stock_etfs_metrics_sector_industry_category'\n",
        "url_columns = '0,1,2,3,4,6,14,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,125,126,59,61,63,64,67,65,66,103,100,109'\n",
        "url_mktcap_rows = ['&r=1', '&r=21', '&r=41', '&r=61', '&r=81', '&r=101', '&r=121', '&r=141', '&r=161', '&r=181', '&r=201', '&r=221', '&r=241', '&r=261', '&r=281', '&r=301', '&r=321', '&r=341', '&r=361', '&r=381', '&r=401', '&r=421', '&r=441', '&r=461', '&r=481', '&r=501', '&r=521', '&r=541', '&r=561', '&r=581', '&r=601', '&r=621', '&r=641', '&r=661', '&r=681', '&r=701', '&r=721', '&r=741', '&r=761', '&r=781', '&r=801', '&r=821', '&r=841', '&r=861', '&r=881', '&r=901', '&r=921', '&r=941', '&r=961', '&r=981', '&r=1001', '&r=1021','&r=1041','&r=1061','&r=1081','&r=1101','&r=1121','&r=1141',]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYhnaEoxwHF6",
        "outputId": "6b5af065-4956-43ec-efca-94274cea2993"
      },
      "outputs": [],
      "source": [
        "urls_mktcap = []\n",
        "\n",
        "for _rows in url_mktcap_rows:\n",
        "    url = url_mktcap + url_columns + _rows\n",
        "    urls_mktcap.append(url)\n",
        "\n",
        "print(f'len(urls_mktcap): {len(urls_mktcap)}')\n",
        "print(urls_mktcap[0:3])  # Print the length of the list of url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vVRlXL0GwHF6"
      },
      "outputs": [],
      "source": [
        "url_etfs ='https://finviz.com/screener.ashx?v=152&ft=4&o=-e.assetsundermanagement&c='\n",
        "url_etfs_rows = ['&r=1', '&r=21', '&r=41', '&r=61', '&r=81', '&r=101', '&r=121', '&r=141', '&r=161', '&r=181', '&r=201', '&r=221', '&r=241', '&r=261', '&r=281', '&r=301', '&r=321', '&r=341', '&r=361', '&r=381']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFL3e4q2wHF6",
        "outputId": "f7056337-6b5c-4c31-d2d8-90ef7383c40b"
      },
      "outputs": [],
      "source": [
        "urls_etfs = []\n",
        "\n",
        "for _rows in url_etfs_rows:\n",
        "    url = url_etfs + url_columns + _rows\n",
        "    urls_etfs.append(url)\n",
        "\n",
        "print(f'len(urls_etfs): {len(urls_etfs)}')\n",
        "print(urls_etfs[0:3])  # Print the length of the list of url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWfH7PLrwHF7",
        "outputId": "ee8b7abc-ba92-4b13-d3e3-e70c986e25c7"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "urls =  urls_mktcap + urls_etfs\n",
        "shuffled_urls = random.sample(urls, len(urls))\n",
        "print(f'len(shuffled_urls): {len(shuffled_urls)}')\n",
        "print(f'shuffled_urls[0:10]')\n",
        "for url in shuffled_urls[0:10]:\n",
        "    print(f'{url}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WyA3q6n9wHF7"
      },
      "outputs": [],
      "source": [
        "# test_urls = [\n",
        "#   'https://finviz.com/screener.ashx?v=152&ft=4&o=-marketcap&c=0,1,2,3,4,6,14,42,43,44,45,46,47,48,49,50,51,52,53,54,57,58,59,67,65,66,103,100,109,120,121,122&r=1',\n",
        "#   'https://finviz.com/screener.ashx?v=152&ft=4&o=-e.assetsundermanagement&c=0,1,2,3,4,6,14,42,43,44,45,46,47,48,49,50,51,52,53,54,57,58,59,67,65,66,103,100,109,120,121,122&r=101',\n",
        "#   'https://finviz.com/screener.ashx?v=152&ft=4&o=-marketcap&c=0,1,2,3,4,6,14,42,43,44,45,46,47,48,49,50,51,52,53,54,57,58,59,67,65,66,103,100,109,120,121,122&r=601',\n",
        "#   ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DgtBvSQtwHF8",
        "outputId": "7a6660fb-de8a-42c3-fbf0-dc4511596c9b"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "import random\n",
        "\n",
        "total_urls_to_download = len(shuffled_urls)\n",
        "\n",
        "df = pd.DataFrame()  # Initialized an empty DataFrame\n",
        "processed_count = 0\n",
        "\n",
        "columns_list = ['No.', 'Ticker', 'Company', 'Sector', 'Industry', 'Market Cap',\n",
        "       'Dividend', 'Perf Week', 'Perf Month', 'Perf Quart', 'Perf Half',\n",
        "       'Perf Year', 'Perf YTD', 'Beta', 'ATR', 'Volatility W', 'Volatility M',\n",
        "       'SMA20', 'SMA50', 'SMA200', '50D High', '50D Low', '52W High',\n",
        "       '52W Low', 'All-Time High', 'All-Time Low', 'RSI', 'Gap', 'Avg Volume',\n",
        "       'Rel Volume', 'Volume', 'Price', 'Change', 'Single Category',\n",
        "       'Asset Type', 'AUM']\n",
        "\n",
        "# for url in test_urls:\n",
        "# for url in shuffled_urls[3:6]:\n",
        "for url in shuffled_urls[slice_obj]:\n",
        "    # Introduce a delay between requests (adjust as needed)\n",
        "    delay_seconds = random.uniform(2, 4.5)  # Sleep between 2 and 5 seconds\n",
        "    # print(f\"Downloading {symbol}. Sleeping for {delay_seconds:.2f} seconds...\")\n",
        "    processed_count += 1\n",
        "    print(f\"Downloading {url}. Sleeping for {delay_seconds:.2f} seconds.  Processed {processed_count} / {total_urls_to_download} urls\")\n",
        "    time.sleep(delay_seconds)\n",
        "\n",
        "    df_temp = download_table(url, selector)\n",
        "\n",
        "    if df_temp is not None:\n",
        "        df_temp_filtered = df_temp[columns_list]\n",
        "        # Discards the original row indices of both DataFrames.\n",
        "        # Creates a new sequential integer index\n",
        "        df = pd.concat([df, df_temp_filtered], ignore_index=True)\n",
        "    else:\n",
        "        print(f\"Failed to download data for {url}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJt8zyFewHF8",
        "outputId": "2f8b8baf-a638-4042-d268-1386bcbf69f5"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "import pytz\n",
        "\n",
        "pst = pytz.timezone('America/Los_Angeles')  # or 'US/Pacific'\n",
        "current_date_pst = datetime.datetime.now(pst).strftime('%Y-%m-%d')\n",
        "\n",
        "# df_finviz_filename = f\"df_finviz_{current_date_pst}.pkl\"\n",
        "\n",
        "# print(f\"df_finviz_filename: {df_finviz_filename}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qwlh1R3qwHF8"
      },
      "outputs": [],
      "source": [
        "# Save the DataFrame to a pickle file\n",
        "df.to_pickle(df_finviz_filename)  # Saves to the Colab's runtime environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "egV3PWJ0wHF8",
        "outputId": "d51e83d9-b96a-4795-c887-76b375cc7814"
      },
      "outputs": [],
      "source": [
        "# # prompt: pickle df and download to my pc\n",
        "\n",
        "# import pickle\n",
        "# from google.colab import files\n",
        "\n",
        "# # Download the pickle file\n",
        "# files.download(df_finviz_filename)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R6_jh9e9WE_8",
        "outputId": "ad331610-e198-4724-cb1c-4a0c553f2e60"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "file_path = '/content/drive/MyDrive/stocks/df_finviz_tickers.pkl'  # Run in Colab\n",
        "\n",
        "df_tickers = df[['Ticker']]\n",
        "\n",
        "# Save the DataFrame to a pickle file in the specified location\n",
        "df_tickers.to_pickle(file_path)\n",
        "print(f\"tickers saved to: {file_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fq0ZyUcN8Hx7",
        "outputId": "c5a87596-9964-4dbf-ad39-2cc4a5e9144d"
      },
      "outputs": [],
      "source": [
        "len(df_tickers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 929
        },
        "id": "UHLEyY839h0V",
        "outputId": "bf1c0459-2efe-4a8d-c698-bf1099d05dcb"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78db6cfhuLqT"
      },
      "outputs": [],
      "source": [
        "# ======================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5VcTdIE3uLqT",
        "outputId": "060b4a54-0d08-4792-d829-ffce0f3ebe5d"
      },
      "outputs": [],
      "source": [
        "# symbols: The string (or sequence) to slice.\n",
        "# start_index: Optional. The starting index of the slice. If None, defaults to 0.\n",
        "# end_index: Optional. The ending index of the slice (exclusive). If None, defaults to the end of the string.\n",
        "# step_value: Optional. The step value for the slice. If None, defaults to 1.\n",
        "\n",
        "# start_index = None\n",
        "# end_index = None\n",
        "# end_index = 3\n",
        "\n",
        "step_value = None\n",
        "slice_obj = slice(start_index, end_index, step_value)  # Create a slice object\n",
        "\n",
        "print(f'slice of symbols: symbols[{slice_obj}]')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZmvZTdHuLqT"
      },
      "outputs": [],
      "source": [
        "from fake_useragent import UserAgent\n",
        "\n",
        "ua = UserAgent()  # Initialize UserAgent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTQzrXttuLqU",
        "outputId": "112dbd04-9a4d-4524-fb0d-b1c90caa7e0c"
      },
      "outputs": [],
      "source": [
        "new_user_agent = ua.random\n",
        "headers = {\"User-Agent\": new_user_agent}\n",
        "headers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Xyosq3FuLqU",
        "outputId": "4730a5ff-f138-4e4e-8841-64365648e254"
      },
      "outputs": [],
      "source": [
        "new_user_agent = ua.random\n",
        "headers = {\"User-Agent\": new_user_agent}\n",
        "headers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6l7V9hk1uLqU"
      },
      "outputs": [],
      "source": [
        "dir_path = 'G:/My Drive/stocks/'  # Run in PC, Replace with your actual directory path\n",
        "dir_path = '/content/drive/MyDrive/stocks/'  # Run in Colab\n",
        "\n",
        "symbols_stocks_file = 'symbols_stocks.txt'\n",
        "symbols_ETFs_file = 'symbols_ETFs.txt'\n",
        "# symbols_stocks_ETFs_file = 'symbols_stocks_ETFs.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NAuxwFy7uLqU"
      },
      "outputs": [],
      "source": [
        "# selector = \"#nimbus-app > section > section > section > article > div.container > div.table-container.yf-1jecxey > table > tbody\"\n",
        "selector = \"#nimbus-app > section > section > section > article > div.container > div.table-container.yf-1jecxey\"\n",
        "col_names = ['Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hVZTyrX6uLqU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "def read_stock_symbols(dir_path, symbols_stocks_file, symbols_ETFs_file):\n",
        "    \"\"\"\n",
        "    Reads stock and ETF symbols from text files in the specified directory and returns them in two separate lists.\n",
        "\n",
        "    Args:\n",
        "        dir_path (str): The directory path where the symbol files are located.\n",
        "        symbols_stocks_file (str): The name of the file containing stock symbols.\n",
        "        symbols_ETFs_file (str): The name of the file containing ETF symbols.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing two lists: (stock_symbols, etf_symbols).  Returns ([], [])\n",
        "               if any error occurs during file reading.\n",
        "    \"\"\"\n",
        "\n",
        "    stock_symbols = []\n",
        "    etf_symbols = []\n",
        "\n",
        "    try:\n",
        "        # Read stock symbols\n",
        "        with open(os.path.join(dir_path, symbols_stocks_file), 'r') as f:\n",
        "            stock_symbols = [line.strip() for line in f]\n",
        "\n",
        "        # Read ETF symbols\n",
        "        with open(os.path.join(dir_path, symbols_ETFs_file), 'r') as f:\n",
        "            etf_symbols = [line.strip() for line in f]\n",
        "\n",
        "        return stock_symbols, etf_symbols\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: One or more files not found in directory: {dir_path}\")\n",
        "        return [], []\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return [], []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I79YQzfFuLqV"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import random  # For a bit of randomness in the sleep time\n",
        "\n",
        "def download_yahoo_finance_table(url, selector):\n",
        "    \"\"\"\n",
        "    Downloads table data from a Yahoo Finance page with rate limiting.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Add a User-Agent header to mimic a browser\n",
        "        # headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}  # Example User-Agent\n",
        "        new_user_agent = ua.random\n",
        "        headers = {\"User-Agent\": new_user_agent}\n",
        "\n",
        "        response = requests.get(url, headers=headers)\n",
        "        response.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)\n",
        "\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        table_body = soup.select_one(selector)\n",
        "\n",
        "        if table_body is None:\n",
        "            print(f\"Error: Table body not found using selector: {selector}\")\n",
        "            return None\n",
        "\n",
        "        rows = table_body.find_all('tr')\n",
        "        if not rows:\n",
        "            print(\"Error: No rows found in the table.\")\n",
        "            return None\n",
        "\n",
        "        # Extract headers from the first row (th elements)\n",
        "        headers_list = [th.text.strip() for th in rows[0].find_all('th')]\n",
        "\n",
        "        data = []\n",
        "        for row in rows:\n",
        "            cells = row.find_all('td')\n",
        "            row_data = [cell.text.strip() for cell in cells]\n",
        "            if row_data:  # Only append if the row has data\n",
        "                data.append(row_data)\n",
        "\n",
        "        if not data:\n",
        "            print(\"Error: No data found in the table rows.\")\n",
        "            return None\n",
        "\n",
        "        df = pd.DataFrame(data, columns=headers_list)\n",
        "        return df\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error during request: {e}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NkcX6TakuLqV"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "import pytz\n",
        "\n",
        "def get_current_pst_time():\n",
        "  \"\"\"\n",
        "  Returns the current time in Pacific Standard Time (PST).\n",
        "\n",
        "  Returns:\n",
        "    A string representing the current time in PST, formatted as\n",
        "    \"YYYY-MM-DD HH:MM:SS\".\n",
        "  \"\"\"\n",
        "\n",
        "  pst_timezone = pytz.timezone('America/Los_Angeles')  # Get the PST timezone\n",
        "  pst_now = datetime.datetime.now(pst_timezone)  # Get the current time in PST\n",
        "\n",
        "  return pst_now.strftime(\"%Y-%m-%d %H:%M:%S\")  # Format the time as a string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gInEy7rtuLqV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def convert_df_data_types(df):\n",
        "    \"\"\"\n",
        "    Cleans and converts a Pandas DataFrame with a MultiIndex to the specified data types.\n",
        "\n",
        "    Args:\n",
        "        df: The input Pandas DataFrame.  Assumes a MultiIndex with stock ticker (str) and date (str).\n",
        "            Assumes columns 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume' as objects.\n",
        "\n",
        "    Returns:\n",
        "        A Pandas DataFrame with the correct data types.\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert the second level of the MultiIndex to datetime\n",
        "    try:\n",
        "        df.index = pd.MultiIndex.from_tuples([(i[0], pd.to_datetime(i[1])) for i in df.index], names=df.index.names)\n",
        "    except ValueError as e:\n",
        "        print(f\"Error converting MultiIndex to datetime: {e}\")\n",
        "        return df  # Or handle the error differently, e.g., raise it\n",
        "\n",
        "    # Convert columns to appropriate data types\n",
        "    columns_to_convert = ['Open', 'High', 'Low', 'Close', 'Adj Close']\n",
        "    for col in columns_to_convert:\n",
        "        try:\n",
        "            # Remove commas *before* attempting conversion. CRITICAL.\n",
        "            df[col] = df[col].str.replace(',', '', regex=False)  # Remove commas first\n",
        "            df[col] = df[col].astype(float)\n",
        "        except ValueError as e:\n",
        "            print(f\"Error converting column '{col}' to float: {e}\")\n",
        "            return df #skip this column and return the original df\n",
        "\n",
        "    try:\n",
        "        # Handle '-' values in 'Volume' BEFORE removing commas\n",
        "        df['Volume'] = df['Volume'].replace('-', np.nan)\n",
        "        df['Volume'] = df['Volume'].str.replace(',', '', regex=False).astype(float).astype('Int64') # Use Int64 to store NaN\n",
        "    except ValueError as e:\n",
        "        print(f\"Error converting column 'Volume' to int64: {e}\")\n",
        "        return df\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OHvVY-VhuLqV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def adjust_prices(df):\n",
        "    \"\"\"\n",
        "    Adjusts Open, High, Low, and Close prices using Adj Close to account for splits and dividends.\n",
        "\n",
        "    Args:\n",
        "        df: Pandas DataFrame with 'Open', 'High', 'Low', 'Close', and 'Adj Close' columns.\n",
        "            Assumes MultiIndex with stock ticker and datetime.\n",
        "\n",
        "    Returns:\n",
        "        Pandas DataFrame with adjusted 'Open', 'High', and 'Low' prices.\n",
        "    \"\"\"\n",
        "\n",
        "    # Calculate the adjustment ratio\n",
        "    df['adjustment_ratio'] = df['Adj Close'] / df['Close']\n",
        "\n",
        "    # Adjust Open, High, and Low prices\n",
        "    df['Adj Open'] = df['Open'] * df['adjustment_ratio']\n",
        "    df['Adj High'] = df['High'] * df['adjustment_ratio']\n",
        "    df['Adj Low'] = df['Low'] * df['adjustment_ratio']\n",
        "\n",
        "\n",
        "    # Optionally, drop the adjustment_ratio column if you don't need it\n",
        "    df = df.drop('adjustment_ratio', axis=1)  # axis=1 to drop the column\n",
        "\n",
        "    return df\n",
        "\n",
        "# Example Usage (assuming 'df' is your cleaned DataFrame)\n",
        "# df_adjusted = adjust_prices(df.copy())  # Create a copy\n",
        "# print(df_adjusted.head())\n",
        "# print(df_adjusted.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnDeGAN7uLqW",
        "outputId": "63991f56-d271-47dc-bd15-dec9549d7a10"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "file_path = '/content/drive/MyDrive/stocks/df_finviz_tickers.pkl'  # Run in Colab\n",
        "\n",
        "loaded_df = pd.read_pickle(file_path)\n",
        "symbols = loaded_df['Ticker'].tolist()\n",
        "print(f\"symbols (len = {len(symbols)}):\")\n",
        "print(symbols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xZ2jju1uLqW",
        "outputId": "9fffb327-1fd2-45a9-909b-ca5f0d4bc71d"
      },
      "outputs": [],
      "source": [
        "symbols_to_download = symbols[slice_obj]\n",
        "\n",
        "# symbols_to_download = slice_string(symbols, symbol_start, symbol_end, symbol_step)  # Adjust the slice as needed\n",
        "total_symbols_to_download = len(symbols_to_download)\n",
        "processed_count = 0\n",
        "\n",
        "print(f'symbols_to_download: symbols[{slice_obj}]')\n",
        "print(f'total_symbols_to_download: {total_symbols_to_download}')\n",
        "print(f'processed_count: {processed_count}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q52YTHcquLqW",
        "outputId": "2ef1ef85-8f42-46a8-8072-5c42378eae90"
      },
      "outputs": [],
      "source": [
        "current_pst_time = get_current_pst_time()\n",
        "print(f\"Start OHLCV download at PST time: {current_pst_time}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Ro6O4nGuLqW",
        "outputId": "b97cd30d-7bc8-46e4-d410-f7b044c161c5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame()  # Initialized an empty DataFrame\n",
        "\n",
        "for symbol in symbols_to_download:\n",
        "    url = f\"https://finance.yahoo.com/quote/{symbol}/history/\"\n",
        "    # Introduce a delay between requests (adjust as needed)\n",
        "    delay_seconds = random.uniform(2, 4.5)  # Sleep between 2 and 5 seconds\n",
        "    # print(f\"Downloading {symbol}. Sleeping for {delay_seconds:.2f} seconds...\")\n",
        "    processed_count += 1\n",
        "    print(f\"Downloading {symbol}. Sleeping for {delay_seconds:.2f} seconds.  Processed {processed_count} / {total_symbols_to_download} symbols.\")\n",
        "    time.sleep(delay_seconds)\n",
        "\n",
        "    df_temp = download_yahoo_finance_table(url, selector)\n",
        "\n",
        "    if df_temp is not None:\n",
        "        df_temp.columns = col_names # Ensure the columns are what is expected\n",
        "\n",
        "        df_temp.set_index('Date', inplace=True) # Set Date as Index\n",
        "        # Create MultiIndex\n",
        "        df_temp.index = pd.MultiIndex.from_product([[symbol], df_temp.index], names=['Symbol', 'Date'])\n",
        "\n",
        "        df = pd.concat([df, df_temp])  # Append to the combined DataFrame\n",
        "\n",
        "    else:\n",
        "        print(f\"Failed to download data for {symbol}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekzZn5ahuLqW",
        "outputId": "279e12a4-5722-4154-cdc8-2a2befa9bb6d"
      },
      "outputs": [],
      "source": [
        "current_pst_time = get_current_pst_time()\n",
        "print(f\"End OHLCV download at PST time: {current_pst_time}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "nW2J6txeuLqX",
        "outputId": "e9c7e5cd-0e88-4237-adf8-2455a7eaffa2"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5elcgkAsuLqX",
        "outputId": "5b0c9f6c-5884-4993-c08a-840a4a790c92"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ed0R2tQduLqX",
        "outputId": "6b5a3fbe-c183-4a18-ccba-8e47695b0f47"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "import pytz\n",
        "\n",
        "pst = pytz.timezone('America/Los_Angeles')  # or 'US/Pacific'\n",
        "current_date_pst = datetime.datetime.now(pst).strftime('%Y-%m-%d')\n",
        "\n",
        "df_OHLCV_filename = f\"df_OHLCV_{current_date_pst}.pkl\"\n",
        "\n",
        "print(f\"df_OHLCV_filename: {df_OHLCV_filename}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cWwmLvyuLqc",
        "outputId": "a2fdb76e-76f3-4821-d894-4f51819d954e"
      },
      "outputs": [],
      "source": [
        "# Drop rows with any NaN values\n",
        "df_dropna = df.dropna()\n",
        "df_dropna.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqENdn0huLqc",
        "outputId": "2e5e4b04-d046-4a02-8a33-03d90d3b42c1"
      },
      "outputs": [],
      "source": [
        "df_converted = convert_df_data_types(df_dropna.copy())  # Create a copy to avoid modifying the original\n",
        "df_converted.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEi6efHUuLqd",
        "outputId": "4a7a302f-d70e-4342-bc10-8122be353cc2"
      },
      "outputs": [],
      "source": [
        "df_adjusted = adjust_prices(df_converted.copy())  # Create a copy\n",
        "df_adjusted.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ebC9_OruLqd",
        "outputId": "d1c7dd50-cba1-4001-9699-3a500d634437"
      },
      "outputs": [],
      "source": [
        "# Save the DataFrame to a pickle file\n",
        "df_adjusted.to_pickle(df_OHLCV_filename)  # Saves to the Colab's runtime environment\n",
        "\n",
        "print(f\"Dropped NaN, converted data types, adjusted OHLC and saved DataFrame saved as {df_OHLCV_filename}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "27_IWmaMuLqd",
        "outputId": "344d0679-4fd4-4bd1-dce7-9db93d7f7482"
      },
      "outputs": [],
      "source": [
        "# from google.colab import files\n",
        "\n",
        "# # Download the pickle file\n",
        "# files.download(df_OHLCV_filename)\n",
        "# print(f\"Downloded {df_OHLCV_filename}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "bb6tONh4uLqd",
        "outputId": "72d6a418-23df-4c4b-da7e-4d1130a6f4ef"
      },
      "outputs": [],
      "source": [
        "df_adjusted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "# Create a zip file containing both files\n",
        "zip_filename = 'data_files.zip'\n",
        "file_list_to_zip = [df_finviz_filename, df_OHLCV_filename]\n",
        "\n",
        "with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
        "    for file in file_list_to_zip:\n",
        "        zipf.write(file, os.path.basename(file))\n",
        "\n",
        "# Download the single zip file\n",
        "files.download(zip_filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import zipfile\n",
        "# import os\n",
        "# from google.colab import files\n",
        "\n",
        "# # Create a zip file containing both files\n",
        "# zip_filename = 'data_files.zip'\n",
        "# with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
        "#     zipf.write(df_finviz_filename)\n",
        "#     zipf.write(df_OHLCV_filename)\n",
        "\n",
        "# # Download the single zip file\n",
        "# files.download(zip_filename)\n",
        "\n",
        "# # Optionally remove the zip file after download\n",
        "# os.remove(zip_filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # prompt: pickle df and download to my pc\n",
        "\n",
        "# import pickle\n",
        "# from google.colab import files\n",
        "\n",
        "# # Download the pickle file\n",
        "# files.download(df_finviz_filename)\n",
        "\n",
        "# from google.colab import files\n",
        "\n",
        "# # Download the pickle file\n",
        "# files.download(df_OHLCV_filename)\n",
        "# print(f\"Downloded {df_OHLCV_filename}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UH7H3fY3vASV"
      },
      "outputs": [],
      "source": [
        "# # prompt: how to allow multiple download\n",
        "\n",
        "# import os\n",
        "\n",
        "# # Define the directory where you want to save the files\n",
        "# download_dir = \"/content/downloads\"  # Example directory\n",
        "\n",
        "# # Create the directory if it doesn't exist\n",
        "# os.makedirs(download_dir, exist_ok=True)\n",
        "\n",
        "# # Modify the file saving and downloading part of your code:\n",
        "\n",
        "# # ... (your existing code) ...\n",
        "\n",
        "# pst = pytz.timezone('America/Los_Angeles')  # or 'US/Pacific'\n",
        "# current_date_pst = datetime.datetime.now(pst).strftime('%Y-%m-%d')\n",
        "\n",
        "# df_finviz_filename = os.path.join(download_dir, f\"df_finviz_{current_date_pst}.pkl\")\n",
        "# df_OHLCV_filename = os.path.join(download_dir, f\"df_OHLCV_{current_date_pst}.pkl\")\n",
        "\n",
        "# # ... (rest of your code) ...\n",
        "\n",
        "\n",
        "# # Save the DataFrame to a pickle file\n",
        "# df.to_pickle(df_finviz_filename)  # Saves to the specified directory\n",
        "\n",
        "# # Download the pickle file\n",
        "# files.download(df_finviz_filename)\n",
        "\n",
        "# # ... (rest of your code) ...\n",
        "\n",
        "# # Save the DataFrame to a pickle file\n",
        "# df_adjusted.to_pickle(df_OHLCV_filename)  # Saves to the specified directory\n",
        "\n",
        "# # Download the pickle file\n",
        "# files.download(df_OHLCV_filename)\n",
        "\n",
        "# # ... (rest of your code) ...\n",
        "\n",
        "# # You can now loop through all the files in the download_dir and download them\n",
        "# for filename in os.listdir(download_dir):\n",
        "#     if filename.endswith(\".pkl\"):  # Or any other file extension\n",
        "#         filepath = os.path.join(download_dir, filename)\n",
        "#         files.download(filepath)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
